,Topic Title,Tags,Leading Comment
0,ValueError using fastText trained bin model for MUSE unsupervised training,"['machine-learning', 'nlp', 'unsupervised-learning', 'fasttext', 'machine-translation']","I trained two fastText models supervised with pretrained data.Thereafter, when I tried to align these two models with MUSE, I got the following ValueError.After viewing where the error occurred, I ran the following test which executed without error."
1,RNN Language Model in PyTorch predicting the same three words repeatedly,"['machine-learning', 'nlp', 'pytorch', 'recurrent-neural-network', 'language-model']","I am attempting to create a word-level language model using an RNN in PyTorch. Whenever I am training the loss stays about the same for the whole training set and when I try to sample a new sentence the same three words are predicted in the same order. For example in my most recent attempt the RNN predicted 'the' then 'same' then 'of' and that sequence just kept repeating. I have tried changing the how I've set up the RNN including using LSTM's, GRU's, and different embeddings but so far nothing has worked.The way that I am training the RNN is by taking a sentence of 50 words, and selecting a progressively larger part of the sentence with the next word being the target. At the end of the sentence, I have an EOS tag. I am using text from Republic by Plato as my training set and embedding it using a pytorch embedding layer. I am then feeding it into an LSTM and then a linear layer to get the right shape. I am not sure if the problem is in the RNN, the data, the training or what so any help would be greatly appreciated.If anyone has any experience in nlp or in language modeling I would greatly appreciate any help you could offer for this fixing this problem. My end goal is simply to just be able to generate a sentence. Thanks in advance!Here is my RNNhere is how I create my input and targetsand here is my training loop"
2,Add additional layers to the Huggingface transformers,"['python', 'tensorflow', 'keras', 'nlp', 'huggingface-transformers']","I want to add additional Dense layer after pretrained TFDistilBertModel, TFXLNetModel and TFRobertaModel Huggingface models. I have already seen how I can do this with the TFBertModel, e.g. in this notebook:So, here I need to use the second item(i.e. item with index 1) of the BERT output tuple. According to the docs TFBertModel has pooler_output at this tuple index. But the other three models don't have pooler_output.So, how can I add additional layers to the other three model outputs?"
3,Topic score/weight varies for seen text Lda,"['python-3.x', 'nlp', 'lda', 'topic-modeling']","for an unseen text, we can find out the topic score using get_document_topics.If it is for seen data, (one of many documents in corpus which is trained for LDA),
topic score varies when i apply ,suppose for 1st document ,"
4,How should I load a large NLP file into Lambda,"['amazon-web-services', 'aws-lambda', 'nlp', 'spacy', 'serverless']","I have a NLP program which uses Spacy to load in a large word2vec file. This file is 3.75 GB and is too large to put directly in a lambda. Currently I have this program running in an extra large EC2 instance, but this option is expensive. The code also only runs on demand around four times a week, so keeping a server running constantly is not what I want."
5,How can I debug wrong predictions when using TfidfVectorizer and RandomForestClassifier,"['machine-learning', 'scikit-learn', 'nlp']","I have a small dataset using which I am trying to predict a store_id(y) for a given item (X). For example, I was expecting store_id (62) when searching for SHARP TV, but I am getting store_id (74) corresponding to item=""Roller Brush"".I reviewed the training data created after calling the TfidfVectorizer.fit_transform(X). It has tf_idf for ""Roller Brush""=0.57735 and for ""SHARP TV""=0.40248.I then tried to apply(transform) the vectorizer on a the search item = ""SHARP TV"", and I see that the transformed data has zeros for all strings except the ""SHARP"", ""SHARP TV"" and ""TV"" with a value of 0.57735 (corresponding to the Tfidf value I saw in the training set for roller brush). Is this the reason why the store_id (74) corresponding to roller brush got predicted ?The issue is seen even with large dataset, so I had reduced the dataset size to help with data inspection/troubleshooting.Any ideas on how I should troubleshoot such issues ?Dataset and tf-idf value in the training set
datasetPredicted data
predicted data** Code **"
6,How can I use NLP to extract the main food word from an ingredient?,['nlp'],Let's say I have the following list of ingredients:What I want to do is extract the main food word from the list of ingredients. So the result should be:How can I use NLP to do this? Or is there something else I can use to do this?
7,Methods for determining whether text is system-generated or user entered?,"['python', 'nlp', 'text-classification', 'topic-modeling']","My goal is to perform entity extraction on a set of notes connected to customer accounts, however I'm already stuck on preprocessing the data. The problem is that the notes vary a lot in nature. Some of them are particularly relevant for being more prose-like, like messages sent by customers themselves (for example, to cancel their membership), or notes made by sales representatives based on conversations they've had with the customers, but unfortunately a large number of them are system generated notes or otherwise formulaic that at most only differ by a number of values (""Order number 123 has been shipped!"", ""Order number 456 has been shipped!"",  or ""Member contacted. Entered by John Doe."", ""Member contacted. Entered by Jane Doe."").So my question is, what methods are available for distinguishing between these two types? I've used topic-modeling before using tf-idf scores and matrix factorization, and I'm also familiar with k-means clustering (outside of NLP). Both of those options seem like a lot of work and computing time just for what's essentially pre-processing, especially when I know that the similarity between the notes I want to remove will be particularly high (additionally, I don't even know how many ""genres""/topics of these system-generated notes exist).Alternatively, I'm thinking that I could preprocess the notes by removing the easy to remove differentiating values (like numbers), then hash the edited texts as keys and the NoteIDs as values, to get a bucket of NoteIDs per ""distinct"" text. From there I could either 1) use the number of NoteIDs per text-type as a threshold to determine its relevancy, or I could 2) then go on to do some type of topic modeling or clustering (maybe even relying on Jaccard similarity between each text-type).I'm planning on using Python throughout this project. I'm relatively new to NLP so linking relevant articles would be highly appreciated -- I suspect that the reason I struggled to find any similar questions or other documents online is because I'm not using accurate key terms or the appropriate references."
8,seq2seq model transformer model - what's the best way to batchify my inputs?,"['nlp', 'pytorch', 'transformer', 'seq2seq']","I'm trying to build a character-level model that matches diacritics for Hebrew characters (each character is decorated with a diacritic). Note that the correct diacritic is dependent on the word, the context and the part-of-speech (not trivial).I built an LSTM based model which achieves 18% word-level accuracy (18% of the words were exactly right in all their characters, on an unseen test set)Now I'm trying to beat that with a transformer model, following pytorch seq-2-seq tutorial, and I'm reaching far worse results (7% word-level accuracy).My training dataset is 100K sentences, most with up to 30 characters, but some go all the way to 80 characters.My question (finally) - what's the best way to batchify these inputs for the transformer? I prepared 30-characters chunks that cover each sentence (e.g. a 55 characters sentence => 30 + 25) and padded with zeros when a chunk is shorter than 30. I'm also trying to split the chunks between words (on spaces) and not in mid-word.Is this the way to go? Am I missing some better (and better-known) technique?"
9,spaCy Doc.sents not Splitting Correctly,"['nlp', 'spacy']","In an NLP text summarization example, I've come across a weird situation. The example uses the spaCy library to process the text. I'm explaining the situation through the two cases below.In the first case (see the first pic), spaCy doesn't split the sentences after the period character, as you see in the red outlined part, ""won by the Whites."".In the second case (see the second pic), after I've moved the sentence up, ending with ""Whites."", spaCy does split the sentences after the period character, as you see in the red outlined part, ""won by the Whites.,"". Note that this time there is a comma at the end of the sentence ending with ""Whites."". That means that this sentence has been split from the next sentence unlike in the first case.I've observed this situation by moving the sentence to another position as well.Nothing has come to my mind except this might be a bug. (I've copied the text to a text editor and then pasted to the notebook to make sure that there is not a special character next to the period.)What do you think?I'm sharing the notebook here so that you can play with it:
https://colab.research.google.com/drive/1MXRIrak0y680U84g0a0glpjX-clkkdtG?usp=sharing"
10,AWS takes the “0”s as a missing value in JSON AI/ML schema,"['amazon-web-services', 'nlp', 'artificial-intelligence', 'future']",What the reason for the below sample codeAny reason why if I give this field as 0 AWS takes it as the missing value
11,nlpcraft: Set base timestamp for relative date parsing,"['nlp', 'opennlp']","I'm working with nlpcraft to build a parsing system for scheduling. Users are asked when they will be doing certain activities and they can respond with relative or absolute dates, such as ""tuesday and wednesday"" or ""not until 8/15"".While nlpcraft has very nice relative date parsing,  near as I can tell it always parses dates relative to the current system time in UTC. Not only does this complicate testing (because the input is relative while the output is absolute), it means that if the server does not parse the input close to the time the user wrote it, relative dates may be parsed incorrectly. For example, if the user says ""tomorrow"" at 11PM on a Sunday, but the server doesn't parse it until 5AM on Monday, it might result in Tuesday instead of Monday.I looked into NCDateEnricher where this all seems to happen and then parsing routine computes a base time as the current system time. I didn't see a way to override this with a config variable or request parameter -- am I missing something?"
12,"Label tokenizer not working, loss and accuracy cannot be calculated","['python', 'tensorflow', 'keras', 'nlp', 'tokenize']","I am using Keras Tensorflow for NLP, I am currently working on the imdb reviews dataset. I would like to make use of the hub.KerasLayer. I want to pass the actual x and y values directly. So the sentences as x and the labels as y in my model.fit statement. My code:Tryingdoesn't work, because training_labels is not in the correct shape/format. My approach now is to make use of applying the tokenizer again, because I then get the result (from texts_to_sequences) in the correct format/shape. For this I have to first transform it into yes/no (or a/b whatever) string.Because I now have 1 and 2 as labels, I need to update my model:I then try to fit it:Problem is that loss is nan and accuracy is not calculated correctly.Where is the mistake?Please not that my question is really about this specific way and why this tokenizer is not working. I am aware that there are other possibilities which would work."
13,Ktrain |my model giving good performance on train/val data during training but no on test data,"['machine-learning', 'deep-learning', 'nlp', 'stanford-nlp', 'text-classification']",Need some guidance.I am training one model.I amm getting good training and validation accuracy (98%) but when i test the model on my test data then accury drops alot.it drops to 80%.any suggestion on how to improve performance on test data?PS: My data is text (tweets only). and m only fine tuning a pretrained model on my data. and my using Ktrain library with Bert modelthanks.
14,Predict user string input (filename) based on document text content [closed],"['machine-learning', 'nlp']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 20 hours ago.I would like to add some AI to my scan software. I did preliminaries researches, but didn't find anything suited ""out of the box"" to my need :As a first baby step, the use case is ""OCR the document"", and ""name it"". I already have the ocr engine, and I'm looking for the text analysis.I did review some of the ML.Net (it's a windows desktop app) samples, but it only contains input dataset with a few words to extract a static ""categorization"" or ""sentiment"".
I will probably start by ocr only the first page, but it's still around 200 words of datas as a single unit to analyze, not just a single sentence.The output will be something like ""Mail from John Doe received the 2020-03-27"", or ""Identity card of John Doe""Is there samples of ML algorithm working on texts this big, or should I fly away of ML and just go with static text analysis ? Or another solution ?
If ML could be pertinent, is there samples (any language) for this nature of data ?"
15,Convert string of unicode character into unicode type,"['nlp', 'python-unicode']","I have a list of string of gujarati unicode characters and i want to convert them to unicode. But the problem is the escape character('').
for egHow to convert it into Unicode which is ('\u0aec')?
Also don't think about using .encode('utf-8') as it will just make it a unicode string and not unicode characters."
16,"Automatic inflectional forms words generator in Polish, Ukrainian and Russian",['nlp'],"I am looking for tools (python library etc.) that can automatically generate list of inflected words from one input word in Polish, Ukrainian and Russian language (it can be different tools for each lanuage).For example:
(Polish): słowo -> słowo, słów, słowach, słowami...
I found something for Polish language but for Ukrainian and Russian it's too hard for me (because of Cyrillic alphabet).Can you recommend any tools?"
17,"How to receive a user input from UI, place it in a python code in Flask?","['python', 'flask', 'nlp', 'word-embedding']",I have to receive a user input from UI and have to place it in as mentioned below. I am using flask to do this.
18,"In Stanford CoreNlp, why are not all proper nouns (NNP) also named entities","['nlp', 'stanford-nlp', 'named-entity-recognition']","I use Stanford CoreNlp for Names Entity Recognition (NER). I've noticed that in some cases that it's not 100% which is fine and not surprising. However, even if a, say, single-word named entity is not recognized (i.e., the label is O), it has the tag NNP (proper noun).For example, given the example sentence ""The RestautantName in New York is the best outlet."", nerTags() yields [O, O, O, LOCATION, LOCATION, O, O, O, O, O] only correctly recognizing ""New York"". The parse tree for this sentence looks likeso ""RestaurantName"" is a proper noun (NNP)When I look up the definition of a proper noun, it sounds very close to a named entity. What's the difference?"
19,Searching an array of strings in a file,"['node.js', 'arrays', 'nlp', 'full-text-search', 'stringtokenizer']","I have a text file say, testFile.txt and an array of strings to be searched in the file as say, ['year', 'weather', 'USD 34235.00', 'sportsman', 'ಕನ್ನಡ']. I can break the file into tokens with NodeJS natural and maybe, create a large array (~100-200x the number of entries in the string array) out of it. Then, sort both the arrays and start the search. Or, use lodash directly?A Found result is when at least one string from the search string array is found in the text file; else, it should be considered as NotFound.What are some of the options to implement such a search?"
20,How to extract information from invoices of different formats?,"['python', 'nlp', 'python-tesseract', 'invoice', 'information-extraction']","I am working on a project in which I have to extract information like Vendor name, Bill to address, ship-to address etc from invoices. Invoices don't have a fixed format and can be of PDF, DOC or image type.I have tried many approaches some of them works on simple pdf but not suitable for variable formats.  I am able to convert all PDF, DOC OR image in unstructured text format.The task is to just extract information like addresses, names, amount etc from the text.The flow I am thinking of isThe format that needs lots of parsing which we do not want because Complex pdfs are not parseable.
I don't have much knowledge of Bi-directional LSTM, BERT, Transformers or any other complex architecture so you can suggest me any approach."
21,How to eliminate duplicate sentences in generated output text from a machine learning model,"['python-3.x', 'nlp', 'duplicates']","I have a python script to take in some text in the form of a string. I'm trying to trim the result (text) to eliminate duplicate sentences and cut it down to perhaps 15 sentences.Some text is in this form and I want to get rid of the last 3 lines and ensure there are no duplicate sentences.any help is greatly appreciated, thanks in advance."
22,"Should the queries, keys and values of the transformer be split before or after being passed through the linear layers?","['deep-learning', 'nlp', 'pytorch', 'transformer', 'attention-model']","I have seen two different implementations of Multi-Head Attention.According to the paper Attention Is All You Need, from what I can deduce from the image the queries and keys should be split before being passed through the linear layers, but from most implementation online they are split after. Are the two approaches similar or is one better than the other?"
23,Break string into individual fields in R [duplicate],"['r', 'nlp', 'stringr']",I have the stringShe was the youngest of the two daughters of a most affectionateand I want to turn this into a vector like belowshe was the youngest etc.I'd like to use stringr if possible.Thank you.
24,How to transform a list: numpy.int64 instance into (sparse/binary) categorial format for model.fit?,"['python', 'list', 'tensorflow', 'keras', 'nlp']","I am working with Tensorflow to make text classification. I want to make use of tensorflow hub. I am using the imdb_reviews dataset. My code so far is as follows:The model.fit command gives the error:Reason is training_labels:The training_sentences and test_sentences are in the following format/shape:The training_labels and test_labels however, have the following format/shape:and output looks like this:I need training_labels (same for test_labels) in the following format/shape to make model.fit work:
(output would look like this:)and so onCould also be [[2], [1], [2], [1], [2], ...] I would be fine with that too. Then I would use SparseCategoricalCrossentropy with units in last layer equal to 2 . Important is the format/shape.How can I make this transformation?I could make us of the Tokenizer and first transform the 0 1 to yes and no (or a and b doesn't matter) and then apply the Tokenizer and as a result get it in the format with values 1 and 2:However, tokenizing it is a workaround and I thought there must be a better way.(Please note that I really want this way / asking about the actual transformation, so no workaround / other solution with using batches of imdb_train or tfds.as_numpy directly applied to the imdb dataset in the first step.)"
25,Improving spacy entity accuracy for location named entities,"['python', 'nlp', 'spacy', 'ner']","This is my first time working with Spacy and apologies if this question is pretty standard. I have gone through a lot of questions here but haven't found something that answers my question.I have block of text (3-5 lines). The first 1-2 lines can contain names of individuals or organization. The next 2 and 3 contain addresses.There are some exceptions but this is generally the standard input.Examples1.
John Doe
9308 EAST MILL AVE.
ONALSKA WI 546502.
/1234567890
JANE DOE
277 LOWER RIVER RD
CAMDEN, NJ 081053.
XYZ CONSTRUCTION GROUP
4 VALLEY RD.
SULPHUR, LA 706634.
/0987654321
ABC FASHION BOARD
3 ARNOLD ROAD
SUITE 6598 MIAMI FL 33126
UNITED STATESThe objective is to identify all persons, organizations, and locations from this block of text.Spacy is having some difficulty classifying locations. I added a few patterns to the nlp pipe and classification improved marginally. Code below:I wanted to know:Since this input is so standard, can we train the spacy model to recognize this and maybe that improves classification?Can we extract a list of Geographical entities to overwrite prediction. not sure if this is a good idea.Any other suggestions on improving the overall accuracy are welcome."
26,installing latest version of linux and venv,"['python', 'django', 'pip', 'nlp']","from many days i'm getting errors while installing various libraries like NLP, Django etc ..After investigating about the errors on stackoverflow , i found the reason is uncompatible python version of myvenv
can anyone help me in creating a venv with latest version of python installed ?
i'm using linux mint"
27,How to choose negative context words in Skip-gram negative sampling? [closed],"['machine-learning', 'nlp', 'word2vec']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed yesterday.I understand the part of random sampling from a noise distribution. What I don't understand is; shouldn't the randomly chosen negatives be checked first that they are NOT positives somewhere else in the corpus or even the target word itself? All materials I found simply state that the sampling is done randomly from the corpus and without any such measures."
28,TypeError: 'NLP' object is not callable [closed],"['python', 'nlp', 'nlg', 'allennlp', 'simplenlg']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed yesterday.Here's the SiteFrom here i just try to ran the sample code provided on site, but am getting this errorTypeError                                 Traceback (most recent call last)
 in 
----> 1 text = nlp(""The virginica species has the least average sepal_width."")TypeError: 'NLP' object is not callableI have installed all packages, but still what might have cause this issue?"
29,Is it necessary to lemmatize the text and remove stopwords for NER?,"['python', 'python-3.x', 'nlp', 'spacy']","I'm working on the project where there are several paragraphs of text in which I must extract certain entities using NER. (I'm using SPACY)Here is the sample data:I must Extract bodypart for which he is being treated, i.e left shoulder & low-back and accident date.
Now if I lemmatize this text the meaning of ""left shoulder"" changes as ""leave shoulder"". And if I remove stop words, the bodypart ""low-back"" will not be recognised as entity as ""back"" will be removed by stopwords the meaning of entire text changes.
I know that certain stopwords can be removed, but what if other words having importance is removed.
I'm not sure whether stopwords & lemmation is helping me.Is is ok if I dont do text pre-processing ? or should I proceed with the same?"
30,what is stemming and lemmatization of the word “meaningful”?,"['nlp', 'artificial-intelligence', 'tokenize', 'stemming', 'lemmatization']",`
31,How to extract chunks with multiple patterns from pos tagged sentences?,"['python', 'nlp', 'chunking']","Given an input sentence that is pos tagged using pos_tag function in nltk :[('Veer', 'NNP'),
('Singh', 'NNP'),
('Rathore', 'NNP'),
('auctioned', 'VBD'),
('his', 'PRP$'),
('gigantic', 'JJ'),
('house', 'NN'),
('in', 'IN'),
('New', 'NNP'),
('York', 'NNP'),
('.', '.')]I need to extract the phrases which follow a certain pattern. For example, 'NNP NNP' or 'JJ NN'.
There can be 'n' no. of patterns that we might want to extract. For example, here we need 2 patterns namely 'NNP NNP' and 'JJ NN'.The output that I want for the above inputted sentence is a list of the phrases like :output :['Veer Singh Rathore', 'gigantic house', 'New York']I have tried something like this :But the output I am getting is in the form of a tree.Output :(S   (Chunk Veer/NNP Singh/NNP Rathore/NNP)   auctioned/VBD   his/PRP$
(Chunk gigantic/JJ house/NN)   in/IN   (Chunk New/NNP York/NNP)   ./.)
(Chunk Veer/NNP Singh/NNP Rathore/NNP) (Chunk gigantic/JJ house/NN)
(Chunk New/NNP York/NNP)
How can this be resolved?I referred this link for Chunking :
https://www.codespeedy.com/chunking-rules-in-nlp/"
32,"how to represent words tag, number, gender and the coreference using Weka?","['nlp', 'weka', 'coreference-resolution']","I'm new at using Weka. I already built a program that solves pronominal anaphora using c# but now I would want to do the same using Weka, and compare both.I'm not sure on how to model my cases. I need to work with word gramatical tags (noun, verb, pronoun etc) and also consider the number and gender from the words. I also considered the distance from the pronoun to the noun when I solved coreference on my c# program.My questions are:How are words represented using Weka? Considering that I must represent if its a verb, a noun, what number and gender they have etc.How each solved coreference case should be represented on the @data to do the training? Something like this? (excuse the incorrect Weka syntax, its just pseudocode)Any docs/examples/whatever on coreference resolution using Weka?"
33,AssertionError on the existing code to analyze sentence structures with spacy,"['python', 'python-3.x', 'machine-learning', 'nlp', 'spacy']","I would like to analyze sentence structure, such as, SVC, SVOC, and SVOO.The below question on stack overflow looks helpful and try to execute the code which was introduced on the answer.Sentence Structure identification - spacyI would like to gain output like ""SVO, SVC, and SVOO"" etc.But the below is the output and I have no idea how to fix the AssertionError.If you have other recommend code, libraries or articles for sentence structures analysis, it is also helpful to let me know them.the repository of the original code:https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.pyThe below is my current version with one change that I explained on  What I tried to do section.Since the error was shown when I execute the original code on Github, I fixed the library import part following the answer on stack overflow.originalchangedImportError: No module named 'spacy.en'Python 3.7.0spaCy version 2.3.1"
34,Parallel processing using concurrent.futures in python for a QnA task,"['python', 'nlp']","So here's what I got, Input 2 lists, query and context. Query is a list of questions, and context is a list of contexts basically which are 3-4 sentences long, each element of context corresponds to a query.So I'm using adaptnlp module's Questionanswering functionality which was built using huggingface's transformer's module.The thing is it takes good amount of time.Like 500 questions takes 10 mins. I would want to run it parallelly, like split 500 questions into batch of 100s maybe.generate is the function which uses adaptnlp's answering functionality. When ran, parallellization is not happening. Can anyone point me where I'm going wrong.TIA."
35,TypeError: forward() missing 1 required positional argument: 'target' when using AdaptiveLogSoftmaxWithLoss,"['python', 'neural-network', 'nlp', 'pytorch']","I am trying to build a next word prediction model with pytorch in google colab. As my vocabulary size is over 1.5 million, I am using AdaptiveLogSoftmaxWithLoss module of pytorch to reduce RAM consumption.The simple BiLSTM model definition is as follows:The model and loss function are called as follows:Inside the epoch the loss is calculated as follows:This is the complete error trace I am getting after running the epoch loop:I tried the same code with a simple nn.Linear() layer and the code runs fine. But when I replace the Linear layer with AdaptiveLogSoftmaxWithLoss, I get the above mentioned error."
36,Name entity reconization,"['python', 'nlp', 'spacy']","Here my spacy is not detecting any entities
I have tried many ways also read documentation of not getting anything?
Here i dont have any error nor any output?
Code is not going inside the for loop"
37,Which coreference chains does each sentence relate to in NeuralCoref?,"['nlp', 'nltk', 'spacy', 'coreference-resolution']","I am using neuralcoref for the task of coreference resolution in a text.I want to know each sentence has mentions from which coreference clusters. For example, sentence1 has mentions from coreference clusters 1, and 4; sentence 2 has mentions from coreference clusters 10 , 14.How can I do this?"
38,What does “skip” mean in skip-gram model?,['nlp'],"What does ""skip"" mean in skip-gram model? I read the original paper ""Efficient Estimation of Word Representations in Vector Space"" but cannot find any explanation."
39,Looking for Machine Learning model to extract Dates from documents [closed],"['python', 'machine-learning', 'nlp', 'nltk']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 2 days ago.What would be way for extracting data using Machine Learning and NLP? I have been able to extract some data based on parser and regular expression (quite simple looks for date format when one of key word appeared), but actually I was thinking to introduce ML model that could find patterns across various document, that would help me to achieve better resultsThanks!"
40,Dependency parsing visualisation,"['python', 'nlp', 'nltk']","How can I represent the following sentence:as follows:I would like to visualise the sentence like shown in the image, i.e. as a tree/network.
Any advice would be greatly appreciate. I am using nltk for word embedding."
41,"Error while using Scispacy,a sub module of Spacy","['python', 'nlp', 'data-science', 'spacy', 'ner']","I am trying to use Scispacy, a submodule of Spacy for entity extraction. But while doing the same I am facing the below error.Currently, I am using Jupyter Notebook as my IDE and python 3.6.10pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gzBelow are my lines of codeIf someone can help me in fixing it, I will be grateful. Thank you :)"
42,How to determine if two sentences talk about similar topics?,"['python', 'algorithm', 'nlp', 'sentence-similarity']","I would like to ask you a question. Is there any algorithm/tool which can allow me to do some association between words?
For example: I have the following group of sentences:What I would like to do is to find a connection, probably a semantic connection, which can allow me to say that the first two sentences are talking about a topic (phone) as two terms (phone and charger) are common within it (in general). Same for the second sentence.
I should have something that can connect phone to charger, in the first sentence.
I was thinking of using Word2vec, but I am not sure if this is something that I can do with it.
Do you have any suggestions about algorithms that I can use to determine similarity of topics (i.e. sentence which are formulated in a different way, but having same topic)?"
43,NLP: Which are the dependency tags associated with a verb?,"['nlp', 'dependency-parsing']","I need to identify all dependency tags associated with a verb. So far, I have identified:'ROOT''xcomp'Are there others?"
44,Segmenting Long text sequence into paragraphs using Python,"['python', 'nlp', 'nltk']",I'm trying to separate a long text sequence into possible no. of paragraphs. I found this SO question and thought of using 'nltk.tokenize.texttiling'. But I'm getting the following error after trying implementing the code in a notebook as given below.Error:A workaround for this or a suggestion of another working library is much appriciated.
45,Determine the part of speech of a number,"['python', 'nlp', 'nltk', 'text-normalization']","For a given number, I want to distinguish whether it is a phone number or a quantity value, depending on the context. One idea I have right now is to figure out its part of speech, but I am having trouble doing that using NLTK.For example: ""How many books do you have? I have 911 books."" In this case, 911 is a adjective, so it should be converted to ""NINE HUNDRED AND ELEVEN"". But in this sentence, ""Can you call 911, there's an emergency!"" 911 is a noun (phone number), and should be converted to ""NINE ONE ONE""When I use the tokenizer in NLTK, it only tells me that 911 is a cardinal number, but is there a way to change it so that it will give me a part of speech that's more specific? (I want noun, adjective, or pronoun)"
46,Reading Company prospects using ML and NLP [closed],"['python', 'nlp', 'corporate']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed 2 days ago.What would be way for extracting data using Machine Learning and NLP? I have been able to extract some data based on parser and regular expression (quite simple looks for date format when one of key word appeared), but actually I was thinking to introduce ML model that could find patterns across various document, that would help me to achieve better resultsThanks!"
47,HuggingFace Transformers: BertTokenizer changing characters,"['nlp', 'huggingface-transformers', 'huggingface-tokenizers']","I have downloaded the Norwegian BERT-model from https://github.com/botxo/nordic_bert, and loaded it in using:This works very well, however when i try to tokenize a given sentence, some nordic characters such as ""ø"" and ""æ"" remain the same, whereas all words having the char ""å"" is replaced with ""a"".
For instance:Yields:Thanks"
48,Topic Rank algorithm taking a lot of time,"['python-3.x', 'nlp', 'topic-modeling', 'topicmodels', 'keyword-extraction']",I am using Topic Rank algorithm for keyword extraction. I am passing abstracts of 50 research papers as input. I have the following code.It is taking a lot of time to process the 'candidate_weighting' method.I don't understand why?
49,Keyword in context (kwic) for skipgrams?,"['r', 'nlp', 'text-mining', 'n-gram', 'quanteda']","I do keyword in context analysis with quanteda for ngrams and tokens and it works well.
I now want to do it for skipgrams, capture the context of ""barriers to entry"" but also ""barriers to [...] [and] entry.The following code a kwic object which is empty but I don't know what I did wrong.  dcc.corpus refers to the text document. I also used the tokenized version but nothing changes.The result is:""kwic object with 0 rows"""
50,Generating Captions from the Images Using Python (and Pythia),"['python', 'nlp']","some months ago I stumbled upon this article on SEJ explaining how to generate captions from images using Python and Pythia framework.
The script used to work fine until some months ago. You can find the original copy here in Colab. However, at some point, Pythia repository in Git was redirect to MMF framework and the script stopped working. Colab returns the following error:Replacing this line of code with:The script goes on for a little bit, but it stops again on these two errors:Right now I am stucked at this point.
Does anyone has an idea of which workaround might be the right resolution for this script?
Thank you"
51,TypeError: an integer is required in Python Spacy-Stopword NLP,"['python', 'nlp', 'spacy']","why getting error like ""Integer is Required"" in Python Spacy???Error"
52,Proper way to add new vectors for OOV words,"['python', 'nlp', 'spacy', 'fasttext']","I'm using some domain-specific language which have a lot of OOV words as well as some typos. I have noticed Spacy will just assign an all-zero vector for these OOV words, so I'm wondering what's the proper way to handle this. I appreciate clarification on all of these points if possible:Pre-train the “token to vector” (tok2vec) layer of pipeline components, using an approximate language-modeling objective. Specifically, we load pretrained vectors, and train a component like a CNN, BiLSTM, etc to predict vectors which match the pretrained onesIsn't the tok2vec the part that generates the vectors? So shouldn't this command then change the produced vectors?
What does it mean loading pretrained vectors and then train a component to predict these vectors? What's the purpose of doing this?What does the --use-vectors flag do?
What does the --init-tok2vec flag do? Is this included by mistake in the documentation?It seems pretrain is not what I'm looking for, it doesn't change the vectors for a given word. What would be the easiest way to generate a new set of vectors which includes my OOV words but still contain the general knowledge of the lanaguage?As far as I can see Spacy's pretrained models use fasttext vectors. Fasttext website mentions:A nice feature is that you can also query for words that did not appear in your data! Indeed words are represented by the sum of its substrings. As long as the unknown word is made of known substrings, there is a representation of it!But it seems Spacy does not use this feature. Is there a way to still make use of this for OOV words?Thanks a lot"
53,Unsupervised Sentiment Analysis pycaret,"['nlp', 'sentiment-analysis']",Is it possible to conduct unsupervised sentiment analysis with Pycaret library if you have an unlabel dataset?Any valid alternative and suggestion will be appreciated too
54,Searching over a list of individual sentences by a specific term in Python,"['python', 'string', 'nlp', 'nltk']","I have a list of terms in Python that look like this.As well as a list of individual sentences that may contain the name of that fruit in a data frame. Something similar to this:And I want to take the sentences in the review column, match them with the fruit mentioned in the text and print out a data frame of that as a final result. So, something like this:Hoe could I go about doing this?"
55,Using different language analyzers with ngram Analyzer in one mapping in Elasticsearch,"['elasticsearch', 'nlp', 'elasticsearch-analyzers']",i want to use english and german custom analyzers together with other analyzers for example ngram. Is the following mapping correct? i am getting error for german analyzer. [unknown setting [index.filter.german_stop.type]. i searched but i did not find any information about using multiple language analyzers in custom type. Is it possible to use language specific ngram-filter?
56,bilstm and attention to find the topic representation of text,"['tensorflow', 'machine-learning', 'keras', 'deep-learning', 'nlp']","Also, I am using glove embeddings 300 Dimensions.Here my X is a matrix of shape (59,139) where 59 = number of samples and 139 = length of maximum sentence in my text rows. These 139 values are filled with the word2idx of my vocabulary.Y is a matrix of shape (59, 14) where 59=same above and 14 = length of my maximum title and filled with word2idx of vocabulary.For example I want this:Input:Output:Please help me out, I have spend so many days to find the approach but I am unable to find it."
57,Create Spacy NER patterns,"['python', 'for-loop', 'nlp', 'spacy', 'named-entity-recognition']","I am trying to create patterns for spacy's named entity recognition. So I have a list (terms) where I store lists of a term and its concept and I am trying to make patterns using the term as the the entity and the concept as the entity's label.I run into problems when the term's are more than one word longer. I wrote the following code to create the patterns but it's not working correctly and I can't figure out why.When I run the code, the pattern for terms[0] should be: 
{'label': 'general_history_and_physical', 'pattern': [{'LOWER': 'History'}, {'LOWER': 'physical'}]}but it returns {'label': 'general_history_and_physical', 'pattern': [{'LOWER': 'H'}, {'LOWER': 'P'}, {'LOWER': 'general'}, {'LOWER': 'history'}, {'LOWER': 'physical'}]} instead. So it's also appending the words from terms[1][0] in the list. How can I fix this? I also tried lower.clear() after the pattern in appended to the patterns list, but that returns [] as the pattern."
58,text cleaning in python,"['python-3.x', 'replace', 'nlp']","If I need to clean text in R, I could chain the multiple operations by:The above code cleans the text column within the ssids dataframe. It performs each operation and sends the results down to the next operation....How do I do this in a similar manner in python?"
59,Is there any way to give an input file to Stanza (stanford corenlp client) rather then one piece of text while calling server?,"['parsing', 'nlp', 'stanford-nlp']","I have a .csv file consists of Imdb sentiment analysis data-set. Each instance is a paragraph. I am using Stanza https://stanfordnlp.github.io/stanza/client_usage.html for getting parse tree for each instance.Right now, I have to re-run server for every instance and it is taking a lot of time since I have 50k instances.Is there any way to pass a file or do batching?"
60,Finetuning German BERT for QA on biomedical domain,"['nlp', 'transformer', 'huggingface-transformers', 'question-answering', 'bert-language-model']","I am relatively new to this field, so please bear with my amateur question. I want to perform question-answering on a German Biomedical text. From what I understand up to now, I need to fine-tune German BERT on biomedical QA datasets. Is there any script/pipeline in transformers that I should be using for this?Thank you very much in advance."
61,Identifying personnal information from column description,"['java', 'python', 'nlp', 'privacy', 'word-embedding']","I have a question about the identification of GDPR (General Data Protection Regulation) related sentences.
Is there a tool / method in Python, Java, ... that identifies whether a database column contains personnally identifiable information from its description only ?We may think about using word embedding to get the ""most_similar"" or ""most_similar_cosmul"" words given a sentence and afterwards identifying keywords related to GDPR (biometric, personnal, id, photo...) but the results depend on the robustness of the word embedding model.Thank you in advance,"
62,Python does text match similar words,"['python', 'text', 'nlp']","I'm a beginner in python and am needing some help (either code, or point me in the right direction for help).I have some free text fields where i need to assess if they reach a benchmark of completion.
ie sales people needing to complete a free text field based on customer's needs - I am needing to flag these as ""pass"" or ""not pass"" for training purposes - ie not pass is where people are lazy and put N/A or junk text, or, where they don't mention something about the topic required. Pass would need to contain relevant info relating to the field topic/theme.I'm getting some examples of what are considered 'good' completions. I'm thinking of just basing the first version around length and presence of keywords - ie is the text long enough (ie needs to be a min of 5 words) - this part is fine, the part i need help with is trying to work out how to determine are any key words listed in the text that match the topic/theme.ie for the field is based on the customer's current situation i might be looking for any words related to situation, current, help, doing, needs etc ...
for the field based on next steps i might look for words relating to going, decision, recommend, chooseAny ideas/help would be appreciated!"
63,Is HuggingFace's BertForTokenClassification an instance of seq2seq model? [closed],"['nlp', 'ner', 'huggingface-transformers', 'bert', 'seq2seq']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 4 days ago.Is HuggingFace's BertForTokenClassification an instance of seq2seq model? If not, please help me understand how it works."
64,How to apply label encoding to text data(list of list),"['python-3.x', 'nlp']","I am novice python data analyst trying to preprocess the text data (jsonl format) before it goes into Neural networks for topic modelling(VAE). I was able to clean the data and turn it into numpy array, further I wanted to apply label encoding to the cleaned text data but fail to do so. **How can one apply label encoding to list of list data format **?. The input data into label encoding is list of list and ouput has to be in same format.The code is this way(after cleaning):(list of list -strings)Error I am getting:"
65,OpenNMT/Machine Translation with Tree Based Corpus,"['nlp', 'pytorch', 'opennmt']","I am currently working on a Neural Machine Translation task as a side project for a science exhibition, so apologies if my question is simple.I parsed my English input to dependency (and constituency) trees to experiment on linguistic information in the EN-IRISH domain (note that Irish [GA] is not parsed to trees, it’s just sequences).I know that a normal seq2seq model will not work with my Tree input, but I can't find much info on making a tree based model anywhere! I've read papers but they don't delve into the implentation, of course. I seen OpenNMT-py has some tools that might be of use (like gated graph neural networks) but I'm not to familiar with how to implement them.Any help is appreciated, whether it's some docs or a paper or anything."
66,Sparse data: SparseSoftmaxCrossEntropy: logits and labels must have the same first dimension,"['python-3.x', 'nlp', 'tensorflow2.0', 'text-classification', 'keras-2']","I am building a softmax classifier based on tweets contents.
The number of categories may reach 200K+, so considering SparseSoftmaxCrossEntropy is more efficient than CategoricalCrossEntropy in this case. However, I am getting this exception during the training process:Here is the full snippet:And here is the fullstack trace:"
67,NLP-Name Entity Recognition : How to map different naming of the entity to the same entity? AMC vs AMC Entertainment vs AMC Theatres,"['python', 'nlp', 'spacy', 'ner']","I am doing a named entity recognition task where I am counting how many times a certain entity was mentioned in a document.What I found is the different naming convention of the same entity has been counted separately.
For example AMC, AMC Entertainment, AMC theatres.
How can I know they are all referring to the same entity and count it 3 times, instead of counting 1 time for each?Currently using Spacy, open to other python solutions.
https://spacy.io/usage/spacy-101"
68,Bidirectional RNN Implementation pytorch,"['nlp', 'pytorch', 'recurrent-neural-network']","Hi I am trying to understand bidirectional RNN.I am returning both hidden state and output while going through tutorials some says that I need to concatenate hidden state (torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) and in some tutorials take output state (x[:,-1,:]) but both of results come difference.What is the correct way of doing Bidirectional RNN."
69,How can I find a common pattern between sentences?,['nlp'],"Let's say I have three sentencesThe war caused not only destruction and death but also generations of
hatred between the two communities.This investigation is not only one that is continuing and worldwide
but also one that we expect to continue for quite some time.The car not only is economical but also feels good to drive.Is it possible to find the not only... but also pattern automatically using NLP, considering I'm not looking for it?Thanks in advance."
70,How to determine plurality of a word in Python without using Inflect or NLTK?,"['python', 'nlp']","I am writing a program using Python which reads a list of words and needs to determine the plurality of each one. I tried using Inflect but, as far as I can tell, it does not provide any great methods for determining the plurality of a generic word. I used the singular_noun and plural_noun methods but they both require that the word to be inflected be both a noun and plural/singular, respectively. According to documentation:""All of the plural... plural inflection methods take the word to be inflected as their first argument and return the corresponding inflection. Note that all such methods expect the singular form of the word. The results of passing a plural form are undefined (and unlikely to be correct). Similarly, the si... singular inflection method expects the plural form of the word.""I have also tried using NLTK pos_tag but it is too inaccurate.Is there a better Inflect method or a different Python package that can take any word of any part of speech and accurately determine its plurality?"
71,if i make my own speech to text model as will it recognize the words i just trained it on or any word [closed],"['machine-learning', 'nlp', 'speech-recognition']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 4 days ago.I am trying to explore the field of machine learning as speech to text conversion is one of the needs of my project.
Recently I came article Learn how to Build your own Speech-to-Text Model (using Python)I am wondering whether the model which will be built will only be able to recognize the words it has been trained on or any word it comes across."
72,How to optimize string detection for speed?,"['r', 'string', 'performance', 'nlp', 'text-mining']","When I do text analysis, I frequently want to figure out whether a large number of documents contains any element of a list of strings. If I have millions of documents (e.g. tweets) and a long list of patterns, this can take a long time.I usually use the following packages to optimize for speed:
data.table
dtplyr
stringrWhat are some best practices to optimize string detection and analysis thereof? Are there packages that would allow me to optimize code like this:I would assume that using data.table directly instead of the dtplyr implementation would increase speed. Are there any other ways to improve performance for this kind of application?I looked at this question and was hoping I could get some similar guidance. Hopefully, the question is specific enough as it is now."
73,Stanford CoreNLP Chinese model training,"['java', 'error-handling', 'nlp', 'stanford-nlp']","I am currently trying to train my own Chinese NER model with CoreNLP however when executing the training command I get an FileNotFoundException. I have seen post about this error being fixed in CoreNLP 3.5.0, however I am using 4.1.0 and it is still occurring."
74,Best approach to find matching tickets in any ticket raising tools,"['python-3.x', 'nlp', 'pattern-recognition', 'keyword-extraction']","We have  internal tools, were we store millions of Customer + Internal Tickets for years. Primarily it has three fields- Ticket Title / Description / Comments. Now the tool only restricted to search by Ticket Title and Description but what I found there are very useful info in Comments section too, which can be useful info to find similar ticket .I am seeking here guidance how to fix this type of problem. I was exploring Keyword extraction can be one way to do it (for Title / Description / Comments and map respective tickets ). But it sounds overwhelming every ticket -> keywords and then look for all tickets with matching keywords. The accuracy might be compromised here. Also I looked into pattern recognition (using clustering algorithm), I can group them into clusters based on keywords.Kindly help me with some best approaches/any reference link to address the above problem."
75,Why any of the ml algo would need to learn n+1 features in the sparse vector representaion of each element of the corpus,"['nlp', 'logistic-regression', 'feature-extraction']","[Here n = |V| ,then also logistic regression need to learn n+1 features]"
76,How to get the bounding box of a single word from a given bounding box of a line [closed],"['text', 'deep-learning', 'nlp']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 4 days ago.I have a dataset and I am actually trying to preprocess it like the following :
the form of my dataset is like this192,169,299,187,TAMAN DAYAThis is a line of text and it's corresponding position (bounding box) in the image .
It's easy to split line into individual tokens but how can I get the bounding box for each word so it looks like this :...,169,...,187,TAMAN...,169,...,187,DAYASomeone suggested that I use heuristic rules but I don't what that really is and how I can do that .
Can someone help me?"
77,NameError: name 'display_nearest_neighbors' is not defined [closed],"['python', 'nlp', 'stanford-nlp']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 2 hours ago.I am running BERT training model it throwing error"
78,How to normalize output from BERT classifier,"['python-3.x', 'nlp', 'classification', 'tf.keras', 'huggingface-transformers']","I've trained a BERT classifier using HuggingFace transformers.TFBertForSequenceClassification classifier. It's working fine, but when using the model.predict() method, it gives a tuple as output which are not normalized between [0, 1]. E.g. I trained the model to classify news articles into fraud and non-fraud category. Then I fed the following 4 test data to the model for prediction:The outputs are:For me label-0 is for non-fraud, and label-1 is for fraud, so that's working fine. But how do I prepare the scoring confidence from here? Does normalization using softmax make sense in this context? Also, if I want to look at those predictions where the model is kind of indecisive, how would I do that? In that case would both the values be very close to each other?"
79,Split sentence into Subject - VP - NP with stanza in python,"['python', 'nlp', 'stanford-nlp']","Let's say I have ""red riding hood hates the reindeer"". Then my expected output will be:I've read few answers from stackoverflow especially Extract Noun Phrases with Stanza and CoreNLPClient
but can't seem to find a way to extract the outermost chunk of each label (Subject, VP, NP). It is also expected that the components cannot overlap with each other (like in the example). There are also some answers on this from NLTK or spacy, but it seems that StanfordNLP is the creme-de-la-creme of nlp packages for now, so I'm exclusive to this package (stanza in python is for stanfordnlp). Any idea?"
80,Develop Question Answer System using BERT,"['tensorflow', 'nlp', 'information-retrieval', 'transformer', 'question-answering']","i'm currently developing a Question-Answer system (in Indonesian language) using BERT for my thesis.
The dataset and the questions given are in Indonesian.The problem is, i'm still not clear on how the step-to-step process to develop the Question-Answer system in BERT.From what I concluded after reading a number of research journals and papers, the process might be like this:What i want to ask are :I appreciate any helpful answer(s).
Thank you very much in advance."
81,string matching with NLP,"['python', 'pandas', 'dataframe', 'nlp', 'string-matching']","I have two dataframes, df1 and df2, with ~40,000 rows and ~70,000 rows respectively of data about polling stations in country A.The two dataframes have some common columns like 'polling_station_name', 'province', 'district' etc., however df1 has latitude and longitude columns, whereas df2 doesn't, so I am trying to do string matching between the two dataframes so at least some rows of df2 will have geolocations available. I am blocking on the 'district' column while doing the string matching.This is the code that I have so far:This produced about 12,000 matches, however I have noticed that some polling station names are incorrectly being matched because their names are very similar when they are in different locations - e.g. 'government girls primary school meilabu' and 'government girls primary school muzaka' are clearly different, yet they are being matched.I think utilising NLP might help here, to see if there are certain words that occur very frequently in the data, like 'government', 'girls', 'boys', 'primary', 'school', etc. so I can put less emphasis on those words, and put more emphasis on meilabu, muzaka etc. while doing the string matching, but I am not so sure where to start.
(For reference, many of the polling stations are 'government (i.e.public) schools')Any advice would be greatly appreciated!"
82,No module named 'utils_nlp' when using MS nlp_recipes in google colab,"['python', 'nlp', 'google-colaboratory', 'recipe']","I want to use the utils_nlp provided in the nlp_recipes github repo from MS in my google colab project. However, I'm getting a ""No module named 'utils_nlp'"" error. This is what I have tried:In the setup from nlp_recipes is stated that:It is also possible to install directly from Github, which is the best way to utilize the utils_nlp package in external projects (while still reflecting updates to the source as it's installed as an editable '-e' package).pip install -e git+git@github.com:microsoft/nlp-recipes.git@master#egg=utils_nlpIn colab I runWhich works perfectlyObtaining utils_nlp from git+https://github.com/microsoft/nlp
recipes.git@master#egg=utils_nlp Cloning
https://github.com/microsoft/nlp-recipes.git (to revision master) to ./src/utils-nlp
Running command git clone -q https://github.com/microsoft/nlp-recipes.git /content/src/utils-nlp
Installing build dependencies ... done
Getting requirements to build wheel ... done
Preparing wheel metadata ... done
Installing collected packages: utils-nlp
Running setup.py develop for utils-nlp
Successfully installed utils-nlpWhen I do !pip list I getutils-nlp                2.0.0           /content/src/utils-nlpWhen I want to import from utils-nlp, for exampleI get aNo module named 'utils_nlp'I have tried using sys.path.append(""/content/src/"")  and many other paths to append but none of those seem to work.Any idea?"
83,"“Response 401: The key used is invalid, malformed, empty, or doesn't match the region” while working with Dispatch","['nlp', 'botframework', 'qnamaker']","I'm getting this error because Dispatch does not like my either my QnAKnowledgebaseId or QnAEndpointKey located in my .env file. I know the Id and Key are correct because I've tripled checked it and made sure the Id and Key came from my qna.ai portal. Somehow when dispatchbot.js is loaded up, it does not like my Id or Key:Anyone have any idea why I am still getting this error?"
84,Increasing accuracy of prediction using a linearSVC classifier,"['python', 'nlp', 'classification']","I am using a linear classifier to predict data. below is my code. How do i improve the accuracy of prediction? The dataset contains around 500k entries, 350k of which are in the test data. I see no parameters to change in the below code. Should I change those within CountVectorizer()? I am very new to python and NLP, would be great if you could help!"
85,Build a multiclass text classifier which takes vectors generated from word2vec as independent variables to predict a class,"['python', 'machine-learning', 'nlp', 'word2vec', 'text-classification']","I am dealing with patient data. I want to predict the top N diseases given a set of symptoms.This is a sample of my dataset: In total I have around 1200 unique Symptoms and around 200 unique DiagnosisWhat I am planning to do with this dataset is to use the Symptoms column to generate word vectors using Word2vec for each row of patient data. After generating the vectors I want to build a classifier, with the vectors in each row being my independent variable and the Diagnosis being the target categorical variable.Shall I take the average of the vectors to generate feature vectors generated from word2vec? If so, any clarifications on the same?"
86,KeyError when cleaning tweets column using stop words in python,"['python', 'nlp', 'nltk', 'tokenize', 'stop-words']","I have a data frame of tweets and I'm trying to clean my 'tweet' column- remove stop words and use lemmatization.Below is my code:The code above gives me the following error: (I included all the traceback)How can I fix the error?Also, how can I get the result in my data frame- add another column with the results?"
87,How to get values which are in RHS part of a equal sign in a string [closed],"['python', 'regex', 'nlp']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 5 days ago.I have a string like below:If I want the value of 'hct' only '36.1%' should come.Could anyone help me with this?Note: separator in the string can be anything(',',';','.',etc,...)"
88,ldaseqmodel runtimewarning Invalid value in double_scalars,"['nlp', 'gensim', 'lda']","I am currently trying to use dynamic topic modeling on some news crawled from the web.
Unfortunately, I receive a warning in the logs:INFO : using serial LDA version on this nodeAfter using google to find out more about this issue, I learned that this numpy error is often produced by NaNs or null values. So with regards to dynamic topic modeling this probably refers to an empty document? but I dont have any empty documents in my dataframe"
89,How can I get NLP Recommender function to work?,"['nlp', 'recommendation-engine']","I am struggling to get my recommender function working.Speech is a dataframe and ""Name"" is a column containing the names of presidents. The cosine_sim is a cosine similarity matrix that performs ok.I get a very simple but frustrating error:Who can point out my mistake?"
90,Using custom Token extensions in spaCy's Matcher,"['python', 'methods', 'nlp', 'spacy', 'matcher']","I just added the following extension to Token in spaCy:So, I want to check if a token has a certain specified dependency name as one of its children, so the following:Outputs True, because 'walking' has a child whose dependency tag is 'nsubj' (i.e. the word 'we').However, I don't understand how to use this extension with spaCy's Matcher. Below is what I've written. The output I expect is walking, but it doesn't seem to work:"
91,How to interpret output from gensim's Word2vec most similar method and understand how it's coming up with the output values,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","I am trying to implement word2vec on a problem. I will briefly explain my problem statement:I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.Note:
1.words with #prefix are diagnosis and the rest are symptomsApplying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach"
92,BERT + custom layer training performance going down with epochs,"['tensorflow', 'machine-learning', 'nlp', 'language-model']","I'm training a classification model with custom layers on top of BERT. During this, the training performance of this model is going down with increasing epochs ( after the first epoch ) .. I'm not sure what to fix here - is it the model or the data?( for the data it's binary labels, and balanced in the number of data points for each label).Any quick pointers on what the problem could be? Has anyone come across this before?Thanks!"
93,How to get results in elastic search in an order with respect to first findings from a given input string?,"['python', 'elasticsearch', 'nlp']","I am new to elastic search and trying to query a string.Below is my query:Output:}In my Query, the order of required tokens is ""RDW-CV, WBC, RBC"" but in output hits, the order changed like ""WBC, RBC, RDW-CV"".Is there any way that we can get the hits w.r.t input order?
Could anyone help me with this?"
94,Why funcion self.spacy_nlp(phrase) doen't no work,"['python', 'nlp', 'spacy']","Forgive me if I say any improper things.
We keep a project developed in python which makes an analysis of the feeling of the customer comments. The steps are simple:The problem is at point 3 and we don't understand what's going on. The problem is this,
The clean_data_path variable receives the file with the clean comments:It just fails to continue and we don't know how to capture the error. We tried to put try i except but it doesn't come out.
If we look at the nlp.phrase_to_words we see the following code:The problem for that is when you call self.spacy_nlp(phrase). Don't run, simply exit.
But the strangest thing is that if I execute the process putting a complete path of the file clean_data_path it works correctly and is able to execute doc = self.spacy_nlp(phrase) (the same file that it uses when it executes the whole process)The process worked perfectly until a week agoSomeone could give me some guidance or have found something like thisGreetings,"
95,Find the most frequently occuring words in a text in R,"['r', 'n-gram']",Can someone help me with how to find the most frequently used two and three words in a text using R?My text is...
96,Similarity between two words with pre-trained NLTK wordnet,"['nlp', 'nltk', 'wordnet', 'word-embedding']","I want to compare two words with similarity score.
I used wordnet from nltk.corpus.I got similarity score, but, it works with only between noun, but, what I need is to compare noun between adjective or other type of word.For example bellow , I need to compare word like ""expensive"" (adjective) with ""price"".I want preferably a library with pre-entrained model because I need a model that can work with any words in any domainWhat about word embedding ?"
97,batch_size != eval_batch_size when training and evaluating a transformer based language model,"['neural-network', 'nlp', 'pytorch', 'transformer', 'seq2seq']","I'm trying to follow pytorch's transformer tutorial. They are training and then evaluating a seq-2-seq language model.I noted that the batch size (length of input vector) is different when training and evaluating:why would I want that? and if I do, how does it work? is it padding the tensor with zeros? where are they doing that?thanks!"
98,Translate software for corpus in Arabic to English [closed],"['python', 'nlp', 'translation']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 6 days ago.I have a large corpus of Arabic .txt files that need to be translated to English for classification purposes.  Of course there's the Google translate method, but that is tedious and will take some time.  The amount of files within the corpus is around 5000.Does any one know of a software where I can drop the files in and the output files will be English?"
99,Well Trained Classifier Does not Perform Well in Same Source of Dataset,"['keras', 'deep-learning', 'nlp', 'classification', 'training-data']","I am doing a tweet classification project, and the task now is to determine whether a tweet is, for instance, road traffic-related or not.I have collected a huge number of tweets(over 1 million). To train a classifier, I first use keywords(such as crash, accident, road, etc.) to get some candidate traffic-related tweets. Then I manually review these tweets and check whether these tweets are traffic-related or not. I also construct another dataset which are not traffic-related. Finally, I get 1000 traffic-related tweets and 2000 not traffic-related tweets.Then I use the CNN-LSTM model based on pre-trained word embedding to classify the tweets. The overall structure of the model is:The dimension of the pre-trained word vector is (300,1). The training data, validation data, and test data account for 60%, 20%, and 20% of the labeled data. I use Adam optimizer with a learning rate of 0.001. Actually, the model performs quite well, reaching a 0.99 F1 score on the test data. The confusion matrix about the performance of the model on the test data is:However, when I use the trained classifier to make the prediction on all the tweets I have collected. The performance is really bad. I manually check the tweet which is labeled by traffic_related by the classifier, and I find that most of the tweets are not traffic-related at all.I am wondering which part I did is wrong. Any suggestions and insights are very appreciated!"
100,Need to implement text prediction in storyline 3 [closed],"['javascript', 'nlp', 'artificial-intelligence', 'articulate-storyline']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 3 days ago.Is there a way to implement AI based sentence auto completion capabilities in Storyline 3 e.g like gmail has the feature to suggest the upcoming sentence while typing."
101,I encountered this problem when trying to a List of tokenized words to a String in Python,"['python', 'list', 'nlp', 'typeerror', 'spacy']","This is the code:When I tried running the code, it gave me this error:TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found.*I have tried using some of the sample codes from here but all of them did not work.So is there anyway to untokenize the words in the list or any alternatives to solve this problem?"
102,An error occurred while calling o266.fit while trying to fit the train data set in sparknlp pipeline,"['machine-learning', 'pyspark', 'nlp', 'data-science', 'johnsnowlabs-spark-nlp']","An error occurred while calling o266.fit while trying to fit the train data set in sparknlp pipeline.Environment:
openjdk version ""1.8.0_252""
Spark NLP version 2.5.0
Apache Spark version: 2.4.4The data set is taken from https://www.kaggle.com/c/nlp-getting-started from where a train.csv is takenCode:Error:"
103,Topic Extraction Using Gensim,"['python', 'nlp', 'gensim', 'lda', 'topic-modeling']","I am trying to extract topics using Gensim library, LDA model from Persona-Chat dataset. After pre-processing part, I try to find the best number of topic in order to get the best keywords regarding to the topics.So, for that I ran the code for different number of topics such as; 50-100-150-200 and 250.The Coherence Score increases whenever number of topics are increased and at the same time, Perplexity Score decreases as well. So, from these information I understood I had to use 250, because as far as I know, Perplexity should be small and Coherence should be high. Below, you can see the scores of each number of topics (K)However, whenever I used 250 number of topics, my keywords have 0 weight and they are repeated in every topic.I made some research and I found out that if keywords are repeated then maybe my number of topics are too much. So I decreased my number of topics from 250 to 50 but I want to know, is there any ways to fix this?Since, the Perplexity and Coherence Scores shows that LDA model would work better with 250 number of  topics.Below you can find the code as well.Best regards,"
104,How to normalize word embeddings (word2vec),"['python', 'nlp', 'normalization', 'word2vec', 'word-embedding']",I have a pre trained Word2Vec model with embeddings. I need to normalize some embeddings to do analyses with the words. Is there a simple line (or block) of code to do this? I've been searching online but can't find a simple answer.
105,nlp: is this dependence tag correct? What does exactly it mean in this situation?,"['nlp', 'spacy', 'dependency-parsing']","I am exploring the amazing python library and I got this:token_pos=[token.pos_ for token in spacy_doc]
token_tag=[token.tag_ for token in spacy_doc]
token_dep=[token.dep_ for token in spacy_doc]token_postoken_tagtoken_depTreeQUESTIONS: I am puzzled about the dependence relation between ""managed"" and ""went"". It is a ""conj"". (1) Is this a classification error? If it is a classification error, what would be the correct classification? If it is not, can you explain why is this happening? Spacy explains this as a ""conjunct"": (2) Is there a way to differentiate this case from the case below?According to stanford dependence manual:A conjunct is the relation between two elements connected by a coordinating conjunction, such as “and”, “or”, etc:“Bill is big and honest”“They either ski or snowboard”conj(big, honest)conj(ski, snowboard)Look at this last sentence now:The relation dependence between ""ski"" and ""snowboard"" is also ""conj"" and in this case it seems to be the correct classification."
106,Expanding vocabulary size and changing layer dimensions for a pretrained model in OpenNMT-PY [closed],"['machine-learning', 'nlp', 'pytorch', 'pre-trained-model', 'opennmt']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 6 days ago.I see this comment in OpenNMT docs:When training from an existing model, some settings can not be changed:I would like to take an existing pre-trained model and use it for transfer learning to adapt to an augmented set of training examples with additional vocabulary.  To do this I need to ""hack"" the .PT file toTo do this I guess I have to hack the PT file.Does anybody have any experience with this and any practical suggestions?"
107,Calculate Impact of Input Tokens on BERT (or other classifier) Output Probability,"['python', 'machine-learning', 'nlp', 'classification', 'huggingface-transformers']","Say I’ve trained a BERT model for classification. I’d like to calculate the proportional impact each input token is having on the predicted output.For example - and this is very general - if I have a model that labels input text as {‘about dogs’ : 0, ‘about cats’ : 1}, the following input sentence:
s = 'this is a sentence about a cat'
should output very close to:
1HOWEVER, what I’d like is to calculate each input’s impact on that final prediction, e.g. (assuming we’re tokenizing on the level of words - which is not how it would be done in practice, I know):
{this : .01, is: .005, a : .02, sentence : .0003, about : [some other low prob], a: [another low prob], cat : 0.999999}Intuitively I’d think this means running a forward pass with the input sentence, then looking at the backprop values? But I’m not quite sure how you’d do that. Thoughts?NOTE
Assume everything is implemented in PyTorch. My current use case is with HuggingFace, but I'd want to generalize this anyway."
108,What ML or Deep Learning model can identify the main topic of a document? [closed],"['python', 'machine-learning', 'deep-learning', 'nlp', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 7 days ago.I have currently been using TF-IDF to extract the main topic of an article. TF-IDF has become a limitation because I am building an NLP pipeline that takes only one document (independent of other documents) and returns the same document. Unfortunately, with TF-IDF you rely on a set of documents (corpus) in order to perform the IDF part. Since I can only input one document in my pipeline I need another option.I am looking into building a classifier that will accomplish this task. I have a pre-labeled dataset that has the text of the article and the main topic of the article. I first vectorized my text data (using CountVectorizer()) and attempted to training a classifier (tried both SVM MultinomialNB) but the results were really really poor.What is the best approach to accomplish this task? Is there a specific model I should be using? If so what is the name of it?"
109,“'\w' is an unrecognized escape” in grep,"['regex', 'r', 'grep', 'pcre']","I'm using grep in some projects in R (which uses a perl=TRUE flag) and for the life of me I can't figure out why R keeps throwing errors. My query is as follows:However, R throws the following error:"
110,Spacy token-based matching with 'n' number of tokens between tokens,"['python', 'nlp', 'spacy']","I am using spacy to match a particular expression in some text (in italian). My text can appear in multiple forms and I am trying to learn what's the best way to write a general rule. I have 4 cases as below,, and I would like to write a general patter that could work with all of the cases. Something like:"
111,Jape file for Basic phone number Detection GATE NLP,"['nlp', 'gate']","I am new to GATE NLP. I started learning and going through GATE tutorials to understand the basics. I Tried some text formats in GATE Developer tool.Steps I followedI wanted to add some new rules in jape files to detect the given phone number formats from word documents.
Format1: +14168878659, Format2: +1-4036187846, Format3: +1.647.400.3581, Format4 : +1 647 400 3581I wrote Jape File  for processing +14168878659Can someone tell me how to write jape for these formats? +1-4036187846,+1.647.400.3581, +1 647 400 3581
I know these may be not valid formats. I am doing this for learning purpose."
112,Using pretrained word2vector model,"['nlp', 'lstm', 'word2vec']",I am trying to use a pretrained word2vector model to create word embeddings but i am getting the following error when Im trying to create weight matrix from word2vec genism model:Code:Im getting the following error:Error
113,TypeError: enable_padding() got an unexpected keyword argument 'max_length' [closed],"['nlp', 'tokenize', 'bert']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 8 days ago.While using Fast tokenizer in BertWordPieceTokenizer, getting this error."
114,"Generating text corpus from a matrix, based on words and their weighted probabilities","['python', 'pandas', 'numpy', 'text', 'nlp']","I have a matrix, and I am trying to generate text corpus.I selected the work chewbacca as my first word.Now I am trying to find pairs for chewbacca, based on probabilities. Two words are here - luke(0.66) and obi (0.33).The second word must be based on weighted probabilities.For instance, if ""luke"" pairs with ""chewbacca"" as 0.66 and ""obi"" pairs with ""chewbacca"" as 0.33, ""luke"" must be selected twice more likely than ""obi"".How to approach it? Appreciate any tips!"
115,What's the most convenient way to analyze a sentence phrases and structure using NLTK or SpaCy?,"['python', 'nlp', 'nltk', 'spacy']","What's the most convenient way to analyze a sentence phrases and structure using NLTK or SpaCy?
The main goal is to get a well organized and clean data in order to apply some inferential statistics on it.Here is a simple example of what I need, as shown in the tree above:"
116,Algorithm for replacing coreferent clusters with head entity,"['machine-learning', 'nlp', 'allennlp', 'coreference-resolution']","The AllenNLP coreference resolution toolkit provides clusters but does not assign a head entity or expose a method for replacing all corereferent mentions of the same entity with the head.  For instance,The dog is old.  He is a lab.I want it to be translated toThe dog is old.  The dog is a lab.This gets trickier since you often have nested or overlapping entities, so the order matters.  I was wondering if there are any elegant algorithms for this."
117,Building a recommendation system using word2vec,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","Now, from the outputs, I am unable to understand the embeddings or how these embeddings are changing or will change with new data.Is this way correct for solving the problem statement and if so how can I optimize this to find the best embeddings for my dataset. Can anyone provide any suggestions on the same"
118,Issue with tokenizing words with NLTK in Python. Returning lists of single letters instead of words,"['python', 'nlp', 'nltk', 'tokenize', 'sentiment-analysis']","I'm having some trouble with my NLP python program, I am trying to create a dataset of positive and negative tweets however when I run the code it only returns what appears to be tokenized individual letters. I am new to Python and NLP so I apologise if this is basic or if I'm explaining myself poorly. I have added my code below:snippet from CSV file for reference:"
119,Can chatbots learn or unlearn while chatting with trusted users,"['nlp', 'chatbot', 'rasa']","Can chatbots like [Rasa] learn from the trusted user - new additional employees, product ids, product categories or properties - or unlearn when these entities are no longer current ?
Or do I have to go through formal data collection, training sessions, testing (confidence rates > given ratio), before the new version be made operational."
120,Domain specific keyword correction [closed],"['python', 'machine-learning', 'nlp', 'keyword']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 8 days ago.I am working on correction of domain specific keywords like the following code.This is straightforward when hardcoded, but I want to apply some machine learning based model, or any other method, to solve the following."
121,Predictions from model.predict() not correct,"['python-3.x', 'tensorflow', 'keras', 'nlp']","I am using model.predict to predict labels for unseen data in a sentence classification task. But the prediction is always 1 even if I give sentence of opposite label.Here is the codeAnd here is the outputThe output of model.predict(X_test) isAs I can see the first string means absolutely nothing. Still it has got a positive label. Also, the 2nd string is a negative one but it also has a positive label."
122,How can I vectorize a series of tokens,"['python', 'python-3.x', 'machine-learning', 'nlp', 'vectorization']","I have a series of tokens that I am attempting to vectorize. However, I keep getting the error message ""TypeError: expected string or bytes-like object"".My text tokens:My code:Again, the error message is ""TypeError: expected string or bytes-like object"". It works if I just inputhowever, trying to apply it to all the rows turns back the error message. Any guidance, explanations, or solutions would be much appreciated."
123,NoneType error when fine tuning BERT CLS embeddings directly,"['tensorflow', 'keras', 'nlp', 'word-embedding', 'bert']","this is my first post on stack overflow. I am attempting to fine-tune the BERT embeddings, instead of fine-tuning the BERT model on a downstream task. Basically I input several sentences and attempt to fine-tune the 'CLS' token directly according to a specific target embedding.I imported keras_bert and loaded the BERT base uncased model. I peel off the NSP and MLM layers and keep the final output as the 'Extract' layer - which is the CLS token embedding. I have a set of Y labels (target embeddings for CLS). I would like to fine-tune for 4 epochs. However, I keep getting a NoneType error which I cannot debug for days now.I tried passing in a few sentences into BERT and just using model.predict() to get their respective output CLS embeddings. That worked out fine. I even double checked them with BERT's built in extract_features.py code. However, I do not understand the cause of this type error and any documentation for it online.Here is a model.summary() of the keras_bert model I'm using:My code:Ultimately, I will want to replace that last 'Extract' layer with a similar one that extracts the embedding of the first token after CLS, but that is a later challenge. For now, could someone help me understand what is going on here?"
124,Is there a better way of building corpus from OpenSubtitle for chatbot training,"['nlp', 'dataset', 'artificial-intelligence', 'chatbot', 'corpus']","I have a Corpus for training a chatbot build like this:Using OpenSubtitle corpus (Jörg Tiedemann, 2009 64), English XML dump (19Gb, 323905 movies, 338M sentences) - consecutive sentences with following properties were extracted:first sentence ends with question marksecond sentence has no question marksecond sentence follows in the movie the first sentence by less than
20 secondsThe extracted pairs were additionally cleaned-up (detokenized, removal sound sequence like (BANG), removal of too complex sentences, normalization of quotes), uniq-ed and shuffled.As seen here : https://forum.opennmt.net/t/english-chatbot-model-with-opennmt/184Is there more optimization that can be done to have a better dataset?"
125,This Text Mining Python Program Needs Efficiency,"['python', 'performance', 'text', 'nlp', 'statistics']","I am relatively new to programming in python, and I am working on a word frequency analysis program that parses through entire books and gives the word frequencies, as well as the positions of each word throughout the text. I have it working, but it takes so long to work. Does anyone have any efficiency suggestions for this code? Also, any additional thoughts you have about directions to take the analysis are welcome. I appreciate any feedback!"
126,Does VoltDB support near-duplcate-content detention?,"['nlp', 'duplicates', 'full-text-search', 'text-processing', 'voltdb']",I want to implement similar comment detection functionality for the comments engine which uses golang+voltdb. The task is very similar to the Near-duplicates detection problem(which is a sort of ad-hoc text processing).Here are books/articles I read about the subject: Near-duplicates and shingling and Syntactic Clustering of the WebSo how does VoltDB deal with it?
127,Classify comment on normal and not normal [closed],"['machine-learning', 'text', 'nlp']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 8 days ago.Is there way to classify text/ comments in my case for Normal/human readable comment and Not Normal(just some bag of words)?
I need to drop off comments which are looks like:
Bla bla bl bla nnnnnnn word wordJust words words words wordsI need to control some how that user will make some logical comment.Maybe some language model?
Thanks"
128,All training samples are not loading during training,"['python', 'tensorflow', 'nlp', 'tensorflow2.0', 'tensorflow-datasets']","I'm just starting with NLP. I loaded the 'imdb_reviews' dataset from tensorflow_datasets.There were 25000 testing samples, but when I run I only train for 782 samples. I didn't use batch_size, just loaded entire dataset at once as you can seeThe other hyperparameters are:Can anyone tell me what I'm doing wrong ?"
129,How to extract and use BERT encodings of sentences for Text similarity among sentences. (PyTorch/Tensorflow),"['tensorflow', 'deep-learning', 'nlp', 'pytorch', 'bert']","I want to make a text similarity model which I tend to use for FAQ finding and other methods to get the most related text. I want to use the highly optimised BERT model for this NLP task .I tend to use the the encodings of all the sentences to get a similarity matrix using the cosine_similarity and return results.In the hypothetical conditions, if I have two sentences as hello world and hello hello world then I am assuming the BRT would give me something like [0.2,0.3,0], (0 for padding) and [0.2,0.2,0.3] and I can pass these two inside the sklearn's cosine_similarity.How am I supposed to extract the embeddings the sentences to use them in the model? I found somewhere that it can be extracted like:Is this the right way? Because I read somewhere that there are different type of embeddings that BERT offers.ALSO please do suggest any other method to find the text similarity"
130,StanfordCoreNLP Spanish module,"['python', 'nlp', 'stanford-nlp']","I am trying to use the Spanish module in the StanfordCoreNLP library, but I keep getting a message AttributeError: type object 'StanfordCoreNLP' has no attribute 'Pipeline'. Any ideas how can I access the Spanish module?"
131,Attribute Error : Can't pickle local object,"['machine-learning', 'nlp', 'pytorch']","I am making a chatbot using pytorch. I developed this chatbot on a linux system which was working flawlessly. But when i am running the same model in windows 10. There is an error :-Traceback (most recent call last):
return self.wsgi_app(environ, start_response)
return super(_SocketIOMiddleware, self).call(environ,
return self.wsgi_app(environ, start_response)
response = self.handle_exception(e)
return cors_after_request(app.make_response(f(*args, **kwargs)))
reraise(exc_type, exc_value, tb)
raise value
response = self.full_dispatch_request()
rv = self.handle_user_exception(e)
return cors_after_request(app.make_response(f(*args, **kwargs)))
reraise(exc_type, exc_value, tb)
raise value
rv = self.dispatch_request()
return self.view_functionsrule.endpoint                      > load(candidateSkillset.get(name))
File
for (words, labels_net) in train_loader:
return _MultiProcessingDataLoaderIter(self)
w.start()
self._popen = self._Popen(self)
return _default_context.get_context().Process._Popen(process_obj)
return Popen(process_obj)
reduction.dump(process_obj, to_child)
ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'load..ChatDataset'
exitcode = _main(fd, parent_sentinel)
self = reduction.pickle.load(from_parent)
EOFError: Ran out of inputI am calling the function with the dataset i want it to train on... Here is my code :-"
132,"python code to compare the pairs of sentences, and see if they are in the same or different blocks in text file","['python', 'nlp']","I've 2 text files. Writing a Python Program for the following
1)I need to compare the pairs of sentences in the 1st file, and see if they are in the same or different blocks, and compare that to the 2nd text file.
2)I need to calculate the percentage of correct classification.
3) I need to  count:
% of sentence pairs correctly classified as in the same block, % of sentence pairs correctly classified as in different blocks"
133,How to go about creating a parse tree in Python for grammar,"['python-3.x', 'string', 'list', 'nlp', 'parse-tree']","I am trying to create a grammar parser for a general sentence structure I am studying. I have a lot of the logic down as the grammar structure is similar between most sentences. i.e. I have hardcoded if statements that check for indices of certain keywords and tokenize them appropriately which works to a degree. However, I would like to find a way to take this as a much more object oriented approach with a tree structure.example sentences: bob one three enemies flank southExample tokenization:bob is a ""name"" token,  one is additional name information since it comes right after the first word in the sentence.enemies is an ""enemy declaration"" token and three is additional enemy information since it comes right before the word ""enemies"".flank is an action token and south is an ""action"" direction since it comes one word after the action.These rules are consistent among all of the sentences in the language I am parsing.So ideally what I want to do is a tree model architecture that parses the whole sentence, one word at a time, and tokenizes the special keywords like ""enemies"" or ""flank"", but I also need a way to capture the additional information that is attached to each keyword in proximity (preceding or succeeding), as given in the examples above. This is likely where my if statement logic would come in.How can I set up an OOP approach to appending extra information into these classes. My early thoughts are to create ClassTokens for the keywords, but how would I handle attaching the descriptive words? Should I make classes for each of those words too or is this redundant?Also, I imagine I need a separate class like a ""Parser"" that parses the entire sentence and maintains the sentence state as well as some lookahead or look prior functions? Just trying to brainstorm ideas and get some thoughts on how to make this flow together. Thank you.E.g."
134,Dimension error with RNN and word classification,"['python', 'keras', 'nlp', 'dimension']","I'm pretty new with NLP and I want to classify different words depending on their language (basically my model should tell me if a word is french, or english, or spanish and so on).When I fit the following model I get a dimension error. The ""dataset"" contains the words, it's a padded tensor of size (1550, 19) and the ""y"" contains the different languages, it's also a padded tensor of size (1550, 10).ValueError: Shapes (None, 10) and (None, 18) are incompatibleDo you see where the problem is?Thanks!"
135,Is CRF(conditional random field) dead?,"['deep-learning', 'nlp', 'crf']","I do not really know about the tech, but I need to select a reseach direction before entering the postgraduate school, and CRF is one of them. The main direction is NLP.
I searched it on google and found that many paper published 10 years ago, which seems not so interesting.
So I wanna know is it still promising?"
136,Get same results as spacy web dependency visualizer,"['python', 'nlp', 'spacy']","I have the following sentence:I passed this to the Spacy dependency visualizer (with ""merge phrases"" checked):

I'm really happy with this result and tried to ""remake"" this in Python:This gives me:So here kimchi, is merged with fermented-vegetable foods whereas I want it to be exact the same as in the example. I checked the github  parse_deps function and this also uses the doc.noun_chunks so I can't understand the origin of the discrepancy and how I can get the desired result?"
137,How can i remove strings from sentences if string matches with strings in list,"['python', 'python-3.x', 'pandas', 'list', 'nlp']","I have a pandas.Series with sentences like this:on the other hand, I have a list of names and surnames like this:l = ['juan', 'antonio', 'esther', 'josefa', 'mariano', 'cristina', 'carlos']I want to match sentences from the series to the names in the list. The real data is much much bigger than this examples, so I thought that element-wise comparison between the series and the list was not going to be efficient, so I created a big string containing all the strings in the name list like this:'|'.join(l)I tried to create a boolean mask that later allows me to index the sentences that contains the names in the name list by true or false value like this:but it returns:which is clearly not ok.I also tried using str.contains() but it doesn't behave as I expect, because this method will look if any substring in the series is present in the name list, and this is not what I need (i.e. I need an exact match).Could you please point me in the right direction here?Thank you very much in advance"
138,How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?,"['nlp', 'pytorch', 'transformer', 'huggingface-transformers', 'bert-language-model']","I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:I think this code is right, but I also notice BertForMaskedLM's paramaters masked_lm_labels, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:"
139,R: Select option by introducing a number,"['r', 'nlp', 'user-input', 'stdin', 'text-mining']","I would like to build a very simple, sketchy, function where you can introduce a word (in Spanish) and get the most feasible next word (kind of a text predictor). So when the user introduces me, suggestions are ha, he, lo, da and gusta. Now, I would like to enable the user to select one of those five suggestions by typing 1, 2, 3, 4, or 5. Is there any way to do this in R?"
140,Preprocessing of Labelled Text Data,"['python', 'data-structures', 'nlp', 'preprocessor', 'ner']","To train NLP models for NER it is necessary to have text data that has named entity labels on the text. In many cases this is given by a character offset (eg.
(""Android Pay expands to Canada"", [(0, 11, 'PRODUCT'), (23, 30, 'GPE')])), BILUO format (eg. ([""Facebook"", ""released"", ""React"", ""in"", ""2014""], [""U-ORG"", ""O"", ""U-TECHNOLOGY"", ""O"", ""U-DATE""])) or something similar (examples taken from the spaCy101). When one wants to do preprocessing on this data it is important to keeps the labels in the correct positions. Eg. for removing stop words or manipulating white-space characters and tokens will be removed. My question is:Is there a data structure that allows preprocessing of labelled text data while moving labels to the correct positions in the new text?If there is already an implementation of this in python I would also be interested in this. Otherwise I might also be willing to code it up myself.I used NER as a motivation for this question but I would also be interested if there was a more general data-structure that can store different types of labels simultaneously.For example I would like to do some operation like this to make all of my document lower case.This is not possible and returning an error.AttributeError: attribute 'text' of 'spacy.tokens.doc.Doc' objects is not writable"
141,StanfordNLP: Unable to identify Date with 7-class-ner,"['nlp', 'stanford-nlp']",I'm using stanfordNLP to get date entities from text. Here's the code that i tried:-I didn't get why it's not extracting Date even though it's very clearly identifiable in the text.
142,Python RuntimeError: input sequence after bert tokenization shouldn't exceed 512 tokens,"['python-3.x', 'nlp', 'ner', 'sliding-window', 'bert']","I try to run NER in Indonesian Language using configs.ner.ner_ontonotes_bert_mult from DeepPavlov http://docs.deeppavlov.ai/en/master/. I got this error RuntimeError: input sequence after bert tokenization shouldn't exceed 512 tokens when I run the code. I am stuck with the bert tokenisation of 512 tokens.I've read some resources, they said that the BERT model has positional embeddings only for first 512 subtokens. So, the model can't work with longer sequences. I can't truncate text to 512 as there will be loss of information in that case.There is also a model for long sequences, the name is 'Longformers'. But I can't use this model because the entity from deeppavlov is more complete than it.Also I found sliding window approach at Pytorch error ""RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows"", but I dont know how to implement it at NER usecase.Could you please help how can I handle this?The input is pandas.core.series.Series 'df'I have been doing split before it is going to NERThe codeWhen I run the code, it only processes 32 of df. After that  input sequence after bert tokenization shouldn't exceed 512 tokens error appearsHere is the 33rd of df : Limnologi mempelajari perairan di daratan Mamalogi mempelajari mamalia Mikrologi meneliti organisme mikroskopik dan interaksinya dengan kehidupan lainnya Mikologi mempelajari fungi Neurosains Neurobiologi mempelajari sistem saraf termasuk anatomi fisiologi dan patologinya Ornitologi mempelajari burung Paleontologi mempelajari fosil dan bukti Geografis kehidupan prasejarah Patologi Patobiologi atau patologi meneliti penyakit seperti penyebab proses ciri dan perkembangannya Parasitologi mempelajari parasit dan parasitisme Penelitian biomedis meneliti tubuh manusia yang sehat dan sakit Psikobiologi mempelajari dasar psikologi secara biologis Sosiobiologi mempelajari dasar sosiologi secara biologis Teknik biologis mempelajari biologi dari sudut pandang teknik dan lebih menekankan pada pengetahuan terapan Bidang ini terkait dengan bioteknologi Virologi mempelajari virus dan agen yang seperti virus Zoologi mempelajari hewan termasuk klasifikasi fisiologi perkembangan dan perilaku Bali adalah sebuah provinsi di Indonesia yang ibu kota provinsinya berNamaa Kota Denpasar Denpasar Bali juga merupakan salah satu pulau di Kepulauan Nusa Tenggara Di awal kemerdekaan Indonesia pulau ini termasuk dalam Provinsi Sunda Kecil yang beribu kota di Singaraja dan kini terbagi menjadi 3 provinsi Bali Nusa Tenggara Barat dan Nusa Tenggara Timur Selain terdiri dari Pulau Bali wilayah Provinsi Bali juga terdiri dari pulau-pulau yang lebih kecil di sekitarnya yaitu Nusa PenidaPulau Nusa Penida Nusa LembonganPulau Nusa Lembongan Nusa CeninganPulau Nusa Ceningan Pulau Serangan dan Pulau Menjangan Secara Geografis Bali terletak di antara Pulau Jawa dan Pulau Lombok Mayoritas puduk Bali adalah pemeluk agama Hindu Di dunia Bali terkenal sebagai tujuan pariwisata dengan keunikan berbagai hasil seni-budayanya khususnya bagi para wisatawan Jepang dan Australia Bali juga dikenal dengan julukan Pulau Dewata dan Pulau"
143,Error 'power iteration failed to converge within 100 iterations') when I tried to summarize a text document using python networkx,"['python', 'nlp', 'networkx']","I got an PowerIterationFailedConvergence:(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations') when I tried to summarize a text document using python networkx as shown in the code below. the error shown at the code ""scores = nx.pagerank(sentence_similarity_graph)"""
144,Word/Phrase classification,"['python', 'machine-learning', 'nlp', 'word2vec']","I have a column containing 5000 string records. These records are individual words or phrases (not a sentence or paragraph). Most of these records are similar or contain similar elements(e.g. ""Office"", ""offise"" ""ground floor office""). Also, someone manually classified 300 of these records into five categories (i.e. Residential, Industrial, Office, Retail, Other) which means I can use it to develop a supervised machine learning model. I did a bit of study on word2vec, but it seems they work on texts, not individual words and phrases. Please advise me on how I can do the classification. Please note that the number of the records in the column is growing and new records will be added in the future, so the solution must be able to classify new records.The sample input and the desired output is as below:Input & Output"
145,How can I extract keywords from a string using c++? I don't want to extract all the words from the string but just a few,"['c++', 'nlp', 'tokenize']","I am making an Eliza style chatbot and I need it to have keyword functionality, so I was wondering if there is a way to extract keywords from the input string so that if someone inputs ""Can you swim?"" the chatbot replies ""Don't you believe that I can swim?"", basically I am having trouble extracting ""swim"" from the string and putting it in the reply string. Right now my code just returns ""Don't you believe that I can do that?"" but I want my reply to contain the key word from the input that is ""swim"" in this case, which would make it more ""human like"", however I don't know how to go about it.Here is all my chatbot code till now:"
146,Type error: can only concatenate list (not “str”) to list while adding <eos>,"['python', 'string', 'machine-learning', 'nlp', 'data-science']","I am trying to do this for encoder but I am getting type error: can only concatenate list (not ""str"") to list.I don't know what to do. Kindly help me."
147,Right tool for querying NLP processed text,"['nlp', 'stanford-nlp']","I am using Stanford Stanza python package for analyzing text. For each sentence it returns a list of tokens(words) each containing token attributes like type of speech, normalized word form and so on.Below is sample output for the sentence ""Alex and Anna ate mushrooms""I would like to know the names of the persons who ate the mushrooms.Right now I wrote some algorithm to extract the names based on simple logic. Something like ""Select words which are tagged as 'name' and are connected to word 'eat'"". The algorithm is very specific and will not work in complex cases when sentence structure is not that simple. I suppose there should be standardized / recommended tools for querying tokenized text where it is easy to specify the rules for data extraction.What tools should I use for querying such data ?"
148,create category of product from product title in pandas,"['python', 'pandas', 'dataframe', 'nlp']","I have dataframe with column of products title :I want to to classify each product (in each row) to a category (example laptop, smartphone..)I created this function to apply it with pandas apply:The issue is some products titles contains many categories keywords without being belonging to one of them, example this title:this title contains Headphones and Earphone and Headset which is good to classify it in phones accessories but the issue is it contains Smartphone and there is a category for smartphones (not accessories) for example this title is for smartphones:when I use .loc to filter, it returns always empty result
This is an example of the dataset:
https://docs.google.com/spreadsheets/d/1oTHP3JU7FlK_wAye5KYSelWqziXRSKBBeXKrLWkeaFA/edit?usp=sharingI also don't know how to find all possible categories using the whole dataset"
149,How to concatenate Glove 100d embedding and 1d array which contains additional signal?,"['numpy', 'nlp', 'concatenation', 'stanford-nlp', 'word-embedding']","I new to NLP and trying out some text classification algorithms. I have 100d GloVe vector representing each entry as a list of embeddings. Also, I have NER feature of shape (2234,) which shows if there is named entity or not. Array with GloVe embeddings is of shape (2234, 100).How to correctly concatenate these array so each row represents its word?Sorry for not including reproducible example. Please, use variables of your choice to explain the concatenation procedure.Using np.concatenate did not work as I have expected but i don't know how to deal with dimensionality of embeddings."
150,Additional training NLP,"['python', 'nlp']","I started working with the Camembert deep learning model, an analogue of Roberta for the French language, and I have a question, how can I retrain such a model for a particular task? Specifically, the task is for the model to learn how to evaluate input sentences for correctnessI have built a layer, which needs to be trained, how to do it in the correct way? (it is especially important to understand which optimizer and loss function to use)"
151,Am i clustering users correctly by using sklearn's cosine similarity method and K-means algorithm?,"['python', 'nlp', 'data-science', 'k-means', 'cosine-similarity']","I have a movie dataset with more than 200 movies and more than 100 users. The users rated the movies. A value of 1 for good, 0 for bad and blank if no choice.I want to cluster similar users based on their reviews with the idea that users who rated similar movies as good might also rate a movie as good which was not rated by any user in the same cluster. I used cosine similarity measure with k-means clustering. The csv file is shown below:According to the scheme, if an original review was 1 (good) then we put 1 in the cell and -1 in the cell if the review was 0 (bad). For no reviews, we put 0 in the cell. The csv file below explains the scheme. The rows are users and M in the column is movie and C is the choice.I measured cosine similarity and then clustered the users with sklearn's cosine_similarity and kmeans clustering algorithm. The code is:With this code, i am getting the cosine similarity for all the pairs and i can filter the pairs based on the value of cosine similarity. I am also getting a list of clusters for the users. I want to know that am i doing it right ? Is it right to calculate the cosine similarity first and then pass the values to kmeans ? As by default, the sklearn's kmeans function uses euclidean distance.I will really appreciate some help.Thanks.."
152,LabelEncoder instance is not fitted yet,"['python-3.x', 'tensorflow', 'keras', 'nlp']",I have a code for prediction of unseen data in a sentence classification task.The code isBut I am getting this errorI have used Sequential model and the model.fit is written as history=model.fit() in the training part. Why am I getting this error?
153,ImputError: cannot import name “function” from “module.py”,"['python', 'pandas', 'dataframe', 'nlp']","In Jupyter Notebook:
repo:I have several functions in featurestorehouse.py, I need to import most of them, and some of them, particularly the ones I wrote earlier in time, work when I import them. However, the more recent ones don't want to be imported as it gives an ImputError.I checked and changed where necessary, the necessary modules are imported in the module file, I tried putting the imports specific to a function within the function definition - which didn't fix anything, and I tried to not import the functions separately but adding 'fs.' in front of each to reference featurestorehouse as fs.The function and module:Some of the functions that do not work:How I tried to implement them in my main notebook:The last set underneath ""### apply function"" gives an error when I import featurestorehouse.py as fs and add fs. to each function in the main notebook (obviously it doesn't get to this point when I'm importing each function separately from featurestorehouse.py).The error:Does anyone know what's up with this? There's no circular dependencies involved as far as I know. There's a lot of other notebooks in the repo but that's never been a problem up till now."
154,Manually tune tf-idf features in document classification,"['nlp', 'text-classification', 'tf-idf', 'feature-engineering', 'document-classification']","I am working on a multi-label document classification task with a very small data set (180 labeled documents) and a fairly large number of labels (20).I found that - ignoring label correlations and turning the problem into 20 binary decision problems - tf-idf features fed into a logistic regression work reasonable well.Since I have domain knowledge I would like to add manual weighs to some features to increase the prediction accuracy. For instance,  assume I train a model to recognise the category ""Gender"", I am thinking about something like this:The same weighting is then applied to the vectorised unseen data that I want to predict.This indeed gives me a better accuracy on the test set and also leads to better predictions on the unseen data, however, I feel quite uncomfortable to ""manually"" change the data.I have not found any information about such manual feature tuning. Is this something that can be  justified? Or is it generally a bad idea?"
155,Is there any way to whitelist spacy labelling?,"['nlp', 'spacy']","I'm new to spaCy and currently trying to use spaCy english large model to identify PERSON from sentences 
All is fine to identify PERSON from sentences until I found some name that's identified is not PERSON. 
E.g. If I put ""Alex is eating apple"". It will successfully return Alex is a PERSON 
But when this case happens, it won't work anymore
E.g. Sun Saw Bee is eating apple or Alexandro Soon is eating apple

I'm wondering if there is anything like whitelist to add in ""Sun Saw Bee"" or ""Alexandro Soon"" as a PERSON without retraining spaCy english model? 
or any way to somehow identify ""Sun Saw Bee"" as a PERSON instead?
if there is any link related to this perhaps can share as well, since my keyword searching might not hitting the right key"
156,Equate strings based on meaning,"['python', 'nlp']","Is there a way to equate strings in python based on their meaning despite not being similar.
For example,I've tried using fuzzywuzzy and difflib and although they are generally good for this using token matching, they also provide false positives when I threshold the outputs over a large number of strings.
Is there some other method using NLP or tokenization that I'm missing here?Edit:
The answer provided by A CO does solve the problem mentioned above but is there any way to match specific substrings using word2vec from a key?
e.g. Key = max temp
Sent = the maximum ambient temperature expected tomorrow in California is 34 degrees.So here I'd like to get the substring ""maximum ambient temperature"". Any tips on that?"
157,BERT Multi-class Sentiment Analysis got low accuracy?,"['machine-learning', 'nlp', 'sentiment-analysis', 'bert', 'ktrain']","I am working on a small data set which:Contains 1500 pieces of news articles.All of these articles were ranked by human beings with regard to their sentiment/degree of positive on a 5-point scale.Clean in terms of spelling errors. I used google sheet to check spelling before import into the analysis. There are still some characters that are not correctly coded, but not much.The average length is greater than 512 words.slightly-imbalanced data set.I regard this as a multi-class classification problem and I want to fine-tune BERT with this data set. In order to do that, I used Ktrain package and basically follows the tutorial. Below is my code:However, I only get a validation accuracy at around 25%, which is way too low.I also tried the head+tail truncation strategy since some of the articles are pretty long, however, the performance remains the same.Can anyone give me some suggestions?Thank you very much!BestXu================== Update 7.21=================Following Kartikey's advice, I tried the find_lr. Below is the result. It seems that 2e^-5 is a reasonable learning rate.learning rate.jpgAnd I just tried to run it with some weighting:Here is the result. Not much changed.============== update 7.22 =============To get some baseline results, I collapse the classification problem on a 5-point scale into a binary one, which is just to predict positive or negative. This time the accuracy increased to around 55%. Below is the detailed description of my strategy:However, I think 55% is still not a satisfactory accuracy, slightly better than random guess.============ update 7.26 ============Following Marcos Lima's suggestion, I made several additional steps into my procedures:remove all numbers, punctuation and redundant spaces before being pre-processed by the Ktrain pkg. (I thought the Ktrain pkg would do this for me, but not sure)I use the first 384 and last 128 tokens of any text in my sample. This is what I called ""Head+Tail"" strategy.The task is still binary classification (positive vs negative)This is the figure for learning curve. It remains the same as the one I posted before. And it still looks very different to the one posted by Marcos Lima:The updated learning curveBelow are my results, which are probably the best set of results that I have got.Note: I think maybe the reason why it is so difficult for the pkg to work well on my task is that this task is like a combination of classification and sentiment analysis. The classical classification task for news articles is to classify which category a news belongs, for example, biology, economics, sports. The words used in different categories are pretty different. On the other hand, the classical example for classifying sentiment is to analyse Yelp or IMDB reviews. My guess is these texts are pretty straightforward in expressing their sentiment whereas texts in my sample, economic news, are kind of polished and well organized before publication, so the sentiment might always appear in some implicit way which BERT may not be able to detect."
158,NLP algorithm for Root cause analysis form maintenance problem summary/data,"['python', 'nlp', 'bert']","""I am trying to extract root cause from a problem description. I have 20 years of maintenance history of a machine with columns = 'problem summary', 'Analysis', 'Engineering deposition'. All of them are text. I want to build a model that can read all the text for each record find the root cause.
Have anyone ever done something similar? Could you guide me how to start on that?
Thanks in advance"""
159,"Python NLP/regex: How to split a long text into two parts, but ensure that sentences are not split up?","['python', 'regex', 'nlp']","I have a text with 50000 characters. I am using an APP to process the text but the APP can only process text up to 10000 characters in length. So I have to separate the text into at least 5 parts.The simple way to separate the text is text[:10000], text[10000:20000], ..., This way may split a sentence into two parts, which is not what I want.Another way is using tokenize.sent_tokenize(text) to separate sentences, but the output of this way is a list of all separated sentences. It is too ineffective because I do not want to separate all sentences.Is there any effective ways to separate a long text into several parts?"
160,Counting term frequency in list of strings in pd dataframe,"['python', 'dataframe', 'nlp', 'tf-idf']","I have a dataframe and one column contains the lemmatized words of a paragraph. I wish to count the frequency of each word within the whole dataframe, not just within the record. There are over 40,000 records so the computation has to be quick and not reach the limit of my RAM.For example, this basic input:would have this desired output:
'complete':1
'health':2
'science':1
'test':1This is my current code:Which works when I manually enter a list of a list of strings (ex/[['completing', 'dog', 'cat'], ['completing','degree','health','health']]), but not when it iterates through the df.I have also tried this:to return the top 20 terms, but the output lists the frequencies of terms within the entry, not the entire dataframe.Any help would be appreciated!"
161,Compute similarity between document and specific keywords,"['python', 'nlp', 'cosine-similarity', 'sentence-similarity']","I have a collection of newspaper (unlabeled, just raw articles) articles on a disease. I also have three set of manually chosen keywords associated with the disease eg: phase-1,phase-2 etc like below.Is there anyways to calculate the similarities between a set of keywords and the news articles, using PYTHON ?"
162,How to apply tf-idf to new documents?,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I am working on a program that identifies the main topic of an article from a news website using tfidf. As you known, new articles come out every day and I need the ability to calculate tfidf on a single new article (or a set of new articles). I understand that tfidf isn't suited for a single document, independent of a corpus.With these limitations in mind, what would be the best way to apply tfidf to a new document in order to identify the main topics?"
163,How to get an old release of a spacy model?,"['python-3.x', 'linux', 'nlp', 'spacy']","I manage to install an old version of spacy with pip3 install spacy==2.2.4.However, when I follow this up with
python3 -m spacy download en_core_web_sm, it downloads en_core_web_sm-2.2.5.tar.gz."
164,NotFoundError: [_Derived_]No gradient defined for op: StatefulPartitionedCall on Tensorflow 1.15.0,"['tensorflow', 'nlp']","I am running a tensorflow model using BERT in the embedding layer. I found this similar question with no answer. Honestly I do not understand why the error is occurring, because the model runs fine for another dataset.When i call model.fitI get this error:Model:Loading BERT from Tensorflow hubLoading tokenizer and encoding textRun model"
165,How to use model.predict in keras?,"['python-3.x', 'tensorflow', 'keras', 'nlp']","I am using keras model.predict after training my model for a sentence classification task. My code isBut the output seems to be an array,I am working on a sentence classification task with 2 labels. So I wanted to predict these sentences as 0 or 1. But instead getting a numpy array. How do I code such that it predicts one of these two labels?"
166,I need some help developing a parser that would read each document and convert it into a vector of words,"['nlp', 'nltk', 'preprocessor', 'cosine-similarity']","If you look into this code, I am reading and preprocessing each document manually. I want someone to help me with creating a parser/iterative loop: that would read each document, preprocess that particular document. Once I get the bag of words for say document_1, I want it to be automatically stored in a file say file_1. Similarly for the rest of the documents too: the for loop reads and preprocesses each document and stores it separately. This means I require 5 different files that would contain their own bag of words. The intent is to calculate tfidf vector score later and calculate cosine similarity between 1-4 documents wrt document 5 respectively: so file_1 would be compared with file_5, file_2 with file_5 and so on..Any help would be appreciated."
167,Gensim vector shape changing,"['python', 'nlp', 'gensim', 'word2vec', 'shapes']","i am trying to vectorize a text and classify it by using gensim and tensorflow.keras.Before the train I have shapes as follows:After the train I have the following shapes:logging for gensimI couldn't understand the part that my dataset losing some of the text, can anyone explain to me the reason why the shape gets smaller?i am trying to learn about the topic. i would be grateful if anyone can explain it"
168,List has no attribute values,"['python-3.x', 'tensorflow', 'keras', 'nlp']","I am making a code for prediction of a sentence after the training part. My code till now is something like thisBut when I run this, it gives an errorWhy am I getting this error? Also when I pass indices like l[1] it gives the error: AttributeError: 'str' object has no attribute 'values'"
169,Implementation of TextRank algorithm using Spark(Calculating cosine similarity matrix using spark),"['python', 'apache-spark', 'pyspark', 'nlp', 'textrank']","I am trying to implement textrank algorithm where I am calculating cosine-similarity matrix for all the sentences.I want to parallelize the task of similarity matrix creation using Spark but don't know how to implement it.Here is the code:Here,cluster_wise_sen is a dictionary that contains list of sentences for different clusters ({'cluster 1' : [list of sentences] ,...., 'cluster n' : [list of sentences]}). cluster_dict contains the 100d vector representation of the sentences. I have to compute the sentence similarity matrix for each cluster. Since it is time consuming, therefore looking to parallelize it using spark."
170,"Unable to extract facts from image text, but works for a normal string?","['python', 'nlp', 'ocr', 'spacy', 'textacy']","I'm working on a program with OCR that identifies text from an image, then use spacy and textacy to extract facts from the image. My problem is that when I convert the image text to a string and print the extracted facts, nothing shows up. But when i use a normal declaration of a string and pass it into the doc variable, it works. Why?"
171,How to solve difficult sentences for nlp sentiment analysis,"['nlp', 'sentence']","Such as the following sentence,
""Don't pay attention to people if they say it's no good.""
As humans, we understand the overall sentiment from the sentence is positive.Technique of ""Bag of Words"" or BOW
Then, we have the two categories of ""positive"" words as Polarity of 1, ""negative"" words of Polarity of 0.
In this case, the word of ""good"" fits into category, but here it is accidentally correct.
Thus, this technique is eliminated.Still use BOW technique (sort of ""Word Embedding"")
But take into consideration of its surrounding words, in this case, the ""no"" word preceding it, thus, it's ""no good"", not the adj alone ""good"".  However, ""no good"" is not what the author intended from the context of the entire sentence.Thus, this question.  Thanks in advance."
172,How to return all extracted text from multiple PDFs in python?,"['python', 'pdf', 'machine-learning', 'nlp', 'pdf-scraping']","This is my code. So far, it'll print all the content of the pdfs to the pages variable. However, I cannot seem to return the same extracted text. I've been testing it by pulling information from random pdfs and placing it in the folder I'm calling. How do I get it to return the extracted text the same way it prints it?"
173,how to override downloader directory with local directory in polyglot in python,"['python', 'server', 'nlp', 'polyglot']","I run polyglot sentiment detection. When I upload it to the server I cannot run the downloader.download(""TASK:sentiment2"") command, so I downloaded the sentiment2 folder and saved it in the same folder as the python file.I tried to set downloader.download_dir = os.path.join(os.getcwd(),'polyglot_data') pointing at the sentiment2 folder location as it says in the polyglot documentation but it doesnt work.How do I override downloader directory so it will access the sentiment2 local folder when it executes the sentiment analysis?Please see the full code below. This code works on my computer and localhost but returns zero when I run it on the server.my localhost returns - 0.1666the server returns - 0.0"
174,Sliding window for long text in BERT for Question Answering,"['nlp', 'text-classification', 'huggingface-transformers', 'question-answering', 'bert-language-model']","I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.From what I understand if the input are too long, sliding window can be used to process the text.Please correct me if I am wrong.
Say I have a text ""In June 2017 Kaggle announced that it passed 1 million registered users"".Given some stride and max_len, the input can be split into chunks with over lapping words (not considering padding).If my questions were ""when did Kaggle make the announcement"" and ""how many registered users"" I can use chunk 1 and chunk 3 and not use chunk 2 at all in the model. Not quiet sure if I should still use chunk 2 to train the modelSo the input will be:
[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]
and
[CLS]how many registered users[SEP]1 million registered users[SEP]Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as -1? For example ""can pigs fly?""[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP][CLS]can pigs fly[SEP]announced that it passed 1 million[SEP][CLS]can pigs fly[SEP]1 million registered users[SEP]As suggested in the comments, II tried to run squad_convert_example_to_features (source code) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like run_squad.py from huggingface uses squad_convert_example_to_features with the s in example.I get the error.The error indicates that there are no tokenizers but it does not allow us to pass a tokenizer. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the squad_convert_example_to_features function?"
175,(Huggingface Transformers) Question for BartModel's output shape,"['nlp', 'transformer', 'huggingface-transformers', 'huggingface-tokenizers']","tune multilingual-Bart with Korean to generate some texts.
while I tried to pass my datas into model, I can't under stand why the output shape of the model is different from what I expected.Settings : I used MBartTokenizer and BartForConditionalGenerationfor batch, I used prepare_translation_batch to make datas as batch, inputs_ids and target_ids.
I also need decoder_input_ids(form like [tgt_lang_code, seqence, eos]), so I made it.Here's the problem.
BartForConditionalGeneration's forward pass needs input_ids necessarily. From the Docs of Bart, If I pass only the 'input_ids' to the model(it can include attention_mask), the model's decoder wouldn't have it's own input, so It takes 'input_ids' as their input.
And the shape of the prediction_scores in returns should be (batch_size, seq_len, vocab_size) and It worked rightBut, When I pass the 'input_ids' and 'decoder_input_ids' together to the model, the shape of the prediction_scores shows (batch_size, 1 , vocab_size) all the time.I think when I pass the inputs together, The shape of prediction_scores in returns should be like (batch_size, decoder_input_seq_len, vocab_size)
I tried with normal BartModel, It also showed just the same shape with BartForConditionalGeneration.[enter image description here][1]I don't know why this happen. Maybe I totally misunderstood the model at all.
The reason I made this topic is I need a clean view of this problem.
Any advice would be appreciated.
Thank you."
176,How can I define sentence position embeddings?,"['python', 'nlp', 'pytorch', 'embedding', 'word-embedding']","My goal is to generate sentence position embeddings like this article: Multiview Convolutional Neural Networks for Multidocument Extractive SummarizationAs described in the above article, first we denote the position of a sentence si
according to the following equation:S1:3 denotes the set containing the first three sentences of a documentS-3:-1 denotes the set containing the last three sentences of a documentThen, the three integers are mapped to a vector space h. The mapping procedure is done like that of word embedding (x = Lw, where w is a one hot vector and L is a word representation matrix).My first problem is: (1) Why the authors use from 0, 1 and 2 values? Are these values important? or can I change them with any other number?Second problem: (2) I think that the beginning and last sentences are more important in the text than middle ones. Is it reflect in 0, 1, and 2 values? Wouldn't it be better, for example, to use a value of 10 for the beginning and end sentences and a value of 0 for the middle sentences so that a larger value indicates more importance for the beginning and end sentences?I want to use embedding layer in PyTorch to implement it. I know that embedding layer is like a lookup table that the input is a list of indices and the output is their embedding vectors. In the case of sentence position embedding,I should assign a unique index to each sentence (0 for the 0th sentence in the document, 1 for the 1th sentence and so on), but my original problem is:Third problem: (3) I can't understand where should I use from p(si) values (0, 1 or 2)? Is the size of embedding matrix (3, embedding_dimension) or (number_of_sentences, embedding_dimension)?Thanks in advance."
177,Get sentence vector for a K-means clustering task,"['machine-learning', 'nlp', 'vectorization', 'gensim', 'word2vec']","I am working on a project which groups jobs posted on various job portals into clusters based on the description of the jobs using K-means.I found the work vector using Word2Vec, but i guess this will not serve the purpose as I will need a vector of the whole job description.I know that I can average out the word vector of a sentence to get the sentence vector but worried about the accuracy as this will loose the ordering of the words.Is there any other way I can get the vectors ?"
178,Groupby with 2 colums - “pandas.core.groupby.generic.DataFrameGroupBy” terminal response,"['python', 'pandas', 'dataframe', 'scikit-learn', 'nlp']","For a current project, I am planning to group a Pandas DataFrame by stock_symbol as first criterium and quarter as second criterium.From other threads, I have seen that a structure like group_data = df.groupby(['stock_symbol', 'quarter']) could be a possible solution for this point. In the given case, I am however only receiving the terminal output <pandas.core.groupby.generic.DataFrameGroupBy object at 0x11fdcbf10>.Does anyone find my thinking error with this line? The relevant code section looks like this:The function to be called in the operations is highlighted below:And the corresponding DataFrame has the following structure:"
179,How to extract pre-defined key words from a sentence in Python?,"['python', 'nlp']","Consider the following example
""10% of on all Artificial Intelligence courses.""
In this example, I have to extract two predefined classes like Artificial Intelligence and courses. Even the program has to classify words like ANN, CNN, RNN, AI, etc. into the Artificial Intelligence category. I have used spacy to train but I am not impressed with the results as it is not labeling correctly. Is there any alternative to extract entities from a sentence in Python?"
180,How to predict the part type based on combination of a part number and part description using Python?,"['python', 'python-3.x', 'nlp', 'nltk', 'prediction']","I'm currently trying to develop a program in python that uses NLP to predict part types. In my organization, each part has a Part Number (12 digit number) and a Part description. I want my program to be able to predict what type of part it is based on these two fields (Part number and description). The 5th and 6th digit of part number indicates the design group, so I can narrow down the part type. For ex. if 5th and 6th digit is 42 I can estimate that the part belongs to Brakes group.
I have a list of part descriptions (~25000 parts) and part types (~350 nos.). I used NLTK in Python to match strings but it is isn't giving good results. Any ideas on how I can improve the matching?
Mine is a manufacturing company with several types of parts (brackets, bolts, nuts, hoses, brakes, etc.) and the part descriptions contain these strings (or their abbreviations e.g. SCR for screws, BKT for bracket) either at the start, end or somewhere in the middle of the whole description string.Any ideas as to how I should proceed with this problem (approach, algo, etc.)? Any suggestions are welcome.I have followed the logic described in this page (https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp)
code below:Hope this clarifies a bit."
181,ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()||| Ktrain| BERT,"['python', 'arrays', 'python-3.x', 'nlp', 'ktrain']","need your support...I am following one tutorial and trying to run on my dataset. I am getting below error ...Pls consider me a beginner so I would appreciate if you can let me know how to fix and what is the reason?Error : ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()Screenshot of errorenter image description hereI found out that I need to use (np.allclose) I am not sure where to use in my code.....ThanksUpdate: New Error after updating the code with the solution provided.
New Error after updating code with the solution provided"
182,Conditional weighting for SparseCategoricalCrossentropy in TensorFlow,"['python', 'tensorflow', 'keras', 'nlp', 'tensorflow2.0']","We are working on an extractive text summarization task in TensorFlow. We have been able to get a baseline model up and running. Now, to get a more holistic model what we want to do is to be able to compensate for the padded sequences and at the same time give more preference to the sequences that are summary candidates originally.Here's what we have done so far:This results in the gradients not found error. Any directions to remedy the situation, in general, or even a better approach toward conditional weighting with SparseCategoricalCrossentropy would be helpful."
183,Increasing batch size decreases trainable parameters,"['python-3.x', 'tensorflow', 'keras', 'nlp']","I am using a LSTM+attention layer for sentence classification task. I have observed that in simple LSTM model, my total trainable parameters were 14705 with batch size of 64, but when I am using attention layer with  LSTM, for the same batch size, the trainable parameters decreases to 230. While for batch size of 4, it increases to 3077. Also, since batch size is 64 in attention layer, it is increasing by 1 only in epochs.How is it possible?This is the screenshot for attention layer for batch size 64And this is the screenshot for LSTM layer for batch size 64."
184,How to fit NLP in CNN model?,"['python', 'keras', 'nlp', 'cnn']","I am doing research on using CNN machine learning model with NLP (multi-label classification)I read some papers that mentioned getting good results in applying CNN for multi-label classificationI am trying to test this model on Python.I read many articles about how to work with NLP an Neural Networks.I have this code that is not working and giving me many errors ( every time I fix the error I get another error )I ended seeking paid FreeLancers to help me fix the code, I hired 5 guys but non of them was able to fix the code !you are my last hope.I hope someone can helpe me fix this code and get it working.First this is my dataset (100 record sample, just to make sure that code is working, I know it is not enogh for good accuracy. I will tweak and enhance model later)http://shrinx.it/data100.zipat the time being I just want this code to work. yet tips on how to enhance accuracy are really welcomed.Some of the errors I gotandhere is my codea sample of my datasettext100.csvResults100.csv"
185,Failing to create DTM for n-grams in R,"['r', 'nlp', 'text-mining', 'tm', 'n-gram']","I've tried to apply the answer to this question, but it doesn't work. I've used VCorpus to get the docs_es corpus.I don't know what's wrong."
186,What is the highest accuracy of the Stanford's Large Movie Review Dataset ever achieved?,"['deep-learning', 'nlp', 'imdb']","I want to know the NLP model that was able to achieve the highest accuracy using the open Stanford's Large Movie Review Dataset, and also its accuracy.
The link to the dataset."
187,How to resolve Kaldi ASR feature extraction error?,"['nlp', 'speech-recognition', 'kaldi']","I am working on the Kaldi tutorial for dummies. I followed every step. Now at the end when I run the main script name run.sh I get these errors. I have been trying to resolve this for hours now. I have setup also manually setup path in path.sh.The problem is here in the decode.sh script. It exits from line 83, instead of going down. I am attaching the decode.sh file for reference. What am I doing wrong?
here is the screenshot of terminal. For both Mono decoding and TRI1 decoding it say that
TRI1 DecodingDecode.sh"
188,Is Asynchronous encoding is better than normal encoding in BERT client,"['asynchronous', 'nlp', 'encode', 'bert']","I am using BERT client to do encoding process. I am currently using bc.encode() to get a fixed vector for every record. I want to understand how bc.encode_async() is faster bc.encode() method. Is there any example to understand the difference between these two?My encoded fields such as title, description.Consider I have 1000 records, each records has title and description. I want to encode using async method.How it will handle for the followingIt should not encode the empty string or array?Any help would be appreciated?"
189,how can I add a dataset as meta to a corpus in R,"['r', 'nlp', 'text-mining']","I have a corpus with many legal decision and their case numbers but also a dataset with the case numbers and characteristics about the case number.
How can I add one or two of the binary characteristics (e.g. won or lost) as metadata to the corpus for textmining?My aim would be to then seperate the documents with regards to the particular value as metadata and do different NLP and text mining techniques separately for the cases which were rejected and for the cases which were not rejected.Thank you very much in advance."
190,Assigning “n” to the total number of words detected,"['python', 'pandas', 'dataframe', 'nlp']","For a current project, I am planning to count the total number of words in a given Pandas DataFrame. The code below is based on SciKit-Learn and assigns a frequency to each word identified but requires to define the total quantity n of words considered.I am however looking to count the total number of words in the DataFrame. Is there any way to set n to an infinite number to cover all relevant words in the DataFrame and to then calculate the sum of these relevant words (i.e. not showing 'manager' : 10, 'office': 5 but 'total words': 15)?In the given example, I am using 10000 as the number of words to be considered (through the line common_words1=get_top_n_bigram_Group1(df[i], 100000)) but am wondering if there is a more flexible/automated way.The relevant code section looks as follows:"
191,Accuracy per epoch in PyTorch,"['machine-learning', 'nlp', 'pytorch']","I have made a chatbot using pytorch and would like to display accuracy on every epoch. I am not quite understanding how to do that. I can display loss but cant figure out how to display my accuracyHere is my code :-My chatbot is working inverselt... i am trying to map the answer to the right question.
Any help would be appreciated."
192,Scaled Co-occurrence matrix with window size calculation in python,"['python', 'matrix', 'nlp', 'stanford-nlp', 'find-occurrences']","Say, I've a dataset in CSV format, which contains sentences/paragraphs in rows.
Suppose, it looks like this:df = ['A B X B', 'X B B']Now, I can generate co-occurrence matrix that looks like thisHere, (A,B,X) are words. It says B appeared where X is present = 4 times
Code that I used for itThe beauty of this code segment is that it allows me to choose windows size. That means if a particular word doesn't appear with a fixed amount of range from total sentence size then it gets ignored. But I would like to scale it.So this means if a word is far from the target word ""to"" then it will be assigned lesser values. Unfortunately, I couldn't find a suitable solution for it. Is it possible with a package such as scikit-learn? Or is there any other way to do it except raw coding?"
193,Added layer must be an instance of class layer,"['python-3.x', 'tensorflow', 'keras', 'nlp']",I am building a Bi-LSTM network and I have included an attention layer in it. But it is giving an error that added layer must be an instance of class layer.Some of the libraries which I have imported areThe attention layer class isThe model looks like thisBut it is giving an error
194,what is use case of Tokenization and Lemmatization in NLP when we have CountVectorizer and Tfidfvectorizer,"['machine-learning', 'scikit-learn', 'nlp', 'lemmatization', 'tfidfvectorizer']","i am learning the NLP and gone through;tokenization,Lemmatization Parts of speech and other basics.
I came to know CountVectorizer  and Tfidfvectorizer are there from sklearn which having internal ability to apply tokenization, Lemmatization.so question is :when i need use the core NLP activities to get the vocabulary instead of using CountVectorizer  and Tfidfvectorizer?"
195,How to add attention layer to a Bi-LSTM,"['python-3.x', 'tensorflow', 'keras', 'nlp']",I am developing a Bi-LSTM model and want to add a attention layer to it. But I am not getting how to add it.My current code for the model isAnd the model summary is
196,How to further pretrain a bert model using our custom data and increase the vocab size?,"['python', 'tensorflow', 'nlp', 'pre-trained-model', 'bert']","I am trying to further pretrain the bert-base model using the custom data. The steps I'm following are as follows:Generate list of words from the custom data and add these words to the existing bert-base vocab file. The vocab size has been increased from 35022 to 35880.I created the input data using create_pretraining_data.py from the bert official github page.Doing the pretrain using run_pretraining.py but facing the mismatch error:ValueError: Shape of variable bert/embeddings/word_embeddings:0
((35880, 128)) doesn't match with shape of tensor
bert/embeddings/word_embeddings ([30522, 128]) from checkpoint reader.Note: I changed the bert_config file with lastest vocab_size as 35880.Please help me to understand the error and what changes should be made, so that I can pretrain with the custom vocab file."
197,Mapping of two text documents with python,"['python', 'dataframe', 'deep-learning', 'nlp', 'text-mining']","I have annotated some textual data and now I am trying to map it with the original text file to get more information out.
I have all information of the annotations in a JSON file, from which I successfully parsed all the relevant information. I stored the information as seen below.My goal now is to include non-annotated text, as well. Not every single sentence or character of a text document has been annotated, but I want to include them to feed all the information into a DL-Algorithm. So every sentence that has not been annotated should be included and showing ""None"" as of entity class and entity label.Appreciate any hint or help on that!Thanks!"
198,Tokenization for Google's Public Word2Vec,"['machine-learning', 'nlp', 'tokenize', 'word2vec']","I'm trying to tokenize my text in a way that is compatible with Google's pretrained word2vec https://code.google.com/archive/p/word2vec/ however I could not find any code associated with the preprocessing nor training here.  I did find this link: https://github.com/dav/word2vec but it seems it also does not have preprocessing.  For example, in Google's word2vec they have vocab items like ####.## which I assume means four numbers followed by a dot followed by two numbers.  I have managed to produce a tokenizer which produces somewhat the expected format but it would be nice to have something official.  Anyone know of anything?"
199,How can I transform verbs from present tense to past tense with using NLP library?,"['python', 'python-3.x', 'nlp', 'stanford-nlp', 'spacy']","I would like to transform verbs from  present tense to past tense with using NLP library like below.There is no way to transform from present tense to past tense.I've checked the following similar question, but they only introduced the way to transform from
past tense to present tense.I was able to transform verbs from past tense to present tense using spaCy.
However, there is no clew to do the same thing from present tense to past tense.Python 3.7.0spaCy version 2.3.1"
200,"How to improve accuracy of model for categorical, non-binary, foreign language sentiment analysis in TensorFlow?","['python', 'tensorflow', 'machine-learning', 'keras', 'nlp']","My aim is to categorize sentences in a foreign language (Hungarian) to 3 sentiment categories: negative, neutral & positive. I would like to improve the accuracy of the model used, which can be found below in the ""Define, Compile, Fit the Model"" section. The rest of the post is here for completeness and reproducibility.I am new to asking questions on Machine Learning topics, suggestions are welcome here as well: How to ask a good question on Machine Learning?For this I have 10000 sentences, given to 5 human annotators, categorized as negative, neutral or positive, available from here. The first few lines look like this:I categorize the sentence positive (denoted by 2) if sum of the scores by annotators is positive, neutral if it is 0 (denoted by 1), and negative (denoted by 0) if the sum is negative:Following this tutorial, I use SubwordTextEncoder to proceed. From here, I download web2.2-freq-sorted.top100k.nofreqs.txt, which contains 100000 most frequently used word in the target language. (Both the sentiment data and this data was recommended by this.)Reading in list of most frequent words:Initializing encoder using build_from_corpus method:Building on this, encoding the sentences:Convert to a tensor each sentence's sentiment:Defining how much data to be preserved for testing:From the pandas dataframe, let's create numpy array of encoded sentence train tensors:These tensors have different lengths, so lets use padding to make them have the same:Let's stack these tensors together:Stacking the sentiment tensors together as well:Define & compile the model as follows:Fit it:The first few lines of the output is:As in TensorFlow's RNN tutorial, let's plot the results we gained so far:Which gives us:Prepare the testing data as we prepared the training data:Evaluate the model using only test data:Giving result:
Full notebook available here.How can I change the model definition and compilation rows above to have higher accuracy on the test set after no more than 1000 epochs?"
201,"(NLTK, WordNet) Adding new sense relations (new meaning, antonym, synonym, entilement)","['nlp', 'nltk', 'wordnet']","I looked around for a way of adding new entries to the synsets of a word in WordNet, but with no luck. I am trying to expand locally what I get with the basic WordNet module with the functionalities to work (preferably also so I could build n-grams at the end, but lets start small :)).Are there any simple ways to expand what I get from the following snippets?1 How could add meanings to the word 'staccato' (also with a deffinition)?2 How could add entailments to a particular meaning?3 How could add antonyms to the new meaning?Thanks for ideas."
202,Corpora annotated for NER,"['nlp', 'ner']","I am currently working on a project on NER of legislative texts of the 16th century in Spanish, Portuguese, and Latin. In order to train the model properly, I need some annotated data, ideally corpora sharing the same genre and/or time period. Up to this point, however, I could not find anything remotely suiting my needs. Maybe someone has came across such datasets or could give directions where they can be found? Would really appreciate any help."
203,Solr 8.1 org.apache.solr.common.SolrException: Cannot unload non-existent core,"['python', 'ubuntu', 'solr', 'nlp']","I am installing the Clarity NLP (local machine setup without docker) and I am following the directions provided at https://claritynlp.readthedocs.io/en/latest/setup/local-no-docker.html#setup-solr. When I run the following commands:the python script should create a Solr core named claritynlp_test, but I am getting an error:And in the solr admin UI logger I get the following message:I am not very familiar with the command line, so I'm just following the directions provided in the document. So how can I fix this issue and create the core?
The ClarityNLP download files provided test csv files that would have been loaded into the solr core through the python script if it had worked... is there a way to create a core and load these files through the admin UI?"
204,Gensim training meaningless word embeddings,"['nlp', 'gensim', 'embedding', 'word-embedding', 'fasttext']","The word embeddings when training with Gensim are meaningless: the similarities on known relationships (King - Man + Woman -> Queen) and others simply don't hold (not even close), they might as well be random.Using the same training data and equivalent parameters, I get meaningful results with Facebook's FastText. In comparison, I've tried the Gensim FastText and Word2Vec classes, both returning meaningless embeddings.To narrow things down, here are some of the settings:Training code below:I would rather avoid using Facebook's FastText as it would require me changing its source code to achieve this vocab size.What could be going wrong, or how can I debug this? I have been able to import Facebook's FastText vectors into Gensim and it works fine."
205,NLP problems to handle sentence with conjunctions,"['python', 'python-3.x', 'nlp', 'stanford-nlp', 'spacy']","I would like to preprocess sentences include conjunctions like below.
I don’t care the tense of verb and transformation following the subject.
What I want to is to hold new two sentences that have subjects and verbs individually.I was able to solve the sentence of Pattern1 with the below code, but I'm stack with thinking the solutions for Pattern2 and 3 with the below code no.2.With using the NLP library spaCy, I was able to figure out conjunctions is recognized as CCONJ.
However, there is no clues to realize what I want to do like the above.Please give me your advice!Pattern1working codepython 3.7.4spaCy version    2.3.1jupyter-notebook : 6.0.3"
206,String matching keywords and key phrases in Python,"['python-3.x', 'string', 'nlp', 'tokenize']","I am trying to perform a smart dynamic lookup with strings in Python for a NLP-like task. I have a large amount of similar-structure sentences that I would like to parse through each, and tokenize parts of the sentence. For example, I first parse a string such as ""bob goes to the grocery store"".I am taking this string in, splitting it into words and my goal is to look up matching words in a keyword list. Let's say I have a list of single keywords such as ""store"" and a list of keyword phrases such as ""grocery store"".Now the issue is this Sometimes my sentences might be simply ""bob goes to the store"" instead of ""bob goes to the grocery store"".I want to find the keyword ""store"" for sure but if there are descriptive words such as ""grocery"" or ""computer"" before the word store I would like to capture that as well. That is why I have the keyphrases list as well. I am trying to figure out a way to basically capture a keyword at the very least then if there are words related to it that might be a possible ""phrase"" I want to capture those too.Maybe an alternative is to have some sort of adjective list instead of a phrase list of multiple words?How could I go about doing these sort of variable length lookups where I look at more than just a single word if one is captured, or is there an entirely different method I should be considering?"
207,Detecting dutch addresses in unstructured text,"['python', 'nlp', 'text-mining', 'street-address']","I need to detect addresses in their many forms in unstructured dutch texts. These addresses may or may not include post codes, abbreviations, and spelling mistakes. I've come across pyap, which does this for US addresses, and Libpostal, which normalises strings which have already been identified as addresses. Does anyone know if there are any existing libraries for detecting addresses specifically in dutch texts, or that can be adapted to do so with minimal effort?Any help would be much appreciated."
208,Should entity examples be part of Intent training data or Entities (when Auto expansion is enabled),"['nlp', 'dialogflow', 'nlu']","The title is not the most concise one because my use case needs some elaboration. If anyone wants to update it, you're welcome.I have defined an Intent in Dialogflow for Product Search. I want this Intent to be able to extract fairly large variations of Product Names without explicitly providing the product names as training data.Now, the product names could be a single word or contain multiple words. E.g:As you can see that there can be unlimited variations in product name. So, I enabled the Allow automated expansion for the Product Name entity.The sentences would follow some structures like Do you have PRODUCT or I need one PRODUCT.Now, I want this agent to work for Do you sell orange juice or any sentence that starts with Do you sell ___ _. Do I add another row in the Intents for Do you sell orange juice or do I add orange juice to the Product Name intent. Which option would make the agent most generic. I know that I have to add more training examples but where should I add those for maximum performance?I tried adding orange juice to the Product Name entity. When I tested with Do you sell juice, the agent extracted orange juice as Product Name. So, I think adding more variations to the Product Name entity would make the agent somewhat biased towards the examples in the Product Name entity. Is this correct?So, all in all, if a want a very generic agent that extracts entities based on sentence structure, where do I add more training data, part of Intents or Entities?"
209,Tensorflow 2.0 Model interpretability for NLP,"['nlp', 'tensorflow2.0', 'tensorflow-datasets']",I am have trained deep model using tensorflow 2.0 for nlp classification task. I looked into Lime and tf-explain and Shap to explain my model's decisions but non worked for me.
210,"I am facing ValueError: Shapes (1, 14) and (1, 139, 14) are incompatible","['python', 'tensorflow', 'machine-learning', 'deep-learning', 'nlp']","=> ((52, 139), (52, 14))array([293,  40, 294, 129,  75, 130, 129, 131, 295, 296, 132,
297, 298,
2, 299,  34,  12,  76, 300,  27, 301,  15,   1, 302, 133,   4,
77, 303,   3, 134, 304,  78,  34, 305,  11, 306, 307,   4,   1,
132, 135,  22,  10, 308,  11, 136,   4,   1, 309,  50,   4, 310,
11,  78, 311, 312,   3,  77,   1, 313, 130,  10, 137,  11,  12,
109,   7, 314, 315,   7,   1,  76, 316,   4, 317, 318,  34, 138,
319, 139, 320,   3,  77, 321,  79, 322,   4,   1, 323, 324,   4,
1, 325,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0])array([1040, 1041,    2, 1042,    0,    0,    0,    0,    0,
0,    0,
0,    0,    0])Here X,y represent the sequences converted using keras tokenizer library. My aim is to learn title of the paragraph."
211,How to add our particular Bag of Words in UBER PPLM?,"['python', 'machine-learning', 'nlp', 'uber-api', 'gpt']","I tried adding the file path of Bag of Words inside the run_pplm python file, which have Bag of words archive map where we have the default Bag of words from the UBER PPLM. I think it is not taking the particular Bag of words which is specified by me in the function."
212,ENTITIES matching conflicting in LUIS,"['machine-learning', 'nlp', 'luis', 'luis.ai']","I have two entities created in LUIS. One entity to identify the AlphaNumeric word and another one to identify a word with a pattern. Both entities are created using a regular expression.To identify alphanumeric I used - \w+\d+ regular expression.To identify the word with a pattern I used - ^venid\d+ (words like venid12345, venid32310...)These two entities are mapped to two different INTENTS. But actually how much I trained the LUIS, still the first entity is only getting recognized. How to overcome this?"
213,How to extract this sentence using regex in python?,"['python', 'regex', 'nlp']","I am trying to extract sentences that has citations from research articles. I have managed to extract all sentences except the one.""Relevance (for the individual undergoing a learning process) appears as triggers in Hidi and Renninger’s model (2006) and can be any of the types of relevance in Priniski et al.’s continuum.""(r'\w.+\(\d{4}\)+\.*', regex = True) is the pattern I have used. I wonder if the words inside parenthesis are to be dealt.Sample paragraph from the article:n \n\nOn Work Relevance of Adult Education: A Case Study Narrative \n\nTone Vold1,2, Hanne Haave2 and Aristidis Kaloudis1 \n1NTNU, Norway \n2INN, Norway \nTone.vold@ntnu.no \nTone.vold@inn.no \nHanne.haave@inn.no  \nAristidis.kaloudis@ntnu.no \nDOI: 10.34190/EJKM.18.02.002 \n \nAbstract: There is an increased focus on relevance of higher education. Mostly it is about enhanced job opportunities or job \nadvancements for the individual. However, relevance of higher education may also be towards solving important issues or \nproblems  at  a  workplace.  There  are  some  necessary  preconditions  as  to  how  an  educational  activity  becomes  relevant. \nFirstly, the student must be capable to discover how generic knowledge and acquired skills may or may not apply to concrete \nsituations at work. This requires experience, understanding of the norms and culture of the organisation and a certain form \nof practical intelligence.I have split & tokenized the sentences and then converted it to a dataframe from which I am trying to match and extract entire sentences with citations using the following codeprint (df[df['sentences'].str.contains((r'\w.+(\d{4})+.*', regex = True)]) is the code I use to extract all rows/sentences with citation from the dataframe (df)I have managed to write different regex patterns that matches the entire sentence in rows of my dataframe. A regex pattern that will match my problem sentence will help me out."
214,ModuleNotFoundError: No module named 'pegasus',"['python', 'tensorflow', 'machine-learning', 'nlp', 'google-colaboratory']","I would like to try PEGASUS to summarize article. https://github.com/google-research/pegasusThe original repo's README suggests to use Google Cloud Compute Engine, but I use Colaboratory notebook. I'm an English teacher in Japan and I hope my students to try this software easily. They can experience both Machine Learning and English passage summarization.I followed this instruction. https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setupThis is my colab notebook.
https://colab.research.google.com/drive/1p95tZcjhfuCLYh23X3S_gZqRoWVhpIlE?usp=sharingThis is my code in the notebook.Then, I get this error message.This error message says that python can't import 'pegasus' module, but I made python path with !export PYTHONPATH=/content/pegasus this command.Could you give me any advice, please?"
215,Need help calculating cosine similarity of a sparse matrix,"['python', 'numpy', 'nlp', 'recommendation-engine']","I'm trying to calculate cosine similarity of a sparse matrixThe thing is I used scikit-learn's cosine_similarity function but I got this error:
memoryError: Unable to allocate 29.7 GiB for an array with shape (3984375099,) and data type float64I googled the error where I was suggested to increase the size of the paging file, but after doing it my PC just freezes and I have to force shutdown and reboot. Is there any way to overcome this?"
216,Python NER: add custom text and labels to update the NER model,"['python', 'nlp', 'spacy', 'named-entity-recognition', 'ner']","I'm using NER to essentially scrub text so that each named entity is replaced with its label (PERSON, ORG, etc.). So ""John works at Apple"" would become ""PERSON works at ORG.""clause_text is my list of sentences. I used the ner-d package to build my NER model and scrub text as follows:Now, I am trying to add custom training data. Basically I want to be able to add a sentence with labels and update the NER model to make it more accurate/specific to what I need it to do. Right now I have this:But when I run this, I get this error:Does anyone have any insight on how to best fix this code, or if there's a better way to add custom entries to update the NER model? Thanks so much!"
217,Separate dictionary and text,"['python', 'dictionary', 'nlp']","I have many sentences like as follows (This is one sentence and not many sentences) :I want to separate the text and dictionary as separate parts like:Splitting by "","" wont help as it will lead to two problems:Note that the text might contain emoji's also in utf-8 encoded form.How can this be done?"
218,Model feature importance in text classification,"['python', 'nlp']","from sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer=TfidfVectorizer(min_df=5,max_df=0.98,use_idf=False)  tfidf_vectorizer.fit(Tweet_balanced['text'])X = tfidf_vectorizer.transform(Tweet_balanced['text'])`X.shape(63828, 8585)``X.columnsAttributeError: columns not found`I was checking the feature importance of the model that I build for text classification but the error emerged 'data has no columns feature'. How can I check the feature importance for my model(linear SVM) since I doubt overfitting in my model?"
219,Will the document vectors generated by Doc2Vec be similar to document vectors obtained through Word2Vec?,"['nlp', 'word2vec', 'word-embedding', 'doc2vec']","I came across few blog posts stating that, Document vectors can be generated not only by Doc2Vec, but also by averaging the word vectors obtained by running Word2vec algorithm.
In that case, would the vectors generated through both the Algorithms be the same?
Which would be the most efficient way to generate the Document vectors and Why?Any reference links in this regard would be of great help!!Thanks in Advance"
220,How can I ensure that my PDF reading code does not return a NaN row and a duplicate row?,"['python', 'pandas', 'machine-learning', 'nlp']","I am working on reading PDF files that are CVs into a DataFrame (pandas). However, after reading the files I find a NaN row and a duplicate row of the last CV (alphabetically). Is there something in the code that does this? I can't seem to figure out why. I have tried changing around the iloc index[0] parts and the fileIndex value, but have found no solution. All help is appreciated.Here is a list of the PDF files currently in the directory:
[PDF files in directory][1]
[1]: https://i.stack.imgur.com/8UJZQ.pngThis is the result:"
221,How do i get word2vec similarity from the mean vector?,"['python', 'machine-learning', 'nlp', 'word2vec']","For example, there are words for 'apple', 'banana', and 'orange'.We will execute the code below to save the distance between apple and banana.But what I want to know is the similarity between 'apple' and 'whole fruits'.
How do i get the similarity of apples and whole fruits?I already got vectors for the whole fruit.
e.g. whole fruits=[0, 0.4, 0.2, 0.2, 0.5, .....]"
222,"No gradients provided for any variable, Attention-based Neural Machine Translation, tensorflow-keras implementation","['tensorflow', 'keras', 'deep-learning', 'nlp', 'attention-model']","I am trying to implement an Attention-based Neural Machine Translation architecture in Keras. I  have encoder (Extends Model class), attention(layer class) and decoder(Model class) classes. Finally, I have a ""combine"" class (Extends Model) that combines all these models.Here is the issue I am facing:How do I provide gradients to all these variables?I have spent days figuring out and trying other alternatives like using GradientTape, but nothing seemed to be working."
223,How to parallelize a text classification model to all kind of sentences?,"['python', 'nlp', 'bert', 'spacy-pytorch-transformers']",".Hi allI'm working on a text classification model with pytorch and using ""BertForSequenceClassification"" from ""Transformers"",which aims to classify text in two outputs(Intent expressed,Intent non-expressed),but when trying to evaluate it on test set,it turns out that it classifies  incorrectly sentences like ""I bought a laptop and i'm verry happy with it""(it classifies it as Intent expressed)...also sentence like""since my childhood i aimed to get my own buisness.which e-learning plateform would u choose in my case to learn buisness?""(it classifies it as Intent non-expressed)Knowing that I collected the Data I used for Training from Twitter API(with keywords ""I want"",""I need"",""I'm looking for"")
And I did a processing step on these data like :-detect negation on texts containing verbs""need"",""like"",""want"" and label them as Intent non-expressed(using spacy)-detect condition expression on these verbs and do the same-(and I don't know how to detect interrogations on these verbs..)And I added some text(Intent non-expressed) using also Twitter API(with keywords ""politics"",""economy""..)Here is the Training and Validation steps :NB: -I used a pre-processing step to delete emoji,links..and manage spacesThe size of positive texts is 25000 ,and negative also 25000Any help would be appreciated"
224,Python named entity recognition (NER): Replace named entities with labels,"['python', 'nlp', 'spacy', 'named-entity-recognition', 'ner']","I'm new to Python NER and am trying to replace named entities in text input with their labels.The output is: [('2008', 'DATE'), ('Jeff Atwood', 'PERSON'), ('Joel Spolsky', 'PERSON')]I can then extract the people, for example:to get ['Jeff Atwood', 'Joel Spolsky'].My question is how can I replace identified named entities in the original input text so that the result is:Stack Overflow is a question and answer site for professional and enthusiast programmers. It is a privately held website, the flagship site of the Stack Exchange Network,[5][6][7] created in DATE by PERSON and PERSON.Thanks so much!"
225,Text comparison of phrases in two data-frames and getting the output at matching phrases with sequence and index,"['python', 'dataframe', 'nlp', 'spacy']","Two datasets df and df1 are in columns in row-wise split, but separated by fullstop '.' as complete sentence.
I want to match the dataset phrases which are present in both and and get the dataset at the matching sentences with the index of superset df.I can only make if the text is plain, but not in the column-wise. If the spaCy or nlp with language model can help to handle this issue?"
226,PCA on word2vec embeddings using pre existing model,"['python', 'nlp', 'jupyter-notebook', 'pca', 'word-embedding']","I have a word2vec model trained on Tweets. I also have a list of words, and I need to get the embeddings from the words, compute the first two principal components, and plot each word on a 2 dimensional space.I'm trying to follow tutorials such as this one: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/However in all such tutorials, they create a model based on a random sentence they use and then calculate PCA on all the words in the model. I don't want to do that, I only want to calculate and plot specific words. How can I use the model that I already have, which has thousands of words, and compute the first two principal components for a set list of words I have (around 20)?So like in the link above, they have ""model"" with only the words from the sentence they wrote. And then they do ""X = model[model.wv.vocab]"", then ""pca.fit_transform(X)"". If I were to copy this code, I would do a PCA on the huge model, which I don't want to do. I just want to extract the embeddings of some words from that model and then compute PCA on those few words. Hopefully this makes sense, thanks in advance. Please let me know if I need to clarify anything."
227,Stanford-NLP Parser incorrectly splits my sentence,"['nlp', 'stanford-nlp']","I'm using the Stanford Parser to parse my corpus (for Machine Translation) to constituency trees. I am able to get the parser to work just fine, both through the GUI and the command line, but one problem I'm having is how it basically 'defines a line'.Usually, when working with a corpus, a sentence is an entire string of words until it reaches a new line. With the Stanford parser, it seems to take a sentence to be up until an 'end-of-sentence character' like a full stop or question mark. In some situations,this ends with incorrectly taking a portion of a sentence to be an entire sentence on its own, which inflates the number of sentences I have and causes misalignment with my target dataset.
Is there any way I can get the parser to take a sentence to be up until the \n newline, or is it just defined this way?"
228,Getting internal server error after deploying the project on Heroku,"['python', 'heroku', 'nlp']","I have deployed NLP project on Heroku server. If I am going to test the webapp by entering the fields, then I get INTERNAL SERVER ERROR:This app is successfully running on localhost. I don't know what happens after deployment.Can anyone help?Here is the screen shot of build
[Here is the code of app.py file
This is the code of spam_classifier.py
These are requirements which I put on requirements.txt
"
229,Adjusting the number of features for TF-IDF/logistic regression sentiment analysis,"['python', 'scikit-learn', 'nlp']","I'm doing a sentiment analysis project on a Twitter dataset. I used TF-IDF feature extraction and a logistic regression model for classification. So far I've trained the model with the following:This logistic regression model was trained on a dataset of about 1.5 million tweets. I have a set of about 1.7 million tweets I'm trying to use this sentiment analysis model on, df_april. On my first attempt, I extract the features as follows:My first thought was to just call predict on X_april but this gives me an error:This made sense to me: the shape of these feature vectors was different:So I know I need to somehow adjust the number of features to match between X and X_april to call predict on X_april. My attempt to do this was:I'm working in a Jupyter notebook, and this code results in a dead kernel every time I've tried it. How can I adjust the features so that I can call the logistic regression model?"
230,Building quick web app to deploy ML models,"['python', 'nlp']","I've almost completed my DL models for an NLP project. Now I want to make a web app.I created models in PyTorch to detect propaganda text fragments in news articles. The models have poor performance. But given a news article, they can return offsets & labels for each offset (in a text file), identifying the type of propaganda in each span of text.Now I want to build a prototype Web app to demo this project with following minimal requirements:"
231,Would Spacy take into account of custom attributes when training for a model?,"['nlp', 'spacy']","If I have custom attribute or custom property setup in Spacy, will these custom attribute be learned by the model when I train one?Specifically, I have a large set of Natural Language that I want to train, each line of data is associated with some non-natural language metadata. For 2 NL that has the same NL but different metadata, the resulting label maybe different.Thanks."
232,Pytextrank - avoid lowercasing tags into key phrases extraction,"['nlp', 'spacy', 'pytextrank']",I want to avoid lowercasing tags in pytextrank. Any suggestions on how that can be achieved?
233,how to extract text from PDF between some patterns,"['python', 'r', 'regex', 'nlp']","I have some thousands of documents (in German) in PDF format. I need to extract a portion of text from each one of them, which usually comes after date and ends before the Date, Location, Address info nearly at the page end. An example is attached. The text I need is highlighted.
What I tried so far is  qdapRegex::rm_between:which did not work. The output is the whole text content including Name, Vorame, etc. (I could live with the date 20.01.2019 at the beginning.) What I am doing wrong is not clear to me, as I am new to regex and nlp and cannot spot it reading the documentation of rm_between or qdapRegex.My first problem is getting this working.The further problem is, these documents are not standard, and some documents may have another type of info, e.g., Ref:1234 instead of date before the highlighted area. This piece may be found in any page number of the document, so going for exact page numbers is not an option.Is there any other solution, library, etc. which can be used to extract more or less this portion of text?"
234,nlp: What is exactly a grammar dependence tag 'attr'?,"['python-3.x', 'nlp', 'spacy', 'dependency-parsing']","I am exploring the spacy nlp python library. I have got this:Spacy explains that ""professor"" is an attribute ('attr') of ""is"".What is exactly an attribute?Where can I find this kind of information?Are there other examples of 'attr' in different grammatical contexts?"
235,Spacy : train POS tagger after tokenization,"['nlp', 'spacy', 'pos-tagger', 'custom-training']","I am trying to train spaCy's POS tagger after customizing the tokenizer.For example the tokenization of the text
""Il est culotté celui-là.""is now ['Il', 'est', 'culotté', 'celui-là', '.']rather than the original one :
['Il', 'est', 'culotté', 'celui', '-', 'là', '.']My problem is that nlp.update() doesn't seem to consider my customized tokenizer, since I can't annotate 'celui-là' as one token, but as 3 :But it should be :However we can see that in the output the customized tokenizer is applied, so my conclusion is that I am training the tagger before applying the custom tokenizer.Here are the code and output :
https://gist.github.com/mariastefan/57606e6f85e3dfbd779b16285ab21760Do you know how to first apply my modifications of the tokenizer before the training of the tagger so I can train it with the right tokens?Thank you."
236,How to replace the alphabet with a hat (or irregular format) in a word,"['nlp', 'text-mining', 'data-cleaning']","I am doing a NLP task on Dutch language.
I am facing a difficulty that is there are a lot of words that have the same meaning but are written in different formats.
For example:hyponatriemie
and
hyponatriëmie.now my algorithm thinks they are two different words. but they are the same with the only difference being the letter ë.
There are a lot of these kind of words. Is there an efficient way / package to replace all of the words with irregular letter to the normal words in the training corpus?By the way, in Dutch there are also only 26 alphabets (just like English). some of them are written in strange letter. it would be great to identify all the strange letter and replace them with normal English-looking-like letter. E.g.   ë ==> e=========UPDATE====================
I have found the perfect answer in Python: replace french letters with english"
237,How to parse a paragraph sentence by sentence,"['ruby', 'parsing', 'text', 'split', 'nlp']",I have a relatively complicated paragraph which I am trying to parse sentence by sentence using a specific rule set. I've tried using the Stanford Parser but I haven't really been able to figure it out. I have a basic level of coding skill and could implement anything if I was told how but I really have no idea how to approach this one my own.Any chance anyone might be able to help out?Thanks!
238,Is it possible to run a pre-trained BERT model on my CPU and fine tune it on my CPU?,"['python', 'machine-learning', 'nlp', 'word-embedding', 'bert-language-model']","I'm trying to run and fine-tune a BERT model using my CPU, then use the word embeddings for a task. Are there are resources on how to do this on my CPU specifically?"
239,Trying to increase the F1 score for an NLP data extraction problem,"['tensorflow', 'machine-learning', 'deep-learning', 'nlp', 'lstm']","We have a problem reaching a decent F1 score when tackling this NLP data extraction problem.When given a group (i.e. Female dogs, male dogs) we want to extract relevant numerical data from a paragraph:Female dogs often have 5 big spots, whereas male dogs have 2 small
spots. Both dogs experience discrimination due to their spot count.
However, while female dogs are discriminated against 60% of the time,
they often receive 30% less harsh treatment, than male dogs. Male dogs
are discriminated against only 20% of the time but will receive harsh
treatment 70% of the time.The female dogs grouping would extract:Male dogs:What we've tried:Our current architecture embeds and vectorizes the words using BERT, while concurrently attaches POS and dependency tags as additional features for each token. We also attach the group words as additional tokens in front and at the end of the paragraph. The sequence of paragraph tokens is then fed through a bidirectional LSTM network, each token has a score (1 if it is relevant to the group, 0 if it is not relevant to the group) and we train the network to predict the relevant words in the paragraph. The training set is broken into 5 subsets for ensemble learning.We have tried just the BERT embeddings without the POS and dependency tags and we lose ~2% in F1 score.We score correctly positive predictions at a 1:3 negative weight (our loss function increases the loss by three-fold on positive tokens to punish missing positive tokens, while this inflates false positives it greatly improved recall for positive tokens at the correct threshold, with an overall boost to F1) to best accommodate the imbalance of negative tokens and paragraphs with no positive tokens (data is imbalanced and sparse).We have approximately 2000 rows of data that we use (1000 rows of positive data and 1000 rows of negative data, at 95% negative tokens and 5% positive tokens), and 3000 rows of negative data we chose to exclude from the training We exclude a random 3000 rows each training iteration to swap the random samples to expose the models to as many negative cases as possible.Our current F1 score is 45% at the optimal threshold.Any suggestions would help!"
240,Loss function negative log likelihood giving loss despite perfect accuracy,"['nlp', 'pytorch', 'loss', 'log-likelihood', 'sequence-to-sequence']","I am debugging a sequence-to-sequence model and purposely tried to perfectly overfit a small dataset of ~200 samples (sentence pairs of length between 5-50). I am using negative log-likelihood loss in pytorch. I get low loss (~1e^-5), but the accuracy on the same dataset is only 33%.I trained the model on 3 samples as well and obtained 100% accuracy, yet during training I had loss. I was under the impression that negative log-likelihood only gives loss (loss is in the same region of ~1e^-5) if there is a mismatch between predicted and target label?Is a bug in my code likely?"
241,How to build pipelines around data preparation for the chatbot model,"['tensorflow', 'model', 'nlp', 'pipeline', 'chatbot']","Im a novice for building pipelines around models. The use case is that, I am trying to deploy my model into a kubernetes platform(openshift) to use in a webapp. Before that, I want to place my steps into a pipeline.
So for data prepping in each intent data, I am tokenizing the , adding the words to corpus, lemmatizing it, sorting it and creating bag of words for each sentence.
So i want to add this and model creation steps to sklearn pipeline.
Can someone help me which steps I should add to the pipeline and how to approach it.My code base: https://github.com/hanvitha/chatbot_notebooksData prep:01_chatbot_data_preparation.ipynbModel creation: 02_chatbot_model.ipynb"
242,How to Do Topic Modelling and Classification on Each Sentence Comment in a Data Frame in R?,"['r', 'nlp', 'text-mining', 'topic-modeling', 'topicmodels']",Is there a way to do topic modelling and classification on a data frame of comments in R?I have 10 columns of comments (where each comment is a open ended sentence of a topic related to a question) and I want to classify each of these comments by topic for each column of comments.I tried to use LDA (Latent Dirichlet Allocation) using the topicmodels package in R (and use DocumentTermMatrix and Corpus before I applied the LDA model). I tried to find the optimal number of topics using the lowest perplexity.The issue is that I don't know what topic each sentence of a comment is classified by. It does put words into a similar topic but not by sentence. So it's a little confusing.I don't know where to go from there and need advice on how to do this.I was able to apply Sentimental Analysis on the same data frame in R using the sentimentr package and it worked but I can't do the same for topic modelling and classification.How can I do this in R for each sentence of comment in a column (for a total of 10 columns)?
243,Loading json file using torchtext,"['json', 'nlp', 'pytorch', 'dataloader', 'torchtext']","I'm working on the dailydialog dataset, which I've converted into a
JSON file which looks something like this:[{""response"": ""You know that is tempting but is really not good for our fitness."", ""message"": ""Say, Jim, how about going for a few beers after dinner?""}, {""response"": ""Do you really think so? I don't. It will just make us fat and act silly. Remember last time?"", ""message"": ""What do you mean? It will help us to relax.""}, {""response"": ""I suggest a walk over to the gym where we can play singsong and meet some of our friends."", ""message"": ""I guess you are right. But what shall we do? I don't feel like sitting at home.""}, {""response"": ""Sounds great to me! If they are willing, we could ask them to go dancing with us.That is excellent exercise and fun, too."", ""message"": ""That's a good idea. I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them.""}, {""response"": ""All right."", ""message"": ""Please lie down over there.""}]So, each item has two keys - response and message.This is my first time using PyTorch, so I was following a few online available resources. These are the relevant snippets of my code:Although no errors are raised, despite having many items in my JSON file, the train, test and validation datasets strangely have only 1 example each, as seen in this image:
Image Showing the length of train_data, test_data and validation_dataI'd be really grateful if someone could point out the error to me.Edit: I found out that the whole file is being treated as a single text string due to lack of indents in the file. But if I indent the JSON file, the TabularDataset function throws a JSONDecodeError to me, suggesting it can no more decode the file. How can I get rid of this problem?"
244,BERT optimizer.py file throwing TypeError:,"['python', 'tensorflow', 'nlp', 'bert']","I am trying to train a BERT model by following this tutorial. When I get to the part in section 3 where I run code from the command line, I get the following error:File ""bert/run_classifier.py"", line 25, in 
import optimizationFile ""/Users/patriciadegner/Documents/BERT_master/bert/optimization.py"", line 87, in 
class AdamWeightDecayOptimizer(tf.train.Optimizer()):
TypeError: init() missing 2 required positional arguments: 'use_locking' and 'name'I have tried going into the optimization.py file and adding the 'use_locking' and 'name' arguments to the call, but that has not worked. I am currently using tensorflow 1.13.1. The BERT documentation says that the code will work with tensorflow 1.11.0, but I have not been able to download that version.I have tried using tensorflow-2.2.0, but then I get a different error:Traceback (most recent call last):
File ""bert/run_classifier.py"", line 25, in 
import optimizationFile ""/Users/patriciadegner/Documents/BERT_master/bert/optimization.py"", line 87, in 
class AdamWeightDecayOptimizer(tf.train.Optimizer()):
AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'This error led me here, which is what prompted me to try an earlier version of tensorflow in the first place.Can anyone help me make this model run?"
245,How to use GloVe word embedding for non-English text,"['csv', 'nlp', 'word2vec', 'word-embedding', 'glove']","I am trying to run a GloVe word embedding on a Bengali news dataset. Now the original GloVe source doesn't have any supported language other than English but I found this which has word vectors pretrained for 30 non-English languages.I am running this notebook on text classification  using GloVe embeddings. My question isCan I use the pre-trained Bengali word vectors with my custom Bengali dataset, and run on this model?this pretrained Bengali word vector is in tsv format. Using the following code I cannot seem to parse it into word-vector lists.and I get the error"
246,Tensorflow Version v.s. Hub Bert Model Accuracy,"['nlp', 'tensorflow2.0', 'kaggle', 'bert', 'tensorflow-hub']","Excuse me! I use the tensorflow-hub bert model to solve kaggle (Real or Not? NLP with Disaster Tweets, https://www.kaggle.com/c/nlp-getting-started) problem, I want to know the difference between mine and others. So, I run the kernel as following as steps:First, I fork this kernel (https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert) by Vitalii Mokin (thanks to Vitalii Mokin).Second, run this kernel and it runs well and obtain high accuracy (around 0.90) for part 10 (10. BERT using TFHub). I found that this kernel run on the version as following as code:Third, I upgrade the version of the tensorflow, keras and tf-hub to the new version, the results are different and lower (around 0.65) for part 10 (10. BERT using TFHub). I only add the code at the start of the kernel as following as description:Why does the accuracy of the new version lower than another version? Other configurations are the same! I didn't change other parts! Thank you!Note:Hub Bert Model Set for the Original VersionHub Bert Model Set for the New Version"
247,Stanford CoreNLP how to skip failure when using a filelist?,"['nlp', 'stanford-nlp']","I am using stanfordCoreNLP with filelist parameter. The program seems to fail at some files in the filelist.lst, and the cmd stopped and quit. How to skip the failure and continue to complete the analysis of the rest of the files?java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props Props.propertiesI have added continueOnAnnotateError Flag to the property file, but it still fails when hitting the bad files.Props.properties file look like this:"
248,"Is it possible to extract specific({Architect, Building}) information from unstructured text chunks using NLP?","['python', 'nlp', 'spacy']","I am working on a task to extract architects and their buildings from unstructured pieces of texts with varying sizes. I started trying a NLP tool called SpaCy but annotations they provide sometimes mixes up.FAC Buildings, airports, highways, bridges, etc.ORG Companies, agencies, institutions, etc.GPE Countries, cities, states.LOC Non-GPE locations, mountain ranges, bodies of water.Building names falls into those 4 annotations. My job would be so much easier if i could get only FAC for building names but it looks like it is not possible or i couldn't be able to make it work.The question is, is it even possible to use NLP tools to extract such information tuples(in my case {Architect, Building}) from a chunk of text?Edit: Some things i have doneFollowing bits are some examples of texts i am using at the momentHe renovated Fatih Mosque and built Laleli Mosque in the name of
Sultan Mustafa IIIMehmed Tahir Ağa built Hamidiyye Complex in Bahçekapı for Sultan
Abdülhamid I.I am giving those texts as data to the spaCy, code bit is here:Output is:Laleli Mosque ORGHamidiyye Complex ORGBahçekapı for Sultan Abdülhamid I. ORG"
249,Identify organization using Stanford NER in java,"['java', 'maven', 'nlp', 'stanford-nlp', 'data-extraction']","How can we check if a given string has a valid organization name using Stanford NER in JAVA.
I am new to this so please help me with a detailed code explanation."
250,What loss function is implemented in BERT pre-trained models? [closed],"['nlp', 'loss-function', 'huggingface-transformers', 'bert']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 8 days ago.Do BERT pre-trained models use categorical cross-entropy loss for text classification? If so, how to change it focal loss? I have imbalanced data."
251,Spares Ops error when build keras model in python,"['python', 'keras', 'neural-network', 'nlp']",I am doing my first Text multi-label Neural NetworkI have processed my text datasetdid TFIDF and everything.I split data into training and test data for features and resultsMy final dataframes areI built this NN modelWhen code reach to the last line history I get this errorMy main question is : why am i getting this error and how to fix it? I tried to change the input_dim and output put still getting same error.My other question : Do I have to one-hot-encode multi-label results to do NLP with Neural Network? will it work if kept it without one-hot-encoding. ( I will probably test that when I get the code working )
252,"How are the matrices Wq, Wk and Wv matrices calculated in the Transformer?","['machine-learning', 'deep-learning', 'nlp', 'transformer', 'attention-model']","I am currently reading the ""Attention is all you need"" paper by Vaswani et.al. ([https://arxiv.org/pdf/1706.03762.pdf][1])The paper says that for each word we generate a query vector, key vector and value vector by multiplying by their respective matrices Wk, Wk, and Wv. My question is where do these 3 matrices come from. Are these initialized randomly and then trained through some backpropagation process or are they precalculated separately using some other technique?"
253,Extract pair ( VERB-Noun) from list,"['python', 'nlp', 'spacy', 'pos']","I want to extract pair (verb-noun) from each row i want to add other column and put all pair there im using Eron dataset i worked the first part preprocessing (remove number, punctuation...) and know i want to detect (verb-noun) any help please"
254,How to draw the dependency tree from the CoreNLPClient parsing result?,"['nlp', 'stanford-nlp', 'stanza']","I wanted a dependency tree for the parsing result. I did the parsing using the code given on the github repo of stanford core nlpI got the result as follows.[jupyter notebook result screenshot][1]I have seen other answers mentioning graphviz and todoformat() but these methods require semanticgraph format input(todoformat does as far as i know). I have been able to convert the result of parsing to the following format but it is a list of strings.
[the new format of result][2]
As I saw other result formats like this. What can I do to get the dependency tree graph?
Is the result that I am getting be changed in a form that works for todoformat?
I am new to this. I would really appreciate your help.
[1]: https://i.stack.imgur.com/qma9n.png
[2]: https://i.stack.imgur.com/Xjhwh.pngCode:"
255,Pre trained Embeddings as an input for huggingface-transformers,"['nlp', 'pytorch', 'huggingface-transformers']",I have pre-trained embeddings that I would like as a input for Bert models.Is there builtin way with huggingface-transformers create a model that gets embeddings as an input? And will not try to do embeddings itself.Thanks.
256,Predicting next week incidents using text analysis on incident reports,"['machine-learning', 'nlp', 'data-science']",I want to do a project but I'm not sure if it's really viable neither which path I should take to try and solve this.I have a dataset with numerous incidents from different places and their risk classification. For example:My idea is to compile the incidents by warehouse by week and give a probability of incidents happening in each warehouse for every risk.But I'm not really sure how to get around this problem. It's not really text classification because I know the classification of every report (risk). Is there a way to use this dataset and get any prediction for the next week?
257,Correctly Writing Arabic and English Text to File Using PdfPlumber and Python's Write,"['python', 'text', 'nlp', 'arabic', 'write']","I am using this code:When I am printing text to the terminal I receive:خيرات ta'rīḳ dating (of a letter, etc.); ta'rīḳ  the ground, ground
(adj.); earthly;  pl. خيراوت tawārīḳ2 date; time; history;
underground, subterranean chronicle, annals│ ةايحلا خيرات t. al-ḥayāh
يكوش يضرا arḍī šaukī artichoke biography; curriculum vitae; ماع خيرات
(‘āmm) world history; خيراتلا ءاملع the  ضرا araḍ (coll.; n. un. ة)
termite;  historians woodworm يخيرات tārīḳī historic(al) ةيضرا arḍīya
pl. -āt floor; ground (also, e.g., of  a printed fabric, of a
painting); ground  خرؤم mu'arriḳ pl. -ūn historiographer,  floor,
first floor (tun.); storage, warehouse  historian, chronicler,
annalist; -- mu’arraḳ  charges dated  مورضرا arḍurum2 Erzurum (city in
NE Turkey) ليبخرا arḳabīl archipelago ةطرا (also ةطروا) urṯa pl. طرا
uraṯ (طروا)  نخرا (άρχων) pl. ةنخارا arāḳina archon, pl.  battalion
(formerly, Eg.; mil.) notables (Chr.-Copt.) ةقطرا arṯaqa pl. -āt
heresy (Chr.) بدرا irdabb (now usually pronounced  ardabb)pl. بدارا
arādib2 ardeb, a dry  عيرات see عير measure (Eg.; = 1981) نغرا urgun
pl. نغارا aragin2 organ (mus. instr.) ةبدرا irdabba cesspool لورا
urgūl, argūl a wind instrument (related to  ندرلاا al-urdunn Jordan
(river and country) the clarinet, consisting of two pipes of  unequal
length) يندرا urdunni Jordanian│ ةيندرلاا ةكلمملا  ةيمشاھلا al-mamlaka
al-u. al-hāšimīya the  قرا ariqa a to find no sleep II to make
Hashemite Kingdom of Jordan (official  sleepless (ه s.o.), prevent
s.o. (ه) from  designation)  sleeping زاودرا (Fr. ardoise) arduwāz
slate  قرا araq sleeplessness, insomnia 1 زرا arz (n. un. ة) cedar
ةكيرا arika pl. كئارا arā’ik2 couch, sofa; throne 2 زرا aruzz rice
ةليكرا argīla pl. ليكارا arāgīl2 (syr.) water pipe,  narghile سرا
arasa i (ars) to till the land  يدنلرا irlandī Irish سيرا irrīs and
arīs peasant, farmer 1 مرا arama i to bite  يطارقتسرا  aristuqrāṯī
aristocratic; ristocrat مرا urram molar teeth│ مرلاا قرح  ةيطارقتسرا
aristuqrāṯīya aristocracy (ḥarraqa) to gnash one's teeth (in anger)
وطسرا arisṯū Aristotle  ةمورا arūma, urūma root, origin; stump  شرا
arš indemnity, amercement, fine, penalty;  of a tree blood money (for
the shedding of blood;  مرئم mi'ram root (of a tooth) Isl. Law)  2
مارآārām (= مارا) pl. of مئر ri'm) white  سبوقسبا يشرا (Gr.
ὰρχιεπίσϰοπος) archbishop antelopes قوديشرا (Fr. archiduc) archduke,
ةقوديشرا  نمرلاا al-arman the Armenians archduchess  ينمرا armanī
Armenian (adj. and n.) ضرا arḍ f., pl. ضارا arāḍīn, نوضارا arāḍūn
earth; land, country, region, area; terrain,  اينيمرا armēniyā Armenia
ground, soil│ ىلفسلا ضرلاا (suflā) the  طوانرلاا al-arnāwuṯ the
Albanians nether world; ةسدقملا ضرلاا (muqad[enter image description
here][1]dasa)  يطوانرا arnāwuṯī Albanian the Holy Land, Palestine يضرا
arḍī terrestrial, of the earth; soil-,  بنرا arnab f., pl. بنارا
arānib2 hare; rabbit│ يدنھ بنرا (hindī) guinea pig  land- (in
compounds); situated on or nearThe problem is that the arabic text is supposed to be written right to left, but the letters here are written left to right.Is there a solution to being able to have the extraction and writing process write the arabic characters correctly? For example: The first line print in the output above is: خيرات . It should  be written from right to left and not left to right.In the image linked below I have shown that my terminal prints the arabic text as it should be print, right to left. However when writing it or copying and pasting it becomes left to right.imgur.com/mmSet.pngI see my options here to bea) Write code to find all arabic words and reverse them in the text file.b) Find another text extraction tool that is able to print/write the Arabic to file correctly.c) Solve PDFPlumber/Python's issue with writing like this by writing more code that would specify to write Arabic in right-to-left.Would anyone be willing to look at this and share what other possible solutions could be to this problem?"
258,How can I fix ValueError to execute spaCy neuralcoref sample code on Google Colaboratory?,"['python', 'python-3.x', 'nlp', 'google-colaboratory', 'spacy']",I would like to run the sample code for spaCy neuralcoref on Google Colaboratory.I'm trying to execute the code below which is same as the official example on spaCy page on Google Colaboratory.There are mainly two errors.One occurs in the command part to install neuralcoref and spacy.Error MessageSecound problem about ValueError is appered on the below part.How can I fix these errors and run the sample code?Error MessageGoogle ChromeMac OS Catalina version 10.15.5
259,Predicting missing letters in a sentence,['nlp'],"I have the sentence below :I want to predict the missing letters, using an NLP model. What NLP model shall I use? Can you give an example ? Thanks."
260,Another error to import spaCy neuralcoref module even following the sample code,"['python', 'python-3.x', 'nlp', 'jupyter-notebook', 'spacy']","I would like to run the sample code for spaCy neuralcoref on jupyter notebook.After I asked my former questeion, Error to import spaCy neuralcoref module even follwoing the sample code, I have tried to install libraries following another answer to this issue on stackoverflow.What should I do to run the sample code of spaCy neuralcoref?This part is executable, but notice and output are shown.noticeoutputWhen I execute the below part, the popup error message is shown.popup error message on jupyter notebookI have tried to install libraries following another answer to this issue on stackoverflow.Mac OS Catalina version 10.15.5Python 3.7.4spaCy 2.1.0neuralcoref 4.0"
261,Calculate the percentage of accuracy with which user made the assigned sound,"['machine-learning', 'web-applications', 'nlp', 'speech-recognition', 'speech']","I want to design a web-app for my cousin who is 2 years of age in which i have implemented a functionality in which when an image is clicked some sound gets played and the user has to make the same sound which gets recorded.
For eg-If i click on image of ""Apple"" the sound made is ""A for Apple"".Now the user has to say those words which get recorded.
Now I want to calculate the percentage of accuracy with which the user spoke.I want to know how can i know the accuracy percentage.I have not used machine learning or Natural Language Processing earlier so i want some guidance on what should i learn about or ways of implementing this functionality.I need some help on that.
Also use nodejs frameworks quite frequently so is there any module in nodejs with the help of which the above requirement can be fulfilled."
262,Error to import spaCy neuralcoref module even follwoing the sample code,"['python', 'python-3.x', 'nlp', 'jupyter-notebook', 'spacy']","I would like to run the sample code for spaCy neuralcoref.I executed the sample code on jupyter notebook, and the attribute error is shown when I execute the below part to add neural coref to SpaCy's pipe.How can I fix the error?Error Message<ipython-input-9-xxxxxxxxxxxx> part is a dummy.The sample code is from the official github page.You can also check it on the spaCy page.There was a similar question on stackoverflow,
Kernel Died when running Neuralcoref, and I tried the follwoing uninstall and install process.However I have been still stacked on the above error.Mac OS Catalina version 10.15.5Python 3.7.4spaCy 2.1.0neuralcoref 4.0"
263,label encoding of text data with k most frequent words,"['python', 'nlp']","A text datasets named 20 newsgroups( training data: 11300 doc) imported from sklearn datasets and was preprocessesmy me . Further my job is to be label encode the text data into TYPE:<class 'numpy.ndarray'> (list of list) as the output given below for each of the 11300 doc based on 2000 most frequent words  to be feed into neural network. I am a novice python text data analyst.
my output has to be as TYPE:<class 'numpy.ndarray'> (list of list) encoded (example of one doc)Vocab:{'afraid': 36, 'info': 846, 'short': 1628, 'story': 1720, 'newspaper': 1176, 'day': 434, 'ago': 40, 'sort': 1670, 'mention': 1065, 'japanese': 887, 'using': 1883, 'like': 970, 'land': 929, 'package': 1240, 'moon': 1118, 'article': 99, 'make': 1021, 'matter': 1043, 'worse': 1973, 'memory': 1063,...........}(2000 words)"
264,How to divide the training plan into training and testing? (Python),"['python', 'nlp', 'spacy', 'training-data', 'ner']","I am using a trained Spacy algorithm to identify the named entities in a text.
In python, I have a dataset like the example:The dataset continue...
After I'm running the training as follows:bold
italicI assume that training and testing are being performed within the same dataset. That would not be ideal, I would like to share the training and testing of my dataset. Or create a dataset just for testing, but how could I use the same algorithm previously trained?
How can I do one of two ways?"
265,Ranking how direct spaCy dependencies are on tree,"['python', 'graph', 'nlp', 'spacy']","I have a SpaCy dependency tree made by this code:That prints out this:SpaCy determines that this entire string is connected in a dependency tree. What I am trying to figure out is how to discern how direct or indirect the connection is between a word and the next word. For example, looking at the first 3 words:Essentially, I want to make a df that would look like this:I'm not sure how to achieve this. As SpaCy makes this graph, I'm sure there is some efficient way to calculate the number of vertices required to connect adjacent nodes, but all of SpaCy's tools I've found, such as:Include connection information, but not how direct this connection is. Any ideas how to get closer to this problem?"
266,A data set in French on home automation and smart homes,"['nlp', 'home-automation']",I looked for a dataset in French in the field of home automation and smart homes and I did not find in French.
267,Assign Topic from NNMF Topic Modelling,"['python', 'nlp', 'nmf']","I have a list of text comments that are fed into a non-negative matrix factorization topic modelling program.The example output may be something like:How can I assign a specific comment from the file a specific topic? e.g., the comment ""My computer has an issue of turning off intermittently"" would be mapped to Topic 1 ""problem"""
268,List all sentences containing specific word and similar words with word2vec,"['python', 'pandas', 'nlp', 'word2vec']","I have a table as following:I want to list all sentences that contain the word ""smell""The output that I get is:However, I want to list also sentences that contain a similar word to ""smell"" such as ""scent"" and I want to use the pre-trained word2vec of Google and set up a condition, if the similarity is above 0.5 to list the sentence as well. Therefore, the desired output is:How can I add word2vec to the above code so that it scans not only for ""smell"" but also all similar words?"
269,Facing issues while trying to deploy app using Flask and Gunicorn (Never did before),"['python', 'tensorflow', 'flask', 'nlp', 'gunicorn']","I am facing a problem here, want to deploy this Integrated_app.py to server. Please take a look at the code below, its only Flask code works fine, but when i did some changes for gunicorn to run, it is giving me errors.After running python Integrated_app.py,i get this error:After running export FLASK_APP=""Integrated_app:app"" and ```flask run``, i get this error:and by using gunicorn -w 2 'Integrated_app:app', i get this error:Please let me know my mistakes and how to run it using gunicorn.
Thank you."
270,how to reduce time complexity of clustering over doc2vec embedding?,"['python', 'nlp', 'hierarchical-clustering', 'word-embedding', 'doc2vec']","I have a bunch of vectors of 300 dimensions each, as part of doc2vec embedding. Each vector is a representation of an article. My goal is to discard the duplicate articles. I was thinking of running DBSCAN clustering over my dataset, and then for each cluster label, discard the duplicates. I chose DBSCAN because it's hierarchical, and it can take cosine distance as metric, which I think makes more sense than euclidean distance for document similarity detection.Issue is, if I choose cosine distance, then sklearn DBSCAN implementation doesn't allow ‘ball_tree’ and ‘kd_tree’ as the algorithm, hence I'm left with ‘brute’, which I'm guessing has O(n^2) complexity when run for all vectors.If I don't go the clustering route, then I see two options. Calculate similarity using the most_similar method for all vectors, either in vanilla doc2vec, or through annoy index. According to this doc vanilla doc2vec has linear complexity for similarity query, whereas annoy gives sub-linear time complexity. I'd like to know, which one would be the best choice given my use case? Is there a better approach I'm missing?"
271,How to generate a sentence from a scene graph,"['nlp', 'spacy', 'huggingface-transformers', 'nlg']",I have been thinking of making a meaningful sentence from a scene graph which has nodes as subjects and objects and the directed edges being predicates. How do I make a meaningful sentence from it and what are the methods. Can someone point me in the right direction?
272,Range of values for individual words which could be given in Vader while customizing,"['python', 'nlp', 'sentiment-analysis', 'vader']","What is the range of numbers which could be assigned to a word while customizing the Vader sentiment package. In customization, I wanted to add more words and assign them the sentiment score."
273,Masking only specific words with Huggingface,"['python', 'nlp', 'pytorch', 'huggingface-transformers', 'huggingface-tokenizers']","What would be the best strategy to mask only specific words during the LM training?My aim is to dynamically mask at batch-time only words of interest which I have previously collected in a list.I have already had a look at the mask_tokens() function into the DataCollatorForLanguageModeling class, which is the function actually masking the tokens during each batch, but I cannot find any efficient and smart way to mask only specific words and their corresponding IDs.I tried one naive approach consisting of matching all the IDs of each batch with a list of word's IDs to mask. However, a for-loop approach has a negative on the performance..Side issue about word prefixed space - Already fixedThanks to @amdex1 and @cronoik for helping with a side issue.
This problem arose since the tokenizer, not only splits a single word in multiple tokens, but it also adds special characters if the word does not occur at the begging of a sentence.
E.g.:The word ""Valkyria"":It is solved by setting add_prefix_space=True in the tokenizer."
274,Divide similar users into social groups [closed],"['python-3.x', 'machine-learning', 'nlp', 'cluster-analysis']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed 21 days ago.I have a dataset containing general consumer reviews of products purchased by users. The dataset also includes the item name, price, stars given by the consumer to the product. Please suggest me a way to approach this problem so as to make clusters of similar users using the given information. As of now I'm extracting keywords from the reviews column. I have shared the dataset preview."
275,Add background distribution to Guided LDA,"['python', 'nlp', 'lda', 'topic-modeling']","I am trying to add background distribution to the current python package GuidedLDA.
The idea of background distribution described in the paper.To incorporate the background distribution, I made changes on the current GuidedLDA codes as follows.
However, I could not run the model but no error messages were shown. The jupyter notebook kernel just stops. Running .py in the terminal just stopped after the first iteration. I suspect that the changes in the following codes may cause such problem but not sure which part. Any help would be appreciate!Original code:Revised code:The revised version gives the same results when I run the guided LDA. So I added background distribution on the revised code.Add background distribution p(w) and nw_[w] +=1 records # of times the word is assigned to background distribution :
(Other parts of code were updated with nw_)"
276,How to find most similar to an array in gensim,"['python-3.x', 'nlp', 'gensim']","I know the most_similar method works when entering a previously added string, but how do you reverse search a numpy array of some word?"
277,Optimal Embedding layer dimensions for Keras,"['python', 'python-3.x', 'numpy', 'keras', 'nlp']","I am training an LSTM model and feeding a custom BERT Word Embedding to the Keras Embedding layer. My embedding matrix which I have added in the embedding layer has the dimensions 30,576 x 24,576. But when I pass this matrix to the embedding layer, the google colab crashes as ram shoots up to the maximum limit.What I can do such that the embedding layer accepts this matrix?Here is my code"
278,What do negative vectors mean on word2vec?,"['nlp', 'word2vec', 'word-embedding']","I am doing a research on travel reviews and used word2vec to analyze the reviews. However, when I showed my output to my adviser, he said that I have a lot of words with negative vector values and that only words with positive values are considered logical.What could these negative values mean? Is there a way to ensure that all vector values I will get in my analysis would be positive?"
279,Error in the conversion of Bidirectional LSTM Text Classification Model to TFLite Model,"['keras', 'nlp', 'lstm', 'tensorflow2.0', 'tensorflow-lite']","My model is trained on the ""imdb reviews dataset"" and works fine when predicting the sentiment of movie reviews. However, when I convert my model for Tensorflow Lite, it outputs:
None is only supported in the 1st dimension. Tensor 'embedding 1 input' has invalid shape '[None, None]'.
When training my model, I did not specify a specific shape, therefore I am unsure of what shape to pass for my model to work with my android app. (As long as I convert the embedding_input shape to something else, the TFLite model will be created, but does not work with my android app)Code for model:"
280,How is a decision tree built during training on tokenized text in scikit learn?,"['python', 'scikit-learn', 'nlp', 'random-forest', 'decision-tree']","My goal is to understand the RandomForestClassifier used in scikit-learn. To understand it I need an answer to the question:How is a tree built in sklearn.tree.DecisionTreeClassifier when it is trained on tokenized texts?Ideally I would like to be able to draw the tree.plot_tree(clf) from the documentation from the output I get from the RandomForestClassifiers prediction or with other words: I want to understand how the trees are built without checking how its built as in this question.This is what my understanding is at this point in time:
Please correct my mistakes/misunderstandings and tell me the correct terms and definitions if I use wrong terms.The RandomForestClassifier votes from a n_estimators number of DecisionTreeClassifiers. When trained on texts, for example kaggle's sms spam collection dataset. I would think, that from the number of features generated each tree only gets a random subset of features.Then these features inside the subset are ordered by gini coefficient and the highest gini coefficient is at the top of the tree in the first node and then whether it is inside the text or not it will continue into the corresponding branch(child) into the next node with the token that has the second highest gini coefficient, and so on until the last branch(child) of the tree called the leaf is reached.And in the end given the top_features of RandomForestClassifier fromI should be able to know which words are usually at the top of these trees and then I believe the structure would be like this:"
281,Cosine similarity with synonyms,"['python', 'machine-learning', 'nlp', 'cosine-similarity']","I have some words which are synonyms that I would like to consider similar to the original word. For instance, word restaurant and bar are considered synonyms in this example.To apply cosine similarity under this scenario, I decided to keep the same word in both vectors, but if one word is considered synonym then I subtract a ""penalty"" to the counter. In this scenario, I have to compare the original v1=['cafe'] against v2=['restaurant']. Then, I have the following:However, if I apply this strategy, I ended up with similarity 1.0 (0.65/0.65). I need to get a similarity below 1.0 because restaurant is not considered the same word, but is synonym.I implemented cosine similarity in the following way:How can I get similarity in synonyms? while keeping the control on which words are considered synonyms. Currently, I am getting those synonyms from database."
282,Expected input batch_size (32) to match target batch_size (19840) BERT Classifier,"['python', 'deep-learning', 'nlp', 'pytorch']","I ran into this error with code:Shapes seem fine, but I am getting the error Expected input batch_size (32) to match target batch_size (19840)"
283,Autosize jupyter notebook,"['python', 'python-3.x', 'nlp', 'jupyter-notebook', 'pyldavis']","When using the pyLDAvis package as follows, inside my jupyter notebook,The resulting plot autosizes the width of my jupyter notebook, making all of the other cells overlap with the boarder - I have tried:with no luck, as well asspending a while searching the docs.... There dosen't seem to be anything about this is the documentation, has anyone dug into the code before and can point me in the right direction?An image of the overlap is shown below:"
284,Effect of number of entity types in named entity recognition & more NER related questions,"['nlp', 'ner']",I'm currently doing a project using named entity recognition and I had a couple of conceptual questions that I had trouble googling.Thanks!
285,Event Detection,"['events', 'nlp', 'information-retrieval']","i need help for a university project.
In particular i have to develop a system for event detection at sentence level. I read different papers but i found them a little too abstract (probably for my inexperience).
For this project i have a dataset which contains texts divided in tokens that are labeled with a type of event.For example: ""spoke"" -> Communication Event, ""war"" -> Hostility Event, etc.The use of this dataset it's not mandatory but maybe it could be helpfull for using the tokens annotated with event like event-trigger.The question is: how should i do to understand if a sentence is considerable as an event, and then eventually extract other information from these sentences, with statistical/ probabilistic or even machine learning approach?Thanks in advance"
286,How to perform K- means clustering over text in python?,"['python', 'scikit-learn', 'nlp', 'k-means']","I have a thousand of thousands of elements like these:I want to cluster the business_id using k-means grouping theme by category.Maybe it not the best option. My idea is to create a kind of Dictionary of Categories, and do it by grouping first all possible categories in any way and then, using the model, grouping the samples as group of business_id by cluster of categories.Can this work? Which is the best way to do that in Python?"
287,How to use masking with Convolution1D layer in keras?,"['keras', 'deep-learning', 'nlp', 'convolution', 'word-embedding']","I am trying to perform a sentiment classification task for which I am using Attention based architecture which has both Convolution layer and BiLSTM layers. The first layer of my model is a Embedding layer followed by a Convolution1D layer. I have used mask_zero=True for the Embedding layer since I have padded the sequence with zeros. This however creates an error for the Convolution1D layer since this layer does not support masking. However, I do need to mask the zero inputs since I have LSTM layers after the convolutional layers. Does anyone have any solution for this. I have attached a sample code of my model till the Convolution1D layer for reference."
288,Aspect based sentiment analysis,"['python', 'nlp', 'pytorch', 'sentiment-analysis', 'bert']","I've read a paper called ""A multi-task learning model for chinese-oriented aspect polarity classification and aspect term extraction"" proposed by Yang et. al (2019).
The code is available here:
https://github.com/yangheng95/LCF-ATEPC
The code works with the SemEval 2014 Dataset but when I try to use my dataset I have a problem to convert it to the structure, which works with code.this my dataset (json file) with the following structure https://github.com/afi1289/LCF-ATEPC/blob/master/atepc_datasets/restaurant/restaurant_sentences_for_ABSA.json:""98497_5"": {""true token"": [""Mussels"", ""-"", ""So"", ""much"", ""flavor"", ""!""], ""label"": [""BA"", ""O"", ""BS"", ""IS"", ""IS"", ""O""], ""reference"": ["""", """", ""1"", """", """", """"], ""polarity"": ["""", """", ""2"", """", """", """"], ""aspect category"": [""f"", """", """", """", """", """"], ""modifier"": [[], [], [""STR""], [], [], []], ""sentence"": ""Mussels - So much flavor!""}Only the following structure works with LCF-ATEPC Model:
But O -1
the O -1
staff B-ASP 0
was O -1
so O -1
horrible O -1
to O -1
us O -1
. O -1we've tried to convert it, then we get:Mussels B-ASP 2Nevertheless, the LCF-ATEPC model could not starts to train?
How I could convert my json Dataset to be able to work with LCF-ATEPC model?Thank you in advance!"
289,Create a count matrix of actor names from movies,"['python', 'matrix', 'count', 'nlp', 'countvectorizer']","I have a dataframe with 2 columns i.e. UserId in integer format and Actors in string format as shown below:It represents actors from all movies watched by a particular user of the platformI want an output where we have the count of each actor for each user as shown below:It is something similar to countvectoriser I reckon, but i just have nouns here.Kindly help"
290,what are the major steps for using a pre-train embedding with tensorflow,"['python', 'tensorflow', 'nlp', 'chatbot']","i am trying to use a pre-trained embedding into my seq-2-seq neural using Tensorflow.
what are the major steps to use the pre-train embedding then integrate into the model."
291,Splitting Text information in a dataframe into single words and detect if they are part of a dictionary R,"['r', 'dataframe', 'nlp']","I am trying write a script to detect if one word of an undefined amount of words is part of a dictionary.To make this problem a bit more understandable, I have the following data:So what I want to do know, is to check the column descriptions if any of those single words is part of a dictionary or self created vector of strings.
So the result should look something like:For this example I just assume a english dictionary. In this specific case its sufficient if only one word is part of a dictionary.I already tried this with the qdapDictionaries library and tokenizers to tokenize the contents of the dataframe cells but I fail to get the check right for cells where I have more than one word.Help is much appreciated,Thank you!"
292,'list' object has no attribute 'shape,"['python-3.x', 'keras', 'nlp', 'word-embedding', 'bert']","I am passing an embedding matrix to the embedding layer in KerasHere all in the embedding layer is my embedding matrix. When I pass this, it gives the following error"
293,Gensim LDA Model breaks,"['nlp', 'gensim', 'lda', 'topic-modeling']","I use Gensim to work with LDA models. My model works if I set the parameter num_topics to a small value (under 100 topics). If I increase the value to a larger (like 200 topics) number the model ""breaks"", it is not returning proper results, only empty topics with all the same most important words and a 0.000 importance value.Is someone having similar problems? What is happening?For information: My dataset has around 7000 small text documents with a minimum length of 5 words and up to 500 words. I know that there are other options I should consider beside LDA, but I want to understand why the model isn´t even delivering any results"
294,Data extracting from Instagram,"['python', 'nlp', 'instagram', 'data-mining', 'network-analysis']",I am going to extract posts that included the specific hashtag to do network analysis. I extract the number of like/view and the other hashtags come with the determined hashtag.I am wondering to know whether I can extract the username of those who post?
295,"Kernel keeps dying Jupyter, Anaconda. Trying to implement Coreference Resolution using neuralcoref","['python', 'nlp', 'jupyter-notebook', 'spacy']","I need to implement a solution which can recognize pronouns associated with the noun in a sentence. Say I have an paragraph about a person, I wanna count how many times the person has been referenced (name or any other pronoun). I want to implement this is Python.After some research I came across neuralcoref and though it could be useful. After several attempts I'm still getting stuck because the kernel keeps dying.It would be great if someone can help with this problem. I am also open to suggestions about other libraries/resources I could use to implement this.Thanks!This is the code I used:"
296,Setting gensim wordVectors init_sim property as True error - ValueError: output array is read-only,"['nlp', 'gensim', 'word-embedding']","Below is the code, I am executing. The error occurs on the 3rd line (vectors.init_sims(True))This is the error stack-Is this a known issue? Can someone provide a solution or workaround to this?"
297,pytorch KLDivLoss loss is negative,"['python', 'machine-learning', 'nlp', 'pytorch', 'loss-function']","my target is training a span prediction modelwhich can predict the position in the BERT output sequencemy input's shape is (batch_size, max_sequence_len(512),embedding_size(768))output's shape will be (batch_size , max_sequence_len , 1) and the third dim is stand for kind a probability , then I will reshape output to (batch_size,max_sequence_len)my label's shape is (batch_size , max_sequence_len), and in the max_sequence_len(512), only one position will be 1 and the others will be zeroand I've already check thisbut when I use nn.KLDivLoss() , output still be negative, I really dont know whycan somebody help me? thanks!Here is my code
Model Codetraining code"
298,"Hands-On Machine Learning Code not working in Jupyter, but works on Google. Why?","['python', 'nlp', 'jupyter-notebook', 'google-colaboratory']","I'm working on interpreting code in the book Hands-On Machine Learning with Sci-Kit Learn, Keras & Tensorflow, but came across an issue with the code in the NLP section. The author changed some of the code in a github repository for the book:https://colab.research.google.com/github/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb#scrollTo=kYZbDNAiilzL. When opening the code in Google Collab, it works just fine, but when I try to use the same code in Jupyter notebooks, I get the following errors:andI'm not sure I understand the error and why it would work in Google Collab and not Jupyter. Any ideas?"
299,Remove custom stopwords,"['python', 'nlp', 'gensim']",I am trying to remove stopwords during an NLP pre-processing step. I use the remove_stopwords() function from gensim but would also like to add my own stopwords
300,How to edit 2 lists so that they match in python,"['python', 'list', 'nlp']","I have two lists, a and b. They look like this:I want to ensure that they can zip together and match. However, they do not as can be seen here:It can be seen that a contains 'So' and b does not. To fix this, I want to drop 'So' from a, which would result in this:Essentially, I a word exists in one list but not the other list within the general index area, I want to remove it, regardless if it is in a or b. I have used the fuzzywuzzy library for fuzzy matching, which does decently well, but it is very slow. Are there more efficient ways to do this?"
301,How to match features in new records for NLP BOW,"['machine-learning', 'nlp', 'feature-extraction']","I have a dataset that has 100,000 recordsdata in this dataset are 2 columns
1- Text
2- ClassWhen I apply BOW of my model I get big list of featuresThat is fine, I managed to work with themmy problem is after building the model and deploying.now if a new text came with new words then the model wont work as it wokds in same feature structureExample
""This is a test, test is important"" , Red
""Adam pass a test"", Greenso my final dataset isonce model created and got this text""test and exam are similar"", Yellowin this case the set of features has new features which areand exam are similarthe model will break coz these features never included in the training modelI wonder how to resolve this issue?"
302,Allennlp ConfigurationError: key “matrix_attention” is required at location “model.”,"['nlp', 'allennlp']","I'm new to Allennlp, and this is my first time trying it out. I already installed all the required libraries !pip install allennlp !pip install --pre allennlp-models and my code should be fine too, but I still get this error message saying: ConfigurationError: key ""matrix_attention"" is required at location ""model.""
Here's my code:Do you have any idea how to fix this error? I'm using macOS Catalina and Python 3.6. I really don't know what to do now, so I really need your help. Thanks in advance!"
303,similarity score is way off using doc2vec embedding,"['python', 'nlp', 'word-embedding', 'doc2vec']","I'm trying out document de-duplication on an NY-Times corpus that I've prepared very recently. It contains data related to financial fraud.First, I convert the article snippets to a list of TaggedDocument objects.A sample TaggedDocument looks as follows:Now I compile and train the Doc2Vec model.Let's define the cosine similarity:Now, the trouble is, if I define two sentences which are almost similar and take their cosine similarity score, it's coming very low. E.g.Just to make sure, I checked with exact same vector repeated, and it's proper.What's going wrong in here?"
304,how to run fastai-nlp function in a terminal,"['nlp', 'fast-ai']","I was trying to run src = Seq2SeqTextList.from_df(df, cols='fr').split_by_rand_pct(0.2).label_from_df(cols='en', label_cls=TextList) from course 7 (fasiai-nlp) in a terminal instead of a Jupyter notebook . But I keep getting <IPython.core.display.HTML object> in the console that stops the process from continuing running. I really appreciate it if somebody could help me with that~"
305,Why doc2vec is giving different and un-reliable results?,"['machine-learning', 'nlp', 'gensim', 'similarity', 'doc2vec']","I have a set of 20 small document which talks about a particular kind of issue (training data). Now i want to identify those docs out of 10K documents, which are talking about the same issue.For the purpose i am using the doc2vec implementation:But, I am facing the strange issue, the results are highly un-reliable (Score is 0.9 even if there is not a slightest match) and score is changing with great margin every time. I am running the doc2vec_score function. Can someone please help me what is wrong here ?"
306,What to do when overfitting does not improve using regularization and droppout? [closed],"['python', 'machine-learning', 'deep-learning', 'nlp', 'lstm']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 21 days ago.I'm testing Conv, GRU, LSTM and simple Dense and I don't get 70 to 80%
My network converges very fast and overfits in the first seasons, could it be the data?lstm_46 (LSTM)               (None, 65, 32)            6272dropout_67 (Dropout)         (None, 65, 32)            0bidirectional_20 (Bidirectio (None, 65, 64)            16640dropout_68 (Dropout)         (None, 65, 64)            0lstm_48 (LSTM)               (None, 32)                12416dropout_69 (Dropout)         (None, 32)                0flatten_28 (Flatten)         (None, 32)                0dense_73 (Dense)             (None, 10)                330activation_72 (Activation)   (None, 10)                0dense_74 (Dense)             (None, 1)                 11Total params: 35,669
Trainable params: 35,669
Non-trainable params: 0"
307,How do I add a CNN layer before Bi-LSTM layer,"['python-3.x', 'keras', 'nlp', 'lstm', 'faster-rcnn']",I want to add a CNN layer with max-pooling before a Bi-LSTM layer for a sentiment classification task but I am getting an error.Here is the code I am using.This is the error I am getting
308,AttributeError while loading textHero library,"['python', 'text', 'nlp']",I am getting an error when importing textHeroError : AttributeError: module 'nltk' has no attribute 'data'I have installed textHero again but error did not get resolved.
309,Keras-bert: the list of Numpy arrays that you are passing to your model is not the size the model,"['machine-learning', 'keras', 'nlp', 'python-3.5']","I am getting the following error when I run a keras-bert model.
'''
ValueError: Error when checking model input:the list of Numpy arrays
that you are passing to your model is not the size the model expected.
Expected to see 4 array(s), but instead got the following list of 3
arrays: [array([['0.2', '[0.00057002]', '[-0.10780919]', ..., '0.4',
'[-0.11742077]', '[0.02901019]'],
['0.2', '0.4', '[0.17480111]', ..., '[0.06685996]',
'[-0.07216114]', '[-0.00741577...I am using a np array, except for the dictionary.  I suppose I could make the dictionary a numpy type?In the code below, I am not sure where it is expecting a list of 3 arrays???
'''"
310,NER (Named entity extraction) with no labelled data,"['machine-learning', 'nlp', 'computer-vision', 'relationship', 'ner']","I want to do NER (Named entity extraction) from a document. The document has actually transaction instructions and the entities I want to extract are either common ones (like BIC code or bank name) but also custom entities like internal reference no etc.  The company does not have labelled data.
To read the document I use tesseract (OCR) and parse it line by line. So the problem is to extract the entities from each line.
Is ruled based approach feasible? Is there any way I can use the true data even though they are unlabelled?Thank you for your feedback."
311,Lucene: How to add a new term to query using a Filter?,"['java', 'search', 'nlp', 'lucene']","I'm trying to write a Lucene filter which replaces terms like 'what's' with 'what is', 'can't' with 'cannot', etc.In incrementToken() if the term is one of the strings I'm replacing, I calculate a replacement string (e.g what's -> what is) and push it onto the CharTermAttribute:termAttr.copyBuffer(replacement.toCharArray, 0, replacement.length)But this doesn't seem to be working, when I search for 'what's', i still get results containing 'what's' rather than the string being treated as though it was 'what is'.What's the right way to accomplish this? Do I need to create a tokenizer? (Ideally I want to keep the StandardTokenizer and add to it rather than replace it)"
312,How can I download a stanza's model manually?,"['python', 'nlp', 'stanford-stanza']","One can theoretically download a stanza's model via Python as follows (mirror):However, the Stanford server is inaccessible from my computer:How can I download a stanza's model manually? I'm hoping the stanza's models are stored in more than one servers.I use stanza==1.0.1."
313,How can I download a stanza's model via command line?,"['python', 'nlp', 'stanford-stanza']",One can download a stanza's model via Python as follows (mirror):How can I download a stanza's model via command line?E.g. with spaCy one can use:I unsuccessfully tried:I use stanza==1.0.1.
314,Vectorization for non-word text data,"['text', 'nlp', 'vectorization', 'tfidfvectorizer']","What is the best approach, if there is one, for vectorizing non-word text data? For example, word text data might be the following sentence:the cat walked over to the doorThis sentence can be easily vectorized, with TF-IDF, Hash Vectorization, Count Vectorization, etc., and used for model training. However, would the same vectorization methods be applicable for non-word text data such as the following atom sequence from a biological database:N H CA C O CB CG CD1 CD2 CE1 CE2 CZFor example, would using TF-IDF Vectorization for the above sequence be representative, or is the another approach for encoding non-word text data in to vectors?"
315,Passing sparse document matrix to keras model using feature_column API tensorflow 2,"['python', 'nlp', 'tensorflow2.0']","I'm trying to translate input sequence approach into batch of documents using the feature_column API of Tensorflow 2. I took an code snippet from here, because I found intuitive the tf.feature_column.input_layer object. Nonetheless, it gives me the following error (and I cannot find the surrogate for TF2):Next you can see the code snippet. Further, is this the correct approach to pass the sparse matrices to the functional API? (without using the Estimator API):Thank you all in advance."
316,How to extract cross referencing in matlab?,"['matlab', 'nlp']","I need help. I have a document 'X'with the titles from 500 papers. I have a folder 'Y'where the references of these papers are saved as 500 unique papers and the papers are converted to word.
how can i use the following code to find my titles in 'X' in each of the papers in folder 'Y'.
Where the inputs go?"
317,Fine tune doc2vec on gensim,"['nlp', 'gensim', 'nlu']",I am new in NLU and I am doing a project on document embedding. I want to fine-tune the doc2vec model in gensim on my small dataset to see if it can help for document clustering. I read the tutorial on the website but they did not mention anything about fine-tuning. Where I can find doc2vec pertained on wikipedia or twitter on any big dataset.
318,Meaning of text annotation in NLP context,"['nlp', 'text-processing']","In Natural Language Processing, what does it mean to annotate a corpus?
Does it simply mean to add labels to text (i.e. ""positive, negative & neutral"" classes in a sentiment analysis task)? Or there is more to its meaning/definition?"
319,How to split SpaCy dependency tree into subclauses?,"['python', 'graph', 'tree', 'nlp', 'spacy']","I am trying to split units of text by their dependency trees (according to SpaCy). I have experimented with much of the docs provided by spacy, but I cannot figure out how to accomplish this task. To visualize, see below:The code above results in this dependency tree graph (which is split into 2 screenshots due to size):Intuitively, this indicates that there are 2 independent clauses in the original sentence. The original sentence was 'I was, I dont remember. Do you want to go home?', and it is effectively split into two clauses, 'I was, I dont remember.', and 'Do you want to go home?'.How, using SpaCy or any other tool, can I split the original utterance into those two clauses, so that the output is:['I was, I dont remember.', 'Do you want to go home?']?My current approach is rather lengthy and expensive. It involves finding the two biggest subtrees in the original text whose relative indices span the range of the original text indices, but I'm sure there is another, better way."
320,Which NLP model could I use for this problem statement?,"['machine-learning', 'nlp']","I am relatively new to ML. The following is my problem statement:I have data on company employees. Data is made up of skills the employees possess.
For example, a description could look like this:I have a Masters degree. My talent is I am flexible and have good problem-solving skills.
I have won a hackathon. My projects are based on SQL, Javascript, Python. I have good analytical
skills and am an independent thinker.I need to perform text analysis on this data and come up with summarization which would represent
company's talent pool, e.g.:I was wondering what model of NLP I could use to solve this problem.
I am thinking of using Automated Keyword Extraction
and try using LDA for topic modeling.
This seems to be an unsupervised NLP problem.
Which model could be used?Also, if there are good resources for the same?"
321,error message on running model of speech recognition,['nlp'],I got error message when running the following command:Can't really figure out why am I getting this error
322,Using Hugging Face Transformers library how can you POS_TAG French text,"['python', 'nlp', 'huggingface-transformers', 'bert']",I am trying to POS_TAG French using the Hugging Face Transformers library. In English I was able to do so given a sentence like e.g:The weather is really great. So let us go for a walk.the result is:Does anyone have an idea how a similar thing could be achieved for French?This is the code I used for the English version in a Jupyter notebook:The output of this notebook is above.
323,How much of an impact does fine-tuning by Masked Language Modeling have when we implement Question-Answering?,"['nlp', 'question-answering', 'finetunning', 'squad', 'simpletransformers']","I'm trying to fine-tune the BERT model using just my text corpus and for this, I would use Masked Language Modeling, using simpletransformers library, using the link here. Now the use-case or downstream task for me would be Question-Answering, for which I'm using the Haystack library.How much of an impact would this have on the performance of this QA model? According to one of the developers of the Haystack library, this would have a negligible impact, as they described here.However, if the model understands the semantics of my domain better, should we expect it to perform better even though after this step it has been fine-tuned for QA on the large SQuAD dataset.Also, how exactly do we use a model fine-tuned using for question-answering using Haystack?You can find more insights into the problem, in my other question."
324,"How do I loop through a column containing geo-coordinate text and extract latitude and longitude, and append as separate columns?","['python', 'string', 'split', 'nlp', 'geolocation']","I have some data from CDC in csv format which includes geo-coordinates in a single text column as follows.  The name of the (Pandas) DataFrame is 'geotest':What I would like to do is extract the latitude and longitudinal coordinates and append them to separate columns to my original dataset as follows:Here is the Python code I have tried but which fails to accomplish what I would like to do. My guess is that the ""("" and "","" characters are key regex items to do this. The code inside the loop works on individual string examples but, like most tutorials on the web, they seldom discuss how to loop through multiple rows of data and are therefore unhelpful at scaling up to real-world applications:Thank you all for any guidance!"
325,How to extract sentences with key phrases in spaCy,"['python', 'nlp', 'spacy']","I have worked with Spacy and so far, found very intuitative and robust in NLP.
I am trying to make out of text sentences search which is both ways word base as well as content type base search but so far, I would not find any solution with spacy.I have the text like:In computer science, artificial intelligence (AI), sometimes called
machine intelligence, is intelligence demonstrated by machines, unlike
the natural intelligence displayed by humans and animals. Leading AI
textbooks define the field as the study of ""intelligent agents"": any
device that perceives its environment and takes actions that maximize
its chance of successfully achieving its goals.[1] Colloquially, the
term ""artificial intelligence"" is often used to describe machines (or
computers) that mimic ""cognitive"" functions that humans associate with
the human mind, such as ""learning"" and ""problem solving"".[2]As machines become increasingly capable, tasks considered to require
""intelligence"" are often removed from the definition of AI, a
phenomenon known as the AI effect.[3] A quip in Tesler's Theorem says
""AI is whatever hasn't been done yet.""[4] For instance, optical
character recognition is frequently excluded from things considered to
be AI,[5] having become a routine technology.[6] Modern machine
capabilities generally classified as AI include successfully
understanding human speech,[7] competing at the highest level in
strategic game systems (such as chess and Go),[8] autonomously
operating cars, intelligent routing in content delivery networks, and
military simulations[9].Artificial intelligence was founded as an academic discipline in 1955,
and in the years since has experienced several waves of
optimism,[10][11] followed by disappointment and the loss of funding
(known as an ""AI winter""),[12][13] followed by new approaches, success
and renewed funding.[11][14] For most of its history, AI research has
been divided into sub-fields that often fail to communicate with each
other.[15] These sub-fields are based on technical considerations,
such as particular goals (e.g. ""robotics"" or ""machine learning""),[16]
the use of particular tools (""logic"" or artificial neural networks),
or deep philosophical differences.[17][18][19] Sub-fields have also
been based on social factors (particular institutions or the work of
particular researchers).[15]Now, I want to extract the sentences complete in multiple with multiple words or string matching. E.g., i want to search intelligent and machine learning. and it prints all complete sentences which contain this single or both given strings.Is there any way that importing model of spacy with spacy can sense the phrase match ..  like it finds all the intelligent and machine learning containing words and print that ? and also with other option, can it also finds as with search machine learning, also suggests deep learning, artificial intelligence, pattern recognition etc?I tried this which is not providing the complete sentence but only the word with matching ID number.in short,"
326,Is there a way to use natural language processing to determine whether the phrase shows agreement or not?,"['python', 'twitter', 'nlp']","I am doing some research related to fake news on twitter. I have isolated timelines of specific fake news stories in python, and I wanted to find out if there was a way to determine if each the text in each tweet was agreeing with the story or refuting it, thus giving me two seperate categories that I could compare over time. I know I could look for key words like 'fake' or 'false' to help classify, however I was hoping there was a more thorough way of doing it.Thanks."
327,How to proceed after annotating text data for ML?,"['python', 'machine-learning', 'nlp', 'text-mining', 'data-handling']","I am currently working on a project where I want to classify some text. For that, I first had to annotate text data. I did it using a web tool and have now the corresponding json file (containing the annotations) and the plain txt files (containing the raw text).
I now want to use different classifiers to train the data and eventually predict the desired outcome.However, I am struggling with where to start. I haven't really found what I've been looking for in the internet so that's why I try it here.How would I proceed with the json and txt. files? As far as I understood I'd have to somehow convert these info to a .csv where I have information about the labels, the text but also ""none"" for thext that has not been annotated. So I guess that's why I use the .txt files to somehow merge them with the annotations files and being able to detect if a text sentence (or word) has a label or not. And then I could use the .csv data to load it into the model.Could someone give me a hint on where to start or how I should proceed now?
Everything I've found so far is covering the case that data is already converted and ready to preprocess but I am struggling with what to do with the results from the annotation process.My JSON looks something like that:Each text is given a classId (e_1 in this case) and a field_value (f_4 given the value 3 in this case). I'd need to extract it step by step. First extracting the entity with the corresponding text (and adding ""none"" to where no annotation has been annotated) and in a second step retrieving the field information with the corresponding text.
The corresponding .txt file is just simply like that:
This is the textI have all .json files in one folder and all .txt in another."
328,Using BERT Embeddings in Keras Embedding layer,"['python-3.x', 'keras', 'nlp', 'embedding', 'bert']",I want to use the BERT Word Vector Embeddings in the Embeddings layer of LSTM instead of the usual default embedding layer. Is there any way I can do it?
329,Seperating premise and hypothesis in MNLI,"['python', 'nlp', 'pytorch', 'huggingface-transformers']","I'm trying to implement Siamese like transformer architecture. Similar work has been done in SentenceBERT paper. I'm facing an issue. To seperate hypothesis and premise, I modify this line from _glue_convert_examples_to_features. Instead I doDo the same thing for examples.text_b. Then I modify the GlueDataset by modifying this line mainly, since this will return two items now (segregated ""hypothesis"" and premise"").
Then __getitem__ is modified accordingly to return self.features_a[i], self.features_b[i].That's the gist of how I'm obtaining segregated ""hypothesis"" and ""premise"". These are then passed to two BERTs (or one BERT if its weights are kept frozen).This is how I've defined the collate_fnThen the dataloader is created in the usual way. So when we iterate like this:I had to modify _training_step and evaluate accordingly in the Trainer class.Now the problem is, the model doesn't learn at all. I tried with using my model and modified Trainer with standard GlueDataset, and it works. This leads to the conclusion that something is off with the data. The model should learn something (even if is not being fed concatenated ""hypothesis"" and ""premise"".The model basically has one BERT and one linear layer. The logits come from linear layer which is then used to compute loss function (typical siamese like architecture).Can you suggest if there's an issue in how the Dataset is being created here, or propose something of your own to segregate ""hypothesis"" and ""premise"" so that they can be fed separately to BERT."
330,"Neuralcoref, wrong pronoun replacement","['nlp', 'coreference-resolution']","I am implementing Neuralcoref by Huggingface for my specific problem. And instead of only replacing pronouns it also replaces nouns?I have this text:CanSat shall fit in a cylindrical envelope of 125 mm diameter x 310 mm length. Tolerances are to be included to facilitate container deployment from the rocket fairing.
The container shall not have any sharp edges to cause it to get stuck in the rocket payload section which is made of cardboard.BlockquoteI want it to transform in the follwoingCanSat shall fit in a cylindrical envelope of 125 mm diameter x 310 mm length. Tolerances are to be included to facilitate container deployment from the rocket fairing.
container shall not have any sharp edges to cause container to get stuck in the rocket payload section which is made of cardboard.What are my results till now:[CanSat: [CanSat, the CanSat],
container deployment: [container deployment, The container, it, The container, The container, the container, it, The container, the container],CanSat shall fit in a cylindrical envelope of 125 mm diameter x 310 mm length. Tolerances are to be included to facilitate container deployment from the rocket fairing.
container deployment shall not have any sharp edges to cause container deployment to get stuck in the rocket payload section which is made of cardboard.Blockquotehow can I fix this?
Thank you for your time!"
331,How to distinguish between primary and secondary input text in Deep learning?,"['python', 'deep-learning', 'nlp', 'text-classification', 'named-entity-extraction']","Given an article, I need to index the important keywords of it using the Entity extraction concept. I am using Spacy NER pipeline to train on my dataset and I created the training dataset in Spacy format as follows.{(Text - Full paragraph in which the keyword occur, starting and ending character position of the keyword,""keyword""),
(..),
...
(...)}For example: {(""K-means is a machine learning algorithm..."",(0,7),""keywords""),..}By analyzing the training data (i.e. article and its keywords), I can see that the words appearing in the title, abstract, conclusion, heading and subheading, tables and figure caption are more probable of appearing in the keywords. So I tried giving the location details in the Text like ""Heading: Text..."" or ""Table caption: Text..."". But still the model performance is low.Is there any way to indicate that some piece of text is important than other in the Deep learning methods. Similarly I want this to extend to text classification too. Any help would be of great help."
332,Get tag-specific accuracy for pos tagger in spacy tagger evaluation,"['python', 'machine-learning', 'nlp', 'spacy', 'pos-tagger']",I'm using the code below for evaluate the pos tagger that I trained in spacy 2.3.0 (built from source- last commit : 9860b8399ed2a3d1d680e1c1cd31d85926422709):and the result is:I wonder if there is a way to get accuracy per every part-of-speech tag?
333,ModuleNotFoundError: No module named 'sklearn_crfsuite',"['scikit-learn', 'nlp', 'ner', 'crf', 'crfsuite']","I am trying to use the sklearn's crfsuite but it is showing an error that no module named 'sklearn_crfsuite', also checked the documentation there also the same thing is mentioned?"
334,Understanding ConvNet Prediction on Text Classification,"['deep-learning', 'nlp', 'text-classification', 'unsupervised-learning']","I'm trying to debug a model that uses 1D convolutions to classify text that was labeled by humans as being ""appropriate"" vs ""not appropriate"" to be posted on some website. Looking at false positives (wrongly predicted ""appropriate""), I see that the text has mostly neutral/positive sounding words, but the idea conveyed is bad (ex: talking about ""capping population""). To address a case like this, I can think of ways to help the model learn that the subject of capping population (in this example) should not be classified as ""appropriate"" for this particular task.The problem I'm having is understanding what caused the model to predict ""not appropriate"" for messages that are in fact appropriate. For example, the following message should be considered ""appropriate"":""The blame lies with the individual who commits the crime.""The model thinks that's not appropriate, but according to the labeling criteria of the dataset, that's a valid message.Given a model with an embedding layer for each word, followed by several 1D convs + dense layer, what are some techniques that can help me what is causing the model to classify that message as such, and potential ways to help the model learn that's ok?Turns out if I take the example phrase above and replace one word at a time, then see how the model classifies the resulting phrase, it classifies the phrase as being ""appropriate"" when I replace the word ""lies"" with just about any other ""positive"" or ""neutral"" word. So it seems like the model learned that ""lies"" is a really, really bad word. Question is: how do I create a feature(s) or otherwise help the model generalize beyond that?"
335,How to use HuggingFace nlp library's GLUE for CoLA,"['deep-learning', 'nlp', 'language-model', 'bert-language-model', 'huggingface-tokenizers']","I've been trying to use the HuggingFace nlp library's GLUE metric to check whether a given sentence is a grammatical English sentence. But I'm getting an error and is stuck without being able to proceed.What I've tried so far;reference and prediction are 2 text sentencesError I'm getting;However, I'm able to get results (pearson and spearmanr) for 'stsb' with the same workaround as given above.
Some help and a workaround for(cola) this is really appreciated. Thank you."
336,Is there a Python library or tool that analyzes two bodies of text for similarities in order to provide recommendations?,"['python', 'python-3.x', 'nlp', 'gensim', 'lda']","First, apologies for being long-winded.I'm not a mathematician, so I'm hoping there's a ""dumbed down"" solution to this. In short, I'm attempting to compare two bodies of text to generate recommendations. What you'll see below is a novice attempt at measuring similarity using NLP. I'm open to all feedback. But my primary question: does the method described below serve as an accurate means of finding similarities (in wording, sentiment, etc) in two bodies of text? If not, how would you generate such a recommendation engine (new methods, new data, etc)?I currently have two dictionaries – one with personality data called personality_feature_dict that includes the personality type and associated descriptor words: {'Type 1': ['able', 'accepting', 'according', 'accountable'...]} and the other called book_feature_dict containing book titles and their own descriptor words, which were extracted using TF-IDF {'Book Title': ['actually', 'administration', 'age', 'allow', 'anti'...]}As it stands, I'm using the following code to calculate the similarity between dictionary values from each to identify % similarity. First, I create a larger corpus using all dictionary items.Then I create an LDA model to identify similarities. My knowledge of LDA modeling is limited, so if you spot an error here, I appreciate you flagging it!Finally, I iterate through sets of values as bags of words and compare how the first personality type or (personality_feature_dict.values())[personality_num] compares to 99 book descriptions/values by finding the Hellinger distance.Finally, I print all instances where the LDA model comes back with a high correlation as a percentage.The result looks something like this:"
337,Is there a simple way to get the position of a token in sequence with spacy?,"['nlp', 'spacy']",I want to the start index and the end index of every token in a sequence. Is there a simple way to do that with spacy?For instance:This is not what I need. I need
338,Text data augmentation,"['text', 'nlp']","I have a more theoretical question regarding the text augmentation. Let's assume I have a text summarization task (or any other sequence-to-sequence task). Should I augment only the source text and keep the target text as the original, or should I also augment the target text (e.g. summaries)? Which one is better?I found a lot of information about the text classification task, which is pretty clear that you augment only the source text and preserve the labels. But what if it's a sequence-to-sequence task?"
339,How to have positive and negative labels in BERT sentiment analysis?,"['python', 'nlp', 'sentiment-analysis']","I am familiarizing myself with Google's BERT, using it for sentiment analysis. I managed to reproduce the most basic examples:The first classify returned [{'label': 'POSITIVE', 'score': 0.9998354911804199}] while the second one returned [{'label': 'NEGATIVE', 'score': 0.9997642040252686}]. Amazing so far, exactly as planned.However, I need to analyze thousands of such short (or not considerably longer, typically at most 3-sentence long) comments in various languages. Luckily, BERT has a multilingual model trained on 104 languages, so I thought this shouldn't be a problem. I downloaded the model usingin the hope that, once it's finished, I can get similar sentiment data on non-English comments as well. However, when I tried inputting some random Chinese, Turkish, and Hungarian texts, I always got such outputs:What did I do wrong? The previous POSITIVE and NEGATIVE labels are gone, instead, I got this LABEL_0 that I am not sure about what it is, and the scores clearly don't match the real positiveness or negativeness of my input comments either."
340,Remove stop words from spaCy Doc object,"['python', 'nlp', 'spacy']","I'm trying to figure out how to remove stop words from a spaCy Doc object while retaining the original parent object with all its attributes.returns:Typical solutions to removing stop words consist in using a list comprehension:Since, now, the parent object is a list of Token objects, and no longer a Doc object, it no longer has the original attributes, so the code returns:So the rather terrible way I could find is to rebuild a string from the tokens, and reprocess it. This sucks as it's double work, and the nlp method is already slow.Now, there must be a better way, right?"
341,Loop optimization to remove pre identified n-grams,"['python', 'optimization', 'nlp']","I have a list of words df_rem pre-identified (grouped by subcats) which needs to be removed from the list of n-grams keywords (grouped by subcats) obtained from the data. The idea is to remove all lower n-grams(in keywords) with respect to each n-gram in (df_rem) if I get a fuzz.token_set_ratio = 100, and to remove the higher order n-grams (in keywords) w.r.t each ngram in (df_rem) if I get a substring match, and to flag higher order n-grams (in keywords) w.r.t each ngram in (df_rem) if I get a fuzz.token_set_ratio = 100.Example: df_rem has a trigram - ""quick brown fox"". I want to remove all bigrams and unigrams in the keywords list which contains either of the words ""quick,brown or fox"". I want to remove a quad-gram with all three of these words but they should be a substring (""quick brown fox jumps""). I want to flag all quad-grams with all three of these words but can be jumbled (""jumps brown fox quick"")I am looking to first identify all the words in the 3 lists and then see if it makes sense to remove them. Is there a way to optimize this code. Its currently taking over an hour to process a list of about 500 words in df_rem and a million words in keywords list. I also tried iterating over a list in place of sets generated in the code below.Is there any way I can optimize my code for a considerably less run-time? Any help would be appreciated. Thanks in advance :)"
342,input/instances formating in tf.serving,"['python', 'tensorflow', 'nlp', 'request', 'tensorflow-serving']","I'm very new to tf.serving and I'm trying to make a request to predict from a model. Running request with my model gives the following error in my output:{'error': '[Derived]{{function_node __inference__wrapped_model_18904}} {{function_node __inference__wrapped_model_18904}} Incompatible shapes: [1,300,768] vs. [1,5,768]\n\t [[{{node tf_bert_for_sequence_classification_2/bert/embeddings/add}}]]\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall]]'}the data I'm passing in is the following:{""signature_name"": ""serving_default"", ""instances"": [[2, 41303, 21069, 38, 4589, 49793, 20252, 3412, 35759, 8830, 8910, 932, 137, 6337, 1517, 193, 5432, 49791, 275, 137, 22927, 20552, 9846, 8871, 23574, 13525, 76, 3,..'And printing out the default serving signature:Again, I might be missing something obvious here but this is the first time deploying with serving so I don't understand how I should change my input to return the proper predictions. Thanks!"
343,How to combine Multiple sentences in a dataframe column to a single list of elements in Python,"['python', 'nlp', 'text-analytics-api']",I have a text column in my dataframe which looks like thisNow I want the text in all four rows to be combined as multiple elements in a listHow to create the output list in the above format in Python
344,"Can we predict a rating based on text, using NLP?","['python', 'python-3.x', 'nlp', 'tfidfvectorizer']","I've used regression and classification in the past to train, test, and make predictions.  Now, I am looking at some NLP sample code and everything is running fine, but at the end, I was hoping to make a prediction of a 'rating' score based on what is contained in a 'text' field.  Maybe NLP can't do this, but it seems like it should be doable.  Here is the code that I am testing.Now, based on specific text, I want to predict the rating a customer will give.Then I get this error: IndexError: Index dimension must be <= 2The actual rating for this actual review is 4.  I was expecting 'y_predicted' to show me a 4.  Maybe there is some other library for this kind of thing.  Again, I think it should be doable.  Thoughts?  Suggestions?"
345,"Sklearn returns error: “Input contains NaN, infinity or a value too large for dtype('float64').”","['python', 'text', 'scikit-learn', 'nlp', 'multinomial']","I am running the following script:This returns:I don't understand what's wrong. The variable ""pipeline"" is described as follows:Maybe, decode_error should be ""replace"", rather than ""strict""?Code has been taken from: https://github.com/DipakMajhi/Classify-Industry-for-Companies/blob/master/Classify-Industry-for-Companies.ipynb.Thanks!"
346,How to fine tune BERT for question answering?,"['nlp', 'question-answering', 'finetunning', 'squad']","I wish to train two domain-specific models:For Domain 1, I've access to a text-corpus with texts from the constitution and no question-context-answer tuples.
For Domain 2, I've access to Question-Answer pairs.Is it possible to fine-tune a light-weight BERT model for Question-Answering using just the data mentioned above?If yes, what are the resources to achieve this task?Some examples, from the huggingface/models library would be mrm8488/bert-tiny-5-finetuned-squadv2, sshleifer/tiny-distilbert-base-cased-distilled-squad, /twmkn9/albert-base-v2-squad2."
347,How to use tf.saved_models.load(),"['python', 'tensorflow', 'nlp', 'bert']","So I have a fine-tuned BERT model and preprocess input from transformers, basically following the tutorial here: https://www.tensorflow.org/official_models/fine_tuning_bert .
However, when I load my model using tf.saved_models.load(path) (the model was previously saved using tf.saved_models.save(path),giving me a model.pb and folders 'assets' and 'variables') and try to predict similar to what they do in the tutorial I get:The code that gives me the error:ValueError: Could not find matching function to call loaded from the SavedModel. Got:
Positional arguments (10 total):
* [<tf.Tensor 'inputs:0' shape=(1, 300) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(1, 300) dtype=int32>, <tf.Tensor 'inputs_2:0' shape=(1, 300) dtype=int32>]
* None
* None
* None
* None
* None
* None
* None
* None
* False
Keyword arguments: {}Expected these arguments to match one of the following 4 option(s):Option 1:
Positional arguments (10 total):
* {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids')}
* None
* None
* None
* None
* None
* None
* None
* None
* True
Keyword arguments: {}Option 2:
Positional arguments (10 total):
* {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids')}
* None
* None
* None
* None
* None
* None
* None
* None
* False
Keyword arguments: {}Option 3:
Positional arguments (10 total):
* {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}
* None
* None
* None
* None
* None
* None
* None
* None
* True
Keyword arguments: {}Option 4:
Positional arguments (10 total):
* {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}
* None
* None
* None
* None
* None
* None
* None
* None
* False
Keyword arguments: {}I've searched around previous posts, some say that you need tf-nightly (which I run on my colab) or just load the keras version instead, but haven't been able to find a proper solution.The input I want to predict on:{'input_ids': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=
array([[    2, 41303, 21069,    38,  4589, 49793, 20252,  3412, 35759,
8830,  8910,   932,   137,  6337,  1517,   193,  5432, 49791,
275,   137, 22927, 20552,  9846,  8871, 23574, 13525,    76,
46377, 17210,    65, 41303, 21069,  1045, 15322,   237,   641,
12517, 16600,   108, 23574,   346,  2248,  1806,  5480,  3412,
1813, 24534,  7714, 18738,   932,   641, 34456,    76, 41303,
21069,    38,  3994,   137,    65, 41303, 21069,  1045, 11597,
817,    48,  6110, 49792,   100, 13097, 35759,    76, 14362,
49817,  1603,  3994,    36,    59,   292,    31,    48, 19122,
82, 11597,    11,   817,    48, 10587,   367,   921,    81,
251,    65,   137,  6337,  1517, 26242,  8111,     3,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0,     0,     0,     0,     0,     0,     0,
0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}Update
After doing some research I realized that the tf.saved_model does not save functions similarly to keras.models.save, so my question is rather not if I read the model wrong but rather how I just load a model from tf.saved_model.load and predict on it in a notebook?"
348,How to apply a function to a set amount of rows in a Dataframe?,"['python', 'pandas', 'nlp']","I have the following code that uses nlp() on every column to determine the type. However, it could take a long time depending on the size of my data. I was wondering how could I apply the function on selected amounts of rows? For example if I only wanted to apply it to the first 100 rows of every column instead?"
349,Huge difference in accuracy when training and testing on random manually splitted data and when splitted using test-train split,"['python', 'scikit-learn', 'nlp']",I have a data set of around 4800 comments for training and 1600 comments for testing and i have to categorize them into 11 different categories USING NLP. I trained my model after vectorising it using tfidf. I applied obj.fit_transform on train data and simply obj.transform on test data. Results had 50% accuracy. But i then mixed the 2 data files and used test-train split. I got 90% accuracy. I even checked that the data that came after splitting had same distribution of number of categories of each of the 11 categories to be predicted. Why is this happening?
350,Using torchtext for inference,"['machine-learning', 'nlp', 'pytorch', 'torchtext']",I wonder what is the right way to use torchtext for inference.Let's assume I've trained the model and dump all Fields with built vocabularies. It seems the next step is to use torchtext.data.Example to load one single example. Somehow I should numeralize it by using loaded Fields and create an Iterator.I would appreciate any simple examples of using torchtext for inference.
351,Using Gensim Fasttext model with LSTM nn in keras,"['keras', 'nlp', 'gensim', 'word-embedding', 'fasttext']","I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like ""Oxytocin"" ""Lexitocin"", ""Ematrophin"",'Betaxitocin""given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gramHow do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.Any idea?"
352,How to use batch to run RNN model with texts that have different length in each row?,"['python', 'nlp', 'pytorch', 'data-mining', 'torch']","I want to use torch.nn.utils.rnn.pack_padded_sequence in pytorch to deal with this problem, But I dont kown how to get torch variable when it has different size in the last dimension?pack_padded_sequenceI will get errors if I run the following code.Help!"
353,Email parsing using NLU/NLP and making actions,"['email', 'parsing', 'nlp', 'nlu']","We have a service desk automation as one of the use cases to solve. Email counts are not that much as it is around 1000 emails per day but we need to parse the email and act accordingly. Currently, its been manually done to read the emails and convert into appropriate tickets which we would like to automate. Please suggest the approach and the right solutions for it."
354,BertLMDataBunch.from_raw_corpus UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 49: invalid continuation byte,"['python', 'nlp', 'bert']","I have trouble fine-tuning Camembert using fast-bert library,
I get this error message when creating a LMDataBunch.
Does anyone know how to fix this ?
ThanksPs logger is initialezed using logging.getLogger()"
355,Replace spaces with new lines if part of a specific pattern using sed and regex with extended syntax,"['regex', 'sed', 'nlp', 'text-processing']","so I have a text file with multiple instances looking like this:word.  word or words [something:'else]I need to replace with a new line the double space after every period followed by a sequence of words and then a ""["", like so:word.\nword or words [something:'else]I thought about using the sed command in bash with extended regex syntax, but nothing has worked so far... I've tried different variations of this:sed -E 's/(\.)(  )(.*)(.\[)/\1\n\3\4/g' old.txt > new.txtI'm an absolute beginner at this, so I'm not sure at all about what I'm doing 😳"
356,How to pass on multiple intents output to one single intent to execute,"['nlp', 'dialogflow']",Consider I want to build a system which plays audio and videoIntents I will have areI want to reuse the intent 2 (player for audio or video) without having two intents one for each audio and video.I'm planning my intents and contexts like belowIs it possible? if so how do I do it?
357,Spacy tagger loss is zero while training,"['python', 'nlp', 'spacy']",I use this snippet of code to train a tagger in spacy 2.3.0.The problem is that the loss value is always zero. What am I doing wrong?
358,About training data for spaCy NER,"['nlp', 'spacy', 'training-data']","I'm new to NLP.
Now I'm trying to create NER model for extracting music artist's name from some text.
But It hasn't gone well.This is what I've done.I got 1500,000 artist's name list.I created training data with string template.like this ""{artist's name} is so sick.""
All 1500,000 training data is like this string.(Maybe this is the reason it doesn't go well?)I used the model after training 30,000 datas.But I didn't work at all.All sentence was extracted as ARTIST.
Below is example. 'Chris Thomas King' is artist's name in this case.Do you have any idea?
Thanks in advance."
359,Word Counter loop keeps loading forever in Python,"['python', 'python-3.x', 'pandas', 'list', 'nlp']","I have a DataFrame comments as seen below. I want to make a Counter of words for the Text field. I have made a list of UserId whose count of words is needed, those UserIds are stored in gold_users. But the loop to create Counter just keeps loading. Please help me fix this.comments
This is just part of dataframe, original has many rows.CodeExpected Output"
360,Word embedding with large files,"['nlp', 'inputstreamreader']","I am new to the world of NLP. I am working on a word embedding program with gensim. It works with files of small size. If I try using a larger file that is several gigabytes, I get an error message saying that it cannot open the file after stalling for two hours.I was looking into this, and it seems like I need to find a way to ""stream"" the file in order to read it into the program but I am very confused how to do this.  I can't find any examples of word embedding with streaming files.Also is this process going to be slow for terabytes of data? Thanks for your help!"
361,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"['python-3.x', 'nlp', 'stanford-stanza', 'coreference-resolution']","I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:I tried three algorithms:deterministic (I got the error below)Questions:Why am I getting this error with the deterministic?Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?Results:The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.Am I missing something?"
362,How do I get a slice of Tensor from a 2D Tensor in Tensorflow 1.12?,"['tensorflow', 'deep-learning', 'nlp', 'sequence']","I have a 2-D tensor X with dimensions (a,b) and I need to get a 1-D slice from it , Is it possible to do an operation similar to numpy in TF such as :"
363,Summarization-Text rank algorithm,"['python', 'machine-learning', 'nlp', 'bert', 'textrank']","What are the advantages of using text rank algorithm for summarization over BERT summarization?
Even though both can be used as extractive summarization method, is there any particular advantage for text rank?"
364,Keras freezes on training of first epoch,"['tensorflow', 'keras', 'nlp', 'dataset', 'speech-recognition']","This is the code I've used from another website : https://github.com/rolczynski/Automatic-Speech-RecognitionI had to change the values a bit (make batch size=1, epochs=2 and change values of features_num, inp_dim, output_dim and rnn_units) because otherwise it'd give the warning :Allocation exceeds 10% of system memorymultiple times and my pc would get stuck there and i'd have to restart.The number of .wav files in the training dataset is 91912, testing dataset is 3995, and development dataset is 4076.When I run the above code, the training happens until the 91911th file and then it freezes there. I'm not really sure why.I'm actually very new to all this so please explain how I'm supposed to make this work. Thank you."
365,I receive this error 'str' object has no attribute 'text' when I try to run the following code,"['python', 'nlp', 'lexical']",I had removed .text and had just written peter_pan.split() but I was not sure whether the output was correct.
366,How do I translate using HuggingFace from Chinese to English?,"['nlp', 'translation', 'huggingface-transformers', 'machine-translation', 'huggingface-tokenizers']","I want to translate from Chinese to English using HuggingFace's transformers using a pretrained ""xlm-mlm-xnli15-1024"" model. This tutorial shows how to do it from English to German.I tried following the tutorial but it doesn't detail how to manually change the language or to decode the result. I am lost on where to start. Sorry that this question could not be more specific.Here is what I tried:"
367,How to get count of words from DataFrame based on conditions,"['python-3.x', 'pandas', 'dataframe', 'nlp', 'pandas-groupby']","I have the following two dataframes badges and comments. I have created a list of 'gold users' from badges dataframe whose Class=1.Here Name means the 'Name of Badge' and Class means the level of Badge (1=Gold, 2=Silver, 3=Bronze).I have already done the text preprocessing on comments['Text']and now want to find the count of top 10 words for gold users from comments['Text'].I tried the given code but am getting error
""KeyError: ""None of [Index(['1532', '290', '1946', '1459', '6094', '766', '10446', '3106', '1',\n       '1587',\n       ...\n       '35760', '45979', '113061', '35306', '104330', '40739', '4181', '58888',\n       '2833', '58158'],\n      dtype='object', length=1708)] are in the [index]"". Please provide me a way to fix this.Note
I had some answers from datascience.stackexchange but they did not work. Link to StackExchange ProblemDataframe 1 (badges)Dataframe 2 (comments)CodeExpected Output"
368,Lucene: How to factor in similarity to original term?,"['java', 'search', 'nlp', 'lucene', 'tf-idf']","Say someone searches for 'shoes', and I have two documents called 'clothes' and 'socks' and both are added as synonyms for 'shoes'.Both would be ranked equally for this term, but I want 'socks' to be ranked higher than 'clothes'.Lets say I have a function which takes two words and returns a numeric similarity score. E.g:similarity(""shoes"", ""socks"") = 0.8similarity(""shoes"", ""clothes"") = 0.65How can I have that similarity score be fed to Lucene to tell it to rank socks higher than clothes?I think ideally, what I want is to multiply the term frequency for each term by its similarity. E.g:tf('socks') * 0.8tf('clothes) * 0.65That way, all the other workings of the lucene scoring formula stay the same, but synonym terms are just given more or less importance in proportion to their similarity to the original term.Is this possible? Function queries? Extending default similarity?"
369,How to split Chinese words and English words in a string using python?,"['python', 'string', 'nlp']","For example, I have some strings look like:'人均收入 Per capital Income','总产值 Gross Output Value'.I want to split them into'人均收入' 'Per capital Income''总产值' 'Gross Output Value'Chinese characters are always before English words."
370,Object of type 'float' / 'int' has no len() error when using nlp() on data,"['python', 'pandas', 'nlp']","Below I applied the nlp() function to one of my columns:However when I try to do the same to all of the columns in my data set, I am getting the error object of type 'int' has no len() or object of type 'float' has no len() Despite trying to ignore those columns with if df[col].dtype == 'float64' or df[col].dtype == 'float' or df[col].dtype == int or df[col].dtype == 'int64':I have also tried:How can I resolve the issue?"
371,Standardizing white spaces in nlp - python,"['python-3.x', 'nlp', 'whitespace']","Since I am working with many different fonts and I would like to standardize all white spaces.I'm looking for something similar to this entry for skip linesor for hyphensI was using something likeHowever, I realize that it also removes ""new lines"". I do not want to remove them."
372,"Standarzing double quotes, single quotes and apostrophes in python","['python-3.x', 'regex', 'nlp', 'quotes', 'double-quotes']","Since I am working with many different fonts and have a special treatment for each of these symbols, I would like to standardize all quote and apostrophe entries in my text fonts.I'm looking for something similar to this entry for skip linesor for hyphensCan you help me?"
373,"I am trying to improve the accuracy of NLP using SVM model , below is code can anyone tell me what more should i do to increase it","['machine-learning', 'nlp', 'data-science', 'svm']","I am using SVM as my classifier
I am trying to improve the accuracy of NLP using SVM model, below is code can anyone tell me what more should I do to increase it"
374,"Python - If I have a bubble plot, is there a python visualization library that reveals text labels as you zoom in?","['python', 'text', 'nlp', 'visualization']","I recently posted this question as a plotly inquiry, but I'm wondering if there are any visualization libraries that allow you to do this.Here's some example code:I'm wondering if there's a way to have the text labels appear as I zoom in, based on the frequency?So, right when you load the plot, you might only see ""two"" and ""started"", but as you zoom in, the other labels appear."
375,KeyError: Selecting text from a dataframe based on values of another dataframe,"['python-3.x', 'pandas', 'dataframe', 'nlp', 'pandas-groupby']","I have the following two dataframes badges and comments. I have created a list of 'gold users' from badges dataframe whose Class=1.Here Name means the 'Name of Badge' and Class means the level of Badge (1=Gold, 2=Silver, 3=Bronze).I have already done the text preprocessing on comments['Text']and now want to find the count of top 10 words for gold users from comments['Text'].I tried the given code but am getting error
""KeyError: ""None of [Index(['1532', '290', '1946', '1459', '6094', '766', '10446', '3106', '1',\n       '1587',\n       ...\n       '35760', '45979', '113061', '35306', '104330', '40739', '4181', '58888',\n       '2833', '58158'],\n      dtype='object', length=1708)] are in the [index]"". Please provide me a way to fix this.Dataframe 1 (badges)Dataframe 2 (comments)Code"
376,sort with top idf values - tf idf vectorizer,"['python', 'machine-learning', 'nlp', 'tf-idf', 'tfidfvectorizer']","I have an assignment to implement TF-IDF vectorizer from scratch. The first task is to Build a TFIDF Vectorizer & compare its results with Sklearn. This I have done successfully.The second task is to Implement max features functionality. The task is described below:As a part of this task, you have to modify your fit and transform functions so that your vocab will contain only 50 terms with top IDF scores.This task is similar to your previous task, just that here your vocabulary is limited to only the top 50 features names based on their IDF values. Basically your output will have exactly 50 columns and the number of rows will depend on the number of documents you have in your corpus.Here you will be given a pickle file, with file name cleaned_strings.
You would have to load the corpus from this file and use it as input
to your TF-IDF vectorizer.Attaching the code for the first task.The above code runs on the dataset which contains just 4 documents. The second task described above has a dataset of 750 documents. I need suggestions on how to sort with idf values and get the top 50 terms here. The idf scores are calculated and immediately used to calculate tf-idf value. So it got tricky for me to solve this. Please help me out. Thanks in advance!"
377,JSONDecodeError: Expecting value: line 1 column 1 (char 0) when using polyglot for NER,"['python-3.x', 'nlp', 'ner', 'polyglot']",I am trying Polyglot for NER. Please find the below given code which I am using.whenever I run Text.entities I am getting Json decode error as follows:
378,HuggingFace Transformers Model for Legal Question Answering,"['nlp', 'question-answering']","I am implementing a Question Answering Implementation using the Haystack library available here. My use case is to use it to answer questions from the Constitution of India. The models I have tried as of now are: deepset/bert-large-uncased-whole-word-masking-squad2, mrm8488/bert-tiny-5-finetuned-squadv2, /distilbert-base-uncased-distilled-squad, bert-large-uncased-whole-word-masking-finetuned-squad, deepset/roberta-base-squad2, albert-xlarge-v2-squad-v2, distilbert-base-cased-distilled-squad, sshleifer/tiny-distilbert-base-cased-distilled-squad.I wanted to know if there is any model which has been fine-tuned for legal documentations which can improve the overall performance of this implementation by understanding the context more than a model that has been trained on generic data. Also is there a model that has been fine-tuned on technical documentation to answer technical FAQ questions."
379,How to store Word vector Embeddings?,"['python-3.x', 'keras', 'nlp', 'word-embedding', 'bert']","0I am using BERT Word Embeddings for sentence classification task with 3 labels. I am using Google Colab for coding. My problem is, since I will have to execute the embedding part every time I restart the kernel, is there any way to save these word embeddings once it is generated? Because, it takes a lot of time to generate those embeddings.The code I am using to generate BERT Word Embeddings is -Here, gen_features is a function which returns word embedding for each i in my list text_list.I read that converting embeddings into bumpy tensors and then using np.save can do it. But I actually don't know how to code it.Please help."
380,NLP in Python - updating NLTKs Vader negate words,"['python', 'nlp', 'nltk', 'vader']","I want to add ""no"" as a negate word in the NLTK Vader package.I understand that you can manually add to the lexicon as per this link: How to edit NLTKs VADER sentiment lexicon without modifying a txt fileHowever, I don't want to add to lexicon. I want to add to the negate word list. I can see from the documentation https://www.nltk.org/api/nltk.sentiment.html that ""nor"" and ""not"" is in the negate word list but I want to add ""no"" as a negate word as well. Is there any way to do that?Thanks"
381,Estimate token probability/logits given a sentence without computing the entire sentence,"['python', 'nlp', 'huggingface-transformers']","I have a sentence like:  ""I like sitting in my new chair and _____ about life"".And I have a SPECIFIC set of tokens like [""watch"", ""run"", ""think"", ""apple"", ""light""]I would like to calculate the probability of each of those tokens to appear as the next word in that incomplete sentence. Hopefully I should get that the probability of ""think"" is higher that ""apple"" for instance.I am working with pytorch-transformers (GPT2LMHeadModel specifically), and a possible solution is to evaluate the score of the full sentence with each of the tokens, but when number of tokens to evaluate is on the order of 100 or 1000 then the computation time starts to be too long.It must be possible to process the sentence only once and somehow use the hidden states to calculate the probabilities of the set of tokens, but I don't know how to do it.Any ideas? Thanks in advanceEDIT:The actual code looks like the one below (estimating the probability for the full sentence every time). For every sentence it takes about 0.1 seconds to run the score() method, which turns into hours if I want to evaluate some thousands of words."
382,spacy train cli compare iterations,"['machine-learning', 'nlp', 'spacy', 'evaluation', 'ner']","So I am running:And I am getting a nice overview in the console about the performance of each iteration, in terms of P, R and F1-score. I unfortunately closed this window, but would really like to have this condensed comparison of each iter, without having to go through 30 different evals myself, and then manipulating the evals to make it just as compact as the cli.Is there a cli for outputting these benchmarks again?Thanks, Rasmus"
383,Summing rows that contain strings from reference table in R [closed],"['r', 'string', 'join', 'nlp', 'sum']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 29 days ago.For a list of strings that exist as rows within a table, I want to identify the frequency of those strings within rows of another data table in R. Simultaneously, I want to sum the values of rows that contain these strings.For example, my reference table which contains a list of strings would look something like this:And my table that I want to analyze would look something like this:I want my resulting table to look like this:I've tried using n-grams and tokenizing but it's not working in a way that makes it easy, as artists names can contain 1, 2 or 3 words commonly. Any help would be appreciated."
384,How to format a text file such that it removes blank files and trailing space,"['python', 'text', 'nlp']","I have test.txt that looks like the screenshotPS: There are trailing spaces in the second line, so it is < space > line 2in result we have to get:line 1
line 2
line 3This is what I have so farBut it is not handling cases when the line starts with space (in the example, line 2) , How do I modify my code? I want to us Python"
385,"In case of text analysis, when I apply fit() method, what exactly happens? And what does transform() do on the text data?","['python', 'machine-learning', 'nlp', 'data-science', 'sentiment-analysis']","In case of text analysis, when I apply fit() method, what exactly happens? And what does transform() do on the data?I can understand it for numerical data type but unable to visualize it for text data.I have a text arrayNow to vectorize it, I use CountVectorizer class:I know that I will get vectors of 4500 length. However, I am unable to visualize what exactly fit method would have done behind the scene and how exactly data would have been then transformed by tranform function? Specially that given data is text type."
386,Unable to install spacy with Anaconda,"['python', 'nlp', 'anaconda', 'spacy']",The error that my pycharm shows me is this:Executed command:C:\Users\Felipe\Anaconda3\Scripts\conda.exe install -p C:/Users/Felipe/Anaconda3/envs/NLP spacy -yError ocurred:UnsatisfiableError: The following specifications were found to be incompatible with each other:Command output:I realy need to get trouth this to keep studing!
387,"How to count and extract questions in a text, in R? [closed]","['r', 'nlp', 'text-mining']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 29 days ago.I have found multiple answers about counting numbers of sentences, but how to extract specifically questions out of text, in R?"
388,My code removed all punctuation from text but do we need few of them for sentimental analysis?,"['python', 'nlp', 'nltk', 'sentiment-analysis']","I am working on the sentimental analysis of amazon product reviews. I am preprocessing the reviews' text and used the above function to remove punctuation. It has removed all of them, but my question is that do we consider some of them for sentimental analysis. Like !. Is it the right approach.Thanks for your help and time."
389,why run “python run_squad.py” doesn't work?,"['nlp', 'google-colaboratory', 'huggingface-transformers']","I want fine tune on squad with huggingface run_squad.py, but meet the following question:1, when I use ""--do_train"" without ""True"" as following code, after 20 minutes runing,there is no models in output_dir:2, when I use ""--do_train=True"" as following code, the error message is ""run_squad.py: error: argument --do_train: ignored explicit argument 'True'"":3, when I use ""--do_train True"" as following code, the error message is ""run_squad.py: error: unrecognized arguments: True"":I run code in colab with GPU: Tesla P100-PCIE-16GBJudging by the running time, I think the code didn't through training process, but I don't know how to set parameters in order to let training go.what should I do?"
390,to get key /values of a list matching the dictionary,"['python', 'nlp']","A training data(data_tr->consists of 11300 doc encoded in numeric values given below as array for each documents). The vocabulary is a dictionay consisting of keys and values of 2000 most frequent words. My problem is to convert the array of data_tr(numeric) to words using dictionary (key,values ) pairs for the whole training data(data_tr). I am new to python programming.I tried to solved the problem as :Here it works fine for one document(data_tr[0]) but fail to produce words (data_tr)for the whole training data(data_tr).I wanted to get back the words for all the documents(11300 doc)"
391,How to find if there is any emotion attached with a sentence in NLP?,"['python', 'nlp', 'nltk']","For example, if I have the sentence:
""I went there yesterday.""- It has no emotion or no emotion word.
But this sentence:
""I was surprised by that"" - It has certain emotion in word surprise.Can we do this using any NLTK library?"
392,Can you use an embedding + CNN model for both text and image classification?,"['tensorflow', 'machine-learning', 'image-processing', 'nlp', 'conv-neural-network']","I have a tensorflow CNN model with an embedding layer for text classification as follows:My colleague is adamant that this is viable but I found this post stating it is not feasible. I understand CNN as an algorithm can be used for text and image inputs, but my understanding is that you can't use the same CNN model for text input and image input: text will use Conv1D and image, Conv2D.The linked post mentions:If I'm on the right track, how I can go about building two sub models (one for text, another for image classification) and merge the latent spaces. Thank you!"
393,Predicting unobservable data using classification modeling?,"['python', 'machine-learning', 'nlp', 'classification', 'logistic-regression']","I have a dataset of teacher evaluations. The question is ""how can the instructor improve?"" I want to be able to classify whether the comment made by the student is a suggestion or not. I have labeled a little over 1000 observations, 1 if it is a suggestion and 0 if it is not.Here is some of my code:Doing all of this gives me an accuracy of 91%.I want to use my model to classify the rest of the comments that I did not label myself. However, when I try to input all of the other observations:I know it's because the bag of words of the bigger set of comments is much bigger than the bag of words used to train the model. I just don't know how I can fix this.What can I do to solve this or is there a better way to use my model to predict unlabeled data with the model I have?Thank you in advance!"
394,How to split sequences based on dependencies with pandas and spacy,"['python', 'pandas', 'nlp', 'nltk', 'spacy']","I have a string that looks like this:Using Spacy, I built a df that prints the word and its dependencies within the string. df.word contains each Spacy token (word) and df.network contains the other tokens that word is connected to in the string. See below:What I want to do is split the string into smaller utterances by the presence of punctuation (keeping the punc char with the last word). However, if a word to the left of a punctation character is found as a dependency to a word to the right of the punctuation character, I want this to override the punctuation split.For example, splitting this string only by punctuation would result in this:'well,', 'you never know'However, this is wrong because well (left of punctuation char) shows up in df.network in the row containing know, which occurs to the right of the punctuation char, I want them grouped together into this:'well, you never know'To illustrate, here is another example with this string:'let me think. i dont know.'The Spacy dependency network looks like this:As you can see, no words to the left of a punctation character show up in df.network for words to the right of a punctuation mark. With this condition satisfied, the string should be split at the punctuation char (keeping the punc char with the last word), like this:'i dont know.', 'let me think'Last, here is a final example. If there are multiple punctuation chars, the pattern should stay the same. See below:'i, i, dont know'Results in:Which should end in:'i, i, dont know'"
395,NLP search with elasticsearch and dense vectors,"['elasticsearch', 'machine-learning', 'search', 'vector', 'nlp']","I'm developing an NLP search engine based on vectors and the MUSE (Multilingual Universal Sentence Encoder) model from Google.The process is very simple:1 - I have a folder with 34 pdf documents (190 MB)2 - I convert this documents to plain text3 - I tokenize the document´s plain text to sentences4 - I use MUSE to encode every sentence in a 512 dimension vector5 - I store the sentences + the vectors in elasticsearch, the vectors are stored in the ""dense_vector""
field6 - When I do the search, I vectorize the search input (with MUSE, obviously) and I use the cosineSimilarity function from elastic to get the most relevant sentences.So far so good, the search engine works with very decent results.The problem here is the size of the index in elasticsearch, to store the 34 documents (190MB) took 560MB!, the index with the sentences + vectors is almost 3 times the size of the original documents in pdf.I have read a lot of publications about the state of the art in NLP search engines and all of them point to sentence encoding (embeddings) and calculate the cosin to get the most relevant results, but any of them talks about a real world implementation.Any of you have experience with something similar?, how do you deal with vectors storage?Thanks!"
396,OpenAI GPT-2 model use with TensorFlow JS,"['tensorflow', 'machine-learning', 'nlp', 'tensorflow.js']","Is that possible to generate texts from OpenAI GPT-2 using TensorFlowJS?If not what is the limitation, like model format or ...?"
397,How to analyze larger texts in spaCy?,"['python', 'nlp', 'spacy']","I would like to analyze larger chunks of text, preferably inside the xlsx or docx document. All I found was the doc = nlp('string'). When I tried to paste a really long string it no longer worked trigerring an ipython syntax error (Jupyter's issue I guess?), so my workaround didn't wor either.What can I do if I would like to analyze something bigger, like, say 20 pages?
Also, can I integrate this code into scattertext somehow?"
398,How to extract Nouns and Verbs from text in android,['nlp'],I tried this method by using nlp. But I didn't get binary path from android.While calling en-parser-chinking.bin file it showing file no Found Exception.If anyone help on this it would be more helpful.
399,Extract sentence embeddings features with Pandas and spaCy,"['python', 'pandas', 'dataframe', 'nlp', 'spacy']","I'm currently learning spaCy, and I have an exercise on word and sentence embeddings. Sentences are stored in a pandas DataFrame columns, and, we're requested to train a classifier based on the vector of these sentences.I have a dataframe that looks like this:Next, I apply an NLP function to these sentences:Now, if I understand correctly, each item in df['tokenized'] has an attribute that returns the vector of the sentence in a 2D array.yieldsHow do I add the content of this array (300 rows) as columns to the df dataframe for the corresponding sentence, ignoring stop words?Thanks!"
400,Joint Probability distribution of a Gaussian and Gaussian mixture variable,"['machine-learning', 'nlp', 'normal-distribution', 'gmm']","If we think that one latent variable x is just the mixture of k components of a gaussian mixture
distribution and another variable y is a simple Gaussian distribution, in that case how to calculate the joint probability distribution of x and y; P(x,y).suppose, a word w1 in document d consists of k distinct Gaussian components as it represents k distinct meanings and another word w2 is a common word in document d doesn't rely on any topics but the global tendency of the document. In that case, is it the right way to compute two different probability distribution for word w1 - Gaussian mixture and for w2 a normal( gaussian) distribution. If so how can we compute the joint probability distribution for w1 and w2 in document d?Note: we don't want to ignore for w2, we want to have a separate distribution for w2 and w1 and finally compute the joint probability distribution of them.I am new in this area, thank you in advance for your valuable time and kind help."
401,How to use NLP in Python to extract the 'subject' of a question using NLTK trees,"['python', 'nlp', 'nltk', 'chatbot']","I am trying to figure out how to understand a question in Python. The aim is to use this for an information retrieval chatbot - i.e. the user will ask for information about something, and then the chatbot will access a knowledge base to return information about that something.Example user inputs may be:""Show me information about WW1."" - where [WW1] should be extracted""I want to know the symptoms of the common flu."" - where [the symptoms of the common flu] should be extractedI have started of by looking at spacy and nltk, specifically nltk trees, which end up looking like these:From here, I'm not sure how to extract the correct subtree as shown above, and I don't want to have to assume it will always be the right hand most tree.I also don't know if this is the best way of tackling this problem?Once I have the string value of which the user wants information about I can use this to find the information, I just need to get this value first."
402,How Can I Use nltk.RegexpParser to Extract Very Specific Tokens?,"['python-3.x', 'nlp', 'nltk', 'grammar']","I use nltk.RegexpParser to chunk English sentences. I have implemented grammar that recognizes specific chunks successfully. Now I would like to improve that grammar to recognize specific constructions, like lexico-syntactic patterns for extraction relationships in a given text.My question is: are there any possibilities for defining grammar that can extract also given words?I have tried several rules like these (without success):For SPEC rules I have tried some syntax such as using apostrophes, omit parentheses, etc. without any success. My question is that is it possible to write a rule, that recognizes the expression ""like other"" using RegexpParser?Using the python regexp package could solve the problem, but that solution makes the application much more complex and less effective and I would like to avoid that if it is possible."
403,program stucks when running transformers example code,"['python', 'nlp', 'huggingface-transformers']","I have some problem when trying to run the example code from transformers github page (https://github.com/huggingface/transformers) using geany, visual studio code or the terminal. I'm using python 3.6 on Windows. The code of the example is as follows:When I run that example, the program get stuck for a long time (several minutes to more than an hour) before producing a very long error message for several times:The chinese charaters says 'the server didn't response in time, connection failed'. So it's probably a problem with internet. So how can I connect to the server, or download the files required by this program by other means?"
404,pyhton wordcloud of customer pain-points from customer reviews of any product,"['python', 'nlp', 'sentiment-analysis', 'word-cloud', 'vader']","I have a case study to work on, in which there are several customer reviews available and I have to do the following predict their sentiment (positive, negative, neutral) based on their
reviewsdisplay a wordcloud of not the frequently occurring words, but
the words which are the pain-points of the customer and of what is
the customer happy about.e.g. If many customers are happy about leather-strap of the watch, then the wordcloud should display 'leather-strap' in the wordcloud of positive sentiments. & If many customers are complaining/unhappy about the dial-size, then the wordcloud should display 'dial-size' in the wordcloud of negative sentiments.Point 1 can be achieved more or less using VADER.But I am not sure how to achieve point 2, as it is not the usual wordcloud of frequently occurring words.Can you please help me on how can I achieve the second task?"
405,What is negative sampling method use- sigmoid or softmax?,"['machine-learning', 'nlp', 'word2vec']","I am recently reading this paper: Word2Vec explained(https://arxiv.org/pdf/1402.3722.pdf)And there's something I can't understand..In page 3, they say that p is defined using softmax$p(D=1|w, c, \theta) = \frac{1}{1+e^{-v_c\dotv_w}}$enter image description herebut i am confused because i have seen that formula in sigmoid function, not softmax function.How you derive that definition from softmax?"
406,How to edit a CSR matrix?,"['python', 'machine-learning', 'nlp']","I'm trying to apply bigram of a set of files in a folder.I know that by passing each file through counterVectorizer along with vocab will give me CSR matrix of that single file.
Now I need to have CSR matrix having all files , each file as a row.so I'm creating an empty CSR matrix, and want to edit it row by row.
Any new ideas are also welcome.I didn't put code because I'm still trying to figure it out."
407,Excluding variations of word based on negated preceding term,"['python', 'nlp', 'data-mining']","I am looking at data mining records for rabies, currently a simplified version of my regex is as follows:Is there a way of having ""not"" ""(something)"" ""rabies"" where the blank is representative of any subsequent negative comment, therefore I do not need to have huge list of possible combinations to say ""not seen rabies""?Many Thanks"
408,Finding out if Input text is for corona virus or not,"['python', 'model', 'nlp', 'classification', 'text-classification']",Hope everyone is doing well. I am new to nlp and I am trying to write a program that can tell me if the user's input is meant to talk one of my three models chatbotsWhat would be the most reliable way to analyze the user's input to choose between the 3 categories?Thank you in advance!
409,Data Privacy Rasa NLU,"['python', 'nlp', 'artificial-intelligence', 'rasa-nlu']","I am using RASA NLU (open source), but I am concerned about data privacy. I want my data to be private, and wanted to ensure that none of my data ever leaves my machine, connects to Rasa's servers (or any servers other than my own —- only running locally), and nothing goes to the cloud. I want all my data to be private and local until deployment. Is this true of Rasa NLU?"
410,Arbitrary threshold for sigmoid activation function for CNN binary classification?,"['tensorflow', 'machine-learning', 'keras', 'neural-network', 'nlp']","I am classifying sentiment of reviews - 0 or 1 - using gensim Doc2Vec and CNN in Tensorflow 2.2.0:I then make predictions and convert my sigmoid probability to 0 or 1 using np.round():I get great results (~96% accuracy) indicating that the threshold of 0.5 is working as expected...However, when I try to predict on a set of new data, the model seems to separate bad reviews from good ones but across approx 0.0:Mind you, the model never saw X_test during training and it is able to predict just fine. It's only when I introduce a new set of review text strings, I run into incorrect predictions. For new reviews, the only preprocessing that I do before calling model.predict() is feeding them through the same tokenizer used for model training:I've been trying to make sense of this conundrum but I'm making little progress. I ran into post and it indicatesSome sigmoid functions will have this at 0, while some will have it set to a different 'threshold'.But this still doesn't explain why my model was able to predict on np.round()'s 0.5 threshold for X_test dataset (which the model never learned on) and then unable to predict on new dataset at the same 0.5 threshold..."
411,Extract the item and description from purchase text description using Python. Part of speech did not work :(,"['python', 'python-3.x', 'machine-learning', 'nlp', 'text-mining']","Hi I hope you all are doing well. I'm currently working on a project which has descriptions about the purchase made. I have to extract the products and if the description is given that as well. I tried using parts of speech that did not give a good result. Example of data is as below:
""I bought a new xbox 360"" - the output should be ""xbox 360""
""Mayo Scissors: ROBOZ RS-6870 straight, 43mm blade length, length 5.5"" - the output should be ""Mayo Scissors 5.5' by Roboz"""
412,"Why is this regular expression not working ({m, n})?","['python', 'regex']","Trying to understand regular expressions and I am on the repetitions part: {m, n}.I have this code:As you can see both the strings are not matching the pattern. Why is this happening? "
413,How do I run the command --evaluator-filename in a PolylingualTopicModel on Mallet?,"['nlp', 'lda', 'topic-modeling', 'mallet', 'topicmodels']","Mallet's PolylingualTopicModel has a command --evaluator-filename that creates a likelihood evaluator.
I try to run it in the guide and help, but it doesn't create the evaluator file. (I was able to create the inference file.)Guide
http://mallet.cs.umass.edu/topics-polylingual.php--helpThe following command will not result in an error. But the evaluator file has not been created.How do I create a file for the evaluator?
I am using mallet-2.0.8 on Windows. Thank you."
414,Test for whether a token is a conjunction head in spaCy,"['nlp', 'spacy', 'dependency-parsing']","Is there a way to detect whether a token is a conjunction head in spaCy?I'd like the following sentence:""Both Americans and Muslim friends and citizens, tax-paying citizens, and Muslims in nations were just appalled and could not believe what -- what we saw on our TV screens.""...to return the following custom_chunks:This sentence contains a sub-conjunction within its main conjunction, which makes the task more complicated:Presently, I'm using the following code that produces the desired answer:The if and elif statements to identify both conjunction and sub-conjunction heads feels somewhat hacky, is there a more affirmative way to identify whether a token is a conjunction head, or could such an attribute be requested?"
415,Extracting the lemma of word using Treetagger,"['python', 'python-3.x', 'python-2.7', 'nlp', 'treetagger']","I wrote this small script to extract lemma of words in a bunch of sentences. The function works but for some reasons it did not extract lemma for some sentences.I use this function but for the word ""donne"" it gives ""donne"" as lemma whereas it is ""donner"" as we see in the example below:The word ""donne"" look like this when using treetagger ""['donne', 'VER:pres', 'donner']"" but it is not extract ? Any idea why ?#Result :"
416,official module in tensorflow examples at tensorflow.org,"['python', 'tensorflow', 'nlp', 'bert-toolkit']","I've been following a tensorflow tutorial https://www.tensorflow.org/official_models/fine_tuning_bertIn the first code snippet, I saw a lot of imports from official moduleAnd problem is that i found no module name official.
I guess this official module somehow related to problem specific or for BERT model(from tf-hub).
As bert model uses specific text preprocessing and official module is providing this.So, where i can find, download, use and make imports from this official module to work? I've been using python 3.7, tf-2.2, tf-hub-0.8.0Please help me out"
417,"Training a GPT-2 from scratch in Greek-text, results in a low perplexity score of 7 after 15 epochs. Is it normal that score?","['python', 'deep-learning', 'nlp', 'transformer', 'huggingface-transformers']","I try to train a GPT-2 from scratch in Greek with the run_language_modeling.py (https://github.com/huggingface/transformers/tree/master/examples/language-modeling) script from HuggingFace repo, but I get a low perplexity score of 7 after 15 epochs.My data for train is about 4.6Gb and is constructed as 5 sentences per line. The data for the evaluation is about 450Mb constructed with the same way. Use of BPE for the encoding with a vocab of 22000 merges.
Any idea is appreciated."
418,NLP Stemming and Lemmatization using Regular expression tokenization,"['python', 'python-3.x', 'nlp', 'nltk']","Define a function called performStemAndLemma, which takes a parameter. The first parameter, textcontent, is a string. The function definition code stub is given in the editor. Perform the following specified tasks:1.Tokenize all the words given in textcontent. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in tokenizedwords. (Hint: Use regexp_tokenize)Convert all the words into lowercase. Store the result into the variable tokenizedwords.Remove all the stop words from the unique set of tokenizedwords. Store the result into the variable filteredwords. (Hint: Use stopwords corpora)Stem each word present in filteredwords with PorterStemmer, and store the result in the list porterstemmedwords.Stem each word present in filteredwords with LancasterStemmer, and store the result in the list lancasterstemmedwords.Lemmatize each word present in filteredwords with WordNetLemmatizer, and store the result in the list lemmatizedwords.Return porterstemmedwords, lancasterstemmedwords, lemmatizedwords variables from the function.My code:Still the program is not working fine. Not passing the 2 test cases. Highlight the mistake in above code and provide alternate solution for the same."
419,Bucket a new keyword based on the synonyms already created,"['nlp', 'data-modeling', 'n-gram']",I have a list of buckets with the synonyms which have been tagged manually. Some examples are shown below:Is there a way I can tag a new n-gram (except the synonyms already present) I obtain from the corpus into one of the already present buckets?  Is there a way to train these synonyms so that my model can predict the buckets based on the new n-gram input it gets.Any help would be appreciated... Thanks in advance :)
420,Albert Model Display and Copying weights inside Albert,"['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert']","I wanted to try something.There are 2 experiments, one on Bert an one on Albert. The task is, Train Bert upto Layers 5 , keeping the other 7 fixed, and then during test time, copy the weights of the 5th layer onto layers 6-12. I was successfully able to do this, as Bert has seperate parameters, and I can manipulate them seperately.Whereas in Albert, the Architecture is such that all the weights are shared from layers 1-12, so, although I can train them till layer 5, they aren't accessible seperately, so, I am unable to copy over the weights onto further layers.Any suggestions how I might go about this?I tried printing the entire model, for both Bert and Albert.And so on, for layers uptil 11. (They are seperately accessible).For Albert:The weights can't be copied directly, as they are the same weights. (I want to have weights trained only till layer 5, and then copy layer 5 weights till layer 12, without training it explicitly. How do I go about this?'''"
421,"ner, spacy,sentence segmentation","['python', 'nlp', 'spacy']","I want to break this sentences in order to process it using spacyI want to result be like this :It means two sentences, the first one should finish after lat. 2° 30' S ^37. but since lat. has a dote , it breaks the sentences after lat.but I did not find the solution till now I have used these 2 approaches:andI  think some small mistakes in the first code.both above methods do not work to split the sentences as desired!generally, what do you recommend in order to segment paragraph to sentences? (especially when we have) such abbreviation cases lie"
422,Training Data Format with Spacy,"['machine-learning', 'nlp', 'artificial-intelligence', 'spacy']","I am trying to build NLP with Spacy, but I am having trouble formatting the training data. I want my app to be able to recognize entities and intents. For example, in ""I want to place an order for pizza"". The intent would be ""place_order"" and the entity would be pizza. How do I format the training data for BOTH entities and intents in Spacy?"
423,CatBoostClassifier for multiple parameters,"['python', 'nlp', 'catboost']","I have the following text data for classifierI want to predict 2 values in my data: country, sport.
Example: 1) USA | basketball; 2) UK | footballCurrently I'm using CatBoostClassifier() to predict a single value (e.g. country):Can I use the classifier to predict 2 or more values and get combined model?"
424,How to apply Named Entity Recognition function to all columns and return column names that meets criteria,"['python', 'nlp']","I have the following code I am using to identify if a column type is ""GPE"" or not, which means that a field contains the name of a Geo-Political Entity.However, it can only be applied to one column at a time and I am wondering how could I modify it so that it could be applied to all the columns within the data frame and then only return the name of columns that contain more than one ""GPE"" as the data type.Desired output (Showing the column 'text' because both rows are GPE) :"
425,spaCy: Scispacy abbreviation large document,"['python', 'nlp', 'spacy', 'chunking']","I found this post looking for a way to identify and clean abbreviations within my dataframe. The code works well for my use case.However, I'm dealing with a large data set and was wondering if there was a better or proficient way to apply this without dealing with memory issues.In order for me to run the code snipet, I sampled 10% of the original dataset and it runs perfectly. If I run the full dataset, my laptop locks.Below is updated version of the original code:"
426,How to write a RNN with RNNCell in pytorch?,"['nlp', 'pytorch', 'recurrent-neural-network', 'seq2seq']",I am trying to rewrite a code from this simple Vanilla RNN to RNNCell format in pytorch. This is the full codeI am trying to emulate the original RNN + fully connected class object from the tutorial and reusing many of its code. I computed the rnn_out and appended its value in a python listbecause its a python list I cant further execute the code and it will result in this errorAttributeError: 'list' object has no attribute 'view'How should I write a RNN using RNNCell?
427,Elasticsearch language Analyzers - return the the retrieved field after text analysis,"['elasticsearch', 'nlp']","i am working on full-text search engine in Elasticsearch and using multilingual data in index time. i used elasticsearch for text analysis and i would like to be able to return the tokens (retrieved index) after the preprocessing. I know about Analyze API but doing this for +200.000 documents is very time consuming. I found ""terms aggregation"" but i am not sure how it works. Any ideas?i used in the mapping language analyzers. Is there any out-of-the-box language detection when using language analyzers or every document is passing by every language analyzer? If so, does it make sense to work with language detection and create multifields for each language? What is the different between using language analyzers in settings or in mappings?"
428,removing triangular bullets from text in python,"['python-3.x', 'string', 'text', 'replace', 'nlp']",I have the following texts that have special triangular bullets - arrow heads in it. How do I remove them and get a clean text trimmed? I pasted the image as I was not able to type the character hereI have tried the following until now that doesn't seem to work:
429,Representing sentence as Graph Neural Networks - NLP,"['python', 'tensorflow', 'nlp', 'pytorch', 'word-embedding']",I am new to the field of NLP and I am looking to express a text sentence in terms of Graph Neural Network with Nodes and Edges where edges define the semantic relationship. Is there any library in Python on the same or is it purely subjective? I know Pytorch Geometric helps us to manipulate our GNNs. But my question is to seek help in bringing the text sentence to a form that can later be processed using Pytorch Geometric.
430,Return rows with some text in them and delete the rest [duplicate],"['python', 'pandas', 'nlp', 'drop']","I have a dataframe df with values as below:I only need the rows where there is certain text. For e.g rows which have blank value like row 3 and row6 need to be removed.but still i get the same result. I feel these are not null values but spaces, so I also tried below:But still same result. Row 3 and 6 are still not removed. What to do?"
431,NLP - Python - Conditional Frequency Distribution,"['python', 'nlp', 'corpus']","I am trying to solve a question in hackerrank, which determine conditional frequency distribution of all the words(lowercase and removing stop words) for the given category 'cfdconditions', and events 'cfdevents'. Also compute conditional frequency distribution of category 'cfdconditions'  and events ending with 'ing' or 'ed'. And then display frequency modal for both distributions.My code is -But result is still failing for 2 test cases, for which my output is -andIf anyone can help me for the solution, it will be of great help."
432,en_core_web_md installation fails,"['python', 'nlp', 'spacy']","I've installed spaCy (version 2.3.0) and now I'm trying to install one of the models, namely en_core_web_md. I'm running the command from their website, which is python -m spacy download en_core_web_md. When I run the command, I get this error: ValueError: cymem.cymem.Pool has the wrong size, try recompiling. Expected 64, got 48Here's the full traceback:How do I fix this? For reference, I am using python 3.8.3"
433,Failed to run the tflite model on Interpreter due to Internal Error,"['tensorflow', 'nlp', 'tensorflow2.0', 'tensorflow-lite']","I am trying to build an offline translator for android. My model is highly inspired from this guide: https://www.tensorflow.org/tutorials/text/nmt_with_attention. I just did some modifications to make sure the model is serialisable. (You can find the code for the model at the end)The model works perfectly on my jupyter notebook. I am using Tensorflow version: 2.3.0-dev20200617, I also was able to generate the tflite file using the following snippet:However when I used the generated tflite model to get predictions on android, it throws the error java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:73 t->dims->data[d] != t0->dims->data[d] (8 != 1) Node number 84 (CONCATENATION) failed to prepare.This is strange because I have provided the exact same input dimensions as I did in my jupyter notebook. Here is the java code that is used to test (dummy inputs) if model runs on android:And here are my gradle dependencies:As the error pointed, there was something wrong with dimensions at node 84. So I went ahead and visualised the tflite file using Netron. I have zoomed the concatenation node, you can find the pic of the node along with input and output dimensions here. You can find the whole generated graph here.As it turns out, the concatenation node at location 84 isn't actually concatenating, you can see this from the input and output dimensions. It just spits out a 1X1X1 matrix after processing 1X1X1 and 1X1X256 matrix. I know tflite graph isn't same as the original model graph since a lot of operations are replaced and even removed for optimisations but this seems a little odd.I can't relate this to the error. And if it runs prefectly on jupyter, is it a framework issue or am I missing something? Also, could anyone please explain me what does the error mean by t->dims->data[d] != t0->dims->data[d] what is d?Please if you have answers to even any one of the question, please write it. If you require any extra details please let me know.Here is the code for the model:"
434,Why is the number of stem from NLTK Stemmer outputs different from expected output?,"['python', 'list', 'nlp', 'nltk', 'stemming']","I have to perform Stemming on a text. The questions are as follows :Below is my code :My code works perfectly with all the provided testcases in hand-on but it fails only for the below test case wheretc = ""I inadvertently went to See's Candy last week (I was in the mall looking for phone repair), and as it turns out, See's Candy now charges a dollar -- a full dollar -- for even the simplest of their wee confection offerings. I bought two chocolate lollipops and two chocolate-caramel-almond things. The total cost was four-something. I mean, the candies were tasty and all, but let's be real: A Snickers bar is fifty cents. After this dollar-per-candy revelation, I may not find myself wandering dreamily back into a See's Candy any time soon.""My Output is :['almond', 'back', 'bar', 'bought', 'candi', 'candi', 'caramel', 'cent', 'charg', 'chocol', 'confect', 'cost', 'dollar', 'dreamili', 'even', 'fifti', 'find', 'four', 'full', 'inadvert', 'last', 'let', 'lollipop', 'look', 'mall', 'may', 'mean', 'offer', 'per', 'phone', 'real', 'repair', 'revel', 'see', 'simplest', 'snicker', 'someth', 'soon', 'tasti', 'thing', 'time', 'total', 'turn', 'two', 'wander', 'wee', 'week', 'went']Expected Output is :['almond', 'back', 'bar', 'bought', 'candi', 'candi', 'candi', 'caramel', 'cent', 'charg', 'chocol', 'confect', 'cost', 'dollar', 'dreamili', 'even', 'fifti', 'find', 'four', 'full', 'inadvert', 'last', 'let', 'lollipop', 'look', 'mall', 'may', 'mean', 'offer', 'per', 'phone', 'real', 'repair', 'revel', 'see', 'simplest', 'snicker', 'someth', 'soon', 'tasti', 'thing', 'time', 'total', 'turn', 'two', 'wander', 'wee', 'week', 'went']The difference is the occurrence of 'Candi'Looking help to troubleshoot the issue."
435,Spacy: automatically find lemma patterns in text,"['nlp', 'pattern-matching', 'spacy']","I'm learning how to use Spacy.
Based on the example below, my goal is to get more patterns of lemmas that are often associated with the word iPhone (I have a text database where such patterns can be found).
For example ""iPhone is the best smartphone"", ""iPhone is too expensive"", etc.
Do I need to find those patterns by hand. Or is it possible to make this automatic (at least to get suggestions or something like that).My final goal is to build a tool that would take as input some text and based on those patterns identify iPhone, Samsung abc, etc..."
436,Webscrape contact information with Python - BeautifulSoup and NLP,"['python', 'nlp']","I've got quite an interesting challenge at my hands. I have been tasked with scraping contact information (name & email) from research websites like this one https://www.ntnu.no/ies/ansatteand put it in a csv document. If only the name is available, then grab the name. The scraper should work for other similar websites too. I have three different ways that I can come up with where each has its pros and cons. I would like to get some feedback as to how I can improve on this or any other approaches.Approach 1Since almost all work emails are in the form of first.last@company.com I can use BeautifulSoup to get all the emails and then just get the name from the email. This is an easy solution but some companies use initials for their emails and in those cases I would be unable to get the name. Additionally, if a persons email is missing I obviously wont get the name either, which ideally I do want.Approach 2Have a textfile with as many names as possible, then check each word on the website and if it is in the textfile then it is a name and I can simply find the matching email. This approach is very slow if the list contains like 50,000 names. Additionally, since many names are Scandinavian or not very common, this approach will miss quite a lot of names.Approach 3Use the spacy package to identify names. This approach is decent and I have tried using different languages but even the best one is missing a lot of names and is classifing a lot of words as name even though they are not. In other words, the accuracy is simply not good enough. My spacy code looks like this:What approach should I go for? Maybe I could try approach 1, and if that fails use either 2 or 3? Is there another better way?"
437,How to save word vectors in spacy,"['python', 'python-3.x', 'nlp', 'spacy']",I have the following code. The goal is to get a vector representation of each word in the list. My intention is to use these word vectors for other application purpose like word clustering.I realize i am doing something wrong in the code above. How to save the word vectors of each word in the list 'category'
438,"Tweets analysis: Get unique positive, unique negative and unique neutral words : Optimised solution:Natural Language processing:","['python', 'pandas', 'twitter', 'nlp', 'textblob']","I have a dataframe train, with a column tweet_content. There is a column sentiment which tells the overall sentiment of tweet. Now there are lot of words which are common in tweets of neutral, positive and negative sentiments. I want to find the words which are unique to each specific sentimenttrainP.S. Note that each tweet or row is a list of words for the column tweet_content.Expected output for the above tweets: (unique_positive, unique_negative etc is for all the tweets in the df. There are 30,000 rows. So unique positive will be the list of words which are unique for positive sentiment for all 30,000 rows combined. Here I have just taken 3 tweets as random egwhereProblem:
The below function runs perfectly and finds unique words for positive, neutral and negative sentiments. But the problem is it is taking 30 minutes to run. Is there a way to optimise this function and run it faster?.Function to find out the unique words for each sentiment:"
439,Luong Style Attention Mechanism with Dot and General scoring functions in keras and tensorflow,"['deep-learning', 'nlp', 'attention-model']","I am trying to implement the dot product and general implementation of calculating similarity scores from encoder and decoder output and hidden states respectively in keras.I have got the idea to do the product of tf.keras.layers.dot(encoder_output,decoder_state) for calculating product score but there is error in multiplication of these two values.I am getting the following error:It is a very simple fix but as I am new to this I am not able to get the exact method needed to be called here.
I have tried reshaping the values of encoder_output but still this does not work.
Request to help me fix this."
440,Is there a faster way to lookup dictionary indices?,"['python', 'dictionary', 'nlp']","I am trying to look up dictionary indices for thousands of strings and this process is very, very slow. There are package alternatives, like KeyedVectors from gensim.models, which does what I want to do in about a minute, but I want to do what the package does more manually and to have more control over what I am doing.I have two objects: (1) a dictionary that contains key : values for word embeddings, and (2) my pandas dataframe with my strings that need to be transformed into the index value found for each word in object (1). Consider the code below -- is there any obvious improvement to speed or am I relegated to external packages?I would have thought that key lookups in a dictionary would be blazing fast."
441,About the regularization used in sklearn Logistic Regression,"['machine-learning', 'scikit-learn', 'nlp', 'logistic-regression']","I am using the sklearn logistic regression function to do a binary classification task on texts.I did the task using three different methods: Bag-Of-Words, TF-IDF, Doc2vec.Inputs are: CountVectorizer for Bag-Of-Words, TfidfVectorizer for TF-IDF, fixed-length numeric document embedding for Doc2vec.I found a weird part about the regularization term I used.I used 1, 5, 10, 30 as the C value to do the experiments for each method.
For Bag-Of-Words, the performance of the model on the testing set downgrades as the C value increases, this is what I expect.However, for TF-IDF and Doc2vec methods, this is not the case, the performance of the model on the testing set doesn't downgrade as the C value increases.For example, for TF-IDF methods, the results are the followingWhy this is the case?In short, why the stronger regularization doesn't increase the testing performance for TF-IDF and Doc2vec, but increase the performance for Bag-Of-Words?"
442,NLP- sentiment analysis using word vectors,"['python', 'nlp', 'nltk', 'stanford-nlp', 'sentiment-analysis']","I have a code that does the following:Now I run a loop taking i words at a time where i ranges from 1 to sentence length. For a particular i, I have few windows of words that span the test sentence. For each window, I take average of word vectors of the window and compute euclidian distance of this windowed vector and the 2 lists.For example i= 3, and test sentence: food looks fresh healthy. I will have 2 windows: food looks fresh and looks fresh healthy for i =3. Now I take mean of vectors of the words in each window and compute euclidian distance with the good_words and bad_words. So corresponding to each word in both lists I will have 2 values(for 2 windows). Now I take mean of these 2 values for each word in the lists and whichever word has least distance lies closest to the test sentence.I wish to show that window size(i) = 3 or 4 shows highest accuracy in determining the sentiment of test sentence but I am facing difficulty in achieving it. Any leads on how I can produce my results would be highly appreciated.Thanks in advance."
443,"Textblob tweets : TypeError: The `text` argument passed to `__init__(text)` must be a string, not <class 'pandas.core.series.Series'> , rows are lists","['python', 'pandas', 'dataframe', 'nlp', 'textblob']",Defining a function to get sentiment out of tweets: (written by me)Calling function to get sentiment:Error:My df is of the type below:'Please help how to either change the function or the way to call the function
444,Replace all collocations in a text file with a dictionary of collocations in python,"['python', 'python-3.x', 'nlp', 'nltk', 'information-retrieval']",I'm trying to replace substrings in a text file [corpus.txt] with some other substrings[collocation|ngram] using python. I have the list of possible substrings in a file sub.txt containing the following:and a corpus.txt containing some texts as below:with the desired outputAnd my python code with multiprocessing (used multiprocessing due to the size of the corpus and sub)I added already initialized corpus and sub variable (in the snippet above) to show how the code works.  Both corpus.txt and sub.txt contains millions (200M+ and 4M+ respectively) of lines in the actual setting. I needed a code that can do the task efficiently and I have tried Multiprocessing with pool but it is going to take weeks to complete. Are there other efficient and fast ways to go about the task??
445,How to split text to sentences when my text has many dots in-between sentences? [closed],"['python', 'text', 'split', 'nlp']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed last month.Requirements are:Starting of the sentences must be a word with its first letter in capitals ( As, The, In) ONLY and the end of the sentence should be a dot.
eg:Input: ""This is a new book. I like to read this book"" Expected Output: ['This is a new book.' , 'I like to read this book']However, if there is citation present at the end of the sentence, it should be included in the sentence. In this case, the dot might be after the citation (or) before and after the citation.eg:Input: ""This is a new book.(Steve and Rasol 2014). I like to read this book (Rashi & Shabana 2015).""Expected Output: ['This is a new book.(Steve and Rasol 2014)' , 'I like to read this book (Rashi & Shabana 2015).']"
446,How to cluster spacy vectors (word embedding) into groups using Annoy or other similar algorithms,"['python', 'nlp', 'spacy', 'nearest-neighbor', 'annoy']","I have a list of words whose vector embeddings I got by using spacy's pre trained model en_core_web_lg.My questions are two foldCan these word vectors be fed into Annoy like algorithm?
Can I get say 20 groups, each group containing around 100 words?I would be grateful for any resources/code."
447,Remove Emoji with PySpark,"['pyspark', 'nlp', 'emoji']","I want to use PySpark to efficiently remove Emoji (e.g., :-)) from 1 billion records. How could I achieve this using pyspark syntax?"
448,"what would be the regex pattern to identify in text citation - ""(author name , year)'?","['python', 'regex', 'pandas', 'dataframe', 'nlp']","I have converted list of tokenized sentences to a dataframe. Now, I need to filter the rows (sentences) that has citation in it.Example dataframe:Expected Output:I have been using the below code with different patterns, r'[(]\w+,\d{4}[)]', r""[(\w+\s+, \d{4})]"
449,How do I get tokens to create NLP model?,"['python', 'nlp', 'valueerror']","I have a code that contains a set of list of items . I need to pass it to an NLP model which is , en_core_web_sm to tag each of the element present in the set.The size of this is 131806. The NLP model accepts only a size of 1000000 with input being a string and not a set. When I convert the set(food_item['Source']) into a list(set(food_item['Source']), I have a size of 3935131. How do I input the whole of this model in order for the NLP to detect?The NLP code is as follows:For this I get an error like this :"
450,"In Spacy NLP, how extract the agent, action, and patient — as well as cause/effect relations?","['nlp', 'spacy', 'dependency-parsing']","I would like to use Space to extract word relation information in the form of ""agent, action, and patient."" For example, ""Autonomous cars shift insurance liability toward manufacturers"" -> (""autonomous cars"", ""shift"", ""liability"") or (""autonomous cars"", ""shift"", ""liability towards manufacturers""). In other words, ""who did what to whom"" and ""what applied the action to something else."" I don't know much about my input data, so I can't make many assumptions.I also want to extract logical relationships. For example, ""Whenever/if the sun is in the sky, the bird flies"" or cause/effect cases like ""Heat makes ice cream melt.""For dependencies, Space recommends iterating through sentences word by word and finding the root that way, but I'm not sure what clear pattern in traversal to use in order to get the information in a reliable way I can organize. My use case involves structuring these sentences into a form that I can use for queries and logical conclusions. This might be comparable to my own mini Prolog data store.For cause/effect, I could hard-code some rules, but then I still need to find a way of reliably traversing the dependency tree and extracting information. (I will probably combine this with core resolution using neuralcoref and also word vectors and concept net to resolve ambiguities, but this is a little tangential.)In short, the question is really about how to extract that information / how best to traverse.On a tangential note, I am wondering if I really need a constituency tree as well for phrase-level parsing to achieve this. I think that Stanford provides that, but Spacy might not."
451,Is it possible to subclass a spacy entity type?,"['python', 'machine-learning', 'nlp', 'spacy', 'ner']","I'd like to subclass the existing GPE, so that it differentiates between GPE-Nation, USA, and GPE-City, New York. I see in the docs how to create new entity types, but not how to subclass what's already there. Can this be done, and if so, how? Thanks."
452,How to obtain contextual embedding for a phrase in a sentence using BERT?,"['nlp', 'bert']","I use https://github.com/UKPLab/sentence-transformers to obtain sentence embedding from BERT. Using this I am able to obtain embedding for sentences or phrases. For example: I can get embedding of a sentence like ""system not working given to service center but no response on replacement"". I can also get embedding of a phrase like ""no response"".However I want to get embedding of ""no response"" in the context of ""system not working given to service center but no response on replacement"". Any pointers on how to obtain this will be helpful. Thanks in advance.I am trying to do this because the phrase ""no response"" has different contexts in different sentences. For example the context of ""no response"" is different in the following two sentences:
""system not working given to service center but no response on replacement""
""we tried recovery procedure on the patient but there was no response"""
453,Counting most frequently mentioned adjectives related to noun using spacy,"['python', 'nlp', 'spacy']","I have a dataframe with text where I want to count the most frequent adjectives related to the word ""quality"" using Python.In this case the result should look like:('great', 2),
('horrible', 1)I have tried using spacy and POS tagging but didn't come close to a solution and I would highly appreciate suggestions or different packages that can help me tackle this problem."
454,learning rate AdamW Optimizer,"['deep-learning', 'nlp', 'huggingface-transformers', 'learning-rate']","I train with BERT (from huggingface) sentiment analysis which is a NLP task.My question refers to the learning rate.Can you please explain how to read 1e-3?Is this the density of steps or is this a value to decay.If the latter, is it a linear decay?If I train with a value 3e-5, which is a recommended value of huggingface for NLP tasks, my model overfits very quickly: loss for training decreases to a minimum, loss for validation increases.Learning rate 3e-5:If I train with a value of 1e-2, I get a steady improvement in the loss value of validation. but the validation accuracy does not improve after the first epoch. See picture. Why does the validation value not increase, even though the loss falls. Isn't that a contradiction? I thought these two values were an interpretation of each other.Learning rate 1e-2:What would you recommend?"
455,TypeError: 'numpy.longlong' object is not iterable,"['python', 'pandas', 'numpy', 'nlp']","For a current project, I am running a number of loops through a Pandas DataFrame to detect the most common word pairs (bigrams) in the set. The output freq is indicating how many times each bigram has been found.As a next step, I would like to sum up the freq numbers received. The command sum(freq) is however yielding the following output:  TypeError: 'numpy.longlong' object is not iterable.Does anyone know how I can sum up the numbers/how to solve the error?"
456,Optimizing the runtime for cleaning n-grams,"['python', 'optimization', 'nlp', 'multiprocessing']","I am trying to clean my n-grams obtained from the text column. I also have 2 list of stopwords I want to remove but only at specific places(when it occurs as a first word in the n-gram or occurs as a last word in the n-gram) and I am also looking to remove my n-grams which contain only numbers or %. The following code takes over 10 minutes to process around a million n-grams.I have also tried multiprocessing but I had to terminate the process after 30 minutes as it was still running. The following is the code for the same:Is there any way I can optimize my code for a considerably less run-time? Any help would be appreciated.
Thanks in advance :)"
457,NLP : Get 5 best candidates from QuestionAnsweringPipeline,"['nlp', 'huggingface-transformers', 'bert', 'question-answering']","I am working on a French Question-Answering model using huggingface transformers library. I'm using a pre-trained CamemBERT model which is very similar to RoBERTa but is adapted to french.Currently, i am able to get the best answer candidate for a question on a text of my own, using the QuestionAnsweringPipeline from the transformers library.Here is an extract of my code.I am currently getting this :{'score': 0.9630325870663725, 'start': 2421, 'end': 2424, 'answer': '{21'} , which is wrong.Therefore, i would like to get the 5 best candidates for the answer. Does anyone have an idea how to do that ?"
458,Working Azure HTTP function app breaks upon publishing,"['python', 'azure', 'nlp', 'spacy']","I am developing a natural language processing function with visual studio code and Microsoft Azure. The function triggers on an http request, and works perfectly on my local host during testing. However, as soon as I publish the function to Azure, the function becomes unresponsive, citing a ""500 internal error."" I believe the problem might have something to do with a specific dependency not being installed upon publishing.My ""requirements.txt"" file looks like thisAnd lastly, in order to run the function on my local machine, I download the ""en_core_web_sm"" package. I am able to quickly do this by running ""python -m spacy download en_core_web_sm"" in my virtual environment. If I do not use the nlp = spacy.load('en_core_web_sm') line in my code, the function will publish successfully.My worry is that this dependency is not included in the function once published. I have tried to solve this problem by adding the github download string - https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz - in my requirements.txt folder, as I am able to download this using pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.3.0.tar.gz. After the addition, the requirements.txt folder looks like this:I have also attempted to configure the necessary command in my ""tasks.json"" folder, changing it to include the command as such:I would appreciate any help as this is a time sensitive issue. Thank you so much!"
459,How to add custom companies as ORG entity in inbuilt spacy model “en_core_web_lg”?,"['nlp', 'spacy', 'ner']",I have few companies which are not in current in-built model en_core_web_lg of spacy.How to add my new companies in this exiting spacy model en_core_web_lg?I have gone through many blogs but none of them has mentioend how to add custom company names in existing model.Is there any way to leverage existing model without training totally new model?For example# https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/
460,How can I transform user Input exactly as processed data that I used to train the classifier? I want to perform sentiment analysis on user input text,"['python', 'python-3.x', 'nlp', 'nltk', 'sentiment-analysis']","I want to perform sentiment analysis using NLTK in python on a sentence that is an input from the user as either voice input or text, but I cannot understand how to perform sentiment analysis. I have searched but most of the search results show movie reviews data or twitter sentiment analysis. Kindly help.
The code I used to Train the classifier is as:Instead of test_set i want to use my own sentence"
461,"Parsing, formatting and generating data based on input","['php', 'machine-learning', 'nlp', 'text-mining']","For some known inputs I have some known outputs/results. Based on this I want the program to generate result based on the input as per pre-filled input-results data.Example input:Enjoy your tea in the morning then have some bread in the lunch. Enjoy the taste of a garlic chicken in the dinner.
Your day starts with cold coffee. In the noon have some rice and fish curry.Example output:Have tea in the morning. Have some bread in the lunch. Have garlic chicken in the dinner.
Have cold coffee. Have some rice and fish curry.I don't want to use string replace or regexp as it will break often. How or where do I start ?"
462,Training SpaCy NER with a custom dataset,"['python-3.x', 'nlp', 'spacy', 'ner']","I have followed this SpaCy tutorial for training a custom dataset. My dataset is a gazetteer. Therefore, I made my training data as the following.I used 'en_core_web_sm' for the training, not a blank model.After training for 20 epocs, I tried to make a prediction with the trained model. The following is the output that I get.I did a prediction using 'en_core_web_sm' for the same test_text. The output is the following.Can someone please instruct me on the errors that I am making while training SpaCy?"
463,Pytorch word embeddings results in nan values,"['python', 'nlp', 'pytorch', 'word2vec']","After training a word embedding model on a large-ish corpus, my embeddings converge to nan values.The model is very simple (skipgram with negative sampling)full model:and training loop and optimizer codeThis model is about as simple as it gets so I am a bit surprised I am having this issue. Everything is fine on a smaller corpus (7k batches of size 128) but gets nan-y with a larger corpus (200k batches of size 128).Anyone see anything immediately wrong or have any tips to figure out what is going?Any help is appreciated. Also pretty new to pytorch - so if you see anything that is a bit dumb feel free to point it out :)"
464,Keras Tokenizer sequence to text changes word order,"['python', 'tensorflow', 'keras', 'nlp', 'tokenize']","I am training a model on DUC2004 and Giga word corpus, for which I am using Tokenizer() from keras as follows:When I try to change the sequences back to texts, the word order changes and gives some weird output.For example:Actual sentence:chechen police were searching wednesday for the bodies of four
kidnapped foreigners who were beheaded during a botched attempt to
free themSequence to text conversion:police were wednesday for the bodies of four kidnapped foreigners who
were during a to free themI tried using the sequence_to_text() method of Tokenier() as well as mapping words using the word_index.I am not able to understand why this happens and how to correct it."
465,“KeyError: True” when matching Pandas DataFrame,"['python', 'pandas', 'dataframe', 'nlp']","I am planning to set up a simple script to see if words from a wordlist can be found in a Pandas DataFrame common_words. In case of a match, I would like to return the corresponding DataFrame entry, while the DF has the format life balance 14, long term 9, upper management 9, highlighting the word token and its occurrence number.The code below is however currently only printing the error KeyError: True with regards to line print('Group 1:', df[df[i].loc[df[i].str.contains(x).any()]]). Does anyone know a smart way to return the DataFrame output word instead of the error?The relevant code section is:The full code segment looks as follows:As requested, I am enclosing below an excerpt from the JSON file:"
466,Can we create dynamic responses using business data from webhook in dialogflow,"['nlp', 'dialogflow', 'chatbot', 'dialogflow-fulfillment']",Can we create textual response mixing with business data from the backend database without writing code for every intent?
467,How to get indices of words in a Spacy dependency parse?,"['python', 'nlp', 'spacy', 'pos-tagger', 'dependency-parsing']","I am trying to use Spacy to extract word relations/dependencies, but am a little unsure about how to use the information it gives me. I understand how to generate the visual dependency tree for debugging.Specifically, I don’t see a way to map the list of children of a token to a specific token. There is no index—just a list of words.Looking at the example here: https://spacy.io/usage/linguistic-features#dependency-parsenlp(""Autonomous cars shift insurance liability toward manufacturers"")Also, if the sentence were nlp(""Autonomous cars shift insurance liability toward manufacturers of cars”), how would I disambiguate between the two instances of cars?The only thing I can think of is that maybe these tokens are actually reference types that I can map to indices myself. Is that the case?Basically, I am looking to start with getting the predicates and args to understand “who did what to whom and how/using what”."
468,Data PreProcessing for BERT (base-german),"['deep-learning', 'nlp', 'data-cleaning', 'bert']","I am working on a sentiment analysis solution with BERT to analyze tweets in german. My training dataset of is a class of 1000 tweets, which have been manually annotated into the classes neutral, positive and negative.The dataset with 10.000 tweets is quite unevenly distributed:approx.
3000 positive
2000 negative
5000 neutralthe tweets contain formulations with @names, https links, numbers, punctuation marks, smileys like :3 :D :) etc..The interesting thing is, if I remove them with the following code during Data Cleaning, the F1 score gets worse. Only the removal of https links (if I do it alone) leads to a small improvement.also steps like stop words removal or lemmitazation lead more to a deterioration. Is this because I do something wrong or can the model BERT actually handle such values?A second question is:I found other records that were also manually annotated, but these are not tweets and the structure of the sentences and language use is different. Would you still recommend to add these records to my original?There are about 3000 records in German.My last question:Should I reduce the class sizes to the size of the smallest unit and thus balance?"
469,Is there an Amazon AWS service we can leverage to detect correlation between two texts?,"['amazon-web-services', 'nlp']","We're a database of Q&A from customers (much like StackOverflow :-P) and we're working on a bot to automatically identify the most likely response to a ticket being opened. e.g.Basically we want to create suggestions for customer care operators pointing them to known answers.We're using Amazon AWS as a platform of choice and we'd like to offload this to existing APIs (rather then resorting to deploying a dedicated NPL solution).To be clear we don't want to write/deploy any code, we just want to use AWS APIs/servicesIs there any service which will automatically detect/classify the ""most similar request"" and allow us to provide suggestions? e.g. some kind of text correlation APIs?e.g. something like"
470,Build vocabulary only from training data or entire data?,"['nlp', 'pytorch', 'recurrent-neural-network', 'word-embedding', 'vocabulary']","Should I build the vocabulary only from train data or all data, wouldn't that effect test data in both ways? I mean :If we only build the vocab from train data, The model wouldn't recognize a lot of the words in the validation and testing data, if the word is not available in the vocabulary.Would considering a pre-trained word embedding help in this situation (i.e. the model learns the new word not from training data but from the pre-trained word embedding)?If yes, Would a randomly Initialized word embedding have the same effect?On the contrary, I've seen many examples where the coders build their vocab from the entire data,  testing and validation data are shared with training data. Wouldn't this be an obvious data leakage problem?"
471,Compare words and return Pandas DataFrame entry,"['python', 'pandas', 'dataframe', 'nlp']","I am planning to set up a simple function to see if words from a wordlist can be found in a Pandas DataFrame common_words. In case of a match, I would like to return the corresponding DataFrame entry, while the DF has the format life balance 14, long term 9, upper management 9, highlighting the word token and its occurrence number.The code below is however currently only printing the search term from the wordlist (i.e. life balance), not the DataFrame entry that includes the occurrence count. I would hence need to find a way to return word instead of the wordlist element. Where is my error in reasoning?The relevant code section is:The full code segment looks as follows:"
472,NLTK Name entity recognition in C# for dutch language,"['c#', 'nlp', 'stanford-nlp', 'ner']",I am working on a C# project where i need to find person names in a paragraph in dutch language. So can anyone help me to find c# program that give people name  output in dutch paragraphs
473,How to run a keyword frequency-count on a pandas df,"['python', 'pandas', 'nlp']","I have a list of articles saved in a column (class type: list) in a Pandas data frame which I would like to run a keyword frequency-count on using a predefined list of words (which I have in another excel file).I'm more or a less a complete novice at Pandas, so would appreciate any insight into how to do this. I've done it once before with a .txt file (last bit of code once I'd finished preprocessing below) so if it's anything like this please do say!Here is an example of one of my articles as a row in my dataframe"
474,Removing compound worded named entities from a document using spacy,"['python', 'nlp', 'spacy', 'named-entity-recognition']","How can one use spaCy to remove named entities from a text if some of the named entities are compound words?I am aware of the question at Removing named entities from a document using spacy
I believe this is not a duplicate of that question, because the accepted answer posted there will fail if the Named Entities are compound words.
Example code for why the accepted answer to the linked question fails appears below.Output:What is the best way to remove named entities from text, including compound word entities?
Thanks.P.S. I would have posted this as a comment to the linked question, but as a new user I lack sufficient reputation to comment.  I tried posting it as an answer there, but since I don't know the solution (only that the accepted answer will fail with compound words) it wasn't a good answer and was deleted.  A new question seemed to be the last recourse left to me, but if this was not appropriate any advice as to the correct course of action in a situation like this would be appreciated."
475,pyspark word2vec error thrown when returning the array from findSynonymsArray,"['python', 'apache-spark', 'pyspark', 'nlp', 'word2vec']","I am using pyspark2.4.5. I have word2vec model . Now I need to find the synonym on list of words from one of the column in data frame . Below is the codeApproach 1 Tried: Using MapApproach 2 Tried: Using UDFInput:From above words is the column which has words for ex: applicable,tuned etcWhen I give for individual word it is working fine:When I tried with list like below working fineBut I need samething to get successful for entire column,Tried above approaches I get error as below :"
476,How to reset RNN model states of a model being served on docker?,"['docker', 'tensorflow', 'nlp', 'recurrent-neural-network', 'tensorflow-serving']","I'm in the process of attempting to deploy an NLP RNN (akin to https://www.tensorflow.org/tutorials/text/text_generation) through Tensorflow Serving. When testing the model locally, I'm able to reset the states of the model on each new batch, but I can't seem to find the solution to resetting the states of models being served through TF Serving.When creating a local model stored in memory, the following simple line of code resets the states on each new batch/prediction:However, using Tensorflow Serving, I can't seem to find a solution to how this can be done through REST API or a call to the model.I'm using Docker to host a container for two models; I've only used a simple implementation as I'm very new to Docker:I've managed to write a piece of code which generates a body of text, on a character by character basis, through individual REST calls.Now the issue here clearly is that once the model is served, it continues to retain model states regardless. Where the ""#model.reset_states()"" sits in the method above would be the ideal place to send a command to the served model, but I can't seem to find any information as to how this can be done, if possible.Any assistance would be greatly appreciated!Thanks"
477,NLP language to launage translation with sentence embedding instead of word embedding,"['nlp', 'pytorch']","I am following this tutorial, and could replace the embeddings with glove. I am wondering if it would make sense to use sentence embeddings instead of word embeddings in that case. options for sentence embeddings that I found are: this and this.Any help in this regard would be appreciated."
478,Using MOSES or PHRASER to translate in python?,"['python', 'nlp', 'stanford-nlp', 'machine-translation', 'moses']","I'm trying to use statistical machine translations in python, and so far, I've been mostly using translate library. However, I heard that Moses, Phraser do a better job of machine translations, so I was wondering if there are ways of using them in python.Is it possible?Any explanation would be greatly appreciated."
479,Differentially generate sentences with Huggingface Library for adversarial training (GANs),"['nlp', 'pytorch', 'huggingface-transformers', 'gan', 'discriminator']","I have the following goal, which I have been trying to achieve with the Huggingface Library but I encountered some roadblocks.The Problem:I want to generate sentences in a differentiable way at training time. Why am I doing this? I want to apply a discriminator to this output to generate sentences with certain properties, which are ""enforced"" by the discriminator. These sentences will also be conditioned on a input sentence, so I need a Encoder Decoder Model.To get around the non differentiability of argmax, I simply take the softmax output of the decoder and multiply it with my embedding matrix. Then I am taking this embedded input and feed it into a transformer discriminator, which simply classifies the input as original/fake. Then I backpropagate through the encoder decoder. Just as one would do it with a normal GAN.So far I have tried to use the EncoderDecoderModel from Huggingface. This class has a method named generate, which generates sentences in a non differentiable way (greedy or beam-search). So I dug through the source code and tried to build my own differentiable generate method. I didn't get it to work though.Questions:Thanks for your help, I would really appreciate it, I have been stuck on this for quiet a while now."
480,Check if word in Pandas DataFrame,"['python', 'dictionary', 'nlp']","I am planning to set up a simple function to see if words from a wordlist can be found in a Pandas DataFrame common_words. In case of a match, I would like to return the corresponding DataFrame entry, while the DF has the format life balance 14, long term 9, upper management 9, while always showing a word pair and its number of occurrence.The code below is however currently only printing the search term from the wordlist, not the results that include the occurrence count. Where is my error in reasoning?The relevant code section is:The full code is:"
481,How to visualize the size of a word depending on its value,"['python', 'matplotlib', 'nlp', 'data-visualization', 'word-cloud']","I don't know if this is possible, or how to even approach this, but is there a way to do the follow?I have a df that looks like this:It is the probability distribution output of a classification problem. I want to print out the words (column[0]), but have the font size reflect the probability value (column[1[) for that row. Kind of like a word cloud, which prints words in bigger font when they have higher frequency.Any ideas on how to approach this?"
482,Huggingface GPT2 and T5 model APIs for sentence classification?,"['python', 'machine-learning', 'nlp', 'pytorch', 'huggingface-transformers']","I've successfully used the Huggingface Transformers BERT model to do sentence classification  using the BERTForSequenceClassification class and API. I've used it for both 1-sentence sentiment analysis and 2-sentence NLI.I can see that other models have analogous classes, e.g. XLNetForSequenceClassification and RobertaForSequenceClassification. This type of sentence classification usually involves placing a classifier layer on top of a dense vector representing the entirety of the sentence.Now I'm trying to use the GPT2 and T5 models. However, when I look at the available classes and API for each one, there is no equivalent ""ForSequenceClassification"" class. For example, for GPT2 there are GPT2Model, GPT2LMHeadModel, and GPT2DoubleHeadsModel classes. Perhaps I'm not familiar enough with the research for GPT2 and T5, but I'm certain that both models are capable of sentence classification.So my questions are:What Huggingface classes for GPT2 and T5 should I use for 1-sentence classification?What classes should I use for 2-sentence (sentence pair) classification (like natural language inference)?Thank you for any help."
483,How do i retain the objects as spacy objects instead of list object after simplifying the code using list comprehension?,"['python', 'list', 'for-loop', 'nlp', 'spacy']",Below is the actual codeI did list comprehension as shown belowdata is now a list which is limiting me from doing some further tasks
484,Which Universal Dependency Relations are helpful to identify modification part?,"['python', 'nlp', 'dependencies', 'spacy', 'linguistics']","ProblemI would like to know which Universal Dependency Relations are helpful to identify modifier parts on a sentence.Universal Dependencies have several tags to express relations between words and therefore, I think it is useful to sepalate each word or clause for sentence constituents.The below case, how can I identify a modifier part and automatically extract it as describe the place?For example,the subject of this sentence is identified as  ""Megan"" focusing on the relation
""nsubj: nominal subject"".the verb of this sentence is identified as  ""play"" focusing on the root.the object of this sentence is identified as  ""tennis"" focusing on the relation
'dobj'What I didI visualized the relatios using python and spaCy.
However, the modifier part, "" at school"" is described with two relations 'prop' and 'probj'.Could you give me advices how to identify modification part and its meaning, such as, 'place'?"
485,Tensorflow Lite input strings,"['android', 'tensorflow', 'nlp', 'tf-lite']",I wanted to run a TfLite model on my android app to do some Natural Language Processing. I was wondering in what format would I have to taken in input for the TFLite model? I'm having trouble finding necessary documentation.
486,How to Group Question Specific Similar Responses?,"['python', 'nlp', 'data-science']","I have a set of Questions and their responses. Each question can have unique, similar or same response. I am trying to create a bucket or cluster so that all similar or same responses fall. Any tips or suggestion would be appreciated. Here is my sample data and CLUSTER field is the one I want to generate.
"
487,Finding cells instead of words in R - tm package,"['r', 'nlp', 'tm', 'corpus']","I have a question about the tm package in R. Using the tm package I wish to make a frequency matrix using TermDocumentMatrix(). It worked for me to create this using the code below. However, I can not seem to select cells with the entire author, but because I need to create a corpus the termdocumentmatrix() function only selects part of the name of the authors.How can I use the termdocumentmatrix() function without losing the entire author names. A screenshot of the data is screenshotted below.As an example, I now get a lot of ""wang"" authors. However, there are a lot of different authors with the same name wang. I need to separate these authors by making the termdocumentmatrix() include the first names of these authors. Do you have any idea how to do this? Would be of great help!My data of authors of papers"
488,"ValueError: Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))","['google-cloud-platform', 'nlp', 'pytorch', 'huggingface-transformers', 'bert']","Reference github: fast-bertI used to run the following notebook to predict the multi label classification using Bert model that means I don't need GPU driver instead I can use CPU memory,
here is the reference for jupyter notebook multilabelIt's not the memory issue, How to resolve this error?While increasing CPU with RAM size, target size also getting increasing,I choose n1-standard-4 (6 vCPUs, 26 GB memory) machine type.Sample code:I have removed this peace of code I use 'cpu' instead of 'cuda'toError Logs:"
489,"How to clean the text from words such as 'Ã§', 'Ã¦' or any other non-meaningful words?","['python-3.x', 'text', 'nlp', 're']","I have a lot of text and trying to clean it up, so far I have used the following code:But still, I see non-meaningful words such as 'Ã§', 'Ã¦', 'mÃ£', 'Ã¯Â¼', etc. How to clean my text of these words?"
490,Issues installing mecab-python3 using pip,"['python', 'pip', 'nlp', 'mecab']","Today I've been attempting-- and failing-- to install this guy (MeCab library for Python 3.5+) for the sake of building a simple personalized Japanese readability analysis tool (as a learner of the language and data nerd).Of course, the first thing I tried was the simple pip install mecab-python3, to see this wall of text (process and errors). The error message is (probably) mojibaked which makes it impossible to know what actually happened.I repeatedly googled important snippets of it that were readable in search of an answer to the problem, but no results resembled exactly what my problem appears to be.I then tried downloading the source from pip and manually building it. It was doing fine until the final python setup.py build command, which resulted in:If you can't read Japanese, basically what that says is ""the specified file could not be found."" Not sure which file this refers to, and what to do about it.I'm using Windows 10.My apologies if the answer should be obvious. I know what it's like to be good at something and watch other people be inept at it, so I'm sorry in advance for the potential frustration.Thank you!"
491,NL training without input,"['nlp', 'bixby', 'bixbystudio']","I'm a beginner at developing using bixby, and I was trying to create and modify the sample Dice capsule that they have on their github. I wanted to add an extra feature of snake-eyes to it, which basically is an extra boolean concept which is true for 1-1 roll on a pair of dice.Another difference is that I am not taking any input as the sample capsule. In the sample capsule, the user gives the input of number of dice to be roller and the number of sides each die has. I have restricted the number of dice to be 2 with 6 sides each, so there is no point of taking input, and it basically boils down to a simple dice rolling application, which spits out the rolls and the sum.In the simulator, I am able to get the output perfectly fine by using the ""intent { goal : .... }"" syntax, but i want to train the model to be able to run on prompt like ""roll die"", although there is no concept to match the roll command to!Is there any way to do this?
Thanks in advance!"
492,Is there a rule of thumb/a way to decide what is a high gamma in an LDA model,"['r', 'nlp', 'lda']","I've built an LDA model in R using the stm package tidy to get a df of gamma per topic. im want to create a viz that shows what % of documents can be associated to each topic using something like:My question is, is there a best practice or a way to decide how to choose the X value in the code above that indicates what should be the gamma threshold to say a document is ""part"" of a specific topic ?"
493,Recommendations table sort dataframe result in descending order Pandas,"['python', 'pandas', 'nlp', 'pandas-groupby', 'data-science']","The recommendation tables output scores are not truly descending, the recommendations don't match the scores of the recommendationTable.Currently, the input does work and it does give a correct recommendationTable_df.Note: the output is correct  recommendationTable_df.head(6)However, when it goes to display the matching results that display id to name in the scored order It does not.At this point, the order is no longer descending or  correctbut it is instead ordering by maybe the id that i am using to match to the namethe order of the score should match the output with the id in the descending order of the score.This is what I am trying to achieverecommendationTable_df  with Exspected Results match up this is not a real dataframeI instead get this as the results which is not matching the decending orderHow would I get it the recommendation data frame to match the order of the recommendation tables output scores.This is the recommendation tables output scores
Which are in the correct orderoutputThis is how it Sorts the ScoreThis is the current recommendations This order is not correctThis is the result I was trying or expecting to getbased on the output"
494,"Dynamic, natural, generative slot filling with pre-trained BERT","['deep-learning', 'nlp']","I'm working on an NLP deep learning model that hopefully will be indistinguishable from human, but hey, who doesn't want that? So let's walk before we can run. The first tasks I'm focusing on is Intent Recognition and Slot Filling/Entity Recognition.I created a model that does great at Intent Recognition by slapping a few dense layers on top of BERT. Now I want to retrieve information from the input text, firstly in categorical form, but later perhaps in a more dynamic form.The current methods of slot filling is not dynamic enough for my liking. It doesn't really retrieve values, it just masks values it recognises aren't tied to slots. I'd prefer a model that can generate an answer from text, given a description of the slot in natural language. Optionally I could also provide the possible answers it could give.Input: User utterance: 'I would like the walls to be green'
Input: Slot description: 'The colour of the wall'
Input: Possible outputs: 'Green,blue,red,yellow'Output - Slot value: 'Green'#############################################Input: User utterance: 'I want there to be unicorns at my wedding'
Input: Slot description: 'Whether or not there should be unicorns at the wedding'
Input: Possible outputs: 'True,False'Output - Slot value: 'True'Note that the output 'True' never appears in the input utterance. I want the model to be able to generate it.As a first step I tried feeding a BERT/XLnet/ELECTRA/RoBERTa the concatenated input like :[CLS] User Utterance [SEP] Slot description [SEP] Possible Outputs [SEP] And through a dense layer tried to match the 'Possible Outputs' to a one-hot encoded vector of which the 1 would be the correct output. This didn't go well and I could only achieve around 54% accuracy. This made me wonder if this is even possible at all?I'm pretty new to NLP so I'm not sure which technology would be able to do this. How do you make a pretrained model like BERT generate text? I'm guessing putting a few dense layers at the end of it wouldn't do the trick. Do you need a transformer decoder?The folks who wrote this paper: https://arxiv.org/pdf/1912.09297v2.pdf seemed to be able to do it, but they didn't release code and didn't go into enough detail.What do you guys think?"
495,memory access error on official example spacy code for entity linking,"['nlp', 'spacy']","Ive tried to follow the instructions on the official documentation at:
https://spacy.io/usage/training#entity-linkerBasicially I use these commands:Of course I didnt change the example code.As a result I get following outputIve tested on Fedora 22, python 3.7, spacy 2.2 and 2.3
Do you have any clue? Or is it a bug"
496,How to find city names in string using NLP and Python,"['python', 'python-3.x', 'string', 'nlp', 'nltk']","As you can see in the title, my question is simple. I want to find city names using NLP. Here is an example:Question:How is the weather today in Istanbul?Answer:I tried this code but it didn't help me so much.How can I get this result? Thanks for your answers."
497,Why computing similarity with gensim needs the size of the dictionary?,"['python', 'nlp', 'gensim']","In order to use the gensim.similarities.docsim.Similarity class to compute similarities between words, one need to provide the corpus and the size of the dictionary.In my case, the corpus are the word vectors computed using a word2vec model.I wonder why gensim needs the size of the dictionary? And also, if it needs here the size of the dictionary used to create the word2vec model, or the size of the dictionary of the corpus, for which I want to compute the similarities."
498,What machine instance to use for running GPU workloads in Google Cloud Platform [closed],"['google-cloud-platform', 'nlp', 'pytorch', 'tensorflow2.0']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed 3 days ago.I am trying to run Elasticsearch BERT application and would like to understand the minimal configuration for fine-tuning the model using GPU. What machine configuration should I be using?Reference github: Fast-Bert"
499,Is there a particular range for good perplexity value in NLP?,"['deep-learning', 'neural-network', 'nlp', 'language-model', 'perplexity']","I'm fine-tuning a language model and am calculating training and validation losses along with the training and validation perplexities. It s calculated by taking the exponential of the loss, in my program. I'm aware that lower perplexities represent better language models and is wondering what the range of values are for a good model. Any help is appreciated. Thank you."
500,How to calculate word similarity with lists of words,"['nlp', 'spacy', 'cosine-similarity']",Supposing I have three lists which contains words of different categories (database attributes):Now I have a phrase such as 'product name' which I want to classify them into one of the categories above. Is there any tools where I can calculate the similarity of the phrases with words in the three lists? This is under the assumption that I don't have a large training data sets (only small samples as shown in example above)
501,Save only best weights with huggingface transformers,"['deep-learning', 'nlp', 'pytorch', 'huggingface-transformers']","Currently, I'm building a new transformer-based model with huggingface-transformers, where attention layer is different from the original one. I used run_glue.py to check performance of my model on GLUE benchmark. However, I found that Trainer class of huggingface-transformers saves all the checkpoints that I set, where I can set the maximum number of checkpoints to save. However, I want to save only the weight (or other stuff like optimizers) with best performance on validation dataset, and current Trainer class doesn't seem to provide such thing. (If we set the maximum number of checkpoints, then it removes older checkpoints, not ones with worse performances). Someone already asked about same question on Github, but I can't figure out how to modify the script and do what I want. Currently, I'm thinking about making a custom Trainer class that inherits original one and change the train() method, and it would be great if there's an easy and simple way to do this. Thanks in advance."
502,How do I use the NLTK sent_tokenize function to loop through a data frame column containing text?,"['python', 'nlp', 'nltk']","I have the following data frame (df) that starts off as a .csv with a few columns in it and which gets loaded into a Jupyter notebook. I would like to use one of the columns as a corpus for nlp scripting.  when I attempt to run sent_tokenize (or even word_tokenize), I get an error.  Below is my script and resulting error:resulting error:
--------------------------------------------------------------------------- NameError                                 Traceback (most recent call last)  in 
----> 1 corpus_tokenized = sent_tokenize(text)NameError: name 'text' is not defined"
503,Testing text classification ML model with new data fails,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'text-processing']","I have built a machine learning model to classify emails as spams or not. Now i want to test my own email and see the result. So i wrote the following code to classify the new email:Knowing that  process_text is a function to process the email, When I run the code i get the following error:What's the problem and how can i fix that please ?"
504,list index out of range to extract text lines from a df column,"['pandas', 'nlp', 'nltk', 'stop-words']","I finally find as a need your guidance and support as I don´t detect what is my error in the next piece of code. It supposes that ""list index out of range"" rises when you initialize counter improperly to the length of df, but what I am attempting is to return the first then lines of the column 'Descripción' as sample (doc) to apply the NLTK stopwords analysis. I appreciate your answers guys, thanks!enter image description hereenter image description hereenter image description hereenter image description here"
505,"Spacy trouble migrating to command line ner training. docs, golds = zip(*docs_golds) ValueError: not enough values to unpack (expected 2, got 0)","['nlp', 'spacy']","I have been using spacy for years now to train custom ner provisions using simple training format sourced from relational db. I am attempted to move this to command line training given additional information and ability to more easily run in cloud. Like users in past, I have struggled with understanding the nuances of documentation but I am hopeful my json format is now correct. When attempting to train ner from command line, it is obvious something is still off. Any guidance people can offer is much appreciated. I'm probably doing something stupid, but can't see it right nowspacy==2.1.8Result:Created blank 'en' model
Training pipeline: ['ner']
Starting with blank model 'en'
Counting training words (limit=0)Itn    Dep Loss    NER Loss      UAS    NER P    NER R    NER F    Tag %  Token %  CPU WPS  GPU WPS[+] Saved model to output directory
C:\home\testModels\widget\devModel\model-finalTraceback (most recent call last):
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy\cli\train.py"", line 281, in train
scorer = nlp_loaded.evaluate(dev_docs, debug)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy\language.py"", line 631, in evaluate
docs, golds = zip(*docs_golds)
ValueError: not enough values to unpack (expected 2, got 0)During handling of the above exception, another exception occurred:Traceback (most recent call last):
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 193, in _run_module_as_main
""main"", mod_spec)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 85, in run_code
exec(code, run_globals)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy_main.py"", line 35, in 
plac.call(commands[command], sys.argv[1:])
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\plac_core.py"", line 328, in call
cmd, result = parser.consume(arglist)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\plac_core.py"", line 207, in consume
return cmd, self.func(*(args + varargs + extraopts), **kwargs)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy\cli\train.py"", line 368, in train
best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy\cli\train.py"", line 425, in _collate_best_model
bests[component] = _find_best(output_path, component)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\spacy\cli\train.py"", line 444, in _find_best
accs = srsly.read_json(epoch_model / ""accuracy.json"")
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\srsly_json_api.py"", line 49, in read_json
file_path = force_path(location)
File ""c:\Users\MPC-LAPTOP01\AppData\Local\Programs\Python\Python36\lib\site-packages\srsly\util.py"", line 11, in force_path
raise ValueError(""Can't read file: {}"".format(location))
ValueError: Can't read file: C:\home\testModels\loan\devModel\model0\accuracy.json"
506,Sentiment about Topics defined by LDA,"['nlp', 'regression', 'sentiment-analysis', 'lda', 'topic-modeling']","For a project I do a research about topics in online reviews, and the effect of these topics on the rating (by doing a regression). Furthermore and the reason I would like to have your help: I would like to know the sentiment of a topic per review.I have the following data:By LDA I have found 15 different topics, I have two documents as output of this LDA topics model.The distribution values of the topic per review, where the columns are the topics and a row a review, in a cell is given the distribution value of that specific topic in a review 1234 (see image):
Distributions of the topics per reviewTerm Topic document, where the a column is a topic (e.g. transport) and a row is a term (e.g. tram) and in the cell is given the probability of the term within that topic (see image)
TermTopicBut now I would like to know the sentiment per topic (aspect) of each review, additionally I would like to know the effect on this sentiment about this topic on the rating? My question to you, what do you think what is the best way to find this?Is this a good way to determine the sentiment per aspect? All other suggestions are welcome.Thank you in advance!!!"
507,Elasticsearch Analyzers for text Analysis,"['java', 'elasticsearch', 'nlp', 'multilingual', 'elasticsearch-analyzers']","i am new to Elasticsearch and willing to use for a full-text search engine.
For Text analysis i need to work with (multilingual) language Analyzers. Elasticsearch offers built in language Analyzers but i am not sure if they cover preprocessing steps like: removing stop words, stemming, removing unwanted characters etc. I will be working with multiple-field, because all (descriptions) languages are indexed in the same fiel in a document. Is a mapping like this correct in this case?i am confused how to use language analyzers to analyze the input-text and when do we use mappings instead of settings?"
508,BERT word embbeding on a simple dataset,"['python', 'nlp', 'word-embedding', 'bert']","I want to learn how can I use BERt for word embbeding. I tried this code:But the problem is that it took a lot and I finally couldn't run it even after 10 hours. I've read that it's a GPU intensive task. I use my own laptop with 8GB GPU.
my question is, there is a way that I can use BERT for a small dataset on my own laptop?"
509,Why a document less relevant to search query receives higher cosine similarity score?,"['python', 'search', 'nlp', 'tf-idf', 'cosine-similarity']","Wile experimenting with TFIDF text search, oddly a document that lacks one of the search query terms receives a higher cosine similarity score.I get that the term 'ASIC' is less frequent in the corpus than 'cybersecurity' which may explain the higher TF and IDF (and surely the cos. sim.) scores, so the first document outweighs the second, even with the missing 'cybersecurity' term, I'm just not sure if this is really the reason or something may be wrong with my calculation / understanding.What could be done to rank the second document higher containing both search terms?Also a wrorking example can be found here: https://www.kaggle.com/edmondvarga/kernel46989d3fbcResults:"
510,Convert a multi value dictionary to a Dataframe in python,"['python', 'string', 'nlp']",The dictionaryNeeded to a data-frame by converting these list of values into a DF with same key
511,How could I generate questions True/False from a given text with python?,"['python', 'python-3.x', 'nlp']","I would like to generate questions True/False from a given text.For example:Text:""Jim's dog was very hairy and smelled like wet newspaper""Questions:-""Jim's dog was very hairy and smelled like a rose"" T/F-""Jim does not have a dog"" T/F"
512,Stanford CoreNLP can not detect sentence with numbering,"['java', 'parsing', 'nlp', 'stanford-nlp']","I have a word document with numbering like 1.  ,2.  etc.
I want to extract sentences from the document.
I use Stanford CoreNLP 4.0.0 and stanford-corenlp-models-current.jar
Normal extraction of sentences retrieve numbers as different sentence.
Suppose document hasSentence extraction gets 1 as a sentence and Abcd efgh as another sentence.Similarly 2 as a sentence and Ijkl mnop as another sentence.I try with boundariesToDiscard properties with different patterns but get same result and also get wrong entity mentions in this case.Please help to resolve this issue.Thanks in advance."
513,How to parse this array from a ML result?,"['python', 'arrays', 'parsing', 'nlp', 'int64']","How would one parse this print out? One would imagine it needs to assigned to a variable.For example, how would one parse 1605?And/or, how would one parse 1605, 1606, 1698, 1607 from the second array?This is a NLP result which is being worked with for the first time and your answer is very much appreciated. Thank you.Ps. To clarify my question, this output is one of many NN NLP outputs to be parsed in a virtual environment, running in the API of an app. So, given this output, how would one parse only the second array, which are rows to be returned from a df? ie: One can not manually separate the arrays. Gervais' solution works in practice by manually typing in ""np."", however this solution does not seem sustainable in a repetitive production environment, at least not by using string formatting. Any further insight into automating the parsing with out without numpy while the app is deployed would also be much appreciated. Thank you."
514,How to know the words associated with a specific class in NLP model?,"['scikit-learn', 'nlp', 'logistic-regression', 'tf-idf', 'multiclass-classification']","I have trained an NLP model for ""Consumer Complaints Classification"" using Logistic regression algorithm and TF-IDF vectorizer. I want to know the words that my model associates with a particular class. I am looking for something like this -
Class 1 =  [""List of words that help my model identify that an input text belongs to this class""]"
515,BERT finetuning : training loss decreases but accuracy remains unchanged,"['python', 'deep-learning', 'nlp', 'pytorch', 'bert']",I have been training bert on a custom dataset for a while now. While training I observe that while the training loss is decreasing the val accuracy remains fixed at 0.5. I have tried out all the different BERT models available in Huggingface and don't know what to do. Here's the output during training:
516,What is the difference between information extraction and knowledge representation?,"['neural-network', 'nlp', 'artificial-intelligence', 'logical-operators', 'information-extraction']","I'm new to AI and the field of neural-symbolic computing, which seemingly relies on 4 parts: Representation, Extraction, Reasoning, and Learning.https://arxiv.org/abs/1711.03902 (Besold et al., 2017)Mind clearing up the differences between representation and extraction?"
517,ALBERT not converging - HuggingFace,"['machine-learning', 'nlp', 'text-classification', 'transformer', 'huggingface-transformers']","I'm trying to apply a pretrained HuggingFace ALBERT transformer model to my own text classification task, but the loss is not decreasing beyond a certain point.Here's my code:There are four labels in my text classification dataset which are:Define the tokenizerEncode all sentences in text, using the tokenizerDefine the pretrained transformer model and add Dense layer on topFinally, feed the encoded text and labels to the modelLoss has decreased from 6 to around 1.23 but doesn't seem to decrease any further, even after 30+ epochs.What am I doing wrong?All advice is greatly appreciated!"
518,Combine two tensors of same dimension to get a final output tensor using trainable weights,"['deep-learning', 'neural-network', 'nlp', 'pytorch', 'question-answering']","While working on a problem related to question-answering(MRC), I have implemented two different architectures that independently give two tensors (probability distribution over the tokens). Both the tensors are of dimension (batch_size,512). I wish to obtain the final output of the form (batch_size,512). How can I combine the two tensors using trainable weights and then train the model on the final prediction?Edit (Additional Information):So in the forward function of my NN model, I have used BERT model to encode the 512 tokens. These encodings are 768 dimensional. These are then passed to a Linear layer nn.Linear(768,1) to output a tensor of shape (batch_size,512,1). Apart from this I have another model built on top of the BERT encodings that also yields a tensor of shape (batch_size, 512, 1). I wish to combine these two tensors to finally get a tensor of shape (batch_size, 512, 1) which can be trained against the output logits of the same shape using CrossEntropyLoss.Please share the PyTorch code snippet if possible."
519,how to predict from manually trained spacy model,"['python', 'nlp', 'spacy', 'named-entity-extraction']","I have the following code to create and train a new spacy model.
i don't know how to predict entities from the new text?can anybody help?"
520,module error in code. Cannot figure out what is wrong,"['python', 'nlp']",I am getting error in my following code. I cant figure out what is wrong in this. Please help.I am getting an errorI tried changing names of every variable but still i cant understand what is wrong. Please help
521,Fine Tuning Bert on Medical Dataset,"['python', 'nlp', 'huggingface-transformers']","I would like to use a language model such as Bert to get a feature vector for a certain text describing a medical condition.As there are many words in the text unknown to most pre-trained models and tokenizers, I wonder which steps are required to achieve this task?Using a pre-trained model seems beneficial to me since the dataset describing the medical conditions is quite small."
522,How do I implement (Brown) cluster represenations of texts from dicts as features for text classifier elegantly?,"['python', 'dictionary', 'scikit-learn', 'nlp']","I'm trying to implement a version of Brown clusters for a series of review texts (SemEval 2014). I am using Owoputi et al.'s(2013) publicly available twitter clusters. They look like the following:where the bitstring indicates the cluster and there are 1000 clusters.I have extracted to dictionary where I have the relevant bitstring as the key and a list of the tokens as value: e.g.{'0000': ['ijust', 'i', '-i'], '000100': ['iyou', #innowayshapeorform] ...}I am just missing the part of how to one hot encode the text mapping the dictionary keys to indexes in a vector (1d array):such that if a word occurs in the text AND the word occurs in a cluster that the value for the cluster is changed from 0 to 1.e.g.
one_hot_vector = [0]*3text 1: I hate catstext 1 vector representation: [1,0,1]text 2: dogs love metext 2 vector representation: [1,1,1]text 3: I dream of sheeptext 3 vector representation: [1,0,0]this example has 3 clusters - the clusters I have would be 1000 dimensions in length."
523,How to Scrape Keywords from Article (URL)? PYTHON,"['python', 'python-3.x', 'web-scraping', 'nlp', 'nltk']","I've been using Newspaper3k to scrape websites, but their articles.keywords function is completely lacking. Is there any way that I could scrape keywords in an article (from a URL, not file), without NLP? This is mainly because NLP doesn't work with my OS."
524,How to understand LSTM performance generated on a graph?,"['matplotlib', 'machine-learning', 'keras', 'nlp', 'data-science']","I used LSTM by applying Keras for the task of Named Entity Recognition. The model present around 95% accuracy. Then I tried to create the graph for accuracy and loss. Now I do know, what do these two graphs present?"
525,Sub-word tokenization in text classification better than word level?,"['nlp', 'lstm', 'sentiment-analysis', 'text-classification', 'unsupervised-learning']","I'm working on a text classification problem. Looked up several recent papers talking about using CNN + LSTM. None of them are very specific on how they tokenize the text, but I can't seem to find much/any evidence that folks use sub-word tokenization, although I do see less rigorous articles online (as opposed to published research) where sub-word is used in text classification.My question: should I expect my model to perform better using sub-words? If so, what's a good intuition as to why?"
526,Huggingface language modeling stuck at data reading phase,"['deep-learning', 'nlp', 'pytorch', 'huggingface-transformers']","I have a large file (1 GB+) with a mix of short and long texts (format: wikitext-2) for fine tuning the masked language model with bert-large-uncased
 as baseline model. I followed the instruction at https://github.com/huggingface/transformers/tree/master/examples/language-modeling. The process seems to be stuck at a stage ""Creating features from dataset file at <file loc>"". I am unsure what is wrong, is it really stuck or does it take really long for file of this size? Command looks pretty much this:Added: The job is running on CPU"
527,CNN on tfidf as input,"['machine-learning', 'keras', 'deep-learning', 'nlp', 'cnn']","I am working on fake news detection using CNN, I am new to ccoding CNNs in keras and tensorflow. I need help regarding creating a CNN that takes input as statements in form of vectors each of length 100 and outputs 0 or 1 depending on its predicted value as false or true.I will really appreciate if someone could give me a working code for this with a little bit of explaination"
528,Using pdfminer on screenplay to output to csv,"['python', 'pdf', 'nlp', 'pdfminer']","I am attempting to use pdfminer on a screenplay of a television show. The ideal output would be a csv file with one column being the character names, and the other column the lines that they spoke. Here is a link to an dialogue excerpt from the screenplay. 
Dialogue From ScreenplayBecause of the unique structure of the script, just reading the text from the pdf is not enough to distinguish which parts are the narrations, and which parts are the dialogue. With the below code, I identified individual text objects and printed out their coordinates. However, here is an example of what was returned: As can be seen, the order of the lines are mixed up right now. I was intending to read the text into csv line by line in a specific order but whenever two characters speak at the same time, the pdf reads the objects differently. Additionally, at (333.0, 591.9) where Bruce speaks, the pdf is not able to read the dialogue as a separate line from the character. I predict there will be many such problems in the screenplay format. I would greatly appreciate any help whatsoever on how the lines can be read in a chronological order, or how to better be able to distinguish between character and dialogue in an easy and orderly manner. Thank you in advance!!!"
529,Combining similar strings using R [Text Analysis] [n-gram]],"['r', 'nlp']","Is there any way to get the count of similar strings:Output should be 1> Have tried splitting the words, 
2> reversing the order of words 
3> Applying antijoin and inner join to identify these type of strings in n-gram approach by creating 2 different dataframes. 
However, struggling to get the total count as an aggregate value.Any help here would be greatly appreciated. since the data is huge, having an optimal solution would be a great help.There is very valid question by a contributor here ""@Lime"". so to make sure that the objective is very well focused.so in my case, anything which is exact same... and should not be containing any other text /character to be map against a similar string. should be having an output like:"
530,Unable to save model architecture (bilstm + attention),"['python', 'tensorflow', 'nlp', 'multilabel-classification', 'attention-model']",I am working on a multi-label text classification problem. I am trying to add attention mechanism with bilstm model. The attention mechanism code is taken from here. I am not able to save the model architecture and getting an error mentioned below. My tensorflow version -2.2.0  Building lstm with attentionError 
531,How to convert list of tokens (after sentence tokenization) in a paragraph format into a numbered list of sentences or convert it to a dataframe?,"['python', 'nlp', 'text-classification']","I read a pdf file using PDFMiner and extracted the text from it for NLP analysis. As I will be dealing with research articles, I did light cleaning of texts by converting the paragraphs of texts into list of sentence tokens. My goal is to select sentences that contains intext citations for my further analysis.for instance, 
the data is in the below format:Expected output:Is this possible to convert this into a dataframe so that I can add labels to each sentences?Or will it be wise to extract only the sentences with in-text citations?"
532,Batch size and epoch,"['machine-learning', 'nlp', 'lstm', 'pyhook', 'batchsize']",I want to train  lstm model in keras for nlp with 63000 sample and label with 3 classes (sentiment analysis in persian) .how can i set the batch size and epoch to good fit my model?and what is standard for choosing num of layers?i tried different epoch but overfitting just happend.
533,Multilingual Search using language Elasticsearch,"['java', 'elasticsearch', 'nlp', 'lucene']","i am working on a project to perform multilingual full-text search using Elasticsearch. The historical training dataset i am using is also multilingual and i am trying now to configure text analysis with language analyzer and language detection.1) i am using the following link as a guide and as it is written in the first paragraph i need to install an Inference Ingest Processor. How can i install it? (i am not familiar with Java and new in elasticsearch)
https://www.elastic.co/de/blog/multilingual-search-using-language-identification-in-elasticsearch2) Elasticsearch offers language Analyzer for many languages i will need to configure Analyzers in 8 languages if i follow this link https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html  i will have to create 8 different custom analyzers which is quite long. Is there any shorter way to write one setting for 8 languages?"
534,text classification using graphs in natural language processing,"['graph', 'nlp', 'classification', 'text-classification', 'multiclass-classification']","I tried to search but couldn't find much helpful information regarding this topic. That's why I am asking it here...I know there are various methods to classify texts (like Logistic regression etc.) and also we have neural network. But, I was wondering if it is possible to 'classify the texts into multiple classes' using graph theory?
If yes, how should I proceed? Please guide me.Example:I like jeansp                    -posI like toyota                    -posI it so-so place                 -neutralI hated that trip                -negI love that shirt                -posthat place was horrible          -negI liked food but service was bad -neutral"
535,Re-implementing TF 1.0 sampled_softmax_loss funtion for seq2seq model in to TF 2 Keras model,"['python', 'tensorflow', 'keras', 'nlp']","I have a TF 1.0.1 code of seq2seq model. I am trying to rewrite it using Tensorflow Keras.TF 1.0.1 code has following decoder architecure:Here is how the sampled_softmax_loss is calculated:And, this is my decoder in Keras:This is my sampled_softmax_loss function I use for Keras model:But, it does not work.
Can anyone help me to implement sampled_loss_funtion in Keras correctly."
536,Shall we lower case input data for (pre) training a BERT uncased model using huggingface?,"['deep-learning', 'nlp', 'pytorch', 'huggingface-transformers']",Shall we lower case input data for (pre) training a BERT uncased model using huggingface? I looked into this response from Thomas Wolf (https://github.com/huggingface/transformers/issues/92#issuecomment-444677920) but not entirely sure if he meant that. What happens if we lowercase the text ? 
537,Can we use sequence labels for question answering?,"['deep-learning', 'nlp', 'pytorch', 'transformer', 'question-answering']","I have some Question, Context and Answer triplets for training and along with that, I have sequences inside the queries that are labelled into a few categories.Example: Suppose 2 kinds of labels are there A and B.Query: How is the weather outside?Label A: How is Label B: weather outsideI wish to encode the question and context using BERT and then use these labels associated with the tokens to extract the answer span from the context.My Approach: I thought of defining 3 different weight vectors (dimension: 768*768) or using weight vector of size (768 using elementwise multiplication with the query tokens) for the tokens labelled A, B and others. Then using the new query encoding and the BERT context encoding in an ensemble for the answer span selection task.Any other ideas to use the sequence labels for the MRC task?"
538,Google cloud NLP integration with UiPath,"['google-cloud-platform', 'nlp', 'rpa']","I am trying to integrate the google NLP (text Classification) with UiPath. But I am facing some problems, please let me know how can I do that."
539,Customize the encode module in huggingface bert model,"['nlp', 'text-classification', 'huggingface-transformers', 'bert-language-model']","I am working on a text classification project using Huggingface transformers module. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:However, my current project requires me to generate customized ids for a given text. For instance, for a list of words [HK, US, UK], I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~"
540,How to save spacy rendering tags in a dict,"['python', 'nlp', 'spacy']","https://spacy.io/usage/visualizers#entI am tryinf to visusalize entities in a sentence by using spaCy. In the link above you can see an example.Now my question. How can I save those entities in a dictionary? I want to analyze 100 sentences and save the frequency of those entities to see which terms are the most common. for example: dict = {""PERSON"": 23, ""ORG"": 2, ""LOC"": 19}Can someone help?"
541,How to convert text into numerical form(vector) in order to use in NLP,"['error-handling', 'nlp', 'classification']","I am working on emotion analysis ,for that i need to try several machine learning models.
Ex:i was working on logiticRegression last ,and i do not know about it , i'm getting error while making a classifier more specifically while fitting the classifier even when I've used countvectorizer and tfidf both ."
542,Preprocessing corpus stored in DataFrame with NLTK,"['python', 'pandas', 'dataframe', 'nlp', 'nltk']","I'm learning NLP and I'm trying to understand how to perform pre-processing on a corpus stored in a pandas DataFrame.
So let's say I have this:Which results in:Now, I load what I need and tokenize the text:Which gives the following output:Now, my problem occurs when removing stop words:Which looks like nothing happened:Can someone help me understand what happens and what I should do instead?After that, I'd like to apply lemmatization, but that doesn't work in the current state:yields:Thanks!"
543,NLP: Generate Text from keywords (NLG),"['python', 'deep-learning', 'nlp']","I would like to create an abstract in the end using Natural Language Processing.As Input I would provide certain keywords belonging to its category.
EGShould result into
„Felix has a dog named Sven. He has him since 2019.“But I don‘t know what would be the best way to approach this. I thought about NNs bc I have enough data like this to Train it. But the results I found online were not pretty leading me into hardcoding a Template but this wouldnt be very flexible if one data value is missing or has two entries. Maybe someone has an idea how I could best approach this.Tyvm TakaIdeally the resulting text should consist out of multiple sentences. Bc more data would follow."
544,How to calculate the likelihood of a sentence using a Word2Vec model?,"['python', 'nlp', 'gensim']","I need to train a Word2Vec model, which I have done, and then I need to use it to calculate the likelihood of previously unseen data. I'm stuck on how to do this though. I've seen WMD but that's for calculating similarity between 2 sentences, while I'm trying to calculate the likelihood of text with my model."
545,"ValueError: operands could not be broadcast together with shapes (2,176525) (300,176525) [closed]","['python', 'nlp']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed last month.I have countVectors of 2 paragraphs of dimension (2,176525) and I want to multiply the learned weight vector of dimension (300,176525) with it. I am using: But getting the error. Any suggestions?"
546,Remove spans in doc but keep POS tags in Spacy,"['python', 'nlp', 'spacy']","I want to apply POS tagging to a text. After that I want to remove some parts of a text (single word or multiple words) but keep the correct POS tags. How can I achieve this? I want to do the POS tagging as a first step, since POS tagging depends on the context of the tokens and removing tokens first will alter that context.I thought about surrounding the spans I want to remove by some characters (like [ or { ), but I noticed that these characters are POS tagged as well. Do these characters affect the POS tagging of other tokens or can I safely use them to mark the spans I want to remove later?Another option would be to compare the tokens in the span to those in the doc and then remove those who match. However, I wish to remove only the specific span and not all occurrences of that span or parts of the span.Edit: I found a solution:I created a Spacy Doc for both the text as well as the span I wish to delete. Then I iterate through each token in the text and try to match it with the first token of the span. If there is a match I check if all next tokens in the text match with the rest of the tokens of the span. If so, I remove the matching tokens from the text."
547,Find a sentence's confidence score/similarity to a word2vec model,"['python', 'nlp', 'word2vec']","I created two different word2vec models for two different topics or categories.Now, if I give a new sentence as an input, how can I calculate the confidence score of that sentence to both the models. I mean, how do I calculate the percentage similarity or closeness of the sentences with both the models ?Is it possible to get a pooled vector for one model so that I can compare it to a new sentence's vector.Example -Say I created two word2vec models for SPORTS and ACADEMICS. I created both the models using various sentences, pre-processing etc.Now, say I give a sentence which is a mix of both SPORTS and ACADEMICS.How do I know how much my sentence is in the SPORTS context and how much in the ACADEMICS using the already present model and the vector of this new sentence ?Like, can I say that my new sentence has 70% similarity to SPORTS model and 50% similarity to academics model ?"
548,How to use SimPack - similarity measures in ontologies,"['nlp', 'ontology']","How is it possible to use SimPack library to measure similarity between ontologies? I compiled the source code of the project, but I did not find any documentation about how to use it.SimPack homepageSimPack is a generic Java library of similarity measures for the use in
ontologies and other application domains.The measures in SimPack are intended to be used out of the box. To use
the measures put all the jar files in the lib directory on the Java
classpath.Requirements:
- Java 1.5 (http://java.sun.com)
- Ant (http://ant.apache.org)Step-by-step installation:1/ Compile the SimPack sources by executing 'ant dist' on the command line.
This results in a SimPack jar in the dist directory.2/ Put the resulting SimPack jar file in the dist directory on the java
classpath together with all the jar files in the lib directory.Have a look at the JUnit tests in tests directory to obtain a basic
understanding of how to use the measures."
549,Jupyter notebook: NoCredentialsError: Unable to locate credentials,"['amazon-web-services', 'nlp', 'amazon-comprehend']","I am trying to analyze text using Amazon Comprehend in a Jupyter notebook. However, I am receiving this error:So I followed Installing the AWS CLI version 2 on Linux and everything ran ok, but I don't have a .aws/credentials file. Help please."
550,Attention Mechanism / Tensorflow Tutorials,"['tensorflow', 'keras', 'nlp', 'recurrent-neural-network', 'attention-model']","I am trying to improve my draft of attention mechanism code where I had basically an iteration of the decoder steps and and LSTM decoder cell getting a context vector at each step from an attention module:I found the implementation from the tensorflow tutorials much cleaner, but I don't see how the decoder is getting at each output step a different context vector from the bahdanau, it looks like the decoder is getting only one context vector, what am I missing???https://www.tensorflow.org/tutorials/text/nmt_with_attention"
551,Tensor flow Model saving and Calculating Average of Models,"['python', 'tensorflow', 'deep-learning', 'nlp', 'bert']","I am trying to implement and reproduce the results of federated Bert pertaining in paper 
Federated pretraining and fine-tuning of BERT using clinical notes from multiple silos.I prefer to use TensorFlow
 code of Bert pretraining.For training in a federated way, initially, I had divided dataset into 3 different silos(each of that contains discharge summary of 50 patients, using mimic-3 data). and then pretrained the Bert model for each dataset using TensorFlow
implementation of Bert pretraining from the official release of Bert.Now I have three different models that are pretrained from a different dataset. for model aggregation, I need to take an average of all three models. since the number of notes in each silo is equal, for averaging I need to do sum all models and divide by three. 
How to take avg of models as did in the paper? somebody, please give me some insights to code this correctly. The idea of averaging the model weight is taken from the paper FEDERATED LEARNING: STRATEGIES FOR IMPROVING
COMMUNICATION EFFICIENCY
.I am very new to deep learning
 and TensorFlow
. so someone please help me to figure out the issue and suggest some reading material for TensorFlow
.In the paper, it is mentioned that It is a good option to overcome privacy and regulatory issues while sharing of clinical data. My question isis it possible to get sensitive data from this model.ckpt files? Then how?Any help would be appreciated. Thanks..."
552,How can I detect if it is a negation with a verb in a sentence using spacy(or other library)?,"['python', 'nlp', 'nltk', 'spacy']","..Hello Everybody! I'm working on an NLP project, and I want to detect if there is a negation with a given verb in a sentence For example : the function ""Is_there_negation"" should return ""True"" with the following parameters :How can I complete this function(I'm really beginner in NLP)Thanks in advance "
553,How do I get Python to plot a histogram of the number of unique words within in a column containing text?,"['python', 'pandas', 'nlp', 'histogram']","I have a data set called 'my_data' that I assign to a generic variable called 'data.' In my dataset I have a column called 'impression.' This 'impression' column contains text of medical notes like, ""Lesion observed in occipital region.""I would like to plot a histogram of the number of unique words occurring in that column. Here is the python script I have am using along with the error it is generating:Note:  this script has worked perfectly on other text columns. I observed that the newer data set I am using also has some numbers in it like,""HISTORY: 1. aneurysm 2. metastasis etc."" and I suspect this is forcing a type conversion in Python that is blowing up my script above but I could be wrong?Can anyone suggest a tweak to my script so that it will convert the data from 'float' to 'int' so that it can pass into the histogram plot?Thank you very much!!"
554,Parsing transcripts into tables,"['python', 'nlp', 'transcription']","My question is very similar to this one: 
    Python parse text from multiple txt file I would like the same kind of output.I am working on using a finite state machine as suggested by mangupt, but I am not sure of the best way to adapt that code for my files. This is what I have so far.Here are some examples of my files:https://pastebin.com/EACpRDvyhttps://pastebin.com/EqGPp4vghttps://pastebin.com/X08vAX5JThank you!!"
555,IndexError: index 22 is out of bounds for axis 1 with size 22?,"['python', 'keras', 'encoding', 'nlp', 'decoding']","while working on english to hindi i got an error ""IndexError: index 22 is out of bounds for axis 1 with size 22"" .LSTM network"
556,Can GPT-2 and BERT be combined to give an higher accuracy than either of them?,"['nlp', 'artificial-intelligence', 'hybrid']",Is it possible to combine GPT-2 and BERT and how accurate the resulting Neural Network might be.
557,Using glove.6B.100d.txt embedding in spacy getting zero lex.rank,"['nlp', 'spacy', 'glove']","I am trying to load glove 100d emebddings in spacy nlp pipeline.  I create the vocabulary in spacy format as follows: glove.6B.100d.txt is converted to word2vec format by adding ""400000 100"" in the first line. Now In the code: gives 
407174
(400000, 100)However the problem is that: gives 0 I just want to use the 100d glove embeddings within spacy in combination with ""tagger"", ""parser"", ""ner"" models from en_core_web_md. Does anyone know how to go about doing this correctly (is this possible)? "
558,Use custom similarity function to rapidly calculate similarity of a set of words with each other,"['python', 'numpy', 'nlp', 'vectorization', 'text-processing']","I have a somewhat odd question about vectorizing calculations in python, which I am not very strong with.I am solving a task of a word normalization. I have a list of companies whose names I want to reduce through standartization. As one of the steps I want to find the similarities between all company names in the list and build a similarity matrix which I then can use to cluster the names.I developed a custom function which calculates the similarity between two words the way I want and I get it into the numpy array. It works fine, but since I explicitly iterate over each pair of words, this function scales really poorly and becomes super slow when I have even just 20K words in a list I am normalizing. I wanted to understand, if there is a good way to somehow use (maybe) numpy and speed up the process through parallelization or something like? Any advice will be much appreciated.Below is the function I developed:"
559,Does using a pipeline with CountVectorizer and TfidfTransform convert the input data into a document-term matrix?,"['python', 'scikit-learn', 'nlp']",Does the following code actually converts the input into a dtm since it's not using the fit_transform method but only fit in order to learn the vocabulary? Is this model sufficient for learning ?
560,How to keep the unicode character codes in my csv file?,"['python', 'csv', 'nlp']","I am handling a large number of incoming emails and many of them have various emoticons in them. I am planning to apply an NLP analysis on the user comments and train a classifier to provide relevant answers, instead of having to manually reply to hundreds of these messages. For this as a first step, I parsed all emails and saved their content in a list called userMessages that I wrote in a csv file. I plan to add further columns to the csv for analytic purposes, such as user name, address, date, and time but this is not relevant for this question now. Here is the code I use to write the userMessages list into a csv file called user-messages.csv:This doesn't run into an error due to the encoding='utf-8' parameter, however, it removes/recodes the emoticons in such a way that it is no longer retraceable, for instance in the following format: ðŸ˜. Ideally, I would like to have the original unicode codes in the csv file, such as '\U0001f604' (smiling face with open mouth and smiling eyes) and later substitute these codes with their (approximate) meaning for the NLP to better understand the context of the messages, for instance in the case of this character ('\U0001f604'), remove the code and add the words 'smile' or 'happy'. Can this be achieved? Or am I overcomplicating things? Any advice would be greatly appreciated. Thank you!Edit: I am using Windows and I open the csv files in Microsoft Excel 2016."
561,What ML algorithm or NLP technique can be best used from extracting information from bank statements,"['python-3.x', 'machine-learning', 'nlp', 'data-science']","I want to extract words related to a specific category from bank narrations
eg:- i want to extract resturant name from all the narrations if it exist in narration else return a blank value or some dummy value"
562,SMS parsing using Node,"['node.js', 'nlp', 'text-parsing']",I want send a json object as follows : from the frontend. Now using node I want to parse this sms to get hold of who is the sender and other stuff. Also is there any NLP library for having different regex format to detect sender recipient or type of messages ?
563,How to convert text in a column to a different format based on a custom dictionary?,"['python', 'r', 'nlp', 'text-mining', 'corpus']","I’m looking to make the education data in my dataset consistent based on a dictionary of university/college names. How do I run code against my dictionary and get the output I want? The data consists of abbreviations and colloquial names.Can someone provide an example of this in R. I’m willing to try it in python also, R is just my preference. This is an example of my dictionary:This is my data:This is what I want:Apologies if there is already a solution for this, I just could not find it. I would appreciate any help."
564,keras and nlp - when to use .texts_to_matrix instead of .texts_to_sequences?,"['keras', 'nlp']","Keras offers a couple of helper functions to process text:texts_to_sequences and texts_to_matrixIt seems that most people use texts_to_sequences, but it is unclear to me why one is picked over the other and under what conditions you might want to use texts_to_matrix."
565,Extracting features from list of strings,"['python', 'neural-network', 'nlp']","I have list of strings and a strings that look like this :I want to compare each of the myString to each string in my list and collect the percentage of similarity in three different lists. So the end result will look like this:So mystring1 will be compare to each string in mylist and calculate the percentage similarity. Same with myString2 and myString3. Then collect each of those percentage in a list as seen above.I read that one can use TF-IDF to vectorize mylist and mystring, then use cosine similarity to compare them, but I never work on something like this before and I will love if anyone has an idea, process or code that will help me get started.Thanks"
566,Read multiple txt files into Dict into Pandas dataframe,"['python', 'pandas', 'dataframe', 'nlp']","I am trying to load multiple txt files into dataframe. I know how to load urls, csv, and excel, but I couldnt find any reference on how to load multiple txt files into dataframe and match with dictionary or viceversa.the text file are not comma or tab separated just plain text containing plain text song lyrics.I checked the pandas documents any assistance welcome.https://pandas.pydata.org/pandas-docs/stable/reference/io.htmlIdeally the dataframethe dataframe I hope to achieve would be like this exampleBasic example
folder structure /lyrics/muscian namesOpen the text files
the files are in directory lyrics/ from where I running my Jupyter notebook.hopefully get result like thisdict_keys(['bonjovi', 'lukebryan', 'johnprine', 'brunomars', 'methodman', 'bobmarley', 'nickcannon', 'weeknd', 'dojacat', 'ladygaga', 'dualipa', 'justinbieber'])import pandas as pd"
567,Calculating cosine similarity: ValueError: Input must be 1- or 2-d,"['python', 'numpy', 'nlp', 'linear-algebra', 'cosine-similarity']","hope everyone's well. I'm trying to use the following method to efficiently calculate cosine similarity of a (29805, 40) sparse matrix, created by HashingVectorizing (Sklearn) my dataset. The method below is originally from @Waylon Flinn's answer to this question. When I try with a dummy matrix, everything works fine.but when I try with my own matrix..I get Now, calling .shape on my matrix returns (29805, 40). How is that not 2-d? Can someone tell me what I'm doing wrong here? The error occurs here (from jupyter notebook traceback):Thanks for reading! For context, calling sparse_matrix returns this"
568,How to replace hyphen and newline in string in Python,"['python-3.x', 'regex', 'nlp']","I am working in a text with several syllables divisions. A typical string is something like that I tried:However, it is not working.I would like to get"
569,Error Running “config = RobertaConfig.from_pretrained( ”/Absolute-path-to/BERTweet_base_transformers/config.json“”,"['nlp', 'google-colaboratory', 'huggingface-transformers', 'bert', 'roberta']",I'm trying to run the code 'transformers' version of this code to use the new pre-trained BERTweet model and I'm getting an error. The following lines of code ran successfully in my Google Colab notebook:Then I tried to run the following code:...and an error was displayed:I'm guessing the issue is that I need to replace '/Absolute-path-to' with something else but if that's the case what should it be replaced with? It's likely a very simple answer and I feel stupid for asking but I need help.
570,BertWordPieceTokenizer vs BertTokenizer from HuggingFace,"['nlp', 'huggingface-transformers', 'bert', 'huggingface-tokenizers']",I have the following pieces of code and trying to understand the difference between BertWordPieceTokenizer and BertTokenizer.Thanks
571,How to check the value of an attribute of Part of speech when using pattern with the library Spacy?,"['python', 'python-3.x', 'nlp', 'spacy', 'information-retrieval']","I am using spacy in order to identify different type of pattern. the pattern are mostly this form :What I am trying to do is check the value of the last post tag of my pattern in a lexicon. As you can see it is either an adj or adv. So if the word at that place is found in my dictionary. I can append the sentences on my new list.I give you here a small edit of my code because, it returns me an empty list. So If you can help figure how to do that , I will appreciate any help.Printing sent_extract give:As you can see ""scrupuleux"" is found in my lexicon but the returned list is empty. I beg If you can help me find what I do wrong ?"
572,ERROR: (gcloud.compute.instances.create) Could not fetch resource: - Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally,"['google-cloud-platform', 'nlp', 'google-compute-engine', 'google-cloud-sdk']","I would like to try PEGASUS to summarize article. 
https://github.com/google-research/pegasusI followed this instruction.
https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setupI checked the region which I can use NVIDIA Tesla V100 and I decided to use us-central1-a
https://cloud.google.com/compute/docs/gpusI used this command.I got this error message.I took 3 hours and tried again, but I got the same result.So, I changed the region from us-central1-a to asia-east1-c.I used this command.Then I got this error message.Is it impossible for me to try PEGASUS? And, does it cost too much to try PEGASUS?"
573,Extracting Emails using NLP - Spacy Matcher and then encrypting and decrypting them,"['python', 'csv', 'encryption', 'nlp', 'spacy']","Image of csv file I have a csv file which looks like the image provided. 
I am reading the csv file, defined a pattern and using spacy Matcher. I am iterating through the rows and columns of the CSV file. My end goal is to identify the email Ids and SSN numbers as sensitive information and encrypt and decrypt them. But unfortunately all the information is getting encrypted and decrypted in the process. I think i am missing something here, not sure if it is EntityRuler or something or some problem with my code. csv"
574,What is the correct way of using RASA's API (rasa.core.processor - Encountered an exception)?,"['python', 'nlp', 'python-requests', 'rasa-core', 'rasa']","I installed the rasa-demo code sample. For turning on the rasa API, I did:How can I query as an API the chatbot? I would like be able to make a request and do the conversation through requests, instead of using the shell. So far, when I tried to make a curl to the rasa server:In:Out:On the rasa run server, I get this response:It is not working. What is the correct way to request Rasa server as an API? After reading the docs, it is not clear to me how to make correct usage of the API.I also tried this:In: Out:"
575,Where I can find the complete list of SpaCy Dependency Parsing labels or annotations?,"['nlp', 'dependencies', 'spacy', 'text-parsing']","I have try to refer to spaCy official website https://spacy.io/api/annotation#dependency-parsing
but I only got list of universal dependency relation which also on https://universaldependencies.org/u/dep/while, when I try to parse some sentences, I also got labels or annotations that not have been listed. Such as: prep, dative, and dobj, even though that those labels can be associated with preposition for prep, direct object for dobj, and ??? for dative. Is there any reference for me to find the complete list of SpaCy Dependency Parsing labels or annotations?"
576,Tensorflow CNN for NLP won't converge,"['python', 'tensorflow', 'nlp', 'tensorflow2.0', 'sentiment-analysis']","I was trying to create a Neural Network based on the model presented on this paper by Yoon Kim (https://arxiv.org/pdf/1408.5882.pdf) for sentence classification. I've built it in TensorFlow Keras and was using padded sentences (with words lemmatized) as input and 3 categories (""positive"", ""neutral"" or ""negative"") as output.  Below is the model I've built:I've tried training this model with just 200 sentences just to see if it overfits the data. But instead of overfitting, the loss value just goes up and down between 0 and 1. I've tried changing the learning rate for a value as small as 1e-8, but it did nothing. Below is the function I've used for training:And the result of the training:Any sugestions on how to make it converge?"
577,"Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2","['tensorflow', 'keras', 'nlp', 'lstm']","I am trying to create an image captioning model. Could you please help with this error? input1 is the image vector, input2 is the caption sequence. 32 is the caption length. I want to concatenate the image vector with the embedding of the sequence and then feed it to the decoder model."
578,What inputs and outputs should be passed to model.fit() for abstractive text summarisation using BERT embdeddings?,"['python', 'tensorflow', 'keras', 'nlp', 'bert-language-model']","I am quite new to NLP and Keras. I am working on a Abstractive text summarisation tool, with BERT as such:1 - Text data2 - Tokenize using BERT tokenizer (hugging face)3 - Pad input text, input summary and respective attention masks4 - Create corresponding tensors, x, y, att_x and att_y5 - Feed data to a encoder - decoder model with BERT embedding layer (huggingface)I feed to my model the input data =x, y, att_x and att_y and as output the embedded y summaries (last_hidden_states_y) as such:Is that correct? Or should I use the tokenised summaries as output data Y? I would assume that the output of my decoder is in itself an array of embedded words, but I am not sure. Can someone help me or maybe explain to me what I should expect my decoder's output to be like? I am not sure what my X and Y in model.fit() .Here is also my model (simplified). The input and outputs are tokenised before then. Thank you !"
579,Translating multiple columns in pandas dataframe to english,"['python-3.x', 'nlp', 'google-translate', 'translate', 'textblob']","I have a data frame where I have some comments mentioned in different languages that needs to be converted to English.My data frame looks like :en_comm is the english comment,
es_comm is the Spanish comment,
de_comm is the german commentEach case represents one type of comment in one language only. So to tackle blanks and translate the comments I am using the following code :This code is taking forever to run on a pretty small data set of 2000 cases and I have not got any output as of now"
580,Problem with csv file generation in NLP project,"['python', 'numpy', 'csv', 'nlp']","So me and my team were doing a NLP project - Tweet Sentimental Analysis on kaggle and while we have completed everything and the required output has been generated. We are facing a problem in file submission.We are requested to submit the final file in following csv file format:We are using this code to generate the csv file of our output:Now before creating the csv file when we try to output the tweet_ids on jupyter we get proper outputs:
As you can see all tweet id_s are generated correctly.The problem is that the generated csv file converts the tweet_id into exponential form and when run on jupyter ntbk displays something like this:As you can see after six digits everthing is displayed as 0.How to convert our data into a proper csv file as we have to submit the precise outputs?"
581,Cannot import BertModel from transformers,"['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert']","I am trying to import BertModel from transformers, but it fails. This is code I am usingThis is the error I getCan anyone help me fix this?"
582,spacy stemming on pandas df column not working,"['python', 'pandas', 'nlp', 'spacy']",How to apply stemming on Pandas Dataframe columnam using this function for stemming which is working perfect on stringBut when i am applying this function on my pandas dataframe column it is not working neither giving any errori've tried different thing but nothing working. like thismy dataset looks like this:
583,How to improve code to speed up word embedding with transformer models?,"['nlp', 'pytorch', 'word-embedding', 'huggingface-transformers', 'bert-language-model']","I need to compute words embeddings for a bunch of documents with different language models.
No problem with that, the script is doing fine, except I'm working on a notebook, without GPU and each text needs around 1.5s to be processed which is by far too long (I have thousands of texts to process).Here is how I'm doing it with pytorch and transformers lib:Do someone has any idea or suggestions to improve speed?"
584,Latest Pre-trained Multilingual Word Embedding,"['nlp', 'word-embedding', 'pre-trained-model', 'fasttext', 'bert-language-model']","Are there any latest pre-trained multilingual word embeddings (multiple languages are jointly mapped to a same vector space)?I have looked at the following but they don't fit my needs:Here is the problem I'm trying to solve:I have a list of company names, which can be in any language (mainly English), and I have a list of keywords in English to measure how close a given company name is with regards to the keywords. Now I have a simple keyword matching solution, but I want to improve it using pretrained embeddings. As you can see in the following examples, there are several challenges:Examples of company names: ""cheapfootball ltd."", ""wholesalefootball ltd."", ""footballer ltd."", ""soccershop ltd.""Examples of keywords: ""football"""
585,"Extract and map similar textresult with base text, convert into two columns using Pandas","['python', 'pandas', 'dataframe', 'nlp']","The following is gui result dataframe.In the above dataframe, Item_id and Result are correlated. Each Item_id has one Results.
Based on the similarity_Id, I need to create two different columns.
One column sentence one is base sentence and sentence2 is similarity sentences.
For example. In similarity_Id four sentences in Result have same similarity_Id.
Item_id of 101,103,104 and 106 have similar Result of Item_id 100.
So, in sentence 1 , I need to have Result respective to Similarity_Id 100, in sentence2 I need similar Results of Item_id 100.The final result needs to be as follows, I tried groupby and merge,melt and unique.
But, desired result not comes.How can I achieve this.
Thanks,
Sundara"
586,Creating classification labels from unstructured data python,"['python', 'pandas', 'nlp', 'nltk', 'wordnet']","Is there a way to create labels from a column of strings in a dataset using Python?  The format of the string varies between 1 to ~10 describing words, where most of the time the category will be more than 1 word (example below).I do not want to pre-populate the list of categories, but I would like the algorithm to create the categories based on common themes.This is what I would like:Input:Output:This is what I have tried:I tried Counter from Collections - but this only counts the number of words in the dataset column - rather than common theme (see example below)nltk.FreqDist but that counted 1 per observation (see second example below)I have tried wordnet from nltk.corpus which seems to categorise 1 word strings onlyFirst example - counter from collections Results:
counter from collections resultsnltk.FreqDist example:nltk.FreqDist results:nltk.FreqDist resultswordnet exampleI understand this is probably quite a complex beast of a subject, but any advice would be greatly appreciated!  I would do it manually but aside from wanting to learn something new, the dataset is around 50K observations"
587,"How To Compute Sentence Bigram Probability Given In A Chart Below,","['nlp', 'probability', 'stanford-nlp', 'n-gram']","Probability Values Are Here  Some other bigram probabilities might be helpful in solving, are given below.
P (I | (s)) = 0.25
P ((s)|food) = 0.20
Please estimate the probabilities with bigram estimates:
P ((s)   (/s)) =?
P ((s) I eat Chinese food (/s)) =?"
588,Natural language processing and discourse analysis,['nlp'],"I'm new to natural language processing and please understand me if my question is too naive. I am wondering if there is an algorithm for detecting whether an essay has specific points. For example, in the introduction, check if the essay mentions motivations. Or, check if each paragraph mentions the main sentence? Can this be only done with discourse analysis?"
589,How does PyTorch conv1D handle the padding for variable length sequences?,"['nlp', 'pytorch', 'conv-neural-network', 'zero-padding']","I have a batch of variable length sequences padded with 0 (padding = post) and would like to perform conv1D to generate feature maps. How does PyTorch conv1D handle padded sequences? For example, the rnn (recurrent neural network) and crf (conditional random fields) layers do allow masking the padded sequences, but I did not see such an option in conv1D. I did check out the conv1D layer and there is an option to pad the sequences, but it does not seem to be similar to the padding and masking I am talking about. I will be really grateful for any help."
590,Logistic regression: X has 667 features per sample; expecting 74869,"['python', 'nlp', 'logistic-regression']","Using a imdb movie reviews dataset i have made a logistic regression to predict the sentiment of the review.prior to this text preprocessing have been applied.
Model performance is just a function to create a confusion matrix.
this all works well with a good accuracy.I now scrape new IMDB reviews:Problem: Now i wish to test the same logistic regression to predict the sentiment:But i get the error: 
ValueError: X has 667 features per sample; expecting 74869I dont get why it has to have the same amount of features as X_test"
591,Difference between model accuracy from test data and confusion matrix accuracy,['nlp'],What is the difference between both the accuracy's which one should be considered as the actual accuracy? and why?
592,Where to find resource of Japanese - Chinese dictionary,"['nlp', 'cjk', 'language-translation', 'machine-translation']",Hey I am trying to provide japanese - chinese translation functionality for my project. I have found Rikaichan which is a chrome plugin that achieves a popup japanese - english translation. Rikaichan uses some online dictionaries for translation. In the link this project provides there are a lot of dictionaries corresponding to different languges. I am wondering is there a Chinese version of this kind of dictionary? If not is there any online resource for japanese - chinese dictionary data I could retrieve?
593,Is there a measure of statistical divergence conditional on number of observations?,"['nlp', 'entropy', 'information-extraction', 'cross-entropy', 'information-theory']","I'm interested in analyzing the semantic distance between two terms (let's call them A and B) and how it changes over a period of 10 years. Term A is new and was barely used at the beginning of the time period but becomes more heavily used towards the end. Term B is heavily used throughout and is a sort of a default category. Theoretically, I'm interested in whether the the semantic distance of A and B increases or decreases over time.I measure a term's ""meaning"" by a count vector of other terms is co-occurs with. I'm aware that there are other, more elaborate ways to vectorize such features but for reasons that would be cumbersome to explain, I am bound to operating with these count vectors.I have been computing various measures of statistical divergence (e.g. KL divergence, Jensen-Shannon divergence) between the count vectors of A and B for moving time windows and I find both to decrease over time, suggesting that A becomes closer to B. However, I have very few observations of term A at the beginning of my time series, which leads to a sparsely filled co-occurrence vector. I worry that this is what is driving the large values for KL and JSD at the beginning of the time series and that both naturally decrease with a larger N of co-occurrence counts. In other words: a small sample even from the same underlying probability distribution is going to have higher divergence on average than a large one.I wonder whether there is a way to account for the fact that I have few observations. Perhaps a measure that accounts for N in some way. Or whether I should try to correct for this by bootstrapping in one way or the other.Below is code to illustrate my point about higher KL values for lower N."
594,Creating document-feature matrix takes very long in R,"['r', 'nlp', 'n-gram', 'quanteda', 'dfm']",I am trying to create a document feature matrix with character-level bigrams in R. The last line of my code takes forever to run and never finishes. The other lines take less than a minute max. I am not sure what to do. Any advice would be appreciated.Code:
595,Python - what is the reasoning for type aliases in this example?,"['python', 'nlp', 'huggingface-transformers', 'roberta']","I was just analyzing the Roberta LanguageModel example:And I wonder what are the benefits of ""typing"" in this line:as I understand it basically says:assign the result from AutoModel.foo() to variable model and remember,
  that its type is RobertaModel.The author could have easily rewrite it to either of: since either solution would be easier to change in the future. I feel like using AutoModel while specifying a variable type is a little bit of a contradiction. Am I wrong? What would be the use of such line?"
596,determining best fit distributions by SSE - Python 3.8,"['python', 'numpy', 'scipy', 'nlp', 'statistics']","I am trying to come up with a way to determine the ""best fit"" between the following distributions:
Gaussian, Multinomial, Bernoulli. I have a large pandas df, where each column can be thought of as a distribution of numbers. What I am trying to do, is for each column, determine the distribution of the above list as the best fit.I noticed this question which asks something familiar, but these all look like discrete distribution tests, not continuous. I know scipy has metrics for a lot of these, but I can't determine how to to properly place the inputs. My thought would be:An example dataset (arbitrary, my dataset is 29888 x 73231) could be:I have some basic code now, which was edited from this question, which attempts this:However, I get:My expected output would be something like, for each column:
Gaussian SSE: <val> | Multinomial SSE: <val> | Bernoulli SSE: <val>UPDATE
Catching the error yields:Why am I getting errors? I think it is because multinomial and bernoulli do not have fit methods. How can I make a fit method, and integrate that to get the SSE?? The target output of this function or program would be, for aGaussian, Multinomial, Bernoulli' distributions, what is the average SSE, per column in the df, for each distribution type (to try and determine best-fit by column).UPDATE 06/15:
I have added a bounty.UPDATE 06/16:
The larger intention, as this is a piece of a larger application, is to discern, over the course of a very large dataframe, what the most common distribution of tfidf values is. Then, based on that, apply a Naive Bayes classifier from sklearn that matches that most-common distribution. scikit-learn.org/stable/modules/naive_bayes.html contains details on the different classifiers. Therefore, what I need to know, is which distribution is the best fit across my entire dataframe, which I assumed to mean, which was the most common amongst the distribution of tfidf values in my words. From there, I will know which type of classifier to apply to my dataframe. In the example above, there is a column not shown called class which is a positive or negative classification. I am not looking for input to this, I am simply following the instructions I have been given by my lead."
597,Extract character-level n-grams from text in R,"['r', 'nlp', 'character', 'n-gram']","I have a dataframe with text and I want to extract the character-level bigrams (n = 2), e.g. ""st"", ""ac"", ""ck"",  for each text in R. I also want to count the frequency of each character-level bigram in the text.Data:"
598,"Incompatible shapes: [128,37] vs. [128,34]","['python-3.x', 'tensorflow', 'keras', 'nlp', 'machine-translation']","I have added attention layer in an LStM model for encoder-decoder.The model.fit functionAnd this is the error I am gettingMy batch size is 128.The generate batch function isHere max_length_src = 34, max_length_tar=37. The error seems to come due to this.Please help."
599,"How to remove substrings “- ” from a string in python, but keeping “ - ” substring?","['python-3.x', 'string', 'replace', 'nlp', 'str-replace']","Example:I need to get the same text but equip- ment to became equipment, and appli- cations to became applications.
Thanks"
600,how can i solve issue on module error tflearn,"['tensorflow', 'nltk', 'tflearn']","after trying so many things i am not able to solve the error.
I started with importing the nltk, after that there's a sub library called tflearn ,it is showing error like no module name tflearn."
601,Extract nouns from sentence (Italian),"['python', 'nltk']","I would need to extract only nouns from these sentence:The nouns are gatto and tavolo. I think nltk should have a package that I can use to extract them from the sentence.This was my try:it works, but partially, as the output includes also sotto and il, which are not nouns but stopwords. I was wondering if there is something more practical than removing stopwords, then extracting nouns."
602,Filter sentencs based on search words using python?,"['python', 'text', 'nltk']",I got a text file containing many sentences. I want to query the text file with search words and return those sentences that contain the query words.Effort so far:file.txt contains these sentencesOutput: for the 'galleries' search  it returns galleries twice but i need to return the sentences.How to query multiple words search and return those sentences containing those combination (not necessarily an n gram or in order) For example if i type 'overall' as one word and 'Layout' as another search word it should return the following sentence. Search words are case insensitiveHELP!
603,ImportError: No module named nltk macos,"['python', 'anaconda', 'pytorch', 'nltk']","Starting on a Python chatbot and using anaconda followed a tutorial and could not import nltk, using pytorch as well.The error after attempting to import nltk"
604,Some extracted mentions in the coreference clusters have bad format in neuralcoref package,"['python', 'nltk', 'spacy', 'coreference-resolution']","I'm using neuralcoref - a coreference resolution package based on spaCy .I have a problem when working with neuralcoref. I want to do coreference resolution for my doc and then split its resolved version to sentences. I expect that the number of sentences in the original doc and its resolved version is the same. but the number of sentences in the resolved version is less. I check the sentences and understand the reason.
In the resolved version, each mention in the text has replaced with the most representative entity (call it MRE) in its coreference cluster. But:
This MRE maybe a mention in the middle of a sentence and so its first word is in lower case.
or
This MRE may be located at the end of a sentence and so it has a dot at the end.The first situation causes us to have a sentence starts with a lower case word and so NLTK sent_tokenizer can not considers it as a sentence.
The second situation causes we to have a wrong dot in the middle of a sentence and so NLTK sent_tokenizer considers that sentence as two sentences.I think neuralcore should upper case MRE when it replaces with the first word in a sentence. And also neuralcoref should drop dot at the end of the mentions in the coreference clusters.Can I set neuralcoref to do these changes?Thanks in advance."
605,Moving all capital letter words in file to beginning of string. Python NLTK regex,"['python', 'regex', 'nltk']","file1.txtDT The JJ quick JJ brown NN fox VBZ jumps IN over DT the JJ lazy NN
dog.DT The JJ quick JJ brown NN fox VBZ enjoyed PRP his NN day.file2.txtDT  JJ  JJ  NN  VBZ  IN  DT  JJ  NN   The big brown fox jumps over the
lazy dog.DT  JJ  JJ  NN  VBZ  PRP  NN      The big brown fox enjoyed his day.What I've tried so far:Pythonand I've tried this"
606,Django webapp (on an Wampserver) hangs indefintely when importing nltk in views.py,"['python', 'django', 'apache', 'nltk', 'wampserver']","I've deployed the web app using Wampserver on a Windows Server 2012. When I import nltk in views.py, the web page refuses to load.Add this WSGIApplicationGroup% {GLOBAL} in the Wampserver htpd.config configuration file as indicated in this post and part of the problem has been resolved, but now when running the application using nltk package, the browser shows Server Error (500)"
607,chunk structures must contain tagged tokens or trees,"['python', 'regex', 'nltk']"," in 
52
53 for pos_tagged_sentence in pos_tagged_text:
---> 54     np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))
55
56~\Anaconda3\lib\site-packages\nltk\chunk\regexp.py in parse(self,
chunk_struct, trace)    1290         for i in range(self._loop):
1291             for parser in self._stages:
-> 1292                 chunk_struct = parser.parse(chunk_struct, trace=trace)    1293         return chunk_struct    1294~\Anaconda3\lib\site-packages\nltk\chunk\regexp.py in parse(self,
chunk_struct, trace)    1096             trace = self._trace    1097
-> 1098         chunkstr = ChunkString(chunk_struct)    1099     1100         # Apply the sequence of rules to the chunkstring.~\Anaconda3\lib\site-packages\nltk\chunk\regexp.py in init(self,
chunk_struct, debug_level)
97         self._root_label = chunk_struct.label()
98         self._pieces = chunk_struct[:]
---> 99         tags = [self._tag(tok) for tok in self._pieces]
100         self._str = '<' + '><'.join(tags) + '>'
101         self._debug = debug_level~\Anaconda3\lib\site-packages\nltk\chunk\regexp.py in (.0)
97         self._root_label = chunk_struct.label()
98         self._pieces = chunk_struct[:]
---> 99         tags = [self._tag(tok) for tok in self._pieces]
100         self._str = '<' + '><'.join(tags) + '>'
101         self._debug = debug_level~\Anaconda3\lib\site-packages\nltk\chunk\regexp.py in _tag(self, tok)
107             return tok.label()
108         else:
--> 109             raise ValueError('chunk structures must contain tagged ' 'tokens or trees')
110
111     def _verify(self, s, verify_tags):ValueError: chunk structures must contain tagged tokens or treesI am getting this error but can not find actually why my line 52 is showing that error. below this link, I did it but could solve the issue. can anyone help me[1]: https://stackoverflow.com/questions/13269543/why-am-i-getting-error-valueerror-chunk-structures-must-contain-tagged-tokens/13289830"
608,error about 'list' object has no attribute 'split',"['python', 'list', 'split', 'nltk', 'stemming']","The code below does not run. The parameter passed to the function is a list of strings.
AttributeError: 'list' object has no attribute 'split'"
609,How to find uncapitalised proper nouns with NLTK?,"['python', 'nltk', 'pos-tagger']","I'm trying to make a 'fix faulty capitalisation' program, and I'm trying to find proper nouns in python using NLTK's pos tagger. The problem is that it doesn't seem to be working very well for text with faulty/missing capitalisation.This is the code I have so far:And the output is:As you can see, there's quite a few mistakes. ""Nice"" gets tagged as a proper noun, as does ""Friend"", while ""bob"" and ""america"" don't.How I can find proper nouns regardless of capitalisation?"
610,how to check if the model in ML is good for the dataset when you apply the model on real data?,"['python', 'scikit-learn', 'nltk', 'sentiment-analysis', 'text-classification']","I have a python script that classify text positive or negative.
I have a dataset i split it to train and test data after i done a pre-process to the text  i gotWhen i tried on real data it give 20% accuracy where is the error ??Trained dataTested dataI am using Logistic Regression as a ML model  and using TfIdf  and cross validation."
611,Get frequency of sentence tokens (instead of words) by group in pandas,"['python', 'pandas', 'nltk']","I have a pandas column, which is titles for online shopping products, classified by categories:I tokenized the title columns by words and it succeed (returns lists of words)My main goal is to get tokens frequency by category, I did it with words tokens, but I can't did it for sentencesThis is the code for frequencies by category for words:I want to do the same for sentences instead of just words
I tried to tokenize the title column by sentences by return the whole title as sentence"
612,Get frequency of tokens by group in pandas,"['python', 'pandas', 'nltk', 'pandas-groupby']","I have a pandas column, which is titles for online shopping products, classified by categories:I try to tokenize the title column, but returns not words, but letters. This is what I did:This is what it returns:I try to do the same to a new column of tokenized text, but don't workEDIT
When I runit returns list of lists of words like below:EDIT 2
I tried this code:andbut both returns this :"
613,How do I link NLTK in Jupyter on a mac?,"['python', 'macos', 'nltk']",I have already downloaded NLTK but Jupyter Notebook still shows:ModuleNotFoundError: No module named 'nltk'
614,How to remove English Words from a column in a CSV file using Python,"['python', 'nltk']","Very new to Python.Problem: I have a csv file that contains rows with alpha-numeric text, and I want to remove all English words. For example, an input is: ""Steam traps on Steam to 56X-233 Butane Vaporizer""
and the desired output is just: ""56X-233""Is the answer like removing stop words with NLTK?Thank you."
615,Extract proper names from a dataframes,"['python', 'pandas', 'nltk']","I would like to extract only proper nouns from this dataframe:I have only the following proper names:however the first one (matt) is not recognised as proper name and it let me think if there might be cases where the proper name is tagged as verb, for example. Probably it depends because the first letter is lowercase.I have tried as follows:For identify NNP, I have tried as follows:but I get this error:ValueError: too many values to unpack (expected 2).However, I would need to remove improper names, for example More or words that, after a period/full stop, start with a capital letter and that they might be wrongly classified as NNP."
616,I need to sum the word that are from the same files in Python,"['python', 'list', 'python-2.7', 'nltk', 'stanford-nlp']","I am trying to find the same words from different files in Python this codes works, but I need to sum the same words from the result that I get from the same files. How can I implement, please help me, I appreciate :)Thank you!!!"
617,Tokenization by date using nltk,"['python', 'pandas', 'nltk']","I have the following dataset:I am applying CountVectorizer as follows:to get the highest frequency values for bi-grams. Since I would be interested in getting this info by date (i.e. grouping by 01/18/2020 and 01/19/2020 to get the bi-grams per each date), what I have done is not enough, sincecreates an empty dataframe with no information about Date. How could I grouped bi-grams per date? If I was interested in one-gram, I would have done something like :I do not know how to do something similar using nltk and CountVectorizer. I hope you can help me.Expected output:"
618,Bi-grams by date,"['python', 'pandas', 'nltk', 'countvectorizer']","I have the following dataset:I am interested in a dataframe which shows bi-grams' frequencies by Date.
Currently I am doing as follows:But it does not show the bi-grams by Date, only their frequency.
I would like expect something like this (expected output):and so on. bi-grams_1, bi-grams_2, ... are just used as example.Any advice on how I can get such a dataframe?"
619,Selecting only a single synset synonym for each item in the dictionary,"['python', 'nltk', 'wordnet']","I have a python dictionary of words where I want to attach a single synonym to each of the items in the dictionary using Wordnet's synsets function. My current code picks up several synonyms but I only want to be able to narrow it down to just one word (i.e. the first in the set). Additionally, if there isn't an available synonym, I would like for it to just use the word as is in the dictionary.I'm fairly new at this so any help would be greatly appreciated. Thanks in advance."
620,Can't import NLTK into Jupyter Notebook,"['python', 'python-3.x', 'pip', 'jupyter-notebook', 'nltk']","I'm pretty new to python, Jupyter notebook, Tensorflow, and that whole lot in general. I'm getting started with a machine learning project. I've gotten to the point where I want to import ""nltk"" into my thing. It doesn't work. I've installed nltk with pip, and conda, and everything, in my terminal. When I do it again in the notebook, it says I've already installed it, which is correct. But when I try to import it it gives me a ModuleNotFoundError:I'm on a macbook, by the way. Any help?"
621,"A way to find action verb, cognition verb, stative verb and trigger words using python","['python-3.x', 'nltk']","I'm trying to find a way in python to identify action verbs, cognition verbs, stative verbs in a text.Below is my code. I found a rule of action verbs. Is it correct? I don't have any idea to find the cognition and stative verbs (rule of cognition and stative verbs). Anyone can help me to find these rules?Output: limit #limit is an action verb in documentAlso, how can I find trigger words at the first of the sentence(i.e. Start with a trigger word)"
622,NLTK stopwords Python,"['python', 'nltk']","I want to cleanup text files to be used as input to NLTK functions.  I have a custom list of stopwords that I want to remove as well as the NLTK conventional stopwords.  I also want to tokenize , convert to lower case, remove numerics and lemmatize  the text file. So I wrote a script to do this but I have encountered an error that I don’t understand or know how to correct.TThe program fails at the #remove stops words with an error message:The custom and NLTK stopwords are in a text file in the form of a list. Following is an abbreviated version.
['abell', 'abler', 'abramson', 'adam', 'adams', 'addison', 'adler', 'ahmed', 'ajello', ''from',  'wasn', 'hasn', 'aren', 'an', 'other']The fileinD (input to fileinE) has the  form;
['test', 'test', 'test', 'abell', 'abler', 'abramson', ,adam', 'adams', 'addison', 'adler', 'ahmed',  'ajello', 'akins', 'al', 'alexander']I will appreciate help on understanding why the script is not working and any suggestions as to what needs to be modified."
623,AttributeError: 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args',"['python', 'nltk', 'wordnet']","I am trying to create multiple execution at same time but getting error which does cleaning of text data but I am getting below errorAppData\Local\Programs\Python\Python37\lib\site-packages\nltk\corpus\util.py"", line 94, in __load     args, kwargs = self.__args, self.__kwargs AttributeError: 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'I am using below cleaning functionBut when I execute script one at a time it works without error
Can anyone please help me in this"
624,Chatbot in Python with custom data,"['python', 'nltk', 'chatbot', 'chatterbot']","I want to make a chatbot for my data. Lets I have stored data of personal information of people. So when I interact with chatbot, I tell bot my personal ID and ask it questions like what is my age? etc. Bot should be able to Understand question, look for information in the data and return answer. Suppose I have data in the json file.I have no experience in building chatbots, so any kind of help will be very appreciated. I have seen different tutorials and libraries (chatterbot, nltk).ExampleMe:What is my age?Bot:Your age is 24Me:How old am I?Bot:You are 24 years old.Me:What are my hobbies?Bot:CricketThe thing I need is that I want the bot to understand question and give the relevent information."
625,Unable to instantiate class nltk.classify.textcat.TextCat,"['python', 'regex', 'nltk']","I'm trying to run the following:I get the following error:I've tried adding import regex  before import nltk but unfortunately I still get the same error.
What have I missed?"
626,I want to able to predict the category,"['scikit-learn', 'nltk', 'data-science']","this is a my ML model to classify resumescleaning the resumes of any punctuations ,url , mentions ... and storing it in the resumeDataSet['cleaned_resume'] :cleaning once more the data of any english stopwords :Convert the categorical variables into numericals :.I want to be able to give to this model a new cleaned_resume and it predicts the category"
627,Generating Shakespearean Text Using a Character RNN,"['python', 'machine-learning', 'neural-network', 'nltk', 'recurrent-neural-network']",I am reading a ML book Hands on Machine Learning (2nd edition) there is a topic on page 526  Generating Shakespearean Text Using a Character RNN I am doing exactly what they are doing but at the time of training it showing TypeError. I did my best to solve this problem on my level.TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'Here is the code
628,MemoryError in processing text data in Pandas (Windows 64-bit),"['python', 'python-3.x', 'pandas', 'nltk', 'stop-words']","I am trying to process a large number of texts amounting nearly 2GB in Windows 64-bit. However, when I run codes to lower, tokenize and remove stopwords I get MemoryError. Here is the code I am using and error I am getting in return."
629,Matrix from bigram?,"['python', 'pandas', 'nltk', 'probability']","I have a list of chordsand I want to turn it into a transition matrix. What I have so far:I want the matrix to be like:where p(FF) is the probability of (F, F) which is in my probabilities list.
How can I do this? Thank you!!"
630,how to do Lemmatization using NLTK package,"['python', 'nltk', 'wordnet', 'lemmatization']",I have python script that must perform a lemmatization for the giving input (dataframe).the problem is when i try to print the result of most existing word in the dataframe it display the below result.where the result must be :code:
631,Tokenize don't to dont using NLTK Python,"['python', 'nltk']",When I use:I getWhat I want is:
632,Perform NLTK on text files using AWS Glue,"['amazon-web-services', 'apache-spark', 'nltk', 'aws-glue']","So I want to perform NLTK on Textfiles which are stored on S3 as a part of the ETL process in AWS Glue.
[https://drive.google.com/drive/folders/1ne16UVNHd0K6790CXsiMdyRRcgZeatjn?usp=sharing][1] is the External Python dependencies I have used for ETL Job.I have changed in data.py file which is located in NLTK Package according to [https://stackoverflow.com/a/45069242/12927963][2] this, but I have used this ""/tmp/nltk_data"" as Path Variable.This is my sample scriptbut I got an error in NLTKand also shows following logsis ""/tmp/nltk_data"" is a proper path for NLTK data in AWS GlueIf I don't add NLTK data in Python External zip, it downloaded on ""/home/nltk_data"", but it's not accessible by Spark Application.Please help me with this use case.
[1]: https://drive.google.com/drive/folders/1ne16UVNHd0K6790CXsiMdyRRcgZeatjn?usp=sharing
[2]: https://stackoverflow.com/a/45069242/12927963"
633,Python 3.x How extract alphabetic representation of numbers from string,"['python-3.x', 'parsing', 'text', 'nltk']","I need to parse text that may contain alphabetical numbers. For exampleorMy goal is get substring sixty six and fourIn internet a lot of approaches when converting number string representation to integer, without additional texts. But i need to next result:"
634,finding word list in string using python parallel,"['python', 'multiprocessing', 'nltk', 'python-multiprocessing', 'python-multithreading']","i know this question answered several times in different places, but i'm trying to find things to do in parallel. i came across this answer from Python: how to determine if a list of words exist in a string answered by @Aaron Hall. it works perfectly, but the problem is when i want to run the same snippet in parrllel using ProcessPoolExecutor or ThreadPoolExecutor it is very slow. normal execution takes 0.22 seconds to process 119288 lines, but with ProcessPoolExecutor it is taking 93 seconds. I don't understand the problem, code snippet is here.single thread takes 0.22 seconds.I have 50GB + single file (google 5gram files), reading lines in parallel this works very well, but above multi thread is too much slow. is it problem of GIL. how can i improve performance.sample format of file (single file with 50+GB, total data is 3 TB)"
635,compute the word coverage for the given fileid associated with text corpus inagurual using nltk,"['python', 'text', 'nltk', 'corpus', 'calculated-field']","I Have tried the below code.#!/bin/python3It is passing only one test case. Remaining all test cases showing the same result. Could you please suggest me what i need to change.These are the outputs.Your Output (stdout)
2
['United', 'accomplished', 'addressed', 'advanced', 'affected', 'allotted', 'assembled', 'awakened', 'called', 'committed', 'compared', 'considered', 'consulted', 'contemplated', 'delegated', 'departed', 'derived', 'distinguished', 'employed', 'enlarged', 'entrusted', 'established', 'exemplified', 'expected', 'experienced', 'filled', 'forced', 'fortified', 'guided', 'honored', 'imparted', 'included', 'instituted', 'judged', 'limited', 'ordained', 'originated', 'palliated', 'persuaded', 'placed', 'pleased', 'produced', 'promoted', 'received', 'rendered', 'repaired', 'required', 'resulted', 'sacred', 'selected', 'staked', 'submitted', 'summoned', 'suppressed', 'swayed', 'transmitted', 'united', 'unparalleled', 'unpracticed', 'untried', 'urged']
8Expected Output
Download
2
['United', 'accomplished', 'addressed', 'advanced', 'affected', 'allotted', 'assembled', 'awakened', 'called', 'committed', 'compared', 'considered', 'consulted', 'contemplated', 'delegated', 'departed', 'derived', 'distinguished', 'employed', 'enlarged', 'entrusted', 'established', 'exemplified', 'expected', 'experienced', 'filled', 'forced', 'fortified', 'guided', 'honored', 'imparted', 'included', 'instituted', 'judged', 'limited', 'ordained', 'originated', 'palliated', 'persuaded', 'placed', 'pleased', 'produced', 'promoted', 'received', 'rendered', 'repaired', 'required', 'resulted', 'sacred', 'selected', 'staked', 'submitted', 'summoned', 'suppressed', 'swayed', 'transmitted', 'united', 'unparalleled', 'unpracticed', 'untried', 'urged']
8TestCase1:Input (stdin)
Run as Custom Input
|1793-Washington.txt
iYour Output (stdout)
2
['United', 'accomplished', 'addressed', 'advanced', 'affected', 'allotted', 'assembled', 'awakened', 'called', 'committed', 'compared', 'considered', 'consulted', 'contemplated', 'delegated', 'departed', 'derived', 'distinguished', 'employed', 'enlarged', 'entrusted', 'established', 'exemplified', 'expected', 'experienced', 'filled', 'forced', 'fortified', 'guided', 'honored', 'imparted', 'included', 'instituted', 'judged', 'limited', 'ordained', 'originated', 'palliated', 'persuaded', 'placed', 'pleased', 'produced', 'promoted', 'received', 'rendered', 'repaired', 'required', 'resulted', 'sacred', 'selected', 'staked', 'submitted', 'summoned', 'suppressed', 'swayed', 'transmitted', 'united', 'unparalleled', 'unpracticed', 'untried', 'urged']
23Expected Output1
['called', 'distinguished', 'reposed', 'united', 'violated']
6"
636,Given a piece of text tokenize location text,"['location', 'nltk']","I am trying to see if there is a handy python library that can take a string and return indexes of words that represent a location?Example, Input text ""New York is a state in United States""Returns tokensNew York
United StatesOr their respective indices in text.I want to use it to highlight locations in display text automatically in my website.Thanks,
Lalith"
637,How to convert REGEX array to String array in Python Chatbot?,"['python', 'python-3.x', 'regex', 'list', 'nltk']","I have a Chatbot with interactive communication.I used nltk library.I have modified Chat class for necessary functions.I want to save session.However I did it.But when I print the list which has session record, just print different way from I expect.How can I convert this array to normal String array ? I just needmatch ='blah blah'part.Thanks all."
638,Is there a simple way to know the gender of a person proper noun in NLTK or spacy?,"['python-3.x', 'nltk', 'spacy']",I need to know if the a person proper noun reffers to a boy or to a girl using spacy or nltk. I tried the wordnet dictionary but I ws not able to find this information.Example:Alexander => MaleSophie => Female
639,How to use nltk nltk.corpus.genesis.words for finding most used phrases,"['python', 'list', 'nltk']","I m trying to find the most common phrases in a list of lists uusing nltk:But I get an error:Which I believe it is because I need to use a text file, but how can I send a list to my method in order to find common phrases and words using nltk?"
640,Calculate the probability of co-occuring keywords in my text file,"['python', 'list', 'nltk']","I have two text files, the first contains one sentence each line and the second contains one keyword each line like this :I want to use PMI to calculate how much more- or less likely some keywords are to co-occur in my text file.Here's what I did :This gives me the following error:Note that program worked fine when I tested it on one sentence and the pmi between its own words. I have more trouble adding the two files. Any help?"
641,"How to take output of nltk.book, common_contexts function to a variable","['nltk', 'nltk-book']",common_contexts in nltk.book returns NoneType and hence how is it possible to store its output to a variablewtc variable above will return NONE.
642,How to build intelligent chat bot ML model,"['python', 'machine-learning', 'artificial-intelligence', 'nltk', 'chatbot']","How to build a ML model that can able to predict and talk to user. basically i was looking for a dialogue type conversation where i keep the list of predefined answers to a question so it can be trained with variety of user question.How we deploy it in real time. Was checking with below links but couldn't get a clear view of it.https://towardsdatascience.com/develop-a-nlp-model-in-python-deploy-it-with-flask-step-by-step-744f3bdd7776
https://towardsdatascience.com/build-and-compare-3-models-nlp-sentiment-prediction-67320979de61I know above examples are bit deviating but just want to know from above example they trying to convey every time when a request comes in all the predict model code need to be executed ?"
643,How to check for specific words in a list of tokenized sentences and then mark them as one or zero?,"['python', 'list', 'nltk', 'tokenize']","I am trying to map specific words in a list to another list of tokenized sentences and if the word is found in the sentence then i append a 1 to a list of its category and 0 to the rest of categories.
For example:Similarly for category_b and category_cI am getting duplicate outputs for each sentence like: i love this product-->[1,0,0]
i love this product-->[1,0,0] and
also like this:[i love this product,i sweat all day]-->[0,1,0]Please help me resolve the issue and get the output in the required format."
644,# string methods TypeError: Column is not iterable in pyspark,"['python', 'pyspark', 'nltk', 'apache-spark-ml', 'lemmatization']","Im trying to re-implement sentiment analysis which is written in python to pyspark as im working with bigdata, im new to pyspark syntax, and im getting an error while trying to apply lemmatization function from nltk packageError: # string methods TypeError: Column is not iterable
Below is the Code and Datafunction lemmatize(function calling)please refer the below link for the error
Error"
645,Running nltk module with python,"['python-3.x', 'nltk']","I have installed python 38 on my PC (Windows 10 Pro, 64 bit).  It is in the following directory;C:\Users\rschafish\AppData\Local\Programs\Python\Python38-32I then downloaded nltk-3.5 (the download is a zip file obtained irectly from the nltk website) and I unzipped it and placed the files in the python Lib directory.C:\Users\rschafish\AppData\Local\Programs\Python\Python38-32\Lib\nltk-3.5I opened python using IDLE (Python 3.8.3 Shell) and the opening line from python includes;(Python 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020 22:20:19 MSC vl1925 32 bit Intel on win32)I then entered >>> import nltk  and received the following error message;Traceback (most recent call last): File”<pyshell#0>”, line 1 in  import nltk
ModuleNotFoundError: no module named ’nltk’It appears that this may be a problem related to the python path however I thought that placing the nltk files in the python Lib directory is what should be done.  I have searched the issue online but did not find any solutions that have worked for me.Help and advice will be greatly appreciated."
646,Shorten Sentence using Python Library like nltk [closed],"['python', 'nltk']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 28 days ago.I am using Nltk to remove stopwords from a sentence.eg. ""I would love to fly again via American Airlines""Result: ""Love to fly American Airlines""I had tried the following Code :This result is an empty string because I think the sentence is too short for Nltk to work. Just researching if there's an easier approach to this, I'm planning to train a model for this."
647,POS using a column (in pandas),"['python', 'pandas', 'nltk']","I would like to extract only nouns from this dataset:I need to apply POS in Text2 in order to extract only ADV. I did as followsbut I have not included the column, as I dn not know what I should put (e.g. df['Text 2'].tolist())What I would need is to extract adverbs from the text and add these in a new array/empty list.
I hope you can help me"
648,"Does anyone know a fast way to retrieve all 9 letter English words? - Lexical Database Searching (wordnet, python)","['python', 'performance', 'nltk', 'wordnet', 'lexical']","I am trying to sift through a lexical database (ideally wordnet via NLTK in Python) and extract all 9 letter words. Does anyone know how to do this? The documentation did not show any promising avenues.I can't just try every 9 letter combination and check if it is defined as this will take forever. However, simply iterating through the lexical database and extracting 9 letter words is feasible.If I could sort the database in advance, I know this could be very fast.So this all seems possible and crossword solvers and dictionary programs must have a way of doing this. Does anyone know how to approach this in Python?"
649,Extract a part of address in python,"['python', 'pandas', 'numpy', 'scikit-learn', 'nltk']","I am new to machine learning and python. I have a dataset that has an address column with""Mankhurd mhada, Zakir Hussain Nagar, Mankhurd, Mumbai"" addresses.I want to keep just the Mankhurd part of the address. A single word like ""Mankhurd"" or 2 words like ""Mankhur west"" determining the area.I would really appreciate help on how to extract a certain part of the text using python.Thank You"
650,VADER LEXICON. How can I change NEGATE WORDS and BOOSTER WORDS?,"['nltk', 'sentiment-analysis', 'vader']","I have adapted the lexicon to Portuguese using this code:Now, I would like to change negate words as well as booster words from English to Portuguese.So, how can I change the code for adding new words?Thanks!"
651,Compute probabilities with bigrams in python,"['python', 'nltk']","The output looks like this:[(die, studiengangsgross),  (studiengangsgross, bachelor),  (bachelor,
ca),  (ca, 6070),  (6070, student),...]So does anybody know how to do approximate matching here? 
Thanks in Advance!"
652,Cleaning text in a pandas column,"['python', 'pandas', 'nltk']","I have a problem in cleaning this dataset:I would need to plot Text column in this dataset:Specifically, I would need to work on the Text column. The rows within that column are the results of a tokenisation, so they are included in brackets. What I would like to do is to plot those words in a words cloud or a simply bar chart for studying their frequency.
The problem that I am having is in the duplicate of the same word, for example 'photo' 'photo' when I try to plot the frequency.My code is for preparing the dataset for a wordcloud.I would really appreciate if you could have a look or try a different approach to fix this issues with duplicate words (frequency is important so I cannot drop any duplicate rows) and eventually with quotation mark '. Thank you"
653,PHP python NLTK integration,"['python', 'php', 'laravel', 'nltk']","I have server in PHP so I'm in the need of a sentence tokenizer so the best until now that I tested was NLTK for python. I used Symphony/Process do the call the script. I couldn't pass long strings so created a temporary file to send the text for the parser.
The problem is mostly is to parse the end result. But I also wish for opinions to better my codefunction that calls the python script:python script:It keeps returning a string like this(some text that I'm using to test the parser):I tried with json_decode with no success even if a remove the last char \r\n
I just wanna know if someone would know how to better resolve this problem.
Thank you in advance"
654,What does the score mean in rake-nltk?,"['python', 'nltk']",OutPut:What does 1.0 stand for in each iteration ?
655,Unicode on Python,"['python', 'utf-8', 'nltk', 'python-unicode']","I have been trying to run the following code:I get the following error:Therefore, I tried substituting the unicode command with 'str' in the 2nd line, but I get the following error:I am getting the script from: https://github.com/DipakMajhi/Classify-Industry-for-Companies/blob/master/Classify-Industry-for-Companies.ipynb.Thanks in advance! :)"
656,"How can I get text summary for each paragraph of an article using Python, NLTK?","['python-3.x', 'nltk']","I need to extract summary for an article say https://pubmed.ncbi.nlm.nih.gov/30897142/Currently the code that I'm using to perform extractive summary is something like this:I want the summary to mandatorily include the words 'Background', 'Methods', 'Results', 'Conclusions' and have the summary displayed for those subheadings in this manner:"
657,Text Analysis using NLTK,"['nltk', 'word-count']","I'm currently working on a project where I'm taking text data, stripping out the unwanted things in it  then I want to calculate various text analysis variables like Sentiment and etc.How can I find Complex Word count from a text data using NLTK python??"
658,On Python TextBlob : limit size of model when update,"['python', 'machine-learning', 'nltk', 'textblob']","I am working on categorize texts using TextBlob on Python and NaiveBayesClassifier.The goal is to do continuous learning, which is made possible on NaiveBayesClassifier by using the update method.But there's a problem : the classifier grow and grow and grow without limit !How I can set a limit or ""clean"" the classifier from useless data after update ?Looking in the sources in NLTK ( https://www.nltk.org/_modules/nltk/classify/naivebayes.html ) I didn't saw anything useful."
659,ImportError: DLL load failed: The specified module could not be found in VScode,"['python-3.x', 'sqlite', 'nltk']",I am using virtual environment. Also activated the virtual environment in vs code but still facing the import error DLL load failed . This is for NLTK module and sqlite3. Please help me to resolve this.
660,Value error when called printing function in pairs in Python Chatbot,"['python', 'python-3.x', 'nltk']","I am working on chatbot.But while use function links in pairs, some errors occured.I want to print subjects in list.And after user can choose the subject which is desired.But while printing subjects, there is some problem which i could't solve.But I got this error :I can not find why is that error return. I check my loop but nothing change."
661,Regular expression: match word not between quotes,"['python', 'regex', 'string', 'quote']","I would like a Python regular expression that matches a given word that's not between simple quotes. I've tried to use the (?! ...) but without success.In the following screenshot, I would like to match all foe except the one in the 4th line.Plus, the text is given as one big string.Here is the link regex101 and the sample text is below:"
662,SystemExit: 2 error with Argparse for SRL (Semantic Role Labeling),"['nltk', 'argparse', 'spacy', 'allennlp', 'srl']",I have a file named srl.csv and I am trying to apply SRL (semantic role labeling with allennlp) for the sentences in this csv. I tried the same code with colab but still had the same error. I would be happy if you could help me to make this code working.I am using jupyterlab through anaconda and use the below code:The error I got;
663,word similarity query with fasttext,"['python', 'nltk', 'distance', 'fasttext']","I have two lists of words, say,list 1 : future proof
list 2 : house past foo barI would like to calculate the semantic distance between each word of list 1 with each word of list 2.
Fasttext has a nice function to display the nearest neighbours but it would be nice if there was a way to read the semantic distance between two defined words out.
Can anyone help, please?Thanks"
664,"NLTK Flask App on GCE failing silently after importing nltk, and just has error about chromium","['flask', 'google-cloud-platform', 'nltk', 'google-compute-engine']","Here is the log from my flask app starting up on GCE. It uses the module nltk to download a list of words:So what is this error? I googled the chromium error and people seemed to say it is nothing to worry about. Additionally, I can't really do anything about it. Any help will be greatly appreciated.here's my code that imports nltk:thanks for the help!"
665,Conditional Frequency Distribution using Browns Corpus NLTK Python,"['python-3.x', 'nltk', 'corpus']","I am trying to determine the words ending with 'ing' or 'ed'. Compute conditional frequency distibution, where condition is ['government', 'hobbies'] and the event is either 'ing' or 'ed'. Store the conditional frequency distribution in the variable inged_cfd.Below is my Code :-I want to output to the in a tabular format, using this above code I am getting the output as :-Whereas the actual output is :-Kindly resolve my issue, and help me get the exact output."
666,Issue with generation of bigrams using Python,"['python', 'nltk', 'data-cleaning', 'stop-words']","I am trying to remove bigrams formed starting with stopwords for eg and_contact. I have a list containing stopwords from nltk library and some manually added stopwords Also, I am getting bigrams that end with _ sign and repeated words with _ sign in between for e.g affect_ and occupied_occupied. I am not sure why I am getting bigrams with _ sign at the end. Please suggest if there is any way to get rid of them while forming bigrams itself.I am using the below code to generate bigrams:Can someone please help me in solving these two issues?"
667,AttributeError: 'list' object has no attribute '_all_hypernyms' what is this error?,"['python', 'nltk', 'wordnet', 'lcs', 'sentence-similarity']","This program is to find similarities between the a sentences and words
and how they are similar in synonyms
I have downloaded the nltk
when i first coded it was run and there were no errors but after some days when i run the program ti give me this error   AttributeError: 'list' object has no attribute '_all_hypernyms'
the error is because of this wn.wup_similarity"
668,"Heroku model deployed success, but nltk.txt not found","['heroku', 'nltk']","I am new to Heroku deployments and am deploying a text classifier (python 3.6.7). I followed Heroku guide and created an nltk.txt file in the root of the app folder. The content is 'stopwords' (for Stopwords Corpus). However, during the deployment, the following log message is observed:The deployed model seems to work fine though, but I suspect there may be hidden issues since stopwords is not used, from the observed msg.Kindly advise, Thanks in advance."
669,Stanford Parser and NLTK,"['python', 'parsing', 'nlp', 'nltk', 'stanford-nlp']",Is it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)
670,how to save bulk of files in python,"['python', 'nltk']","I would like to fix some textual errors in txt files using regular expression in python.
The total number of txt files is 487.It seems like loading all files was successful. Yet still have no idea how to save the reviewed new file (after adjusting regular expression process) in separated txt files.additional code is attached below."
671,Debugging the error “gcc: error: x86_64-linux-gnu-gcc: No such file or directory”,"['python', 'gcc', 'makefile', 'autotools']","I'm trying to build:
https://github.com/kanzure/nanoengineerBut it looks like it errors out on:x86_64-linux-gnu-gcc definitely exists in /usr/bin (It's a symlink) and the target definitely exists as well. It looks to me like the Makefile wasn't generated correctly, perhaps there is a flag that should be passed before specifying x86_64-linux-gnu-gcc? I am unsure as well what specifying x86_64-linux-gnu-gcc is supposed to accomplish.Finally, this makefile was generated by configure, so once we narrow down the cause of the error, I'll have to figure out what files to modify in order to fix this. (I'm a CMake kind of guy myself, but of course I didn't choose the build system for this project.) My OS is Debian.I've tried building this branch as well:
https://github.com/kanzure/nanoengineer/branches/kirka-updatesIf you can try getting this to build on your system, I would greatly appreciate it! Thanks!"
672,Error in nltk package in Python2.7 Widnows,"['python', 'python-2.7', 'pip', 'nltk']","I installed nltk using pip but when I import, it shows the below error."
673,NER using Stanford CoreNLP API through NLTK,"['python', 'nltk', 'stanford-nlp', 'ner']","I'm trying to extract named entities using the Stanford CoreNLP API following this example.However, the code above throws the following error:Other API calls like parse are working just fine. I'm using the latest version (4.0.0) of Stanford CoreNLP."
674,GCP AI Platform custom prediction routine fails downloading nltk resources,"['google-cloud-platform', 'nltk', 'google-cloud-ml']","I created my custom prediction routine for an email classifier. At pre-processing, I'm using nltk. Model creation is successful but when I send a request, GCP fails to download required nltk files. When my preprocessing file is like thisI'm getting following error:And if I will add nltk.download('punkt') after import statement, I'm getting another error like this:"
675,Get the type of a NLTK Tree(),"['python', 'indexing', 'tree', 'nltk']","I am using NLTK's Semcor module:semcor.tagged_sents() iterates over the same sentences with additional annotation including WordNet lemma identifiers.When I use indices in this list, I get the following output:When I use one more index, I get the tokens from the list as output:My goal is two-fold:What code can I use to get the Lemma as output? So the output would be:And what code could I use to get the type of tree as output? In the case of this example:"
676,Error accessing tree in semcor.tagged_sents(),"['python-3.x', 'nltk', 'lookup']","I am using semcor.tagged_sents() module from NLTK.Semcor.sents() iterates over all sentences represented as lists of tokens:And semcor.tagged_sents() iterates over the same sentences with additional annotation including WordNet lemma identifiers.My goal is to create a function that takes as input a sentence from SemCor and extracts a list which contains, for each token of the sentence, either the corresponding WordNet Lemma (e.g. Lemma('friday.n.01.Friday')) or None.Now, I want to access the second element in the last list from above (Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])])). However, when I run:I get the following error:However, the output is still: What is the meaning of this Lookuperror? And should act upon it?"
677,How do I use NLTK to extract numbers from a text string in Python,"['python', 'numbers', 'integer', 'nltk']","I have been working on a program lately and I wanted to add a functionality where it would take in user speech such as ""Show me my schedule from the next five(or 5) days"" or something like that and then extract the number ""Five or 5"" as a number and use that in a different part of the code to request data from the google calendar, the google part is mostly done but I how do I get it to extract the numbers such as ""Five"" or letter based numbers, I found this code earlier when I was looking around and it only returns true or false and I'm not sure how to make it return the actual number, your help would be greatly appreciated!is there a way to make this release the numbers in integer format? like for exampleString says ""Show my schedule for the next five days""
it'll return the number ""5"" as a separate int"
678,Problem with threshold in assigning topics to texts using LDA,"['python', 'nltk', 'gensim', 'text-classification', 'lda']","Unfortunately I do not know how to introduce the issue that I have been having with LDA, trying to assign topics to texts. 
I am getting this error when I try to determine the threshold for assigning the topics: ValueError                                Traceback (most recent call
  last)  in 
       32 
       33 print(scores)
  ---> 34 threshold = sum(scores)/len(scores)
       35ValueError: operands could not be broadcast together with shapes (1,2)
  (0,)Investigating a bit, I found that the output from Output: is causing the issue, as I should have a list of values and not a list of list. 
I am reporting below the steps to get the LDA model, just to see if I have set something wrong in my code. I hope you can help me to go through it. Example of output from data_lemmatized: Creation of Dictionary:Example of output: Then the code at the top of the question. 
Could you please tell me what that error means and how I can fix it?
Thank you"
679,How to use PCKimmo package from NLTK,"['python', 'nltk']","I  would like to conduct a derivational/morphological analysis with the use of PCKimmo package theoretically available from nltk.Is there anyone who would be able to tell me how can I use Kimmo package that is available from nltk.download?
I cannot find any commands that work, so as to e.g. call recognize function from PCKimmo package.Thank you for any suggestions!"
680,How to use txt file instead of Article ? (Python),"['python', 'python-3.x', 'nltk', 'python-newspaper', 'newspaper3k']","I wrote a code for reading article and self-learning AI. First, I read the article with URL and download it.Then I parse the article and use it for my AI's learning text.But now I want to read text from txt file.How can I assign txt file's text to Article object ? (Please check code to clear my wish )
Thanks all.PS:I want to use txt file instead of URL. But I do not remove article from code because it will be necessary  again."
681,How to represent a dict_items into a scatter on Plotly?,"['python-3.x', 'nltk', 'n-gram']","I want to represent the following data on a scatter through Plotly-express:CODE:I don't know how to transform the dict_items into list to put on X, and Y. How to deal with it?"
682,Error While Performing NLTK on Spark RDD using spark-submit,"['apache-spark', 'pyspark', 'nltk']","I have set pyspark_python as python3 and I want to perform NLTK on Spark RDD.
But while performing NLTK it's shown below Error.It works well when I'm running spark application on HDP Cluster, but doesn't work on local system spark-submit."
683,"NLTK word tokenize all words except words with a dash, e.g ('hi-there', 'me-you')","['python', 'nltk', 'tokenize']",I am not sure how can I use the nltk.word_tokenize method if I want to tokenize everything except the words with a dash (i.e excludes all words that have a dash in between). example: I have tried using the RegexpTokenizer and writing a regex but I somehow make it fail to act like the word_tokenize method and exclude '-'.Input: 'hello I am an artificial-human'Output im looking for:
684,How can i use Word Sense Disambiguation with spanish?,"['python-3.x', 'nltk']","I'm doing an udemy course (All the examples in English) but the problem is always when i start using Spanish, always there is a lack of libraries or compatibilities. I downloaded from https://www.datos.gov.co/Ciencia-Tecnolog-a-e-Innovaci-n/LAS-WordNet-una-WordNet-para-el-espa-ol-obtenida-c/8z8d-85m7 the data in CSV, but i'm trying to execute the following code but crashes with NoneType error, because of the description of the context, does anyone has any idea about how to deal with it? Thank youERROR:"
685,"Python NLTK Chunking specify <IN> phrase, e.g. chunk 'of' but ignore 'with', 'from' and 'in'","['python', 'nltk', 'chunking']","I am doing NLTK NP chunking in Python using chunking grammar.I have sentences like: ""the engine from the car"" and ""air mass flow of the intake air"".I used chunking grammar {<DT>?<NN.*>+<IN><DT>?<NN.*>+which gave me the entire phrases described above. However, in the chunking grammar, I would like to specify what <IN> phrase to be chunked, e.g. only create chunks with 'of', not 'from'.How do I do this?thx "
686,How to identify names from a text using Stanford NER in python 3?,"['python-3.x', 'nltk', 'stanford-nlp', 'ner']","How can we identify different names using stanford NER. I tried the below code , but was getting an error that it could not find the jar file, though i downloaded the model and jar file and gave the location.Is there a different way to do this in version 3 ? "
687,most frequently occurring words of a text file excluding stopwords,"['python', 'nltk']","I have a text file in french that I want to count its most occurring words, without taking in consideration stop words. Here's the code: This returns the most occurred words including the stop words. Any better idea how to do this?"
688,Remove punctuation and stop words from a data frame,"['python-3.x', 'pandas', 'scikit-learn', 'nltk']",My data frame looks like - I want to remove punctuation and stop words from this data frame. I have done the following code. But its not working -
689,Pandas concat function timing out when combining large dataframes,"['python', 'pandas', 'dataframe', 'nltk']","I am attempting to combine three dataframes that total over 120,000 features and 206,000 rows. All three of the dataframes are document-term matrices. Here is the code: As you can see I am using the concat function to combine the three dataframes across the horizontal axis. Currently, Jupyter Notebook times out and restarts the kernel after a while of this code running (I assume because of the number of features). Any insight into ways to reduce the size of these dataframes or combine them would be very much appreciated."
690,Trying to create a loop for applying sentiment,"['loops', 'nltk', 'vader']","New to this, but trying to create a loop that will apply a 1,0 sentiment score for each line of a dataframewhen I run this on my df   I get Help would be appreciated!"
691,Improving DOC2VEC Gensim efficiency,"['python', 'nltk', 'gensim', 'word2vec', 'doc2vec']","I am trying to train Gensim Doc2Vec model on tagged documents. I have around 4000000 documents. Following is my code:I have tried modifying the Doc2vec parameters but without any luck.On the same data I have trained Word2vec model, which is much accurate in comparison to the doc2vec model. Further, ""most_similar"" results for word2vec model is very different from the doc2vec model. Following is the code for searching most similar results:Further, the output of the aforementioned is cited below:"
692,Removing stopwords from pandas tokenised column before plotting word frequency,"['python', 'pandas', 'matplotlib', 'nltk']","I am having difficulties to remove some stopwords (default stopwords plus other words manually added) from a plot. 
This question is related to other two questions: Raw data: This is an exercise on Text Mining and Analytics. What I have been trying to do is collecting words that are more frequent per each date. To do this I tokenised sentences saving in a new column called 'Clean'. I used to functions, one for removing stopwords and one for cleaning texts. Code:where Clean is defined by: After cleaning text, I tried to group words by Date selecting tops ones (the dataset is huge, so I only selected the top 4). Then I applied the code for plotting stacked bars reporting most frequent words as follows (I found the code in Stackoverflow; since it was not build from scratch by me, it is possible that I missed some parts before plotting):However, even after adding some new words to exclude from the results, I still get the same variable on the plot, and this means that it was not correctly removed. Since I am not understanding and figure out where the error is, I hope you can help me. 
Thank you in advance for all the help and time that you will spend helping me.  "
693,Topic classification,"['python', 'dictionary', 'nltk', 'data-science']","In the above example based on the text when it encounters delighted it should give [Happy], and when it encounters icecream it should give [Summer] so on and so forth.I tried but couldn't think of a solution. Any help will be appreciable. Thank you."
694,Is it possible to combine these two pieces of code in Python using PRAW and NLTK to produce one data frame?,"['python', 'python-3.x', 'pandas', 'nltk', 'praw']","I am trying to do sentiment analysis on reddit headlines using PRAW and NLTK for a research project, but at the moment I have one piece of code for my PRAW data, and another for the NLTK sentiment analysis. This means that I have two separate data frames, and I'd like to find away to get it all in one table. That would make it considerably easier for me to do data analysis, so I was wondering if there was a way to combine the two codes so I get all the information I need in one easily digestible data frame with pandas?My NLTK code looks like so:That works great, and gives me the headlines while also giving me the sentiment scores. But then my PRAW code looks like this:This also works great, but is an entirely new data frame. My goal is to end up with a code that produces a single data frame that has the raw data I want from Reddit, like the post title, number of comments, etc. while also having NLTK do sentiment analysis on the title, producing polarity scores in the same data frame that ""does it all"" so to speak, that I can then export and do data analysis on. How would I go about doing this?"
695,AttributeError: 'Tree' object has no attribute 'tree'. The class is 'nltk.tree.Tree',"['python', 'nlp', 'nltk', 'stanford-nlp', 'dependency-tree']","I'm getting into the NLP world by reading and coding along with this article. Everything was working fine until I wanted to do Dependency Parsing. I want to leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree, the code is The variable ""sentence"" is defined earlier, and the paths are correct because I used them earlier. When I run this code in Jupyter notebook the output should be ""(beats (unveils US (supercomputer (world 's) (powerful most)))
 China)"" but instead I get this error Any piece of information on this would be extremely helpful, thank you. "
696,How does updating Vader Lexicon scores work?,"['python', 'nltk', 'sentiment-analysis', 'vader']","I am using NLTK for python and I am trying to update the scores for a set of words. Whilst it appears that the scores are updating, they don't appear to update in the way that I am specifying. I'm wondering if anyone knows how this process works? I have attached a minimum working example below, showing the scores before and after the updateThe scores from the before and after can be seen below"
697,Missing some form of words not being generated with wordnet,"['python', 'python-3.x', 'nltk', 'wordnet']","With reference to this link, I tried to sample the word ""play""And I got the following output:-Is there anyway to include words like 'played', 'plays' with the above result?"
698,OSError: [WinError 193] %1 is not a valid Win32 application - nltk,"['python', 'python-3.x', 'nltk', 'corpus']","So, I keep getting this error:I believed it to be because of my environment variables. So, I fixed that, but still keep getting the error. I'm at a loss currently. Here's the complete error output:Edit: Here is the code I'm trying to run.Edit 2: My os is Windows 10 and running on x64"
699,Tokenisation by date for text classification by topics,"['python', 'pandas', 'nltk', 'lda']","I would need to tokenize the following column by dateSomething like:I would need this for topics classification using LDA. 
I have tried as follows: However the output does not give me the expected tokenisation. Next step would be apply the tok_text_list toin order to determine topics by date. How can I get this?"
700,How to remove duplicate sentences from paragraph using NLTK?,"['python-3.x', 'nlp', 'nltk']","I had a huge document with many repeated sentences such as (footer text, hyperlinks with alphanumeric chars), I need to get rid of those repeated hyperlinks or Footer text. I have tried with the below code but unfortunately couldn't succeed. Please review and help.Error message for the above code :Desired Output :"
701,How can I count words based on the column?,"['python', 'pandas', 'dataframe', 'nltk', 'part-of-speech']","Hello. I stuck here.
Could you tell me how can I count words based on the tags on the second column?I want to find mostly used words using  .most_common() using the categorize: most 10 in VB(Verb), 10 in Noun. "
702,Group nltk.FreqDist output by first word (python),"['python', 'nltk', 'distribution', 'frequency']","I'm an amateur with basic coding skills in python, I'm working on a data frame that has a column as below. The intent is to group the output of nltk.FreqDist by the first wordWhat I have so far I have 10000+ rows in my output.My Expected OutputI would like to group the output by the first word and extract it as a dataframeWhat I have tried among other solutionsI have tried adapting solutions given here and here, but no satisfactory results.Any help/guidance appreciated."
703,"Python, NLTK, how to set a rule to parse subordinate clause?","['python', 'nltk']","I'm trying to write a rule for parsing subordinate clause, but when I run it, nothing comes out. what is the problem?"
704,Split list of paragraphs at punkt (“.”),"['python', 'string', 'list', 'nltk']","I have a list of paragraphs:I would like to separate this paragraphs at the punkt (""."") and get a list with each sentence and wrote this code therefore:I got a list of lists:Instead I would like to get:How can I get this?"
705,splitting string into different ngrams based on probabilities (python),"['python', 'nltk', 'n-gram']","The problem: I want to split a string with almost 2 million words into uni-, bi- and trigrams based on the probabilities of them co-occurring. The string was initially part of a pandas df text['description']. I did some preprocessing (all lower case, remove special characters, white spaces, single letters, remove stopwords and those that only occur once in the whole text, lemmatize). After that, I put them all in one big blob using text1 = "" "".join(text['description']).text1 = 'buyer opportunity create personalised ideal home accommodation arranged laterally lower groundfloor comprises reception room feature fireplace spacious separate kitchen utility area principal bedroom ensuite bathroom second bedroom ensuite shower room principal bedroom provides direct access communal patio lead onto private south west facing rear garden also accessible via side passageway priory road ideally positioned benefit shop restaurant cafe society west hampstead st john wood kilburn whilst combination jubilee line overgrou thameslink bakerloo line transport....'I built a basic language model, which is a dictionary containing probabilities of words co-occuring:That part runs very quickly. Now I want to iterate over each word and if two or three words are more likely to occur consecutively than they are not (e.g., probability > .05), I will make them a bigram or trigram, otherwise I'll keep them as unigram. This part has now become very clunky and runs way too long. Does anyone have any suggestions on how to speed this up / simplify?Also the use of a and b seems silly, but I wasn't sure how to skip one or two items. I.e., whenever the first if condition is met, I append the trigram and want to skip the two words that have now become part of the trigram (and same logic for bigrams)."
706,Still LookupError in punkt after putting the file into site-packages file,"['python', 'nltk']","I was trying to tokenize the text i got from web through this:And then LookupError comes:I realize this might occur because i haven't downloaded 'punkt', and then  i tried to download from python:but the result comes as follows:I think maybe internet connection has some problem? 
So i also downloaded punkt package from web and put it into the nltk file in my site-packages.
But still the same problem i had at the first beginning occurred. Don't what to do about this now LOL! Any advice!"
707,NLP - Fetch words which is not part of English grammar,"['nlp', 'nltk', 'text-mining']","I have a requirement where I have to read a text and extract only the SQL Table Names and Column names.Eg statement: ""Direct move by joining the Table_A with Table_B on Column_1In the above example I have to read only the table name and column name. Similarly I have even complex and large statements, from which I have to extract table and columns names. I have tried using the POS tagging from NLTK package. But I am not sure if the POS tagging is a valid approach in this requirement, because the words I am trying to extract is not an exact English grammar word.Any idea on how I can approach this? Please throw some light."
708,Clean list from stopwords,"['python', 'nltk']","This variable: needs to be clean of stopwords. 
I tried with but it has not worked.
What is it wrong?"
709,Select n-grams which contain at least one number,"['python', 'nltk']","I have a list of n-grams created byI would like to select only the n-grams which contain at least one number, e.g. Could you please help me to select it?"
710,Python wordnet from nltk.corpus. How can I get the definition of each word in several sentences using wordnet?,"['python', 'nltk', 'wordnet']","I started learning wordnet, but everything in the examples is tied to a specific word. How can I get the definition of each word in several sentences using wordnet?I tried to do something like this:but:"
711,Sentiment analysis of Italian sentences,"['python', 'nltk', 'sentiment-analysis', 'fasttext']","If you have any experience on sentiment analysis, could you please tell me how I can analyse these sentences, which tool, library, module should I need?I know that many of you are English speaker native. There are so many tools for English language, less for Italian one. 
I have tried with Fasttext and it did not work. I also thought to build one from scratch, but I did not know how to do it, which elements should be taken into account (top words, adjectives, nouns, n-grams...) to create a dictionary. 
If you could please provide an example of sentiment analysis with an Italian sentence (or with the two that I shown above), it would be great to understand what I would need. Thank you "
712,Finding NLTK Wordnet Synonymity Percentage,"['python', 'algorithm', 'nltk', 'wordnet']","Given 2 words, I need to find out their synonymity. I'm dealing with two words where one will be unknown (something like user input) and one will be known during compile time. I decided to use nltk and wordnet since they have the best chance of identifying a unknown word.Example: ""Tree"" and ""plants"" would have a high value whereas ""tree"" and ""animal"" would have a low score.I tried to use nltk.corp.wordnet.wup_similarity(). However, antonyms are getting a high similarity score. I need a % measure of synonymity. I also tried to use nltk.corp.wordnet.synsets() recursively, but I don't know how to get a percentage value.I searched a lot but couldn't find any function that does the job. Is there any function that does the job?If there's no function, I'm okay if I can get an algorithm or a pseudo-code on how to achieve this."
713,Getting error while using custom tagging “object has no attribute 'choose_tag'”,"['python-3.x', 'nltk', 'pos-tagger']","Wile using perceptron tagger as backoff in unigramTagger , getting and error  ""object has no attribute 'choose_tag'"". 
I used :"
714,Tokenisation for topics extraction with LDA,"['python', 'nltk', 'lda']","I am trying to extract topics from some texts. I have a column called 'Texts' likeand I would need to extract topics using lda. I have been using this codeHowever, there is a problem with dictionary_LDA as I am using df['Texts'].tolist() and this is not list of list of tokens like[[""word1"",""word2"",""word3""], [""word4"",""word5"",""word6""],...] (I think it should be required something like this). 
I will be also have probably some 'bad'results as the texts includes stopwords... but this is another story. Could you please tell me how to get a list of list of tokens or something that can allow me to use the code above?
Thank you"
715,Lemmatization for Root Word,"['python', 'nlp', 'nltk']","I'm just starting to learn/play with NLP, and have come across the following behavior:Using nltk, I extract the part-of-speech for words, and then lemmatize as follows:Doing pos_and_lemmatize (or just wordnet_lemmatizer.lemmatize) on the following words: variability, variables, variance, variation just returns the same word, instead of something like vary. Any way to accomplish this goal?"
716,How to use edit_distance() from nltk.metrics in this example?,"['python', 'python-3.x', 'nltk', 'nltk-book']","I have a bit of problem with using edit_distance() in the following example.
I need to print words from the languages mentioned in the languages list in 5 columns, which is not a problem. I have done that: This parts works as it is suppose to work. Now I need to measure the Levensthein string edit distance between words from 'be' langauge and the equivalent of this word in other languages. And the distance should appear after each word in the brackets. So it should look like, for example:tamto(0) acela(5) oni(5) то(3) What would you suggest to be the best idea to measure it? I was thinking about crating dictionaries:And then calculate edit distance somehow, but I cannot execute this. Especially beacue one of the languages - Russian has different script which means that I have to uste translit (correct me if I am wrong, this is what I found online). Do you have any tips how to go about it? I am new to programming so maybe it is a simple question for you, but I am still trying to figure out my way around everything in nltk.
Thank you in advance!"
717,how to minimize the time taken by NLTK in python to get data nouns and verbs from text in oracle database?,"['python', 'nlp', 'nltk', 'part-of-speech']","I am using NLTK in python to get nouns and verbs form text which is in the database columns.
But I have around three million records and its taking too much time to get the nouns and verbs from the database text data and then to ingest the nouns and verbs into the table.
How can I reduce this time? I there any way to fetch nouns and verbs from text in less time?Below is my python code :"
718,NLTK Stopwords Google Cloud Platform,"['python-3.x', 'google-cloud-platform', 'nltk']","I'm trying to use NLTK stop words while performing sentimental analysis using flask.I've tried downloading the stop words ""English"" and have pointed it in app.yamlPlease find error below:Please suggest!!!"
719,"Python, NLTK, how to write cfg parser include compound noun?","['python', 'nltk']","I'm beginner in learning NLP. I'm practicing writing a basic set of rules to parse sentences, but I don't know how to write a rule to parse a compound noun. My rule for parsing the sentence ""the manager wrote an email to the customer"" is this.But when it comes to parse ""the shop manager wrote an email to the customer"", what should I change?"
720,Name of task - splitting up complex sentences?,"['nlp', 'nltk', 'stanford-nlp', 'spacy']","I've go a question about the name of an NLP task - Splitting up a complex sentence into simple ones.
For example, if I have this sentence:""Input t on the username input box and password input box.""I'd like to split this sentence into simpler sentences:""Input t on the username input box""
""Input t on the password input box""What would this problem be called? I've tried clause extraction here but I don't want clauses, but rather, fully formed sentences. I've also tried 'sentence simplification' but it exceeds what I'm trying to do, with its lexical simplification and all. Thanks "
721,lemmatize() missing 1 required positional argument: 'word',"['python', 'nlp', 'nltk', 'stemming', 'lemmatization']",When I try to pass this on to the lemmatizer like this:i get following errorwhen i let it run like this:i get this errorNameError: name 'token_text' is not definedWhat do i have to do? I am trying to apply the function on punctuation and stop words removed sentences. the steps stemming and tokenization are not applied. DataSetDataSet newFull example first partFull example second part
722,removing NLTK StopWords,"['nlp', 'nltk', 'data-cleaning', 'stop-words']","I am trying to remove stop words of my data set.I have two problems with that. First, the output is given character by character (separated by a comma), although I run the check against the list of stopwords with words. I can solve this problem with a join command, but I don't understand why it is split into characters. The second and real problem is that the removal of stop words does not work. Words that are clearly in the list are not removed from the sentences. Where is my mistake in this?image"
723,How to clean a string to get value_counts for words of interest by date?,"['python', 'pandas', 'nltk']","I have the following data generated from a groupby('Datetime') and value_counts()I would like to remove a specific name (in this case I would like to remove 'Paul') and all the numbers (03, 10982360967 in this specific example). I do not know why there is a character 'l' as I had tried to remove stopwords including alphabet (and numbers). 
Do you know how I could further clean this selection?Expected output to avoid confusion: I removed Paul, 03, 109..., and l. Raw data:I cannot put all the raw data as I have more than 100 sentences. The code I used is: Since I got a Key error, I had to edit the code as follows:To extract the words I used str.extractall"
724,GitHub Actions fails when NLTK “punkt” is needed,"['python', 'continuous-integration', 'nltk', 'github-actions', 'building-github-actions']","I have written some code which needs to use NLTK's punkt. I have included nltk in the requirements.txt and in the setup.py. However, when I run the build of my project using GitHub actions, it fails with this error.What is the standard way to tell GitHub actions that it needs 'punkt' without hard coding nltk.download('punkt') somewhere into the code?
Should I add a line in the ci.yml file, and what is the best way to do it?"
725,How can I use NLP to group multiple senteces by semantic similarity,"['python', 'nlp', 'nltk', 'sentence-similarity']","I'm trying to increase the efficiency of a non-conformity management program. Basically, I have a database containing about a few hundred rows, each row describes a non-conformity using a text field.
Text is provided in Italian and I have no control over what the user writes.
I'm trying to write a python program using NTLK to detect how many of these rows report the same problem, written differently but with similar content.
For example, the following sentences need to be related, with a high rate of confidenceI already found the following article describing how to preprocess text for analysis:
How to Develop a Paraphrasing Tool Using NLP (Natural Language Processing) Model in PythonI also found other questions on SO but they all refer to word similarity, two sentences comparison, or comparison using a reference meaning.In my case, I have no reference and I have multiple sentences that needs to be grouped if they refer to similar problems, so I wonder if this job it's even possible to do with a script.This answer says that it cannot be done but it's quite old and maybe someone knows something new.Thanks to everyone who can help me."
726,How to get similarity % from 2 resumes of which one resume is the best for x job role and other fresher resume needs to compare with the best one?,"['python-3.x', 'nlp', 'nltk', 'gensim']",Can someone help me with the flow process using NLP and python? I'm totally new to this NLP. 
727,Sorting Vader Sentiment Analysis Results in Dictionary,"['python', 'pandas', 'dictionary', 'nlp', 'nltk']","Is there a commonly used method for sorting multiple Vader Sentiment Analysis Results in DictionaryI am trying to sort by 'compound' Vader Sentiment Analysis Results in the review Dictionary.I just started learning nlp and Sentiment Analysis and got my first project to 95% so far been learning books, tutorials and here.I was hoping to sort closest scores 5 overall sentiments scores to a particular one in a large set of multiple dictionary results.this what i tried
Might be also due to printing it being a string I assume i need to convert this to string or object, just not sure next steps.I was think I might need to loop through dictionary or convert the dictionary. Any pointers would be greatly appreciated.I also triedThe Vader results are printed from dictionary.This is the format of the results which i think is standard output"
728,Extracting top words by date [closed],"['python', 'pandas', 'nltk']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed last month.I would have some doubts regarding how to extract top words used in texts grouping by date.I have a dataset... and so on.
what I would like to do is to extract the most frequents words by dates to see if there is a pattern. 
For each date, then, I should have a new column that lists these words.Any suggestions on how I could do this?"
729,Any way to import Python's nltk.download('punkt') into Google Cloud Functions?,"['python', 'google-cloud-platform', 'google-cloud-functions', 'nltk']","Any way to import Python's nltk.download('punkt') into Google Cloud Functions? I've found that adding the statement manually into my code block in main.py significantly slows down my function processing, since punkt has to be downloaded every time it is run. Is there any method to eliminate this by calling punkt in some other way?EDIT#1:- I edited my code and program structure to match what Barak suggested, but I keep getting the same error:"
730,is there any host to upload python and php togather?,"['import', 'nltk', 'host']",When l upload my web project to the server cpanel (My project has a Python model and php ) :import error : no module called nltkIs there a way to import this library on line from a site ? or installing libraries on the server ? is there any host to upload python and php togather?
731,Identify domain related important keywords from a given text,"['python', 'nlp', 'nltk', 'chatbot', 'text-processing']","I am relatively new to the field of NLP/text processing. I would like to know how to identify domain-related important keywords from a given text.
For example, if I have to build a Q&A chatbot that will be used in the Banking domain, the Q would be like: What is the maturity date for TRADE:12345 ? From the Q, I would like to extract the keywords: maturity date & TRADE:12345.
From the extracted information, I would frame a SQL-like query, search the DB, retrieve the SQL output and provide the response back to the user.Any help would be appreciated.Thanks in advance."
732,Could not find stanford-parser\.jar jar file at .\stanford-corenlp-4.0.0,"['java', 'python', 'nlp', 'nltk']","I'm new to nltk and seem to be following an outdated tutorial to get started with StanfordDependencyParser in nltk.I've installed Stanford Core NLP and their English models from https://stanfordnlp.github.io/However, running the script below returns the following error:Would appreciate any pointers!Using nltk 3.5 and stanfordcorenlp 4.0.0 with Python 3.7Script attached below."
733,Handling <class 'nltk.corpus.reader.wordlist.WordListCorpusReader'> in Python,"['python', 'nltk', 'word-cloud']","I am using the ""wordcloud"" package in Python and trying to ingest my own tri-lingual (English, French and German) stopword list.This ""EN_FR_DE"" stopword list is stored in the NLTK data folder alongside the standard NLTK stopword lists on my PC because I am using it in other contexts, too. I would now like to import this custom stopword list, turn it into a string and use it in ""wordcloud"". I used ""string"" to convert the list. When I check its ""type"", however, the output still is:Of course, ""wordcloud"" does not accept this list and ignores all the stopwords (see graph image linked below).I wasn't even aware that this was a class of its own, and I could not find instructions how to handle this particular type of class. What is the proper way to convert it into a string?My full script for building the wordcloud is:And this is the output:Graph Image"
734,Wrong number of items by the list creation,"['python', 'list', 'loops', 'tuples', 'nltk']","I'm creating a list of documents that consists of tuples, each tuple consists of a list of tuples and a string, so it looks like this:I'm using nltk to generate the list:The problem is, len(corpus.fileids()) is 84 (what is correct), but len(documents) is 7056‬. So, somehow, I managed to square the number of documents. I'd like the list to have only 84 items.I noticed that documents[0] and documents[84] are identical (so do documents[1] and documents[85] etc., of course). I could slice the full list of 7056 items, of course, but this does not explain anything... I'm new to Python and programming, so any help would be appreciated."
735,unable to get polarity scores from Vader Sentiment Anlayzer,"['python-3.x', 'nlp', 'nltk', 'sentiment-analysis', 'vader']","I am trying to add these new words and their corresponding polarity scores from a csv file into Vader Sentiment LexiconIt also reflects in the vadersentiment object when it is updated:But as soon as I try to get the polarity scores for the newly added words, it throws an error:I am confused as to what is happening even though that the word is present in the vader dictionary:Does anyone know why is it happening? "
736,How do I extract degrees / education and year from a resume? in python using NLTK,"['python-3.x', 'regex', 'pandas', 'nlp', 'nltk']","I have tried this below code but unable to extract correct education and year from a resume.Text: Output for the above is not showing anything .. -- Desired output: Can someone help me to correct this code and fetch passing year of that respective education?
Thanks in advance! "
737,How to check if a sentence begins with a non-english word in python?,"['python', 'list', 'nltk']",I have tried the following code to check if the sentence has non-english words.But I want to check if the beginning of the sentence itself has the non-english word and hence remove that entire sentence. I have also tried the startswith() method along with the isalpha() method but didn't work.So if there are a list of non-english and english sentences I want to recognize the non-english ones and remove them from the list. Please help me resolve this.
738,Generate all possible correct sentences from a set of words,"['python', 'nlp', 'nltk', 'generator']","I have a set of 276 different words, I want to generate all possible correct sentences from them. It can be composed of two words only or more.I used NLTK tagger to give every word a class. For instance, ('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN') but this technique has drowbacks when the words are independent from each other (An array of 276 words in my case).If anyone has an idea of how I could do it, it would really help me. Thanks! "
739,How to use NLTK library in ASP.NET?,"['python', 'asp.net', 'nlp', 'nltk']","Is there any way to use NLTK(an open source Python library for Natural Language Processing) in ASP.NET as an API?
I need to call some of its functions like Tokenization, Remove stop words and Stemming."
740,CountVectorizer takes too long to fit_transform,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'nltk']","Given above code, CountVectorizer takes too long (ran for 60 minutes but it did not finished) to fit but if I remove line if w not in stopwords.words() it just take 5 minutes to fit, what could be problem and possible solution with this code. I am using stop words from nltk.corpus.Note: tokenize function works fine, using separately for any text input.Thank you"
741,"preprocessing tweets, remove @ and # , eliminate stop words and remove user from list of list in python","['python', 'nlp', 'nltk', 'spacy']","i wrote the code below but now I want to p reprocess, so I transformed to lower, I wrote some word to eliminate stop words but it does not work, and I want to remove @ and # and also remove user , can you help me?  "
742,Faster ways to POS tag using Python,"['python', 'nltk', 'tokenize', 'pos-tagger']","I have ~2 million rows of text. Each row can be one or multiple sentences. I need to POS tag the entire corpus. The corpus is a list of strings, for example:The corpus has around 2 million strings, which I'm reading out of a database.My ideal code for POS tagging looks like this, where I tokenise sentences, then tokenise words, and then POS tag:This, however, is extremely slow. I read about pos_tag_sents() but I'm guessing that's going to take forever to do 2 million data points in one shot. Is there any other, faster way of doing this? I'm looking for a way to speed this up at least 2-3x. My main objective is to capture major word forms (nouns, verbs, question words, etc.), so I'm open to other POS taggers, provided they can speed up the process by 2-3x."
743,Scores for sentiment analysis,"['python', 'nltk', 'sentiment-analysis', 'text-classification']","I would need to study the sentiment of the following strings: What I have tried to do is to tokenise the three strings and use pos tagging for extracting parts of speech:What I would like to do, but I do not know how to do it, is to use POS tagging to extract adjectives, adverbs and verbs (i.e. parts that might be more relevant in sentiment analysis) and trying to manually assign a score (+1,0,-1) to each of them. So my question would be how to extract all these parts and assign 'manually' a score. 
I have tried with but I would need to check also the other parts of speech and assign them a score, looking at the previous and next words. I know: it will be not too easy when there are many strings (around 100) and many issues can be arisen from the context (I think for this could be useful the use of n-grams).
What would be a good approach to do sentiment analysis from your point of view? I think the best precision could be got by checking and assigning a score manually (to avoid sarcasm, double negative sentences,...), but I would be interested in models accuracy (or how to improve the already existing models). It is my first time that I have run a sentiment analysis. My willing would be starting a model from scratch, collecting all relevant parts of speech.
Of course, if you know an already (better) existing model/library that I could apply for this exercise, I will be happy to take it into account. "
744,How to extract ngrams Character from diacritical Arabic text?,"['python', 'nltk', 'arabic', 'n-gram']","In my project I have 2 data sets: The first an Arabic text without diacritics and the second with. I need to extract from this data sets ngrams of characters  For example, the sentence: علم الانسان ما لم يعلم. And for n=2 the result is:  I wrote a Python program for this, but for the second data set (with diacritics), I can't combine the characters with there diacritic in the same character.  the correct output must be :  Any ideas?"
745,TextBlob 0-Values for Polarity and Subjectivity,"['nltk', 'sentiment-analysis', 'textblob']","I am using TextBlob with python 3 to create sentiment values for a larger corpus of documents. I reviewed the distribution of polarity and subjectivity values and noticed a big share of values equal ""0"", see the distribution in the image (for polarity values on 1300 documents). I thought this might just be because TextBlob returns 0 as a default value if it wasnt able to calculate the sentiments right or for some other reason. I didnt find any docu on that, but maybe one of you can tell me where the high amount of zeros might origin in.Best, Nero 
"
746,How to insert calculator inside chat bot that written by Python,"['python', 'python-3.x', 'nltk', 'calculator', 'chatbot']","I have write a chat bot program by using Python 3.6. I study the syntax from several website and the most of my chat bot content is come from here: https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e. This program have no problem for perform nltk on text. I plan to input physics knowledge to my chat bot. So far, the bot can answer all the things relate to physics such as Newton second law and even give out the formula. However, I plan to make this bot can perform simple calculation on equation such as F=m*a. I search so many website in google in these few days, yet still cannot find the solutions. I also got search in stack overflow, and yet the nearest post to my answer seems not an answer for me. (how do I add a calculator to my chatbot).  If it is for normal python script, I know how to write it and I think I will write in this way:However, how am i gonna implement this thing in chat bot? So far the only way i can think is using chat bot platform like chatterbot.AI, Dialogflow, Chatfuel all this. If I only want use python, how can I improve the chatbot, so it can perform the calculation that I have stated above. Thank you for stack overflow experts helpings and advises."
747,Deploy Django project to heroku NLTK problem,"['django', 'heroku', 'nltk']",I have a problem when I deploy Django project to Heroku in nltk package I used nltk.txt but I got application error despite deploying successfully but the problem in nltk packages any help.
748,Words frequency through time,"['python', 'pandas', 'nltk', 'word-cloud']","I have a dataframe with four columns:Another column, Token_text was got from tokenising and removing stopwords from Text column as follows:I would need to create word cloud  or just histogram of words frequency (top 10) taking into account texts by Date. I need to examine potential topics and change in them through time. 
So my expected outputs would be: and so on.Can you tell me how to consider these tokenised text by date?"
749,NLTK tokens - creating a single list of words from a pandas series,"['python', 'pandas', 'nltk']","Good afternoon Folks,I'm looking for some help with NLTK, or anything other library that could help me solve the problem I'm facing.I'm no Python expert (I actually only started learning Python 4 months ago, and my thing is more SQL), but I've made quite some research before asking you Folks for help:Tokenizing words into a new column in a pandas dataframe
Passing a pandas dataframe column to an NLTK tokenizer
etc...Here's what I have: a dataframe that contains quite a bit of information about what our students look for (it's the website for a campus) when they're searching for information on our website.It looks a bit like this:What I would like to have, is one big list that looks like:
['query', 'exams', 'session', 'june', '2020', 'when', 'are', 'the', exams', 'exams', 'what', 's', 'my', 'teacher', 's', 'email', 'address]
===> one string, all the words (no sentences), no punctuation.I have tried:===> that gives me an individual string for each row===> a bit better, but not what I want eitherI hope that the formatting for my post is ok, and apologies if I'm wasting anybody's time.Thanks for reading me,
Julien"
750,python pandas RecursionError: maximum recursion depth exceeded in comparison,"['python-3.x', 'pandas', 'dataframe', 'nltk', 'stop-words']","length of cleanedtrain1 is 223549the code below takes a lot of time  :then gives this error : what is the solution in this case?
Note : i saw few solutions in stackoverflow but those are not duplicate because they are not having such issue while using pandas dataframe. "
751,(How) Can I get multiple tags per pattern in RegexpTagger?,"['python', 'string', 'text', 'nlp', 'nltk']","I have a task to label sentences according to multiple regular expressions per label. I'm using RegexpTagger from nltk to hold the regular expressions, but it returns only one match per sentence. Is there another solution to get multiple sentences and their labels?[('Healthy is important.', 'health or death'), ('Pancakes cause death.', 'health or death'), (""I don't like pancakes."", 'pancake'), ('Pancakes improve health.', 'health or death')]"
752,Error - NLTK WordLemmatizer is giving alphabets instead of lemmatized words,"['python-3.x', 'nlp', 'nltk', 'wordnet', 'lemmatization']","The above code is iterating over all characters in the row. I want it to consider the row as a list.
Following is my dataframe.
I want to pass Stopwords Column to the lemmatizer. The current output is present in Lemmatized Text column"
753,How to improve nltk human name identifier?,"['python', 'python-3.x', 'scrapy', 'nltk']","I am trying to extract human names from text.Does anyone have a method that they would recommend?It is fetching so much other data also. Why is this happening tried doing so much thing to fetch human name out of the file but always some error. Because i want to fetch the human name with each sentence and then match that name with my db and then link this sentence with that human name. But not able to achieve.The output of the code ['Samantha Ruth Prabhu Were', 'Pooja Hegde', 'Mohenjo Daro', 'Gulshan Kumar', 'Bhushan Kumar', 'Twitter Is', 'Brandon Routh', 'Pooja Batra', 'Her Is Pure Gold', 'Shraddha Kapoor', 'Highlights Shraddha', 'Siddhanth Kapoor', 'Son Are', 'Legally Blonde', 'Amitabh Bachchan', 'Kabhie Kabhie', 'Kabhi Kabhi Mere Dil Mein', 'Kabhi Kabhi Mere Dil', 'Rishi Kapoor', 'Neetu Kapoor', 'Varun Dhawan', 'Highlights Varun', 'Varun DhawanYou', 'Kiara Advani', 'Gabriella Demetriades', 'Arjun Rampal', 'Arjun Kapoor', 'Sonam Kapoor', 'Harshvardhan Kapoor', 'Rakeysh Omprakash Mehra', 'Sister Nupur Gave', 'Nupur Sanon', 'Akshay Kumar', 'Kareena Kapoor', 'Fevicol Se', 'Malaika Arora', 'Munni Badnaam Hui', 'Ranveer Singh', 'Sylvester Stallone', 'Joe Exotic', 'Amar Akbar Anthony', 'Amazon Prime', 'Father Veeru', 'Veeru Devgan', 'Ajay Devgn', 'Namrata Shirodkar', 'Son Gautham', 'Karan Johar', 'Aditya Chopra', 'Kuch Kuch Hota Hai', 'Kabhi Khushi Kabhie Gham', 'Kabhi Alvida Naa Kehna', 'Shah Rukh Khan', 'Shah Rukh', 'Priyanka Chopra']"
754,How to create a text spinner in python?,"['text', 'nltk', 'spinner']",I have been trying to create some program for the spinning of the text but i am not able to get the correct context after execution. What are the basic algorithms or key points required for the building of a spinner and the libraries used?
755,Generate specific sentences from fromal/context free grammar?,"['python', 'nltk', 'context-free-grammar']","I'd like to generate a number of specific sentences from a formal grammar. I know to generate a number of sentences from a grammar, however I am wondering if there is an way to kind of ""manipulate"" or control Parsing of the grammar, thath the output is one specific sentence? "
756,removing non English words from text in df.columns words contain letters and numbers,"['python', 'nlp', 'nltk', 'preprocessor', 'word']","How to removing non English words from text in  df.columns words contain letters and numbersExdf['text']'the interiors nrd studio | happy mothers day   ”there is no influence so powerful as that of the mother.” —sara josepha hale... happy mother’s day mom & to all the mothers around the world! lots of light natasha
0wet3bxtfl''but still missing you every day happy mothers day francis mcclafferty (mccool) 9wlhju7cxf'from the above 2 rows I need to remove the  word  '0wet3bxtfl' & '9wlhju7cxf'"
757,NLTK FreqDist to a table using pandas,"['python', 'pandas', 'nltk']","I have this frequency distribution that i got using NLTK:I would like to convert it into a table/dataframe, preferably using pandas."
758,I restricted the words in the tweets to content words now I want to Transform the words to lower case and add the POS with an underderscore,"['python', 'pandas', 'nlp', 'nltk', 'spacy']","I wrote the code below and restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives, Now I want to Transform the words to lower case and add the POS with an underderscore. E.g.:love_VERB old-fashioneds_NOUN
but I dont know how, can anyone help me?"
759,Why in NLTK “not” is considered as stopping word in English?,"['python', 'nltk', 'sentiment-analysis', 'stop-words']","I am currently learning nltk in Python where I am writing a program for sentiment analysis. While working on it, I found out that ""not, nor, never"" are considered stopping words. So my question is why that is because these kinds of words can change the entire meaning of sentences and could lead to failure in sentiment analysis."
760,nltk: Unable to download package punkt while building docker,"['python', 'docker', 'proxy', 'dockerfile', 'nltk']","I am running this behind corportate proxy
Used the command: RUN python -m nltk.downloader punkt"
761,Trying to make a standalone executable file of a flask application using pyinstaller,"['python', 'flask', 'nltk', 'exe', 'pyinstaller']","I am using pyinstaller to make a standalone executable version of  flask app. On running the created executable file a fatal error popup is shown which states I create the executable file by running the following command: pyinstaller -w --onefile --add-data ""static;static"" --add-data ""templates;templates"" --add-data ""models;models"" app.pyMy directory has the following structure:"
762,Which of NLTK tokenizer is good before POS tagging?,"['python', 'nlp', 'nltk', 'pos-tagger']","NLTK has a number of tokenizers like belowFor a 'part of speech' recognition task(probably using nltk-Average perceptron tagger), is there a tokenizer that can give optimal results ?"
763,Errrors while installing NLTK,"['python', 'linux', 'pip', 'nltk', 'gcc-warning']","while trying to install NLTK by pip install nltk i got the following error To solve the error error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 i tried sudo apt-get install python3-dev as seen in another stackOverflow post but got the error Can someone explain what have gone wrong and what is gcc error?P.S:- I installed virtualenv by python3 -m venv myenv
"
764,I have installed tensorflow and tflearn for creating chatbots during importing it shows errors,"['python', 'tensorflow', 'nltk', 'tflearn']","this was my codeerror!ImportError                               Traceback (most recent call
  last)
  c:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py
  in 
       57 
  ---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
       59   from tensorflow.python.pywrap_tensorflow_internal import versionc:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py
  in 
       27             return _mod
  ---> 28     _pywrap_tensorflow_internal = swig_import_helper()
       29     del swig_import_helperc:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py
  in swig_import_helper()
       23             try:
  ---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname,
  description)
       25             finally:c:\users\acuwin\appdata\local\programs\python\python37\lib\imp.py in
  load_module(name, file, filename, details)
      241         else:
  --> 242             return load_dynamic(name, filename, file)
      243     elif type_ == PKG_DIRECTORY:c:\users\acuwin\appdata\local\programs\python\python37\lib\imp.py in
  load_dynamic(name, path, file)
      341             name=name, loader=loader, origin=path)
  --> 342         return _load(spec)
      343 ImportError: DLL load failed: A dynamic link library (DLL)
  initialization routine failed.During handling of the above exception, another exception occurred:ImportError                               Traceback (most recent call
  last)  in 
        1 # Libraries needed for Tensorflow processing
        2 
  ----> 3 import tensorflow as tf
        4 import numpy as npc:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow__init__.py
  in 
       22 
       23 # pylint: disable=g-bad-import-order
  ---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
       25 
       26 from tensorflow._api.v1 import appc:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python__init__.py
  in 
       47 import numpy as np
       48 
  ---> 49 from tensorflow.python import pywrap_tensorflow
       50 
       51 # Protocol buffersc:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py
  in 
       72 for some common reasons and solutions.  Include the entire stack trace
       73 above this error message when asking for help."""""" % traceback.format_exc()
  ---> 74   raise ImportError(msg)
       75 
       76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-longImportError: Traceback (most recent call last):   File
  ""c:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
  line 58, in 
      from tensorflow.python.pywrap_tensorflow_internal import *   File ""c:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"",
  line 28, in 
      _pywrap_tensorflow_internal = swig_import_helper()   File ""c:\users\acuwin\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"",
  line 24, in swig_import_helper
      _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File
  ""c:\users\acuwin\appdata\local\programs\python\python37\lib\imp.py"",
  line 242, in load_module
      return load_dynamic(name, filename, file)   File ""c:\users\acuwin\appdata\local\programs\python\python37\lib\imp.py"",
  line 342, in load_dynamic
      return _load(spec) ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.Failed to load the native TensorFlow runtime.See https://www.tensorflow.org/install/errorsfor some common reasons and solutions.  Include the entire stack trace
  above this error message when asking for help."
765,How to find and remove invalid / meaningless text in python?,"['python', 'pandas', 'nlp', 'nltk', 'corpus']","I want to remove meaningless or invalid data on cell.
(A combination of meaningless alphabets or only numbers in cells)My data is below.Expected output as below.How can I this?"
766,How to do information extraction from a fragmented text in python?,"['python', 'python-3.x', 'pandas', 'nltk', 'spacy']","I am reading a large text file in Python, which looks like the following (contains many Code and Description information).From the above free text I need to store only code and description values into two lists like following.How can I do it in Python?Note: The Code can contain a ""Blank (or Blank1)"" keyword or a numeric value. Sometimes code Description is fragmented in multiple lines. In the above example, I am showing that one Code and Description block contains two codes and two descriptions. However, one Code and Description block can contain one or more than one codes and descriptions.  "
767,How to find the root of a word from its present participle or other variations in Python?,"['python', 'algorithm', 'nlp', 'nltk', 'linguistics']","I'm working on a NLP project, and right now, I'm stuck on detecting antonyms for certain phrases that aren't in their ""standard"" forms (like verbs, adjectives, nouns) instead of present-participles, past tense, or something to that effect. For instance, if I have the phrase ""arriving"" or ""arrived"", I need to convert it to ""arrive"". Similarly, ""came"" should be ""come"". Lastly, “dissatisfied” should be “dissatisfy”. Can anyone help me out with this? I have tried several stemmers and lemmanizers in NLTK with Python, to no avail; most of them don’t produce the correct root. I’ve also thought about the ConceptNet semantic network and other dictionary APIs, but it seems far too complicated for what I need. Any advice is helpful. Thanks!"
768,"Create tuples of (lemma, NER type) in python , Nlp problem","['python', 'nlp', 'nltk']","I wrote the code below, and I made a dictionary for it, but I want Create tuples of (lemma, NER type) and Collect counts over the tuples I dont know how to do it? can you pls help me? NER type means name entity recognition"
769,"how can I convert entities(list) to dictionary? my tried code is commented and not working, NLP problem","['python', 'nlp', 'nltk', 'spacy']","how can I convert entities(list) to dictionary? my tried code is commented and not working, or instead of converting how can I rewrite entities to be like a dictionary?
I want to convert in dictionary to be able to find 5 most frequently named people in the first 500 sentence."
770,"remove all words that are not nouns, verbs, adjectives, adverbs, or proper names. spacy python","['python', 'nlp', 'nltk', 'spacy']","I wrote the code below and I want Print out the words in the first 10 sentences, and i want to remove all words that are not nouns, verbs, adjectives, adverbs, or proper names.but I dont know how?  can anyone help me? "
771,nltk path in azure functions python,"['python-3.x', 'azure-functions', 'nltk']","I am tring to use nltk in one of my folder but it can't find itwhat i try to use:import nltk
nltk.data.path.append(""nltk_data"")in a one of my filesThe file tree:What is the current way to path it?Thank you"
772,"Problem with NLTK on MAC 10.15.4, Python 3.8","['python', 'nltk', 'python-3.8']",I have a problem when I try to install NLTK on my Mac. The error is :
773,How to use conditions in pairs while building a chatbot using python,"['python', 'nltk', 'chatbot']","We are building a chatbot using python, nltk and reflections. When we use refelections how can we use conditional statements to produce output based on users input.
Suppose we would like to print pass if percentage is greater than 75 and fail if less than 75. How can we write this in chatpairs.Above is the example of the chatpairs that we have used."
774,How to effectively utilize nltk in a voice assistant?,"['python', 'nlp', 'nltk', 'voice-recognition']","first question ever here so bear with me if my etiquette is poor. I'm currently working on a project where the goal is to implement a voice assistant using python. We were recommended to use natural language processing to help the assistant parse problems more effectively and I've successfully installed nltk for this. I'm totally new to natural language processing so I've run into some confusion.Right now my code will take a verbal input to the mic such as:and sucessfully tokenize it, remove the stopwords and tag it as follows:Output:Essentially my problem is that I'm not sure where to go from here. I haven't really found any good examples of how text like this should be used with API's to actually return the weather in Chicago. Would I be right to simply use if/else statements like in this pseudocode?:To summarize, when you have tokenized/tagged nltk text, what's the best way for your code to determine what to do next so that the relevant information is used by the correct library?"
775,NLTK VADER Sentiment Analysis - CANNOT Figure Out This ERROR,"['python', 'runtime-error', 'nltk', 'sentiment-analysis', 'vader']","i removed all special characters that i thought could possibly cause errors. still keep getting this error that i can't figure out:here's what i have:if anyone knows how to get around or fix this, it'd be very very much appreciated. thank you in advance!example of the .csv file:"
776,How to create a bag of word in Python,"['python', 'nltk']",Dataframe test after I cleaned and tokenized it. outputI would like to create a bag of word for my data. The following gave me error: 'list' object has no attribute 'lower'The second one: AttributeError: 'list' object has no attribute 'split'Can you please assist me either method or both. Thanks!
777,Reading a Specific JSON Column for Tokenization,"['python', 'nlp', 'nltk']",I am planning to tokenize a column within a JSON file with NLTK. The code below reads and slices the JSON file according into different time intervals. I am however struggling to have the 'Main Text' column (within the JSON file) read/tokenized in the final part of the code below. Is there any smart tweak to make this happen?
778,Using WordNetLemmatizer.lemmatize() with pos_tags throws KeyError,"['python', 'text', 'nlp', 'nltk', 'lemmatization']","I just read that lemmatization results are best when assisted with pos_tags. Hence I followed the below code but getting KeyError for calculated POS_tags. Below is the codeOutput after 3rd line (after calculating POS Tags)
Error:"
779,Unexpected lemmatize result from gensim,"['nlp', 'nltk', 'gensim', 'lemmatization']","I used following code lemmatize texts that were already excluding stop words and kept words longer than 3. However, after using following code, it split existing words such as 'wheres' to ['where', 's']; 'youre' to ['-PRON-','be']. I didn't expect 's', '-PRON-', 'be' these results in my text, what caused this behaviour and what I can do?"
780,NLP - Match topic to document,"['python', 'nlp', 'nltk', 'spacy', 'lemmatization']","We are processing a large lists of documents (similar to product descriptions) and want to figure out if they refer to a given topic (i.e. Gambling). Our current approach is to manually define a set of keywords and then use Spacy's Phrase Matcher to find any hits. We use all the pre-trained attributes like lower and lemma. Nevertheless the process is not very efficient. Are there any other libraries available? Or fundamentally different approaches?As we don't have the data to train the model ourselves we are looking for pre-trained models.Additional approaches included to use NLTK's stemmers (Lancaster and Snowball).One further requirement is languages (texts are in English, German, Italian and French)."
781,fit() missing 1 required positional argument: 'y',"['python', 'scikit-learn', 'nltk']",Getting this error despite giving the y parameter 
782,Error “resource Punkt not found” when deploying Python Flask with nltk to gcp,"['python', 'flask', 'google-cloud-platform', 'nltk']","I'm trying to deploy my Python Flask app to Google Cloud. It runs fine on my local machine but I get an error when deploying to the cloud. I get ""Resource punkt not found"". From what I can see, I've set the NLTK_DATA variable to be the correct path. It even shows that nltk is looking in that directory for the punkt folder. I'm deploying with App Engine on Google. Using textblob/nltk for sentiment analysis. The error I'm getting is ""textblob.exceptions.MissingCorpusError"". I'm stuck now so turning to you guys for help.env variables and folderserror"
783,For Loop Returns Column Headers,"['python-3.x', 'nltk']","I have the following code but my dataframe is returning column headers instead of the data itself. When I run the code, the final print(dfd) shows all 8000 rows of data in my dataframe (correctly) but one I print(mylist) all I see is a list of the 8 column headers. Any idea what's going wrong?"
784,How nltk.TweetTokenizer different from nltk.word_tokenize?,"['python', 'nlp', 'artificial-intelligence', 'nltk', 'tokenize']","I am unable to understand the difference between the two. Though, I come to know that word_tokenize uses Penn-Treebank for tokenization purposes. But nothing on TweetTokenizer is available. For which sort of data should I be using TweetTokenizer over word_tokenize?"
785,Need to get actor name out of the Json file,"['python-3.x', 'scrapy', 'nlp', 'nltk', 'spacy']","i want to get actor name out of this json file page_title and then match this with database i tried using nltk and spacy but there i have to train data. Do i have train for each and ever sentence i have more than 100k sentences. If i sit to train data it will takes a month or more. Is there any way that i can dump K_actor database to train spacy, nltk or any other way. "
786,"for nlp, Is there any text corpora with pronunciation alphabet?","['python', 'nlp', 'nltk', 'corpus', 'tagged-corpus']","I'm student who started to study nlp with python and nltk.Since I'm trying to find heteronyms in the text, it looks essential to look for pronunciation aspect of text.Therefore, I am looking for the text corpus which is provided with correct phonetic alphabet of sentences and words.https://linguistics.stanford.edu/resources/resources-corporaI thought some of above link's corpora might be help, but it lseems only standford student can get access to those corpora.In conclusion, I want to ask where can I find some text sources with, or tagged by correct phonetic alphabet.I'm not sure I asked politely in the right format because my English is not good and this is my first question on the stack oveflow. Sorry for it :(Thanks!"
787,How can I compare tokenise word in two column in python data frame,"['python', 'string', 'nltk', 'token', 'summary']","I have a CSV file that has a record of IT incidents.I have a 'summary' column and a  'category' column 
 I generated the tokenize word for each row in this column.I want to compare the token in the summary column with the category column"
788,"NameError: name 'word' is not defined ,python","['python-3.x', 'nltk']",I'm trying to create a chatbot using nltk. Here I have a list which I'm converting to lower case and to its root word using LancasterStemmer. I get this error: NameError: name 'word' is not defined.How can make this code better?I'm new to python.
789,Project Gutenberg accessing text with url,"['python', 'nlp', 'nltk', 'project-gutenberg']","I'm trying to access a text file from project gutenberg's url.
Hence I've copyed the same code from nltk book's, the result was different.This was from nltk book. When it worked properly, it should print out,But when I tried the same on my computer, it came out with this,I think it's a problem with the headers in project gutenberg. Could you help me how to deal with this?"
790,ValueError: Length of values does not match length of index in nested loop,"['python', 'pandas', 'for-loop', 'nltk', 'list-comprehension']",I'm trying to remove the stopwords in each row of my column. The columns contains rows and the rows since i already word_tokenized it with nltk then now it's a list which contains tuples. I'm trying to remove the stopwords with this nested list comprehension but it says ValueError: Length of values does not match length of index in nested loop. How to fix this?My text dataAfter i word_tokenized it with nltkThe Traceback
791,How to detect the language of a text (.csv) by its title using Python? [closed],"['python', 'text', 'nlp', 'nltk']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 months ago.For a research-purpose work I should:I am trying to do the 2nd and 3rd point using Python with its library NLTK,
Could you give me some tips if you ever did something like it?Thank you in advance!"
792,How can the php execute all the Python libraries with exec instruction?,"['python', 'php', 'localhost', 'nltk', 'exec']","I'm trying to use a python inside a php using an exec() instruction and turn on php code by localhost 
The bython code has libraries like nltk, and I think these libraries don't work outside the anaconda environment 
When I played the python file from inside the .cmd 
It showed me the following :But the python file works very well inside the Anaconda"
793,Install older (but stable) NLTK version compatible with python 2,"['python-2.7', 'nlp', 'nltk', 'python-2.x']","I would like to install an older (but stable) version of NLTK for python2.7.
I tried to run the command: pip install nltk===x.x.x but the terminal reports many errors.
I was wondering if there's a repository where nltk can be downloaded or whether there are some other ways to solve the problem. 
Thanks  "
794,Generating the plural form of a noun,"['python', 'nlp', 'wordnet', 'linguistics']","Given a word, which may or may not be a singular-form noun, how would you generate its plural form?Based on this NLTK tutorial and this informal list on pluralization rules, I wrote this simple function:But I think this is incomplete. Is there a better way to do this?"
795,Transforms of NLP dependency trees into binary trees?,"['nlp', 'nltk', 'stanford-nlp', 'spacy']","Spacy (and Core NLP, and other parsers) output dependency trees that can contain varying numbers of children. In spacy for example,  each node has a .lefts and .rights relations (multiple left branches and multiple right branches):Pattern pattern matching algorithms of are considerably simpler (and more efficient) when they  worked over predicates trees who's node have a fixed set of arities.Is there any standard transformation from these multi-trees into from binary trees?For example, in this example, we have ""publish"" with two .lefts=[just, journal] and, one .right=[piece].  Can sentences such (generally) be tranformed into a strict binary tree notation (where each node has 0 or 1 left, and 0 or 1 right branch) without much loss of information, or are multi-trees essential to correctly carrying information?"
796,pyspark RDD word calculate,"['apache-spark', 'pyspark', 'nltk', 'rdd']","I have a dataframe with text and category. I want to count the words which are common in these categories. I am using nltk to remove the stop words and tokenize however not able to include the category in the process. Below is my sample code of the problem.output : ['today', 'birthday', 'lets', 'die', 'house', 'happy', 'fun', 'going', 'office']I want to count word(after the preprocessing) frequencies with respect to categories. For example: ""today"" : 3 as it is present in all three categories."
797,Gensim Word2Vec model floating point,"['python', 'nlp', 'nltk', 'gensim']","I have trained a word2vec model using gensim. In the models matrix some values' floating point looks like this: ""-7.18556e-05""I need to use the values on the matrix as a string. Is there a way to remove those ""e-05"",""e-04"" etc.?"
798,How to merge multiple lists into one list in python? [duplicate],"['python', 'list', 'join', 'merge']","Possible Duplicate:
Making a flat list out of list of lists in Python
Join a list of lists together into one list in Python I have many lists which looks like I want the above to look like How do I achieve that?"
799,Distance between occurrences of a word,"['python', 'nlp', 'nltk', 're']","I've got a text a file having some sentences. Suppose there are three sentences ""Rahul backed from the market."",""We are going to market"", ""All the shops are closed in the market.""Now I need to calculate the distance between the occurrences of the word ""market"".Here it would be 5 and 8 because of the word ""market"" occurs after 5 words from the 1st occurrence of the word ""market"" and so on. I'm using nltk word tokenizer to get the words. Actually I need to do it for most of the words present in the corpus."
800,Is this simply an error from my code or is NLTK really bad at detecting words?,"['python', 'python-3.x', 'nltk', 'word']","I just started developing a very simple program that gets a txt file and tells you the misspelled words according to it. I looked up what would be the best program to use and I read that NLTK and use 'Words'. I did it and noticed that it is not doing its job correctly or maybe I'm not doing something correctly and it's actually my fault but can someone please check it out. The program runs fine but please give me some help in figuring out if NLTK is actually bad at detecting words or it's failing in my program. I'm using this program with testing.txt which is a file with the famous John Quincy Adams letter from his mother. The output on the terminal is this: Screenshot OutputAs you can see in the picture, it just prints out a lot of words that shouldn't even be confused such as 'ages', 'heaven' and 'contest"
801,"NLTK: TypeError: must be str, not list","['python', 'django', 'nlp', 'nltk', 'newspaper3k']","I'm using newspaper3k in a docker container. I downloaded all the needed nltk data, however I'm having this problem when I run article.nlp() then article.nlp() and article.summary.When I used the same code in a Flask app it worked, now I'm testing it on a Django (+ DRF), and I'm having this errors:It seems that there's a problem finding tokenizers/punkt/english.pickle, but when I check the nltk_data, it's there.Do you have any idea, from where this may come?Update:The code is very simple. This is my Django view:Since I'm using Django Rest Framwork, I'm serializing using this field:"
802,Split Sentence using RegexpTokenizer in a dataframe [duplicate],"['python', 'pandas', 'dataframe', 'nltk', 'tokenize']",I am trying to input dataframe into my word processor to split into sentences first and then into words.An example text:Required OutputThe code I tried so far
803,Need help for ValueError: substring not found,"['python', 'nltk', 'valueerror']","I want to make the sentences as the following:(N(Hace calor.)-(S(De todas formas, no salgo a casa.)))(N(Además, va a venir Peter.)-(S(Sin embargo, no lo sé a qué hora llegará exactamente.)))But the program can only gives me the first sentence and gives an error as ValueError: substring not found for the second sentence. Any one can help? Thanks!Here is my code:"
804,How to create a SPACE between word and open bracket in list of sentences,"['python', 'regex', 'list', 'nltk', 're']","In below list, there are actually two dupes. But due to difference of SPACE between second word of sentence and (, its treating them as unique sentences. By using Python - Regular Expressions, how to create addition space between words. (example: 1st item) 'United States(US)', should be changed to 'United States (US)'   (same as 2nd item)Expected Output list is Actually, i am trying eliminate duplicate sentences from the list and considering this is one of the approach by making the sentences similar first. 
Please suggest."
805,How to ignore other instances of a word when looping it via pandas.groupby.agg?,"['pandas', 'dataframe', 'group-by', 'nltk', 'pandas-groupby']","I have a code (see below) that I used to match word occurrences per Location. My problem is that it reads all instances of the word.FOR EXAMPLE: This is what I was hoping it to do, but the code below counted all occurrences of 'help' including 'helping' and 'helped'I got positive_freq.most_common() using the following codes. It returns"
806,Spacy custom sentence segmentation on line break,"['python', 'nlp', 'nltk', 'data-science', 'spacy']","I'm trying to split this document into paragraphs. Specifically, I would like to split the text whenever there is a line break (<br>)This is the code I'm using but is not producing the results I hopedA similar solution could be achieved by using NLTK's TextTilingTokenizer but wanted to check whether there is anything similar within Spacy"
807,python: groupby (merge) next lines with previous line if they start with same match pattern in text data,"['python', 'dataframe', 'text', 'nltk']","I have a file.txt with data group (AAA-(n)) that is very large. Many lines in the file have the same tag (ex. AB) between line AAA -(n)  to  AAA-(n+1) in the file. I want to put them into one line. For example:My desired output is:I tried this:Out:plz, help me to avoid grouping other tags with AB and remove tags after group?"
808,Split Sentences at Bullets and Numbering?,"['nlp', 'nltk', 'sentence']",I am trying to input text into my word processor to be split into sentences first and then into words.An example paragraph:I tried the following codesThe Output I gotRequired OutputHow to split the sentence at Bullets and Numbering?Solutions on spaCy  also would be very helpful
809,How to perform Kneser-Ney smoothing in NLTK at word-level for bigram language model?,"['python', 'nlp', 'nltk']","From the nltk package, I see we can implement Kneser-Ney smoothing only using trigrams but it throws error when I try to use the same function on bigrams. Is there a way we can implement smoothing on bigrams?"
810,How to ignore punctuation in-between words using word_tokenize in NLTK?,"['python', 'nlp', 'nltk', 'tokenize', 'multi-word-expression']","I'm looking to ignore characters in-between words using NLTK word_tokenize.If I have a a sentence:The word_tokenize method is splitting the S&P into Is there a way to have this library ignore punctuation between words or letters?
Expected output: 'S&P','?'"
811,How to use NLTK Regex patterns to annotate financial news with UP/DOWN indicator?,"['python', 'regex', 'nlp', 'nltk']","I'm working on replicating an algorithm describe in this paper: https://arxiv.org/pdf/1811.11008.pdfOn the last page it describes extracting a leaf defined in the grammar labelled 'NP JJ' using the following example: Operating profit margin was 8.3%, compared to 11.8% a year earlier.I'm expecting to see a leaf labelled 'NP JJ' but I'm not. I'm tearing my hair out as to why (relatively new to regular expressions.)  "
812,How to find a capital letter words from an NLTK corpus using regex?,"['python', 'regex', 'nlp', 'nltk', 'corpus']","I'd like to make a word list with a regular expression which is consists of all capital letters. 
the data set is a bunch of biological theses text files called corpus. The result for len(corpus.fileids()) is 487 which means that there are 487 theses in the corpus.The main reason for this is to collect word list to filter biological words like gene name and etc(ATP, BRCA)here are some codes that I've been trying. 
(p.s. I'm using python3)I'm stuck with making functions to call out all files in the corpus.
for a single file, I think this would work.but the thing is that I have to go through all the words in the theses txt files in the corpus and have no idea. 
1st trial2nd trialThird trial"
813,From a sentence count distinct words per line in a pandas dataframe,"['python', 'pandas', 'loops', 'nlp', 'nltk']",I am analogizing data where I have sentences one in each row example This are all sentences I need to count the words per line how many times same words and get something like thisThis image is what I really need to get toI did this and it will separate the words per commasNow I am trying to count them this is just counting all words together PhaseFinal is a list.. that I cleaned up the data removing some things 
814,can someone tell whats remove_punct_dict command is doing ?? and whats the output of last line command?,"['python', 'nltk']",can someone tell whats remove_punct_dict command is doing ?? and whats the output of last line command ?
815,How to replace special charachters in Pyspark?,"['apache-spark', 'pyspark', 'nlp', 'nltk', 'sql-function']","I am fairly new to Pyspark, and I am trying to do some text pre-processing with Pyspark.
I have a column Name and ZipCode that belongs to a spark data frame new_df. The column 'Name' contains values like WILLY:S MALMÖ, EMPORIA and ZipCode contains values like 123 45 which is a string too. what I want to do is I want to remove characters like :, , etc and want to remove space between the ZipCode.
I tried the following but nothing seems to work :I tried other things too from the SO and other websites. Nothing seems to work."
816,How to find abstractness of a word using hyper-/hyponyms in wordnet?,"['python', 'nlp', 'nltk', 'wordnet']","I have 2 words, let's say computer and tool.
Computer is a concrete noun whereas tool is relatively abstract.
I want to get level of abstractness of each word that will reflect this.
I thought the best way to do it is by counting number of hyper/hypo nyms for each word.Thanks!"
817,How to print Categorical features in Machine Learning?,"['python', 'machine-learning', 'nlp', 'nltk', 'data-science']","assume I have a train datasetr1: cheap, expensive -> pricer2: excited          -> entertainmentr3: hot, summer      -> weatherr4: money            -> pricer5: rain             -> weatherthen I want to display it in this pattern:price         -> cheap, expensive, moneyentertainment -> excitedweather       -> hot, summer, rainanyone knows? I'am doing a NLP research. thankyou. "
818,How can I parse a text from an URL and put the clean text in a DataFrame?,"['python', 'pandas', 'beautifulsoup', 'nltk']","I have an Excel file of 147 Toronto Star news articles that I've compiled and created a dataframe. I have also written a Python script that can extract the text from one article at a time. However, I'd like to improve my script so that Python will cycle through all the URLs in the dataframe, scrape the text, append the scraped, stopworded text to the row (or perhaps to a linked text file?), and then leverage that data frame for a classification algorithm and more exploration. Can someone please help me with writing the loop? (I have no background in programming.. struggling!)"
819,"Cannot feed value of shape (8,) for Tensor 'InputData/X:0', which has shape '(?, 150)","['python', 'csv', 'machine-learning', 'deep-learning', 'nltk']","Working on Text classification using Deep Neural Network in Python and
 getting the error that I mentioned in the title.MY CSV FILE CONTAINS 3 COLUMNS rating, review, and text all are in string datatype. CSV file contains 150 rows. Done in Google Colab Python. Using Deep Neural Network Text Classification is done"
820,Counting Sentences using NLTK (5400) and Spacy(5300) gives different answers. Need to know why?,"['python', 'nlp', 'nltk', 'spacy', 'sentence-similarity']",I am new to NLP. Using Spacy and NLTK to count the sentences from JSON file but there is a big difference in both of the answers. I thought that the answers will be same. Anyone who can tell me that?? or any web link which will help me about this. Please I'm confused here
821,"Error in having a sequence - TypeError: sequence item 0: expected str instance, list found","['python-3.x', 'pandas', 'nltk', 'token']",I need your help in joining a list of words into a string. I have tried to map it to a string but to no avail. Here is a snippet of my for-loop.This is what is stored in  tokenized
822,Python based tokenizers for non English languages,"['nltk', 'tokenize', 'spacy']","We currently build text based models using scikit learn library in Python. Scikit learn has default support of tokenization for English laungauge. We want to add support for non-english languages as well (Spanish, French, German, Italian, Japanese, Turkish). I am looking for a python library that support above languages. I came across SpaCy and NLTK but I am looking if there are any other python libraries and if there is comparison chart in terms of benchmarking, memory usage, accuracy, support for multiple languages, stability and community support.
I found this https://spacy.io/usage/facts-figures but I am wondering if anyone did some research on other python libraries also and have a similar comparison chart so that it helps me in choosing a right library for my work."
823,Finding the List of words in List of Sentences and return the matching sentences,"['python', 'nlp', 'nltk', 'list-comprehension', 'trigram']","From the List of Sentences and List of words, how to return the list of Sentences, only if all three words are matching from words Lists (Trigrams).Please suggest. Below are example lists.Output list should be first & last sentences, as they have three matching words in listwords.  Expected output is:"
824,"construct the unigrams, bi-grams and tri-grams in python","['python', 'nltk']","how to construct the unigrams, bi-grams and tri-grams for large corpora then to compute the frequency for each of them. Arrange the results by the most frequent to the least frequent grams."
825,Is it possible to get classes on the WordNet dataset?,"['nlp', 'dataset', 'nltk', 'wordnet']","I am playing with WordNet and try to solve a NLP task.I was wondering if there exists any way to get a list of words belonging to some large sets, such as ""animals"" (i.e. dog, cat, cow etc.), ""countries"", ""electronics"" etc.I believe that it should be possible to somehow get this list by exploiting hypernyms.Bonus question: do you know any other way to classify words in very large classes, besides ""noun"", ""adjective"" and ""verb""? For example, classes like, ""prepositions"", ""conjunctions"" etc."
826,Why does the nltk stopwords output does not match nltk word_tokenize output,"['python', 'text', 'nltk', 'tokenize', 'word']","I am currently using nltks stopwords and word_tokenize to process some text and encountered some weird behavior.printing the following:Focusing on the words containing the "" ' ""-char. We can see, that the stopword list clearly contains words separated by it. At the same time all the words in my sample sentence are included in the stopword list, so are its parts. (""doesn't"" -> included, ""doesn"" + ""t"" -> included).The word_tokenize function however splits the word ""doesn't"" into ""does"" and ""n't"".
Filtering the stopwords after using word_tokenize will therefore lead to the removal of ""does"" but leaves ""n't"" behind...I was wondering if this behavior was intentional. If so, could someone please explain why?"
827,Pyinstaller - Python exe when run shows error “Failed to execute script pyi_rth_nltk”,"['python', 'nlp', 'nltk', 'pyinstaller']","I have developed a simple software in python with GUI. I'm actually working on Natural Language Processing and I've just put the whole NLP process in a GUI.I tried to convert the whole project to a exe file in python using Pyinstaller. I successfully converted it to an exe file but when I run it , it shows an error message something like this
This is the image of my error"
828,What's the best way to classify text data in ML?,"['python', 'tensorflow', 'keras', 'nlp', 'nltk']","Let's say i have a dataset consisting of a review column with exactly 100 words for each review, then it may be easy to train my model as i can simply tokenize each of the 100 words for each reviews then convert it into a numerical array and then feed it into a Sequential model with input_shape=(1,100). But in the real world, reviews are never the same size. If I use a function such as CountVectorizer, then the structure of the sentence is not reserved, and one hot encoding may not be efficient enough.So what is the proper way to preprocess this particular dataset so that i feed it into a trainable NN"
829,ModuleNotFoundError NLTK,"['python', 'python-3.x', 'nltk']","Tokenization is working fine but when I try to do Named Entity Recognition
namedEnt = ne_chunk(tagged, binary=True)It gives the following error
I did install the NumPy from cmd using pip install numpy but still giving the errorError:"
830,What's the best way to vectorise text data in NLTK if i want to preserve the order of sentence?,"['tensorflow', 'keras', 'nlp', 'nltk']",I'm classifying text data and want to feed it into a model but I am stuck with an issue. I don't want to use CountVectorizer because it doesn't preserve it's structure but also don't want to manually convert each word into an array to due inefficiency.What methods can I use that will help in such a context.Thanks
831,How can I download NLTK corpora via `requirements.txt` using `pip install -r requirements.txt`?,"['python', 'pip', 'nltk', 'corpus', 'requirements.txt']","One can download NLTK corpora punkt and wordnet via the command line:How can I download NLTK corpora via requirements.txt using pip install -r requirements.txt?For example one can download spacy models requirements.txt using pip install -r requirements.txt by adding the URL of the model  (e.g. https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0
in requirements.txt)"
832,Skipping tuples without attributes Python NLTK,"['python', 'python-3.x', 'pandas', 'jupyter-notebook', 'nltk']","I have a script that is mostly working for the Natural Language Tool Kit. It works by using NLTK to tokenize and label (categorize) individual words. When my list includes names and entities it works fine. Where it breaks down is if the list includes articles of speech such as ""The"", ""a"", ""and"" etc.These are words that are not going to receive labels from NLTK (Persons, Organization, Geographic Location etc..)  My question is there is a way to skip the tuples that will give me an error because they will not return a label attribute?Example dataframe:Code:Output:Example dataframe 2:Error:The ""to"" is causing it to crash because ""to"" would not get a label. If I'm dealing with thousands of words it would not be practical to find all the words that would cause it to crash and remove them manually. Ideally I would like to skip problematic lines, but I'm not sure if it is possible. Thanks for the help."
833,how to fix “ Error loading stopwords” in pycharm,"['python', 'ssl', 'nltk']",I am using Pycharm and when I run the below code:I get this error msg?Could you please help me?
834,ModuleErrorNotFound: no module named 'nltk',"['python', 'macos', 'nltk']","I'm trying to write a basic script using nltk, which I've already installed with pip on my computer, but whenever I try to run my code with import nltk at the top I keep getting a module not found error, this occurs with other programs I write as well, always at the first line, and I'm not sure what to do."
835,Tweet Tokenizing A column in Dataframe ; Getting Error,"['python', 'pandas', 'nltk', 'tokenize']",I have a dataset with comment_texts. I want to tokenize these using code below:I am getting below error by doing it:The data I am using is train set of Kaggle competition: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data
836,"'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader';","['python', 'flask', 'heroku', 'nltk']","I have developed a NLTK based flask app. Now I have deployed it in Heorku. While deploying it I am getting an error: remote: /app/.heroku/python/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour. And also some pages and functions are not working properly. Please suggest me something to solve this problem."
837,text processing: how to filtering type of word only noun,"['python', 'nlp', 'nltk']",I have a coding like this :and the output:how to delete/remove a type of words other than noun?Thankyou 
838,NLTK Named Entity Category Labels,"['python-3.x', 'pandas', 'nltk', 'jupyter']","I keep hitting a wall when it comes to NLTK. I've been able to Token and Categorize a single string of text, however, if I try to apply the script across multiple rows I get the Tokens, but it does not return a Category which is the most important part for me. Example:Output:That is exactly what I'm looking for. I need the Category not just NNP. When I apply this across a table I just get the token and no Category.Example:Input:Code:Output:I'm getting the tokens and it's working across all rows, but it is not giving me a Category 'PERSON'.I really need Categories. Is this not possible? Thanks for the help. "
839,"How do I resolve RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'?","['python', 'heroku', 'runtime-error', 'nltk', 'dashboard']","While trying to deploy an app on Heroku I keep getting this Runtime error. I've tried replacing on the main app file (app.py) with but I still get the warning, which is preventing the dash app from displaying correctly on the Heroku server.Any help would be appreciated!"
840,NLTK ne_tree Word Tokenize chunk from Column rows (Python/Pandas/Jupyter),"['python-3.x', 'pandas', 'nltk', 'jupyter']","I just started learning about the Natural Language Took kit. I'm trying to categorize words. I'm basically looking for things Persons, Places, and Organizations.So far defining a single line of text works in the script. Output:My question am I able to specify an entire column with this script?My table is as below.ex2:Order is basically the index I created. The idea is later I can break sentences down to words, maintain a key, and melt. Text is what I want to tokenize.When I run this code I get the below error. Perhaps I'm calling it incorrectly, and I need to specify the column? Thanks for the help.Error:"
841,First steps into creating my own CFG based on tokenized sentences from a Corpus?(DUTCH),"['nlp', 'nltk']","I'm quite new to NLTK and I've come quite far. However, I seem to be stuck.
I have to devise a grammar that will be able to parse the sentences of my corpus. 
I have tokenized the sentences, tagged the part of speech using a Dutch Tagger, and I have  fixed the errors within. I can't seem to figure out how to start doing this. Should I make parse trees per sentence myself and then compare all of them and combine it into one? I have a total of 104 sentences. Is there a certain way of doing this? I'm still new to this and the NLTK book is quite confusing. I hope I've made my question clear, but if not, please let me know.Thank you!"
842,check similarity or synonyms between word in Python,"['python', 'nlp', 'nltk', 'spacy']","I want to find synonyms of words.If word is tall building then i want to find all synonyms of this word like ""long apartment ,large building""etcI used Spacy.I can't use this because it takes a lot of time neither I can use PhraseMatcher for thisPlease help methanks in Advance"
843,nltk.downloader gives xml.etree.ElementsTree.ParseError,"['python', 'python-3.x', 'nltk']","I am trying to build a docker image. While executing the given command:RUN python -m nltk.downloader punkt The build fails by throwing the below error: nltk.downloader gives xml.etree.ElementsTree.ParseError : unclosed token: line 108, column 4nltk version used: 3.4.5. Also tried with 3.5.0Python: 3.5 is usedWhat is causing this ?"
844,"Pandas and NLTK: Replace empty cells with subsring of adjacent column, if substring is contained in NLTK tokens","['python', 'pandas', 'nltk']","I have a table consisting of PRODUCT NAMEs and MAKERs. Some of the maker cells are empty hence i want to write a code to replace the empty cells in maker column with the substring in the product names. here is the table for info:In order to identify the substrings i want to use, iam using NLTK library.here is the code i have written so far:So far i have written the code to generate the tokens however iam not sure how to move on from here.ultimately, i want to append the dataframe to be as follows by allowing the code to match substrings in Product Name with the items in the tokens dataframe(dfb) and appending maker column accordingly:"
845,Can you download NLTK data through Poetry?,"['nltk', 'python-poetry']","I'm building an app that uses both NLTK and Spacy, and manages dependencies via Poetry. I'm able to download the Spacy data by adding this line to my pyproject.toml under [tool.poetry.dependencies]Would it be possible to do something similar for the NLTK data? I'd rather have it all specified in one place instead of downloading it as part of some setup script"
846,Invalid Syntax when I imported nltk in python 2.7,"['python', 'python-2.7', 'syntax', 'syntax-error', 'nltk']",when I wrote the line in python 2.7 interpreter terminal it is showing SyntaxError:Invalid SyntaxHow to fix this?
847,Parse Tree in StanfordCoreNLP and Stanza giving different result (representation structure),"['python', 'nlp', 'nltk', 'stanford-nlp']",I did dependency parsing using StanfordCoreNLP  using the code belowAnd I got the output:How can I get this same output in Stanza??In stanza It appears I have to split the sentences that makes up the sentence. Is there something I am doing wrong?Please note that my objective is to extract noun phrases. 
848,Extracting Key-Phrases from text based on the Topic with Python,"['python', 'machine-learning', 'nlp', 'nltk']","I have a large dataset with 3 columns, columns are text, phrase and topic. 
I want to find a way to extract key-phrases (phrases column) based on the topic.
Key-Phrase can be part of the text value or the whole text value.I'm having big trouble with finding a path to do something like this, because I have more than 50000 rows in my dataset and around 48000 of unique values of phrases, and 3 different topics.I guess that building a dataset with all football, basketball and tennis topics are not really the best solution. So I was thinking about making some kind of ML model for this, but again that  means that I will have 2 features (text and topic) and one result (phrase), but I will have more than 48000 of different classes in my result, and that is not a good approach.I was thinking about using text column as a feature and applying classification model in order to find sentiment. After that I can use predicted sentiment to extract key features, but I do not know how to extract them. One more problem is that I get only 66% accuracy when I try to classify sentiment by using CountVectorizer or TfidfTransformer with Random Forest, Decision Tree, or any other classifying algorithm, and also 66% of accuracy if Im using TextBlob for sentiment analysis.Any help?"
849,Pandas NLTK - Tokenizing all rows in a column for natural language processing,"['python', 'pandas', 'nltk', 'jupyter']","==Using Juypter Notebooks==I got NLTK working on a single string of text. Output:This is okay, but not that useful because I would like automate this on many rows in a data frame. Basically tokenize the words while maintaining an index key so I can reassemble the tokens I want in a new field. For example I'm looking for human names in particular excel column that contains over 1,000 rows.When I try this out on a dataframe this is the problem i run into. Running the code with this data frame I get TypeError: expected string or bytes-like objectIs this not possible or is there some conversion command that needs to happen? Thanks for the help.ERROR:"
850,Finding words with specific criteria in a text in python,"['python', 'nlp', 'nltk', 'spacy']","I have a text and want to find e.g.Is there a ""simple"" way to do this?Im completely new to NLP.
Im not even sure if this is a NLP Problem in the first place.
Or does this just sound simple and is actually very complex?If you have a tip for adding/removing tags on this question all feedback is welcome."
851,Python NLTK extract synonyms from a nested list of synonym sets,"['python', 'nlp', 'nltk']","I have a nested list of 65 synonym sets, in NLTK format.
egI want to create a new nested data frame (length 65) whose ""outer"" elements are the 65 words, and whose inner elements are each word's synonyms. How do I do this? 
Here's what I have so far:
    import os
    import pandas as pd
    from nltk.corpus import wordnet
    import nltk
    nltk.download('wordnet')I am getting error:  'list index out of range'"
852,Extracting all Nouns from a CSV File Using NLTK,"['python-3.x', 'pandas', 'nltk']","I am new to both Python and NLTK. I would like to ask how can I extract all nouns from a list of sentences in CSV file using nltk? the list of sentences is in CSV file and is in this form of:   1    I like to eat bread
2    I am excited to watch this movie
3    I sit on the bench and read my favorite book  "
853,How to use a Dutch tagger for a list with sentences in a list?,"['python', 'nlp', 'nltk']","I've managed to find a nice Dutch NLTK tagger to tag POS in my text that I have to annotate.
My nested list looks like this:and it has a total of 1096 words and 105 sentences.The tagger I used:to get the output of the first element.Maybe it is because it is still pretty early, but I can't wrap my head around how to use this tagger for all the sentences.Any hints, tips, guidance is appreciated. Thank you!"
854,how to remove numbering in list using word tokenize function in python ? I am getting the output but i need without numbers,"['python', 'pandas', 'csv', 'nltk']","I dont need the list numbers (i.e.) 0,1 etc. I need to print elements 
  without numbering how to remove numbering in list using word tokenize function in python ? I am getting the output but i need without numbers"
855,How to extract word from string and use as variable?,"['python', 'regex', 'nltk', 'chat', 're']",I am working on using NLTK in python and setting up some responses. in the last item in my pairs I want to extract the last word then match it back to a time from the hours dictionary based on the day that is input. I am stuck trying to get this to work using regex and might be going down the wrong path.
856,Python NLTK Remove Internal Punctuation NOT Part of a URL,"['python', 'nltk']","I'm using NLTK in Python, and I'm having an issue with trying to remove internal punctuation from text where there is supposed to be a space after the period to start a new sentence.Here are a few examples:'on.How''time.Jerry' 'me?What'How can I remove the punctuation from the preceding examples, but still keep punctuation in URLs like stackoverflow.com or nltk.org?Thanks!"
857,Stop StanfordCoreNLP from Connecting to StanfordCoreNLP server,"['python', 'nltk', 'stanford-nlp']","Whenever I create StanfordCoreNLP for a parsing task, I get this logging On applying the parser, self.nlp_src.parse(sentence), I got another logging of the form below:{'properties': ""{'annotators': 'pos,parse', 'outputFormat': 'json'}"", 'pipelineLanguage': 'en'}Lastly, I got this error in the process of using the parser in the stanford coreNLP as explained above. requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9032): Max retries exceeded with url: /?properties=%7B%27annotators%27%3A+%27pos%2Cparse%27%2C+%27outputFormat%27%3A+%27json%27%7D&pipelineLanguage=en (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe04121d6d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
858,"VSCode, keep running into a module error despite installing said module in my python shell","['python', 'visual-studio-code', 'nltk']","So I've been trying to clean up my word dictionary data by removing stop words and words where the count is less than 1000.totalWordsDict is a dictionary containing a word key and a count value.
When I run this code after commenting out the import and the code after the Stopword filter, it works fine.This is the error I keep running into:
ModuleNotFoundError: No module named 'nltk'I've pip installed nltk for Python 3.8.2 (same installation that I'm running this code on), as well as downloaded all of its packages through the shell by using nltk.download('all'), but I'm still running into this error."
859,Iterate through raw text in Python to separate questions and answers in a PDF,"['python-3.x', 'pdf', 'nltk']","Using Python 3.7:I have a PDF document that I am converting to raw text using textract. The PDF consists of a Table of Contents, and then a series of questions and answers, for example:-TOC:content of TOC-Question 1:content of question 1-Answer 1:content of answer 1-Question 2:content of question 2-Answer 2:content of answer 2etc...I managed to convert the PDF to text:My question is: How can one iterate through the text to isolate the question and its respective answer to get the question and answer in a single string?I tried to use NLTK to search for the phrase ""Question 1:"", but I could not figure out how to isolate all the text between ""Question 1:"", and ""Question 2:"".I also tried to convert the PDF to HTML to try to preserve the headings of the questions and answers, but there are many other items being flagged as headings with the word ""Question :1"" from the Table of Contents.Any thoughts as to what can be done here?Thanks!"
860,What is the meaning of “drop” and “sgd” while training custom ner model using spacy?,"['neural-network', 'nlp', 'nltk', 'spacy', 'ner']","I am training a custom ner model to identify organization name in addresses. 
My training loop looks like this:-Can someone explain the parameters ""drop"", ""sgd"", ""size"" and give some ideas to how should I change these values, so that my model performs better. "
861,Sentiment analysis in realtime feed using NTLk,"['python', 'python-3.x', 'nltk', 'sentiment-analysis', 'praw']",I would like to know if its feasible analyse headlines news  (using NTLK /sentiment vader) in realtime/streaming news.Below the code that feed my system of news (headlines)Using  SentimentIntensityAnalyzer  I built:I can´t see anything displayed in the console... I'm expecting to see something like (in realtime)
862,Scrapy Webcrawler - How to filter out crawled content which is in a specific language?,"['python', 'scrapy', 'web-crawler', 'nltk']","I am a python / scrapy newb - so please don't be to harsh on me :-).I did build the following scrapy spider which crawls through some sites and extracts the crawled content (consisting of three different categories of data) and puts it into a list of dictionaries. The spider looks as follows:As said, I now would like to filter out crawled content which is in French language and take such entire dictionaries out of the list (of dictionaries). All the rest of the data is in German. I tried to achieve this as follows (example only for the 'facts' variable):But this does not work.'merged_facts_II' should be of data type str; it is actually not a list of Strings. This is why I directly pass it into TextBlob.I am not sure what the issue is. As said, I am a complete Python newb and hence it may be an absolute trivial problem here.Many thanks for your help!"
863,Excluding words in Word cloud,"['python', 'nltk', 'stop-words', 'word-cloud']",I am trying to exclude some extra words from nltk stopwords library for spanish words but whenever i apply the following code is not reflected in the final world of cloud. I believe is related to the list structure which is not being really aplied the words filter criteria but i cannot realized why.To this list i apply the following functionIt doesnt do anything and i still having the excluded words in lines. I believe maybe it's because i didnt tokenize lines but whenever i apply :this error is shown:
864,Syntax Error with nltk import when running it with wsgi server,"['python-3.x', 'flask', 'nltk']","I wanted to host my python chatbot project on an EC2 instance and I installed all the packages and it is running locally but when i try to run it on a wsgi server, I get the following error. I tried re installing nltk and that also wouldnt work, I cant seem to figure what the problem, and i am trying to run this an apache 2 wsgi serverI wanted to know how to fix that.Python code:WSGI.py:Thank you For the help"
865,How can I use natural language processing to split a sentence in half?,"['nlp', 'nltk', 'spacy']","I've never done natural language processing, so please excuse any wrong terminology. I'm open to using any language/library such as NLTK or spaCy, but no paid API.I'm trying to make a game where the user has to finish a famous quote with their own.Example input:A room without books is like a body without a soul.Desired output:A room without books...I'd like to be able more complex sentences such as:I've learned that people will forget what you said, people will forget what you did, but people will never forget how you made them feel.Desired output:I've learned that people will forget what you said...Another example:Don’t walk in front of me… I may not followDon’t walk behind me… I may not leadWalk beside me… just be my friendDesired output:Don’t walk in front of me… I may not follow...I feel like extracting noun/verb phrases/chunks is almost what I want, but not quite. Maybe something like (pseudocode):Is this too complicated to achieve using NLP? Are there too many syntactical variables in the English language to cover to get consistent results? Should I just manually split up the quotes? I'm open to suggestions instead of working answers, like ""Read up on blah blah"", I just don't know where to even begin. But I'd like to know if this is feasible before I spend a lot of time learning NLP just to find out this isn't possible."
866,python NLP and regex missing white space between words,"['python', 'nlp', 'nltk', 'whitespace', 're']","I'm trying to process a large text file using NLTK and regular expression. After a few codes, I found that I have missing space between some words. Below are my codes and examples of those words clustered together because of the missing white space. I'd really appreciate it if you could help me see which part of the re code is causing the problem.The data I started with is:news
99¢ Only Stores_6-9-2008 99¢ Only Stores(r) Issues Precautionary Recall...
APP Pharmaceuticals_3-26-2011 APP Pharmaceuticals Issues a Nationwide Volunt...
Abbott Diabetes Care_12-22-2010 Abbott Diabetes Care Announces Recall of Certa...
Abbott Nutrition_1-19-2009 Abbott Nutrition Announces Voluntary Recall of...
Abbott_5-25-2007 Abbott Announces Voluntary Nationwide Recall o...Some examples of the wrong output:abbottabbott, abbottdocument, alexandraspmcommunicationscom, americacustomers"
867,Stopword Removal Dilemma,"['python', 'nlp', 'nltk', 'stop-words']","I am facing a dilemma with the stopwords function in NLTK. I am processing user-generated content from a social media platform by removing stopwords using NLTK. However, the dilemma is I want to keep personal pronouns in users' text, which are important for the classification task. These include words such as ""I"" ""you"" ""we"", etc.Unfortunately, the stopwords function deletes these words, too, and I need them to be present. How can I solve this problem?"
868,How to deal with cases where more than one pronunciation is found using Arpabets (NLTK)?,"['python', 'nltk']","If a word has two possible pronunciations, for example 'wind', which can be either a noun or a verb, which case corresponds to each item in the result provided for Arpabet?'They wind back the clock, while we chase after the wind'"
869,Python NLTK Prepare Data from CSV for Tokenization,"['python', 'csv', 'nltk', 'tokenize']","I'm new to Python and NLTK. I'm trying to prepare text for tokenization using NLTK in Python after I import the text from a csv. There's only one column in the file with free text. I want to isolate that specific column, which I did.... I think. When I run the code, the output it gives me is ['values'] which is the column name. How do I get the rest of the rows to show up in the output?Sample data I have in the 'values' column:The way was way too easy to order online.Everything is great.It's too easy for me to break.The output I'm hoping to receive is:"
870,How to complete natural language sentences respecting concordance and grammar?,"['nlp', 'nltk', 'nltk-trainer', 'nltk-book']","I'm been doing some language processing with nltk. In this project i'm using markov chains coupled with POS to try to complete sentences until the end. Unfortunately, to add a new word markov chains only take in consideration the last 3 or 2 words, so the meaning of the sentence being completed is unreacheable. How can one get around this limitation to autocomplete with sentences that take more of the message of the sentences being completed into consideration?"
871,How to get original token value of text using Nltk RegexTokenizer,"['python', 'nltk', 'tokenize']",I am able to remove the punctuation from text using Nltk RegExTokenizer. I want to get the original index of tokens item from the original masked_ocr_text without punctuation removed. How i can achieve it? Following is the codeFor example :I want to get the index from actual_tokens for some required tokens value.
872,NLTK path_similarity is giving ValueError,"['python', 'numpy', 'nltk']","I am currently doing a coursera aassignment with NLTK to find Path_similarity between two doccuments,But got stuck.My issue is that this test funciton instead of returning a float value & it is giving This ValueError:This is basically because the path similarity beween synsets2[-1] and every synset of synsets1 is givng a None value.
But it should not be like this according to the instructions ,I tried for so much time but could'nt figure out how to avoid this and get a float value.As the instructors may take days to respond,I came here for help,Please look into this,if you could.
edit:these are the synsets2 and synsets1."
873,Pyspark - Sentence Tokenisation in a dataframe column of strings,"['python', 'pyspark', 'apache-spark-sql', 'nltk', 'tokenize']","Let's say I have the dataframeWhat I need is to tokenize the sentences of each row of the column txt.
For instance, I have the code:Followed by:I get the output for column sent_tokenzWhich is not what we want. That would be something likeAny help on what is going on is welcome."
874,NLP: How do I combine stemming and tagging?,"['python', 'nlp', 'nltk', 'tagging', 'stemming']","I'm trying to write code which passes in text that has been tokenized and had the stop words filtered out, and then stems and tags it. However, I'm not sure in what order I should stem and tag. This is what I have at the moment:At first glance, this works just fine. However, because I stemmed first, pos_tag often mislabels words. For example, it marked ""hous"" as an adjective, when the original word was really the noun ""house"". But when I try to stem after tagging, it gives me an error about how pos_tag can't deal with 'tuples' - I'm guessing this has something to do with the way that the stemmer formats the word list as [('come', 'VB'), ('hous', 'JJ'), etc.Should I be using a different stemmer/tagger? Or is the error in my code?Thanks in advance!"
875,Is countvectorizer in sklearn only meant for English?,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'nltk']","I am trying to apply count vectorizer for Telugu and Hindi which are Indic language.But the vectorizer is stemming the words automatically.the output is as follows:It is clearly evident that it is stemming the telugu and hindi words automatically, is there any way to avoid that?"
876,How to Categorize the Data With Highest matched keywords count using two data frames in Python Pandas,"['python', 'pandas', 'dataframe', 'nlp', 'nltk']","I've two Dataframesdf1 - Categories column with list of categorykeywords columnsDataframe1another dataframe with description column and keywords pulled from description column using rakeData Frame 2Now I want to assign a category to the decription with the highest matched keywords, For example consider the last row of description, even though all the keywords are available in categ1,categ2,categ3,catge7 but it has additional keywords available in categ 5 ('i.ve Vue) hence assign the catgeory to that description as Categ5 becacuse it has highest matched keywords count, and crate a new column as assignedcatgory and populate the catgory if no catgory available then set it to default.
 Please find the below Expected output resultPlease Help me how can i achieve the below resultExpected OutputMyCode:"
877,FileNotFoundError: [WinError 3] The system cannot find the path specified: 'aclImdb/train/',['nltk'],For this line of code:I am getting File not found error as:
878,How to get subjectivity score of the text in NLTK?,"['machine-learning', 'nltk', 'sentiment-analysis']","I need a method in NLTK that calculates the score (real number) of the text subjectivity. 
Is there anything like that in NLTK?"
879,Extracting Related Date and Location from a sentence,"['python', 'nlp', 'nltk', 'linguistics']","I'm working with written text (paragraphs of articles and books) that includes both locations and dates. I want to extract from the texts pairs that contain locations and dates that are associated with one another. For example, given the following phrase:The man left Amsterdam on January and reached Nepal on October 21stI would have an output such as this:>>>[(Amsterdam, January), (Nepal, October 21st)]I tried splitting the text through ""connecting words"" (such as ""and"" for example) and work on part as follows: find words that indicate a location (""at"", ""in"", ""from"",""to"" etc.) and words that indicate a date or time (""on"", ""during"" etc.), and join what you find. However, this proved to be problematic, as there are too much words that indicate location and date, and sometimes the basic ""find"" method cannot distinguish between them.Assume that I am able to identify a date as such, and given a word that starts with a capital letter, I am able to determine if it is a location or not. The main issue is connecting between them, and making sure they are.I figured that tools like ntlk and scapy will assist me here, but there isn't enough documentation to help me find an exact solution to this kind of problem.Any help would be appreciated!"
880,"Not sure how to fix this: TypeError: string indices must be integers, not unicode","['python', 'twitter', 'nltk', 'sentiment-analysis', 'vader']","I'm trying to gather some sentiment data on some tweets that I've parsed into a spreadsheet. I'm not the best coder and haven't coded in a good while so I'm super rusty. I am using an old script that I had for a project a few years back doing the same thing and am having trouble understanding it in order to figure out my error. Any help would be appreciated! :) Traceback (most recent call last): line 40, in  sentiment_data = get_sentiment(rating_data) line 25, in get_sentiment print ss['neg']TypeError: string indices must be integers, not unicode"
881,Tokenize list of words in a DataFrame,"['python', 'pandas', 'nltk']","I have  acurrent DataFrame with a column that contains a list of words:I want to tokenize the elements in my lists and get a result like below if possible, its important for me that words in the list arent seperated:When i use the current code, all the words in the list gets seperated and i didnt found anything in the nltk docs to get the result i want. I also tried to work around the mwe Module in nltk but i dont know how to use it in my case as i dont have any sentences :Mwe code:"
882,Using Vocabulary parameter in TfidfVectorizer : ValueError: Duplicate term in vocabulary: 'p',"['python', 'nltk']","I am vectorizing a list of strings using TfidfVectorizer.
I want to be able to use the ""vocabulary = mapping"" option in TfidfVectorizer. If I can use “vocabulary” parameter then I have the vocabulary(feature names) mapped to corresponding index in csr matrix itself. This would allow me to save the csr file and reuse in a different code with the feature names.If I don’t use vocabulary parameter, I have to use .get_feature_names() for extracting feature names. 
Then, I am creating a dataframe with feature names and counts and save the dataframe so that its reusable. The error I was getting was : ValueError: Duplicate term in vocabulary: 'p'
I even added stopword to remove single characters (“_stopwords.append(""\w"")), but still I got the same error.Can anyone please say what I might be doing wrong here?"
883,Python - Is there an NLTK Corpus for english GB words?,"['python', 'nltk']","I am in the process of learning Python and trying to create an anagram creator/solver in flask.I'm using nltk and have a basic script set up which descrambles a group of letters and finds the word from the corpus. I know my method may not be perfect - remember I'm still learning what is available to do in Python - but it works in principle and I've created a similar script to find all the words within a group of letters.My problem is that it only uses American English, so in the example below 'favro' becomes 'favor' which is the American spelling, but 'favrou' doesn't become 'favour' which is the British spelling.Is there anything out there which distinguishes between American and British English?I've tried to use 'enchant' and it works fine on the solver part, but when I try to create a list of words within a word it is incredibly slow. For example, when I try to find all the words within 'colours' nltk takes 0.08 seconds and enchant takes 2.5 seconds. This time difference increases as the number of letters increases, so enchant is not viable.Any ideas?Steve"
884,How to filter a model with respect to text and then use most_similar?,"['python', 'model', 'nltk', 'word2vec']","I have the text and I want to filter the model with respect to the text. It is OK?If it is OK how to filter in the results (model_filter ) of the most similar function (modelo_filtrado.most_similar_cosmul????), those that belong to the text?
Thx."
885,Line Extract from text file where a character range(xtoy) exists,"['python-3.x', 'nltk']","Suppose I have a text file as follows:
line1:  ""Hello, i am fine. How are you.
line 2:  I am good too. What would you
line3:   like to have"".Lets say 'too' exists in character range of 20-22. I want to extract the line in which too this character range(20-22, already known to me) exists also the next sentence until sentence completion.The output should be;""I am good too. What would you like to have."" given the character range of [20-22] too we already know. "
886,"Compare List of Sentences and List of words and return complete Sentences , if word is present","['python', 'list', 'nlp', 'nltk', 'list-comprehension']","From the List of Sentences and List of words, how to return the list of complete Sentences , if a word is present.  Please suggest.   Below are example lists.'people' is present in two sentences, 'Covid-19' present in one sentences and 'Lockdown' in one sentence.
output list should have these four matching complete sentences from listsent. "
887,Compare two bigrams lists and return the matching bigram,"['python', 'list', 'nlp', 'nltk', 'list-comprehension']","Please suggest how to compare 2 bigrams lists and return the matching bigram only. 
From the below example lists, how to return the matching bigrams ['two', 'three']."
888,How can I get tagged entity in xml format in NLTK,"['python', 'xml', 'nltk', 'stanford-nlp', 'ner']",I put code below of NER taggerThis give me output as :Is there any possible way to get the output in XML format e.g.
889,how to use nltk to unify alternative spellings?,"['python-3.x', 'nltk']","Is it possible to use ntlk to unify alternative spelling in English please? So that ""colour"" will be turned in ""color""? It is useful because I have both spell variants in my data and it appears that GoogleNews-vectors only contains ""gray"" but not ""grey"".If I used glove.6B, both variants are found, but they their embeddings are not equal.I have tried in WordNetLemmatizer but it didn't work:Output:"
890,NLTK Tokenizer with Twitter API,"['python', 'nltk', 'twitterapi-python']","I'm trying to figure out the frequency distribution in a series of tweets, but the frequency distribution is counting each tweet uniquely rather than the entirety of tweets. How can I fix this?"
891,while using keras.preprocessing.text.text_to_word_sequence() I am getting an error suggesting there is no translate attribute,"['python-3.x', 'keras', 'nltk', 'tokenize']","I am trying to pass a list called articles having a list of sentences to text_to_word_sequence() from keras preprocessing text
The idea is to have an array of numbers or vectors generated from the sentences."
892,How to find relation between 2 different set of corpus data,"['python', 'nlp', 'nltk', 'data-visualization', 'data-science']","I'm currently  working on two different sets of text data. Can you suggest any methods to verify on how to extract the matching information in between two corpus.Also, do you have any suggestions how to plot the similar information received from above question?"
893,How to train TextBlob MaxEntClassifier more quickly?,"['python-3.x', 'nltk', 'sentiment-analysis', 'textblob']","I'm using Textblob MaxEntClassifier for sentiment analysis, just the positive and negative classes. On a fairly small training set, 3,800 short Tweets, the training takes way too long, approximately an hour and 15 mins. TextBlob goes  through ""Training (100 iterations)"" even when the Log Likelihood and Accuracy stopped changing after the 32 iteration. (On a side note, I have 12gb of RAM and 3.4ghz processor with not much else running in the background). I see that the Textblob MaxEntClassifier is a wrapper around the NLTK. I found in the Textblob docs that mentioned ""train(*args, **kwargs) - Train the classifier with a labeled feature set and return the classifier. Takes the same arguments as the wrapped NLTK class...""Since the TextBlob MaxEntClassifier train takes the same args as the wrapped NLTK class, I reviewed the NLTK docs for a param that might shorten the training time. In the NLTK.classify docs under the MaxEnt Classifier I found the param ""min_lldelta=v: Terminate if a single iteration improves log likelihood by less than v."". I was hopeful that I could pass this param through TextBlob to the NLTK classifier to shorten the training time, however it seems to be ignored. Is it possible to pass max_iter, min_ll, or min_lldelta through Textblob MaxEntClassifier to the NLTK to shorten the training time? If not, are there other ways to shorten this training time? My snippet is below:"
894,NLTK issue: name 'save_file' is not defined,"['python', 'nltk', 'nltk-trainer']","How can I solve this issue? NameError: name 'save_file' is not defineddemo_sent_subjectivity(text) is a method inside nltk.sentiment.utils which then calls another method demo_subjectivity(trainer, save_analyzer=False, n_instances=None, output=None)These methods are below:save_file method is called inside demo_subjectivity method, but i did not understand, where is source code for it. I noticed, that save_file method exists in SentimentAnalyzer class, but why here it was called just like save_file, not sentim_analyzer.save_file?"
895,Tokenizing based on certain pattern with Python,"['python', 'nltk', 're']","I have to tokenize certain patterns from sentences that have Sentences like abc ABC - - 12 V and ab abc 1,2W. Here both 12 V and 1,2W are values with units. So I want to tokenize as abc,ABC and 12 V. For the other case:  ab , abc , 1,2W. 
How can I do that ? 
Well nltk word_tokenizer is an option but I can not insert any pattern, or can I ?
word_tokenize(test_word)"
896,How to tokenize in numpy.ndarray?,"['python', 'numpy', 'multidimensional-array', 'nltk', 'tokenize']","I have the following ndarray: X_train: [[<'title'>, <'description'>]]So, now I need to tokenize title and description.
I've written the following function:The question is: how do I process data with ndarrays faster and more efficient?
I do not want to use nested loops. Supposedly there is a way to do it fast with numpy.I've tried:However, it hasn't changed and I get:But I want this:Would be grateful for any possible help."
897,Convert a NLTK tree into a array of dictionaries,"['dictionary', 'tree', 'nltk']","I would like to convert the following nltk Tree into a array of dictionaries:I have created next code to get it:But for any reason only de second part is shown, which is the reason I don't get next output?"
898,extract name entities and its corresponding numerical values from sentence,"['python', 'nlp', 'nltk', 'spacy', 'ner']","I want to extract information from sentences. Currently, I am able to do the following using spacy.  However, I am trying to do the following.Is there any way that I can perform the task using the NLP method through python library such as Spacy? 
The pattern of the sentence is not fixed. Using regular expressions is not working.Thanks"
899,Is it possible to do Integration of machine learning algorithm in python on the web,"['javascript', 'python', 'scikit-learn', 'nltk']","I've designed a machine learning algorithm for the chatbot in python which uses different libraries sklearn, NLTK etc. This algorithm takes an input from the user and replies based on the query user has entered. "
900,Entity recognition based on context,"['python', 'nltk', 'spacy', 'rasa-nlu', 'ner']","Is there a way to get the probability of a word belonging to an entity based on context of the sentence.
For example (entity : server_name)""I want to check mogo server""
And the result would be for example:Mogo : server_name , 0.5688999""Check status of server mogo""
And the result would be for example :Mogo : server_name , 0.6272772One solution would be for example to map all server names in a txt file or entity reco to do so...but in this case the names of servers can be quite a big number so each time we want a probability of a word being indeed a server name ,and so check the database/json for the server name ( if it exist we continue) if not we ask the user based on that probability if (mogo)is indeed a server name 
Like that we add it to the database 
So any way to use nltk,spacy or rasa to extract a word as an entity based on the sentence (not on words .txt or labels) and get the probability of the word with highest potential of belonging to a named entity?"
901,Python TweetTokenizer TypeError: expected string or bytes-like object,"['python', 'nltk', 'typeerror', 'tokenize', 'tweets']","I am trying to TweetTokenize column tweet from my dataset.
Here is my code:I am getting this error"
902,split string into sentences keeping new line separator,"['python', 'string', 'nltk']","let's say I have a string like:I am currently separating this string into sentences by . using I need to do some subtitutions in each sentence, something like this:But if after separating them I want to join them again, and keep the line separators \n, the output is not correctThis should be the correct output:Any ideas?"
903,"How to extract particular text from the string, using spacy and nltk?","['python-3.x', 'nltk', 'spacy', 'text-extraction']","I have search this question but none of the questions and solution fit my scenario.
This is the from which I want to extract twelve (12) monthtext = 'Thereafter,this Agreement will automatically renew for successive twelve (12) month periods (each, a “Renewal Term”).'And for this I have written this patternand it just returns (12) month.
 how to extract twelve (12) month"
904,Why does importing NLTK in python gives error,"['nlp', 'nltk', 'token']","I upgraded NLTK to latest version and while import nltk, I get the following errorFile ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tag\sequential.py"", line 210
    print(""[Trained Unigram tagger:"", end="" "")
                                         ^
SyntaxError: invalid syntaxI commented this line but then another error. python version used is 2.7"
905,Count frequency of multi-word terms in large texts with Python,"['python', 'nlp', 'nltk', 'corpus', 'word-frequency']","I have a dictionary with close to a million multi-word terms (terms containing spaces). This looks something likeI would like to count their frequency in many gigabytes of texts. As a small example consider counting these four multi-word expressions in a Wikipedia page:This gives something like {'multilayer ceramic': 7, 'multilayer ceramic capacitor': 2, 'multilayer optical disk': 0, 'multilayer perceptron': 0} but it seems sub-optimal if the dictionary contains a million entries.I've tried with very large regular expressions:This is dead slow (6 minutes for 1000 lines of text on a recent i7).
But if I use regex instead of re by replacing the first two lines, it goes down to around 12s per 1000 lines of text, which is still very slow for my needs:Note that this does not do exactly what I want as one term may be a subterm of another as in the example 'multilayer ceramic' and 'multilayer ceramic capacitor' (which also excludes approaches of first tokenizing as in Find multi-word terms in a tokenized text in Python).This looks like a common problem of sequence matching, in text corpora or also in genetic strings, that must have well-known solutions. Maybe it can be solved with some trie of words (I don't mind the initial compilation of the term list to be slow)? Alas, I don't seem to be looking for the right terms. Maybe someone can point me in the right direction?"
906,Change some words without touching HTML tags and get HTML output,"['python', 'python-3.x', 'beautifulsoup', 'ckeditor', 'nltk']","I have two CKEditor fields, one for input and one for output (second is disabled for editing).
What I do is using nltk to replace some words in text from input field, and I need same text with replaced words in output filed (so with all HTML like in input).All I managed to do is to strip all HTML tags but that leaves me unformatted text.How I can do this? Can someone point me to the right direction?UPDATEIf it's any help, I'm using BeautifulSoup to strip tags, maybe through BeautifulSoup is possible to do something."
907,How to use a text to calculate target from its similarity in a word2vec model?,"['model', 'nltk', 'word2vec', 'target', 'sentence-similarity']","I´m a newbie to nltk and I have a question.
I have a text in a var and it's preprocessed and clean.text = ""since the history of writing predates the concept of the text most texts were not written with this concept in mind most written works fall within a narrow range of the types described by text theory...""I have a loaded model.Now I have to use the text with the model to search for a term and its similarity.But I don't know how to pass the text to the model and then search for n target candidates. 
Thank you very much."
908,Why does nltk word counting differs from word counting using a Regex?,"['python', 'nlp', 'nltk']","We have two 'versions' of the same text from a txt file (https://www.gutenberg.org/files/2701/old/moby10b.txt):What I am missing is why nltk_text.vocab()['some_word']) returns a smaller count than len(re.findall(r'\b(some_word)\b', raw_text))). "
909,Extended specific stopwords doesn't affecting the result in topic modeling,"['python', 'text', 'nlp', 'nltk', 'stop-words']","I am using nltk.corpus library for stopwords on survey data, extending the stopwords with some specific words for better topic modeling result. But in the result all the stopwords which are extended are there in the topic modeling result.How to improve the result by removing the stopwords?"
910,How to get phonemes for abbreviations with Python?,"['python', 'machine-learning', 'nlp', 'nltk', 'phoneme']","Does anybody know how to get correct phoneme for abbreviations? I use g2p module and I found that their predict function quoted below returns wrong predictions for abbreviations.For example g2p(""http"") returns ['T', 'AE1', 'P', 'T', 'IY1']"
911,I can't get my test accuracy to increase in a sentiment analysis,"['python', 'machine-learning', 'nltk', 'svm', 'text-classification']","I'm not sure if this is the right place but my test accuracy is always at about .40 while I can get my training set accuracy to 1.0. I'm trying to do a sentiment analysis of tweets on trump, I have annotated each tweet with a positive,negative or neutral polarity. I want to be able to predict the polarity of new data based on my model. I've tried different models but the SVM seems to give me the highest test accuracy. I'm unsure as to why my data model accuracy is so low but would appreciate any help or direction.This is an example of one of the strings in the text column of the dataset"".@repbilljohnson congress must step up and overturn president trumpâ€™s discriminatory #eo banning #immigrants & #refugees #oxfam4refugees""Data set"
912,what is the meaning of heads in spacy training data?,"['python', 'json', 'nlp', 'nltk', 'spacy']","I'm trying to train a model on my own data and I'm using Spacy library. But I'm confused about ""# index of token head"" in a code example.what exactly heads mean here? "
913,TypeError: lemmatize() missing 1 required positional argument: 'word,"['machine-learning', 'nlp', 'nltk']",I have an array of for each row in a csv file as followed:When I try to pass this on to the lemmatizer like this:I get the following error:Why is that?As a side question: Do I need to do this before passing this for Vectorization? I am building an machine learning model and saw the function CountVectorizer in sci kit learn but could not find any information that it does lemmatization and so on beforehand as well.
914,Why User site-packages are not visible in virtualenv?,"['python', 'nltk', 'virtualenv']","While installing the nltk module, one of these prerequisites, using the commande pip install --user -U nltk
in a venv called dl4cv, I got the error : (dl4cv) hdafa@hdafa-HP-ProBook-450-G5:~$ pip install --user -U nltk
ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.

Any idea to make thse tools visible in the venv, so I can install nltk in dl4cv ?"
915,Cannot import nltk in Jupyter even though I can import it in Python Console,"['python', 'pip', 'installation', 'nltk', 'python-import']","I installed nltk and seemed to successfully install. But when I try to import it, it says no module named nltk. However, when I try to reinstall it, it says requirement already satisfied. How could that be the case? What can I do to fix this? Thanks!Additionally if I try to import nltk using python shell (under the same virtual environment that I am running the jupyter notebook from), I get the following: "
916,How to find ngrams by a grouped column in Python,"['python', 'dataframe', 'nltk', 'n-gram']","I've been able to figure out ngrams from a single list before, but now I want to group it by a column in a dataframe.When I try to take my original snippet, for the list, and apply it to this dataframe I keep on getting a typeerror for ""expected string or bytes-like object"". I tried googling, and I wasn't able to find a solution for this.My dataframe is just set up as two columns, with about 50,000 rows.My ideal result from this dataframe would be showing the ngrams (between two and four) and the frequency of them. So it should be something like this. For 2-grams:Is this possible? Here is what I have so far:"
917,Count occurrences of list of strings in text,"['python', 'nltk']","I want to count occurrences of list elements in text with Python. I know that I can use .count() but I have read that this can effect on performance. Also, element in list can have more than 1 word.I can do this:This way works, but what If my list has 500 elements and my string is 3000 words, so in that case, I have very low performance.Is there a way to do this but with good / fast performance?"
918,Configure New NLTK Path,"['python', 'append', 'nltk']","I am working with nltk in python. I imported the package and downloaded the additional data just fine, but I want to be able to append a new directory to store nltk_data. When I tried this fix found at this link (How to config nltk data directory from code?) I received this error:What am I doing wrong?"
919,Sentiment Analysis does not display correct results,"['python', 'python-3.x', 'nltk', 'sentiment-analysis']","Above is defining polarityAbove is reading line by line the txt file as well as printing the sentence, sentiment and polarityUnfortunately, when running the file, it shows .5 polarity for every single sentence. I am unsure how to fix it. "
920,Split the sentence into its tokens as a character annotation Python,"['python', 'python-3.x', 'nltk', 'tokenize', 're']","After a long search I didn't find any answer to my question that is why I decided to put my question here. I am trying to achive some specific result with RE and NLTK. 
Given a sentence, on each character I have to use the BIS format, that is, tagging each character as B (beginning of the token), I (intermediate or end position of the token), S for space. 
For instance, given the sentence: The pen is on the table.The system will have to provide the following output:which can be read as: My result is kinda close but instead of: I get:Meaning I get space between table and dot .
The output should be: Mine is : My code so far:"
921,How to compute cosine similarity between 2 different CORPUSES?,"['python', 'nlp', 'nltk', 'spacy', 'gensim']","I'm trying to estimate the cosine similarity between each document i in a Corpus A and all documents in a Corpus B.Any idea how I can do this efficiently? I'm working with pretty large datasets.Essentially, I want to get the document(s) in Corpus B which is (are) most similar for each document within Corpus A."
922,Printing only the first 25 concordances in python,"['python-3.x', 'nlp', 'nltk', 'text-processing', 'corpus']",I am working on a task from my university and I have no idea how to print only the first 25 concordances of a word instead of the whole  list.
923,Fuzzy match columns and fetch value of another column in Python,"['python', 'nltk']","I have below requirement in PythonFile  A - Col1, Col2File B - Col1, Col2, Col3Join File A and File B based on Col 1 and Col2 (fuzzy match) and then fetch value of Col3 (from File B) and place in File ALet me know how to achieve this fuzzy join in python code?"
924,I need to create a list only with the repeated items from another list [duplicate],"['python', 'string', 'list', 'nltk', 'operation']","I have a list with repeated items like: You can see that there are two repeated items, and I need created a list with only this elements like that: Order is important. I know how delete repeated items but I don't know how to do the opposite. "
925,How to calculate the occurrence of specific sentence in a text?,"['python', 'pandas', 'nlp', 'nltk']","How can I use below code to calculate how often the bigram is appearing in the example_txt?
Right now I think I retturn whether 'order' appears in the total variable. I would like to calculate the percentage of bigram in total.So given that we do bigram of total, this is the result:
[('order', 'intake'), ('intake', 'is'), ('is', 'strong'), ('strong', 'for'), ('for', 'q4')]
meaning, the output of my code should be 0.20, as 'order intake' is there 1/5.Edit: new title"
926,How can I look for specific bigrams in text example - python?,"['python', 'pandas', 'nlp', 'nltk']","I am interested in finding how often (in percentage) a set of words, as in n_grams appears in a sentence. Below you see two examples. The first one works, the last doesn't.How can I make the find_ngrams() function process bigrams, so the last example from above works?Edit: Any other ideas?"
927,remove punctuation and stop words from all “.txt” and “.docx ” files in a zip folder,"['python-3.x', 'string', 'nltk', 'glob', 'file-handling']","Actually I have Eight "".txt"" files and two "".docx"" files in file_list. My task is to eliminate all stop words and punctuations from all files in file list.I just provided some part of code where I am facing error. can someone please guide me how to eliminate stop words and punctuations from my all file types. I tried the following code but am unable to execute it succesfully. I am also providing folder here.file_list. Also when I am running the following code I am facing UTF-8 error for .docx files."
928,TypeError: cannot use a string pattern on a bytes-like object ( Spam detection using Pyhton),"['python', 'machine-learning', 'nltk', 'spyder', 'kaggle']",I am facing this  while executing the code in Kaggle notebookTypeError: cannot use a string pattern on a bytes-like object.The same code is executed properly in Spyder notebook.Message outError details: label  message
929,Improve performance of large document text tokenization through Python + RegEx,"['regex', 'python-3.x', 'nltk', 'gensim']","I'm currently trying to process a large amount of very big (>10k words) text files. In my data pipeline, I identified the gensim tokenize function as my bottleneck, the relevant part is provided in my MWE below:Calling the preprocessing function for the given example takes roughly 66ms and I would like to improve this number. Is there anything I can still optimize in my code? Or is my hardware (Mid 2010s Consumer Notebook) the issue? I would be interested in the runtimes from people with some more recent hardware as well.Thank you in advance"
930,Searching for words that are composed entirely from a list of root words,"['python', 'nltk']","I'm new to coding in general and thought now would be a good time to teach myself some Python.What I'm currently trying to achieve is to determine a list of words that are entirely made up from a variable set of root words, syllables, prefixes and suffixes that I will choose.This is where I'm up to so far;With the help of the NLTK install with the Words corpus downloaded, I'm able to search through a list of English words and output a list of the words that are composed of individual letters in my syllables list (shown above) e.g cat, tact.However this does not output words that are composed of the strings that are longer than one letter e.g consistent, considerate. Could someone please explain why these multi-letter strings aren't being used to find the words that they could spell out?
Also any advice on how to resolve this would be much appreciated.Thanks,
Marcus"
931,How to remove items from a list while iterating?,"['python', 'iteration']","I'm iterating over a list of tuples in Python, and am attempting to remove them if they meet certain criteria. What should I use in place of code_to_remove_tup? I can't figure out how to remove the item in this fashion."
932,How to apply Word Net Lemmatizer on pyspark data frame?,"['pyspark', 'nltk', 'lemmatization']","I'm trying to apply WordNet Lemmatization on one of my Data Frame columns.My dataframe looks like this:So each row is the list of tokens. Now I want to lemmatize each token. I've tried with:I did not get any error message, but my dataframe did not change.Is there any error in my code? How should I apply lemmatizer? "
933,Why nltk word_tokenize is not working even after doing a nltk.download and all the packages are installed correctly?,"['python', 'nltk', 'python-3.7']","I am using python 3.7 64 bit. nltk version 3.4.5.When I try to convert text6 in nltk.book to tokens using word_tokenize, I am getting error.code is done in idle 3.7Below is the error when I execute the last statement.Please help. Thanks in advance.While doing some troubleshooting I have created a sample nltk.text.Text object and tried to tokenize it with nltk.word_tokenize. Still I am getting the same error. Please see the below screenshot.But while calling the nltk.word_tokenize() on string, its working."
934,Wordnet: Getting derivationally_related_forms of a word,"['python', 'nltk', 'wordnet']","I am working on an IR project, I need an alternative to both stemming (which returns unreal words) and lemmatization (which may not change the word at all)So I looked for a way to get forms of a word.This python script gives me derivationally_related_forms of a word (e.g. ""retrieving""), using NLTK and Wordnet:The output is:But what I really want is only : 'retrieval' , 'retriever' , not 'think' or 'recovery'...etcand the result is also missing other forms, such as: 'retrieve'I know that the problem is that ""synsets"" include words different from my input word, so I get unrelated derivated formsIs there a way to get the result I am expecting?"
935,How to display the value that in the same row which matched with the input?,"['python', 'pandas', 'nlp', 'nltk']","The data has 2 columns as title and genre. So I am trying to give the title value of the row which matched by genre with user input.Here what i try:But the result is: Series([], Name: title, dtype: object)How can i solve it ?Edit: 
Data samples for titleAnd for genres:"
936,Remove sentences in text file using spacy,"['python', 'nlp', 'nltk', 'spacy']","I have a sales statement sheet in form of text file. Text FileI have to extract the comma separated values from the text file. I have extracted it by removing all the unwanted texts from the file using the following code.My question is, can I implement the same using spacy? I think it can be done through token matcher, but I couldn't find sentence removal in spacy... Please assist"
937,How to check if input in the list,"['python', 'pandas', 'nltk']","I am trying to build a suggestion tool based on a movie dataset. More specifically it will suggest a movie by title based on genre keyword.But I couldn't pass the loop/check part of the script, here is what I have tried:The output of the tokenized_genre like this:The output of the loop:I guess the mistake in the list of the tokenized words but I couldn't solve it."
938,NLTK stopwords on Google App Engine Standard,"['google-app-engine', 'nltk']","I'm trying to use NLTK in my Flask app on Google App Engine standard. But I'm unable to find a neat way to download / load NLTK stopwords on GAE standard.I saw this solution for Django (How to download all nltk data in google cloud app engine?) which suggests downloading data, hosting it with all the other files on GAE, and linking nltk.data.path to it. However, that seems quite hacky and I'd also like to keep my total GAE directory size low.I have tried to replicate this situation in GAE Flexible. There I'd just add ""RUN python -m nltk.downloader all -d /usr/local/nltk_data"" to my Dockerfile.Are there any good solutions for GAE Standard?"
939,Unhashable When Comparing Lists,"['python-3.x', 'nltk']","I am not familiar with the unhashable error that I am receiving here. I have the following dataframe 'dfd' that I am isolating role descriptions on. From there, I split each word within the role descriptions out and consolidate the entire list together into a single list. From this list, I try and compare this to a list of stop words that will filter out the clutter.This code fails at the if statement:
    if w not in stop_words:
TypeError: unhashable type: 'list'Could someone explain what the issue is? I feel like this should be straightforward. "
940,Nltk Net - using of unwrapped functions of NLTK in C#,"['c#', '.net', 'nltk', 'wrapper', 'ironpython']","I'm currently using NltkNet - a wrapper for the python project NLTK in VisualStudio .Net.
One can find that project right here: https://github.com/nrcpp/NltkNetThis wrapper is not complete and now, I need to use the Chunking function of Nltk, which
is not part of the wrapper library. I want to try this example (https://www.nltk.org/book/ch07.html under 2.1):The GitHub Project shows some example of using unwrapped code under What if there is no required NLTK feature in wrapper?but... uff, I'm not really skilled in python and that is to far away from what I need, to transcode it :)Maybe someone of you can give me a leed in the right direction. Would be very nice.Thank you and regards,
Jan"
941,What am I missing when getting nouns from sentence and reversed sentence using nltk?,"['python', 'nltk']",I Have a is_noun definition using nltk:then I have this in a function:then I call the function:and get:then call same function but with reversed sentence and get:results:I am expecting to get same amount of nouns but that is not the case. What am I doing wrong?
942,How do we create hyperlinked strings in Python? Is there a way?,"['python', 'pandas', 'hyperlink', 'nltk', 'sklearn-pandas']",I am building a chatbot using nltk.util pairs. I am using regular expressions for the combinations! I want one of the responses to be “ Visit Google” where “Google” should be a hyperlink that should take you to https://www.google.com!
943,"In python, how to be sure that the given 2 or more location names belong to same location or different locations","['python', 'nlp', 'nltk']","I have been given 2 DataFrames with geographical structure as Dataframe 1:-Dataframe2:-Target: - Extract all the unique barangay values based on the similar 'province' and 'municipality' values.Now by using Fuzzy logic, based on the values of 'province' and 'municipality' I am able to map in the following manner(I have taken only the value which has the highest fuzzy ratio and that too more than 90):- Final output:-Expected output:-i.e., I want all the barangay values like 'brgy bell', 'brgy. bell' and 'bell' to be considered as same barangay name, 'city of cauayan' to be treated similar as 'cauayan city' (1 white space) and 'cauayan  city' (2 white spaces) and 'magarao' should be similar to 'magarao' and 'magarao city'."
944,Why is inline FreqDist failing?,"['python', 'python-3.x', 'nltk', 'frequency-distribution']","I wrote a simple function.This is stuck.Also, [w for w in set(text5) if len(w) > 7 and FreqDist(text5)[w] > 7 fails. It just get stuck.However, this works:Does it not work like that in python? Why is that? Also, why is it stuck, if this is wrong, it should come out as an error, syntax or runtime.This works, flawlessly and fast:"
945,How to apply nltk.pos_tag on pyspark dataframe,"['pyspark', 'nltk', 'pos']","I'm trying to apply pos tagging on one of my tokenized column called ""removed"" in pyspark dataframe.I'm trying withBut all I get is Value Error: ValueError: Cannot apply 'in' operator against a column: please use 'contains' in a string column or 'array_contains' function for an array column.How can I make it? "
946,Define and Use new smoothing method in nltk language models,"['nlp', 'nltk', 'smoothing', 'language-model']","I'm trying to provide and test new smoothing method for language models. I'm using nltk tools and don't want to redefine everything from scratch. So is there any way to define and use my own smoothing method in nltk models?Edit:
I'm trying to do something like this :"
947,How to find what strings (in a big list of strings) are in a text in Python?,"['python-3.x', 'nltk', 'n-gram']","I'm trying to find out what names of list are in a news text.I have a big text file (around 100MB) with many place names. Each name is a line in the file.Part of the file.and the news texts are like this:For instance, in this text the strings Australia and Queensland should be founded.
I'm using the NLTK library and creating ngrams from the news.To do this, I'm doing this:This is too slow. How can I improve it?"
948,"NLTK - Stopwords, hashing over list","['python', 'list', 'nltk']","I'll try to make this as easy to understand as possible as I can imagine how infuriating long and drawn-out problems can become. I have a list of tweets, all stored within a variable called 'all_tweets'. (This is because some tweets fell into the 'text' category, while others fell into 'extended_tweet', so I had to merge them together.I tokenised the tweets, and it all worked perfectly. I got a list of each tweet and each word within a tweet all seperated.I am trying to now implement stopwords into the code so I can filter out, you guessed it, any stopwords. My code is as follows:I get the following error:I'm well aware I cannot hash over a list. I looked at my tweets and each set of words is all within their own list. I'm very well aware of whats going on but is there any workaround to this issue?Any help would be appreciated thanks."
949,Automatic Generation of question in python,"['python', 'nlp', 'nltk']","im currently working on a simple chatbot in python. The goal of this chatbot is to discriminate some products of a product list in order replace the search bar on some website.
So im currently working on the automatic generation of question to ask to the user, using a keyword.
Do you have any algorithm in mind or some keyword in order to help me in my research ?
Thank you !Ps: an example of use.
QuestionGeneration(""colour"") -> ""What color is the product you are looking for ?"""
950,"Resource punkt not found. But, it is downloaded and installed","['python', 'python-3.x', 'dataframe', 'nltk', 'tokenize']",I have the following columns in a dataframe.I am trying to run this small piece of code.The last line of code throws an error.  I'm getting this error message.I'm pretty new to all the tokenization stuff.The sample code is from this site.https://github.com/AustinKrause/Mod_5_Text_Summarizer/blob/master/Notebooks/Text_Cleaning_and_KMeans.ipynb
951,How to do lemmatization using NLTK or pywsd,"['python', 'nltk', 'sentiment-analysis', 'lemmatization', 'part-of-speech']","I know that my explaination is rather long but I found it necessary. Hopefully someone is patient and a helpful soul :)
I'm doing a sentiment analysis project atm and I'm stuck i the pre-process part. I did the import of the csv file, made it into a dataframe, transformed the variables/columns into the right data types. Then I did the tokenization like this, where i choose the variable I wanted to tokenize (tweet content) in the dataframe (df_tweet1):The output is a list of list with words (tokens).Then I perform stop word removal:The output is the same but without stop wordsThe next two steps are confusing to me (part-of-speech tagging and lemmatization). I tried two things:1) Convert the previous output into a list of strings since I thought that would enable me to use this code to do both steps in one:I got the this error: 
TypeError: expected string or bytes-like object2) Perform POS and lemmatizaion seperately. First POS using clean_sents as input:The output is a list of lists with words with a tag attached
Then I want to lemmatize this output, but how? I tried two modules, but both gave me error:The errors were respectively:TypeError: expected string or bytes-like objectAttributeError: 'tuple' object has no attribute 'endswith'"
952,Trying to pass data between Python(using nltk and flask) and a html template,"['python', 'flask', 'nltk']","I have a Python code which is using NLTK and Flask for creating a chatbot that works on a local server.
After I run or execute the code, the html page opens on the local server and I give a input, but the input doesn't seem to pass to my python code. A prompt appears on my python console where the chatbot takes the input and runs.I have tinkered a lot with the code, running different forms of it about 30-40times, debugging it and doing a lot of trial and error.
The below code is the only one that seems to run without any error, but the output displayed by the bot on the Html page is ""none"".Any help or advice is appreciated. I'm new to Flask and NLTK. Thank you.This is my Python codeThe html template used is - new.html, the following :"
953,"import nltk eror, No module named '_sqlite3'","['sqlite', 'nltk']","I'm in ubuntu, python 3.7.6here what i do:I try to import nltk library.line (1): import nltkcan some one help me with this?Thanks for your time !Edit (26.03.2020)sqlite3 -version:"
954,How to split a string into a list?,"['python', 'list', 'split', 'text-segmentation']","I want my Python function to split a sentence (input) and store each word in a list. My current code splits the sentence, but does not store the words as a list. How do I do that?"
955,nltk.download() urlopen() error [Errno -2] Name or service not known,"['python', 'nlp', 'nltk']","nltk.download() givesException in Tkinter callbackTraceback (most recent call last):File ""/usr/lib/python3.6/tkinter/init.py"", line 1705, in callreturn self.func(*args)File ""/usr/lib/python3.6/tkinter/init.py"", line 749, in callitfunc(*args)
File ""/home/mannya/.local/lib/python3.6/site-packages/nltk/downloader.py"", line 2165, in _monitor_message_queueself._select(msg.package.id)
AttributeError: 'str' object has no attribute 'id'"
956,NLTK corpora : IndexError: list index out of range,"['python', 'nlp', 'nltk', 'corpus']",Here when I run this particular codeI get 
957,what should i do to remove the error of importing nltk as i have already installed it?,"['python', 'django', 'pycharm', 'nltk']","I have installed nltk and now it's not working, and I need assistance figuring out what's wrong. I installed via pipIt is showing these error.Please use the NLTK Downloader to obtain the resourceHow can I solve this?"
958,Text Preprocessing Python,"['python', 'nltk', 'text-processing']","I have a text input='The quick brown fox. Jumped over the lazy dog.'
And I want the out to be as below:[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]Please let me know how to do this.I just split the sentence into words but not sure what to do next?"
959,Looping through tree to create a dictionary_NLTK,"['python', 'nltk', 'grammar']","I'm new to Python and trying to solve a problem looping through a tree in NLTK. I'm stuck on the final output, it is not entirely correct.
I'm looking to create a dictionary with 2 variables and if there is no quantity then add value 1.
This is the desired final output:{ quantity =1, food =pizza }, {quantity =1, food = coke }
,{quantity =2, food = beers}, {quantity =1, food = sandwich }Here is my code, any help is much appreaciated!''''''"
960,DataFrame - remove rows that contains stop words or numbers in the 'word' column,"['python-3.x', 'pandas', 'dataframe', 'nltk']","I have dataframe with two columns(words, and count how those words), ordered by count:What I want, is to remove the stop words, and the numbers (like '347' and '348'). e.g. in the example, remove rows 0, 2, 6051, 6053 ('the', 'who', '347', '348').This is how I created the DataFrame:I also got the stop words:But how can I remove those stop words (and preferably the numbers) from the DataFrame?I saw in this blog post that they managed to remove the stop words from trump's tweets dataset, but I didn't manage to make his code to work on my dataset. This is his code:"
961,nltk extract nounphrase with RegexpParser,"['parsing', 'nltk']","I want to extract nounphrases from text and i use python with NLTK.
There is a pattern I found in internet of using RegexpParser as follows:I want to modify the grammar variable to add the case 'Noun of Noun' or 'Noun in Noun' (""cup of coffee"" or ""water in cup"" for example)
My test string is : 'postal code is new method of delivery'
I want to receive list of phrases : ['portal code', 'new method','new method of delivery']"
962,Spacy - NLTK: Language detection,"['nltk', 'spacy']","I am currently working on a project dealing with a bunch of social media posts.
Some of these posts are in English and some in Spanish. My current code runs quite smoothly. However, I am asking myself does Spacy/NLTK automatically detect which language stemmer/stopwords/etc.  it has to use for each post (depending on whether it is an English or Spanish post)? At the moment, I am just parsing each post to a stemmer without explicitly specifying the language.  This is a snippet of my current script:"
963,Intersect bigram trigram in python,"['python', 'nlp', 'nltk']","I have a bigram and I have a trigram, how can I match a bigram that is in the trigram in python?I have tried examples but I do not understand because I'm newbie, thank you very much.Result: [('red', 'car'),('new', 'york')]"
964,Nltk lemmatizers do not recognize the plural form of chemical names,"['python', 'nlp', 'nltk', 'lemmatization']","So, I must admit, I'm a total noob in nlp, and I have no idea whatsoever about nltk, I'm just trying to use a legacy code left by the previous developer. I need to lemmatize words, mostly from chemical and biotech publications. I generally use WordNetLemmatizer. Most of the time it works.returns cat.But then I tryit returns 'dehydrogenases'. I want it to return 'dehydrogenase'. How can I do that?"
965,How to recognize composite verbs using Python nltk,"['python', 'python-3.x', 'twitter', 'nltk']","For my university project, that includes a sentiment analysis of a given Twitter dataset, I'm trying to recognize all of the composite verbs (such as don't like, give up, write out etc.) With Python nltk during the lemmatization process.
All of my efforts are not paying off until now.
Can someone show me an example if that is possible?"
966,How to calculate a distance in python on 2 words,"['python', 'database', 'nltk']","I have to extract a database and put it in a script in python.  I did it for all name of database.
Now I have to calculate the difference of letter between the name 1 and the name 2   and between the name 1 and the name 3 .... and between the name  2 with the name 3 .....I did it for the moment.
I show you what a little part about what  I did who gives me a problem :And the result is hereMy problems are why I have only 1  at the end of the comparaison between the 2 words and why I have a "","" at the end of the words,  It should be different and I don't unterstand why I got this.  I did the same code of a different database with tittle of books and it worked.  Thanks you very much if you can help me !"
967,How do cluster data below using NLP?,"['nlp', 'nltk']","My Data looks like this. I would like to clean, vectorize and cluster using NLP. How should I approach the problem? I am pretty new to NLP"
968,"regex pattern include alpha, special, numeric","['python', 'regex', 'nltk', 'special-characters', 'numeric']","Following are my sentences:
  ex: this is first: example    234 -this is second (example)   345 1this is my third example    (456) 3expected output:  I tired using python, nltk word token and sentence token, split(), and str1 = re.compile('([\w: ]+)|([0-9])')
str1.findall('my above examples')
please suggest me a module which can provide my expected output or let me know where is my mistake in regex"
969,UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 11: ordinal not in range(128),"['python', 'unicode', 'nlp', 'nltk', 'codec']","Using the nltk library, I need to see which is the longest and which is the shortest phrases of a corpus. I think the algorithm I used is functional:Although, the output is: I have no idea what it means, and how to solve it. I know that my input file is an UTF-8 plain text. any ideas? "
970,Natural language processing to compare 2 paragraphs,"['tensorflow', 'nlp', 'nltk']","Good morning, everybody, I would like to know the best approach to perform text similarity based ideas using NLP, I have checked spacy similarity but thing seems to do not work well, I want to develop it from scratch but I do not know the actual process. 
To goal is to compare 2 paragraph and see whether they are sharing the same idea "
971,TypeError: expected string or buffer in stopword list,"['python', 'string', 'nltk', 'stop-words']",I want to erase a few words that a stopword list doesn't have. I made stopwords_id.txt dictionary from the words that I want to erase. I have 74 pdf files to convert to txt and after that save it as txt.but I found error in this area and I don't know how to solve this one. Any idea why there is this TypeError? I think there is something wrong in removeStopwords.
972,Using guess_language to read 30000 tweets,"['python', 'pandas', 'nltk']","I am using guess_language to detect the language of the tweets for a school project. I used pandas to read the .csv file. I have around 30000 rows.However, my problem is that the guess language can only read one tweet at a time. guess_language(""Top story: â€˜Massive Mental Health Crisisâ€™ "")'en'I am very new at python and been trying to figure out the loop and if statements for this for almost a day now and they keep just returning one tweet.Thank you and apologies if the question is lame.I used the code suggested below by Kareem.from guess_language import guess_language resdf = nodupdf[ nodupdf['text'].apply(guess_language) == 'en' ]It worked for the small file (100 csv), but when I applied it on the bigger one. It gave me this error.TypeError Traceback (most recent call last) in 9 10 for chunk in noeng: ---> 11 chunk['text'].apply(guess_language)== 'en'~\Anaconda3\lib\site-packages\pandas\core\series.py in apply(self, func, convert_dtype, args, **kwds) 4040 else: 4041 values = self.astype(object).values -> 4042 mapped = lib.map_infer(values, f, convert=convert_dtype) 4043 4044 if len(mapped) and isinstance(mapped[0], Series):pandas_libs\lib.pyx in pandas._libs.lib.map_infer()~\Anaconda3\lib\site-packages\guess_language__init__.py in guess_language(text, hints) 322 """"""Return the ISO 639-1 language code. 323 """""" --> 324 words = WORD_RE.findall(text[:MAX_LENGTH].replace(""’"", ""'"")) 325 return identify(words, find_runs(words), hints) 326TypeError: 'float' object is not subscriptableThinking it was a memory error, I used chunk.noeng=pd.read_csv(r'C:\Users\jean\nodupdf.csv', chunksize=10) for chunk in noeng: chunk['text'].apply(guess_language)== 'en'I still got the same error."
973,WSL: ImportError: cannot import name 'textblob',"['python', 'nltk', 'textblob']","I have installed NLTK and TextBlob on Python3 on WSL. No matter what I do, I always obtain the same error. Indeed, trying to install shows that all packages are installedBut when I try to update corpora or use from textblob import TextBlob, this is the error message:I cannot understand where is the problem. Any help would be really appreciatedEdit: When I try to import any other module of NLTK library, same error happens. It's like if previous calling to 'tokenize' would have blocked the module or something"
974,best practice to start a sentiment analysis project?,"['python', 'nltk', 'sentiment-analysis']","After i did a lot of research about AI and sentiment analysis i found 2 ways to do text analysis.After the pre-processing for text is done we must create a classification in order to get the positive and negative, so my question is it better to have example:first way:second way:
100 records of text to train and make a vocabulary for bag of word in order to train and compare the tested records based on this bag of word.if i am mistaking in my question please tel me and correct my question."
975,RESULT How to use IDLE with anaconda3. import nltk error,"['nltk', 'python-import', 'python-idle', 'python-3.8', 'anaconda3']","I was facing the Problem of anaconda3, that my created env worked using Anaconda Promt (anaconda3) --> conda activate python38. 
But if I opened IDLE via ..\user\anaconda3\envs\python38\Scripts an simple import of import nltk failed. 
This was one of the errors. I was able to fix the sqlite3 dll error.UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-serviceProblem you are facing:
anaconda is not activated while opening IDLE on this way!
you can check this by if the result is False conda was NOT activated. Open IDLE like this: 
Anaconda Promt:If you check the activation of conda again with os.getenv(""CONDA_PREFIX"") == sys.prefix
you should receive the result True."
976,Python nltk incorrect sentence tokenization with custom abbrevations,"['python', 'nlp', 'nltk', 'tokenize']","I am using nltk tokenize library to split up english sentences.
Many sentences contain abbreviations such as e.g. or eg. thus I updated the tokenizer with these custom abbreviations.
I found a strange tokenization behaviour with a sentence though:So as you can see the tokenizer does not split on the first abbreviation (correct) but it does on the second (incorrect).The weird thing is that if I change the word Karma in anything else, it works correctly.Any clue why is this happening?"
977,How to get the frequency of specific words for each row in a dataframe,"['python-3.x', 'pandas', 'nltk']","I'm trying to create a function that gets the frequency of specific words from a dataframe. I'm using Pandas to convert the CSV file into a dataframe and NLTK to tokenize the text. I'm able to get the count for the entire column, but I'm having difficulty in getting the frequency for each row. Below is what I have done so far. Output:Goal: Get freq for each rowCVS file Format: What have I tried:output:If anyone can offer any guidance, tips, or help, I would appreciate it so much. Thank you. "
978,Python error: TypeError: Expected string or bytes-like object,"['python', 'nltk', 'typeerror', 'sentiment-analysis', 'nltokenizer']","I'm currently working on a sentiment analysis project using nltk in python. I can't get my script to pass in rows of text from my csv to perform tokenization on. However, if I pass the text in one entry at a time it works fine. I am getting one persistent error: 'TypeError: expected string or bytes-like object' when I try and pass the whole csv in.
Here is the printed data frame and python code I'm using. Any help to resolve this issue would be great.Attached is the full stack trace error. 
EDIT: print statementEDIT: Output"
979,Paraphrase generation,"['python', 'machine-learning', 'keras', 'nlp', 'nltk']","Can u please give some hint how can we create utterances for example
I have input say -
""I want my account details""
The output should be like"
980,"Extract Date from text using nltk and pos tagging , python","['python-3.x', 'text', 'nlp', 'nltk']",I am trying to extract dates from text with formats being like January 2017 to February 2018 or Jan 2018 to Feb 2018. I am using nltk and i am getting pos tags of each sentence.I want to extract tuple of all the dates with date's in date range like January 2017 to February 2018. How do i get the relevant information. Currently i am using pattern matching with regex and extracting sentences with set of words in it. Is there any other better approach and how can i capture the required tuples.My code:res_lines contains list of all the tokenized and pos tags of sentences. To capture this pattern i can capture postags where NNP is followed by CD for January(NNP) 2018(CD).Sample text:How do i execute this logic?
981,What is the best python data structure to use in this scenario?,"['python', 'data-structures', 'nltk']","I am trying to figure out what the best data structure to use in my code, I have considered dictionaries, list of dictionaries, classes etc but unsure what would be most efficient and fastest to use.The program I wrote opens multiple text files and selects words based on a certain criteria, I then need to keep a track of unique words selected, the sentences they appear in, the files they appear in and a count of how many times they appear in total throughout the process.I need to check if each selected word has already been added to the data structure as I iterate through the selected words (it will contain thousands of words). If it has already been added then add the file it came from to a list as well as the sentence the word sits in and increment the count.If not already there then add the word to the data structure, file and sentence and initialize count to 1.I am not really constrained by memory but speed is an important factor so I am thinking that something like a C style trie could work, but not sure what would be the best way to implement that in python.How would you do it?"
982,"Wrong probability calculation in context-free grammar (NLTK, Python 3)","['python', 'nltk', 'probability', 'context-free-grammar', 'text-parsing']","I have a problem with showing the most likely constituency structure of some sentence using NLTK's probabilistic grammar. Here is my sentence ""Ich sah den Tiger under der Felse""Here is my code:Here is what I get:But the desired result is:(I didn't add the probability here, but it should be displayed as well)According to the grammar, the probability to form VP from VVFIN and NP is higher than from VVFIN, NP and PP. But the parser shows the second structure.What am I doing wrong?Would be grateful for suggestions!"
983,Transform RDD (PySpark/ Python) to pull back sorted POS with highest word count per Part of Speech (POS) tagged words,"['python', 'pyspark', 'nltk', 'rdd']","This is Python PySparks RDD type of problem. There is a function already setup called 'pos_count' that tags the words with Part of Speech (POS).text = sc.textFile('data_files/data.txt')counts = text.filter(lambda line: len(line) > 0) \
    .filter(lambda line: re.findall('^(?!URL).*', line)) \
    .flatMap(pos_tag_counter) \
    .map(lambda x:(x[1], x[0]))\
    .map(lambda word: ((word[0], word[1]), 1)) \
    .reduceByKey(lambda x,y:x+y) \
    .sortByKey(lambda x:x[1])counts.flatMap(lambda x:x[0:1]).collect()Output:[('CC', 'and'),
 ('DT', 'a'),
 ('DT', 'the'),
 ('IN', 'at'),
 ('IN', 'for'),
 ('IN', 'in'),
 ('IN', 'into'),
 ('IN', 'of'),
 ('IN', 'on'),
 ('JJ', 'applied'),
 ('JJ', 'comprehensive'),
 ('JJ', 'critical'),
 ('JJ', 'michigan'),
 ('JJ', 'multidisciplinary'),
 ('JJ', 'new'),
 ('JJ', 'real'),
 ('JJR', 'more'),
 ('NN', 'approach'),
 ('NN', 'collection'),
 ('NN', 'computation'),
 ('NN', 'computer'),
 ('NN', 'coursework'),
 ('NN', 'information'),
 ('NN', 'insight'),
 ('NN', 'interest'),
 ('NN', 'intersection'),
 ('NN', 'list'),
 ('NN', 'master'),
 ('NN', 'program'),
 ('NN', 's'),
 ('NN', 'school'),
 ('NN', 'science'),
 ('NN', 'technology'),
 ('NN', 'university'),
 ('NN', 'world'),
 ('NNS', 'admissions'),
 ('NNS', 'analytics'),
 ('NNS', 'data'),
 ('NNS', 'hands'),
 ('NNS', 'people'),
 ('NNS', 'problems'),
 ('NNS', 'projects'),
 ('NNS', 'skills'),
 ('NNS', 'statistics'),
 ('PRP', 'we'),
 ('PRP', 'you'),
 ('PRP$', 'its'),
 ('PRP$', 'our'),
 ('RB', 'fully'),
 ('TO', 'to'),
 ('VB', 'develop'),
 ('VB', 'help'),
 ('VB', 'join'),
 ('VB', 'offer'),
 ('VBD', 'embedded'),
 ('VBG', 'applying'),
 ('VBG', 'using'),
 ('VBN', 'applied'),
 ('VBN', 'pleased'),
 ('VBP', 'criteria'),
 ('VBP', 'focus'),
 ('VBP', 'invite'),
 ('VBP', 'online'),
 ('VBP', 'provide'),
 ('VBP', 'teach'),
 ('VBZ', 'is')]However, this is not fully right. There are problems with this output.I need it to return to HIGHEST counted WORD for each POS like 'CC' or 'NN' etc.  I need the highest counted one. For example, 'NN' should be 'NN' 'science' because the count for 'science' is 4 which is greater than other counts for 'NN'.See below a more descriptive result that includes the count.  I tried reduceByKey(max) but MAX isn't working in the reduceByKey because I think the count isn't the main index and because of that MAX doesn't know how to find the highest count? Must just look at last record in list of each POS type.[('CC', 'and'),
 6,
 ('DT', 'The'),
 1,
 ('DT', 'a'),
 2,
 ('DT', 'the'),
 1,
 ('IN', 'For'),
 1,
 ('IN', 'at'),
 1,
 ('IN', 'in'),
 2,
 ('IN', 'into'),
 1,
 ('IN', 'of'),
 3,
 ('IN', 'on'),
 3,
 ('JJ', 'applied'),
 1,
 ('JJ', 'comprehensive'),
 1,
 ('JJ', 'critical'),
 1,
 ('JJ', 'multidisciplinary'),
 1,
 ('JJ', 'new'),
 1,
 ('JJ', 'real'),
 1,
 ('JJR', 'more'),
 1,
 ('NN', 'Coursework'),
 1,
 ('NN', 'approach'),
 1,
 ('NN', 'collection'),
 1,
 ('NN', 'computation'),
 1,
 ('NN', 'computer'),
 1,
 ('NN', 'information'),
 2,
 ('NN', 'insight'),
 1,
 ('NN', 'interest'),
 1,
 ('NN', 'intersection'),
 1,
 ('NN', 'list'),
 1,
 ('NN', 'master'),
 1,
 ('NN', 'program'),
 2,
 ('NN', 's'),
 1,
 ('NN', 'science'),
 4,
 ('NN', 'technology'),
 1,
 ('NN', 'world'),
 1,
 ('NNP', 'Information'),
 1,
 ('NNP', 'Michigan'),
 1,
 ('NNP', 'School'),
 1,
 ('NNP', 'University'),
 1,
 ('NNS', 'admissions'),
 1,
 ('NNS', 'analytics'),
 1,
 ('NNS', 'data'),
 4,
 ('NNS', 'hands'),
 1,
 ('NNS', 'people'),
 1,
 ('NNS', 'problems'),
 1,
 ('NNS', 'projects'),
 1,
 ('NNS', 'skills'),
 1,
 ('NNS', 'statistics'),
 1,
 ('PRP', 'We'),
 2,
 ('PRP', 'we'),
 1,
 ('PRP', 'you'),
 1,
 ('PRP$', 'its'),
 1,
 ('PRP$', 'our'),
 2,
 ('RB', 'fully'),
 1,
 ('TO', 'to'),
 3,
 ('VB', 'develop'),
 1,
 ('VB', 'help'),
 1,
 ('VB', 'join'),
 1,
 ('VB', 'offer'),
 1,
 ('VBD', 'embedded'),
 1,
 ('VBG', 'applying'),
 1,
 ('VBG', 'using'),
 1,
 ('VBN', 'applied'),
 1,
 ('VBN', 'pleased'),
 1,
 ('VBP', 'criteria'),
 1,
 ('VBP', 'focus'),
 1,
 ('VBP', 'invite'),
 1,
 ('VBP', 'online'),
 1,
 ('VBP', 'provide'),
 1,
 ('VBP', 'teach'),
 1,
 ('VBZ', 'is'),
 1]THANK YOU IN ADVANCE for your help!"
984,Read lines between start and end words in Python 3 using Rule Based Processing,"['python', 'python-3.x', 'nltk', 'spacy']",I have long text lines and want to read text between two words. It is a big text file in a standard format as follows:I am looking for text between Fist Paragraph and Second Paragraph. I have tried couple of approaches using spacy but unable to get what I want.What am I doing wrong? Should I be using some other library? Any help would be appreciated. Thank you.
985,How to adjust/re-train NLTK SentimentIntensityAnalyzer,"['python', 'nlp', 'nltk', 'sentiment-analysis', 'tweets']","I'm using SentimentIntensityAnalyzer from NLTK to get a polarity of tweets about airline servies. There are many tweets about food quality and punctuality etc. With the code below I can obtain the polarity of a word. However, there are many words that only returns ""neutral"".These words are frequently used for expressing the service quality of airlines, so I want to get a proper polarity somehow. Any ideas would be appreciated!"
986,Training Tfidf-Vectors with keras and get words (that caused spam or ham),"['python', 'keras', 'scikit-learn', 'nltk']","I'm currently stuck with a problem that I cant get in my head..I have bunch of texts and convert them into a tfidf-vector with sklearn - pretty basic. But now I'd like to use keras for training instead of a sklearn model (That also works fine - Note: I convert the sparse-matrix to a np.array())So after my model is trained, I want to see which words of each text resulted in the according classification. Say, which words caused ham and which words caused spam.I know I have to get somehow the weight-matrix or something.. run a text through the algorithm and see which words ""caused"" the according classification. Any help/hints are appreciated ;-)Edit:
I found my question not as trivial as I tought. The standard sklearn classifier uses just one layer. Here I used two.So I could build a model like:From which I could manually retrieve the weights:So I assume for a single sample, I could just rank the result by using the weights. The higher the weight, the more the word contributes to being classified as 1. But how do I do that for a model with a second relu layer?"
987,Python NLTK FreqDist - Listing words with a frequency greater than 1000,"['python', 'pandas', 'nltk']","I'm trying to output every word that appears in my tokens more than 1000 times (> 1000) and save it to freq1000. This is my current code, I'm completly stuck after this and I'm not sure if there is a freqdist function I can use to help. I have saved the FreqDist to fd_1 successfully. I'm just unsure how to get an output of the words that appear more than 1000 times and save it to freq1000. I would appreciate any help you can provide."
988,How to count the frequency of words existing in a text using nltk,"['python', 'nlp', 'nltk', 'word-frequency']","I have a python script that reads the text and applies preprocess functions in order to do the analysis.
The problem is that I want to count the frequency of words but the system crash and displays the below error.File ""F:\AIenv\textAnalysis\setup.py"", line 208, in tag_and_save
       file.write(word+""/""+tag+"" (frequency=""+str(freq_tagged_data[word])+"")\n"") TypeError: tuple 
  indices must be integers or slices, not strI am trying to count the frequency and then write to a text file.I expect the output like this :('*****/DTNN') 3based on the answer of i changed the function get_freq() into :but now it display the below error :File ""F:\AIenv\textAnalysis\setup.py"", line 217, in tag_and_save
    file.write(word+""/""+tag+"" (frequency=""+str(freq_tagged_data[word])+"")\n"") TypeError: listindices must be integers or slices, not strHow to fix this error and what should I do?"
989,Owner Search for given Server SNO,"['python', 'nlp', 'nltk', 'categories']","I am a newbie to NLP.
I have a excel sheet with following columns: Except the Server_SNo, other columns may or may not have data.  For some records there is no data except Server_SNo which is the
    first column.So, out of 4000 records, about 50% of data contain a direct mapping for a server with the owner.  Remaining 50% of data have a combination of other columns (Owner, Hosting Dept, Bus owner, Applications hosted, Functionality and comments)Here is my problem, I need to find the owner for the given Server_Sno for 50% of data which have a combination of other columns (Owner, Hosting Dept, Bus owner, Applications hosted, Functionality and comments).I have just started to build the code using Python and NLTK.Is this an NLP problem? Am I going in the right direction using Python and NLTK for NLP?Any insights are appreciated.-Mani"
990,"Regex , Find the sentence, all of which are capital letters","['python', 'regex', 'nltk', 'text-extraction', 'stringtokenizer']","I need your help. Currently I'm using this code section for my work;sentenceIndex   = this is a list. It contains tokenized sentences from a paragraph. For example:Sample Paragraph:VODOFONE ARENA ŞANSI  Ama asıl önemli olan nokta Murat Çetinkaya, Cumhurbaşkanı Erdoğan ve Başbakan Davutoğlu’nun ittifakıyla seçildi. O süreci ayrıntılı olarak aktaracağım. Hatta Cumhurbaşkanı ve Başbakan’ı aynı isim üzerinde ittifak etmeye götüren kriterlere de değineceğim. Ama bir şey var ki aktarmasam olmaz. Merkez Bankası Başkanı’nın kaderi Dolmabahçe ile Vodafone Arena arasındaki yolculukta belirleniyor.sentenceIndex:['VODOFONE ARENA ŞANSI  Ama asıl önemli olan nokta Murat Çetinkaya, Cumhurbaşkanı Erdoğan ve Başbakan Davutoğlu’nun ittifakıyla seçildi.','...................','.................']I need a regex, which finds all the capital letter words in the sentences.""VODOFONE ARENA ŞANSI"" ı need to find and extract this section. current regex that I am using is not working. I need help on this regex thing. NOTE:
[Ö|Ç|Ş|Ü|Ğ|İ]
I am working on turkish text. Thats why I need to pay attention this letters too.Thanks for the people who will spare their time and helped me on this issue :)"
991,Laplace smoothing function in nltk,"['nlp', 'nltk', 'nltk-trainer']","I'm building a text generate model using nltk.lm.MLE, I notice they also have nltk.lm.Laplace that I can use to smooth the data to avoid a division by zero, the documentation is "
992,Is it possible to create a custom annotator with attributes for Name Entity Recognition in NLP libraries?,"['pyspark', 'nltk', 'stanford-nlp', 'spacy', 'johnsnowlabs-spark-nlp']","I would like to extract the entities for the drug treatment, for example, from this sentenceAs a result I need to get the entity 'Drug' with the following attributes :Questions:I created such annotators for Apache UIMA and I am interesting is it possible to do the same via other NLP libraries."
993,How to use spacy to do Name Entity recognition on CSV file,"['python', 'pandas', 'csv', 'nltk', 'ner']","I have tried so many things to do name entity recognition on a column in my csv file, i tried ne_chunk but i am unable to get the result of my ne_chunk in columns like soInstead after using this code, i got this errorSo, i am wondering if i could do this using spaCy which is another thing that i have no clue about. Can anyone help?"
994,How to use spacy to do Name Entity recognition on CSV file,"['python', 'pandas', 'csv', 'nltk', 'ner']","I have tried so many things to do name entity recognition on a column in my csv file, i tried ne_chunk but i am unable to get the result of my ne_chunk in columns like soInstead after using this code, i got this errorSo, i am wondering if i could do this using spaCy which is another thing that i have no clue about. Can anyone help?"
995,Words in WordNet corpus clarification,"['nlp', 'nltk', 'wordnet', 'nltk-book']",I want to get the length of words in the WordNet corpusCode:I get the output as 147306My Questions:
996,Formatting a string with elements in a list based on conditions,"['python-3.x', 'nltk']","So I've created a series of grammers for use within a method in a class I've created. Each list can be n elements long so placing each word via list index is prettings straight forward wordlist[1:], however I need to use an | operator and that can't be done with explicit string indexes (at least I think so). This is what I've written so far:Ideally, I'd like to be able to pass a list of n number of pronouns and nouns and have the strings be formatted with each element without explicit string indexes, so something like this:However, I'm not sure how to implment the | operator, much less any conditional formatting when doing string formatting. The grammer formatting is based on nltk's grammer constructor used in this context:Its a bit of a confusing question, so I'm happy to try and clarify any confusion!"
997,Partial keyword match not working when I am trying to create a new column from a pandas data frame in python?,"['python', 'regex', 'pandas', 'dataframe', 'nltk']",I have a data frame Description  as mentioned below    I am trying to do a keyword search on the description column and I have list of keywords as a list .My current code checks only exact matches not partial matches.If there are multiple keywords present in the row these will be separated by a delimiter  and populated new column.My codeHow can this be done?
998,How can I replace the nltk dictionary with my txt in this code?,"['python', 'dictionary', 'nltk', 'boggle']","I found this code here (Many thanks to Naren for developing it) it is a boggle solver, the question is that I now live in Brazil and I would like to take it to Portuguese or maybe, Spanish, so I want to use my own dictionary (Separated by line break , not commas) So how can I replace the nltk dictionary with my txt in this code? I already tried to convert it to csv, open it with the open () and nothing works u.uNote: Dictionary with which I am testing: here"
999,How to get the index of value with list comprehension,"['python', 'nlp', 'nltk', 'stanford-nlp', 'rasa-nlu']",I am working in Brown Corpus using NLTK. I want to separate out the tokens that has tokens tagged with DTMy code:The above code returns the value tagged with DT but I need the index too. I am trying to get the value and the index of the value in return. For example the output should be:This code does not work:
1000,Google Translate Problem CSV File for Python,"['python-3.x', 'nltk', 'google-translate']","hello I'm trying to translate csv file with googletrans.But I get the error""'numpy.int64' object is not iterable"".I will be grateful if you could help me. Thanks in advance."
1001,AssertionError by '.sents' function of NLTK,"['python', 'nltk', 'assertion', 'corpus', 'sentence']","I have a corpus of texts that I'm analyzing with NLTK package (and some trained tagger).For some mysterious reason I get Each time I start iterating through the sentences of some (!) of my texts. Moreover, if delete the first line of such a text everything works just fine (this line, for instance):These lines do not seem to contain anything special.Is there any reasonable explanation to this phenomenon and any solution except of just deleting lines from the text material?Here is my code (if it's important):vds_nennen is a list that contains all forms of the German verb ""nennen"" as strings, and tagger.tag() is the POS tagger created with the help of the ClassifierBasedGermanTagger (described here)So the code basically returns all sentences from several texts that contain the verb ""nennen"" in any of its grammatical forms as lists of tokens packed into the list of lists, and it works. But I'm curious why I need to delete the first lines in order to avoid Assertion Errors.Any suggestion would be appreciated!"
1002,removing bigrams that contain common stopwords,"['python', 'nltk', 'stop-words']",I have a function as below. It returns all bigrams and trigram in a sentence. I would like to keep only bigrams and trigrams that dont contain any stopwords. How could I use from nltk.copus import stopwords to do the same? I know how to remove remove stopwords before creating bigrams and trigrams. But I would like to remove stopwords after creating bigrams and trigrams. 
1003,Ngrams from pandas column,"['python-3.x', 'pandas', 'nlp', 'nltk', 'trigram']","I have a pandas dataframe, with the following columns :Column 1 Column 2etc.My target is to count the bigrams, trigrams, quadrigrams of the dataframe (and specifically, the column 2, which is already lemmatized).I tried the following :However, I have the following error My final target is to be able to print the top X bi grams, trigrams etc."
1004,Why do we neglect 'not' in NLP?,"['python-3.x', 'machine-learning', 'nlp', 'nltk', 'corpus']","While cleaning the texts, we remove the words like 'the', 'this' and even 'not'.As I have analyzed, the y_pred vector has a 1 for the second review because it neglected 'not'. If it had considered 'not' also, maybe the accuracy would have been increased.Is there a way/advanced method to include such important role-playing words?"
1005,Extracting all the forms of “Be” using NLTK,"['python', 'nlp', 'nltk']","I need to figure out a solution to extract all the forms of the verb ""be"" of a sentence. Will it be a good approach to get the verb's base form (VB) from a pos tagger and do a check to solve this?? :) Your replies would and help are much appreciated."
1006,Pickling Error while using Stopwords from NLTK in pyspark (databricks),"['pyspark', 'nltk', 'stop-words']","I found the following function online:and then I am doing the following:The error I am getting is the following:PicklingError: args[0] from newobj args has the wrong classFunny thing is if I rerun the same set of code, it runs and throws no pickling error. Can someone help me resolve this issue? Thank you!"
1007,can i open .bracket file using python?,"['python', 'nlp', 'nltk', 'data-science']","can i open .bracket file using python?I tried to make tree using nltk. 
If its only 1 sentence i have no problem, for example:But i have hundreds of sentence which is in a .bracket file example:i tried using :but  i got error : TypeError: expected string or bytes-like object. (i'm sorry im a newbie in NLP) "
1008,How to go from type theory to first-order logic lambda-expressions,"['types', 'nlp', 'nltk', 'formal-semantics', 'nltk-book']","As can be seen in the O'Reilly NLTK book, Chapter 10, when I want to model the syntax tree of sentence “Bob loves Alice,” namelyinto first-order logic lambda-expressions, I get the following:where on the left I have the tree of types and on the right the tree of λ-expressions. I have chosen to type-raise both Bob and Alice.My question is the following: from the tree of types I can easily calculate that the type of ""loves"" must be <<<e,t>,t>,<e,t>> but how can I deduce from this that the corresponding λ-expression must be λR.λx.R(λy.loves(x,y))Is there some method to obtain the λ-expression of a leave of the syntax tree from its type and from the surrounding λ-expressions?"
1009,how to Define a new variable of type set and store the text list of words in it?,"['python', 'list', 'set', 'nltk']","**code is **the error here File """", line 1, in 
    sorted(set(all_tokens))
TypeError: unhashable type: 'list'"
1010,NLP: Rule Based vs Machine Learning [closed],"['nlp', 'nltk', 'spacy', 'rasa-nlu']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 5 months ago.I am trying to build an agent/chatbot.The human will be contacting the agent using a certain phraseology (very well specified).I am new to this field, and I discovered that I can use Rule-based or machine learning. I have read that Rasa is not a rule based framework, but at the same time it uses Spacy.Can somebody explain this to me."
1011,Unigram tagging in NLTK,"['nlp', 'nltk', 'stanford-nlp', 'allennlp']","Using NLTK Unigram Tagger, I am training sentences in Brown CorpusI try different categories and I get about the same value. The value is around 0.9328... for each categories such as fiction, romance or humorWhy is it that the case? is it because they are from the same corpus? or are their part-of-speech tagging is the same?"
1012,Identify Location Within the Sentence where the Missing Word Belongs,"['python', 'nlp', 'nltk', 'pos-tagger', 'pos']",I have the code below:The output of the code above are the tokenized words with their corresponding parts of speech. Example :The text is supposed to be and supposed to have the POS sequence ofHow will I make the program locate within the sentence the position of the missing word?
1013,Python NLP Text Tokenization based on custom regex,"['python', 'nlp', 'nltk', 'tokenize', 'spacy']","I am processing large amount of text for custom (NER) Named Entity Recognition using Spacy. For text pre-processing I am using nltk for tokenization..etc. I am able to process one of my custom entities which is based on simple strings. But the other custom entity is a combination of number and certain text (20 BBLs for example). 
The word_tokenize method from nltk.tokenize tokenizes 20 and 'BBLs' separately each as a separate token. What I want is to treat them (the number and the 'BBLs' string) as one token. I am able to extract all the occurrences of this using regex: Note: I am doing that because Spacy standard English NER model is mistakenly recognizing that as 'Money' or 'Cardinal' named entities. So I want it to re-train my custom model, so I need to feed it with this pattern (the number and the 'BBLs' string) as one token that indicates my custom entity. "
1014,"word_tokenize with same code and same dataset, but different result, why?","['python', 'nltk', 'tokenize', 'text-mining']","Last month, I tried to tokenize text and create a of words to see which word shows up frequently. Today, I want do it again in the same dataset with the same code. It still works but the result is different and obviously today's outcome is wrong because the frequency of appearing words decrease significantly. Here is my code:Here is a sample of my datasetThis dataset is from Kaggle and the name of it is ""Wine Reviews""."
1015,Hazm: POSTagger(): ArgumentError: argument 2: <class 'TypeError'>: wrong type,"['nltk', 'python-3.6', 'text-mining', 'pos-tagger', 'farsi']","I have got an error for running the below code. May you give me some help?Error:I am using ubuntu18.04 on windows 10. Also, I put mentioned files in resources file beside of code.
Python 3.6.9
Package of hazmI have no problem to run Chunker one from this packege!"
1016,Encoding for ä ü ö ß etc,"['python', 'utf-8', 'nlp', 'nltk']","I have a list of strings, some of which contain letters such as  ä ü ö or ß. Tring to print them results in strings such as NatÃ¼rlichÂ or BedrÃ¼ckung. As suggested in other threads, I attempted solving the problem by goinghowever, now I receive strings like this: Verkehrsl\xc3\x83\xc2\xa4rm, Windb\xc3\x83\xc2\xb6, Gro\xc3\x83\xc5\xb8stadtThe values were scraped from a book using spacy and nlp."
1017,NTLK nltk.ConditionalFreqDist - Plot ngrams,"['python', 'plot', 'nltk']","Here are two examples, one that works and is derived from the https://www.nltk.org/book/ch02.html
and another that does not. The first examples plots single words frequencies, here ['america', 'citizen']. The second is a modified version (evidently incorrectly) that attempts to plot frequencies of the bigram ['america citizen']. I would like to plot ngram frequencies such as for a bigram like ['america citizen'].Plot Example 1
Plot Example 2 - failed"
1018,Is it possible to drop sentences from the text with NLTK in Python?,"['python', 'nlp', 'nltk']","For example, I have a text that consists of several sentences:""First sentence is not relevant. Second contains information about KPI I want to keep. Third is useless. Fourth mentions topic relevant for me"".In addition, I have self-constructed dictionary with words {KPI, topic}. 
Is it somehow possible to write a code that will keep only those sentences, where at least one word is mentioned in the dictionary? So that from the above example, only 2nd and 4th sentence will remain.ThanksP.S. I already have a code to tokenize the text into sentences, but leaving only ""relevant"" ones is not something common, as I see."
1019,NLP text classification based on 2 different input,"['deep-learning', 'nlp', 'nltk', 'text-classification', 'nltk-trainer']","Good mornings
I am new to natural language processing (NLTK), and I would like to have some advice to perform the idea below in the best way  The system is base on text comprehension, where questions are asked and analyze with keywords 'regex',  the goal is to improve the actual system with NLP.
Example: 
1 - we have any question base on the text 
2 - We have a possible keyword to evaluate the answer ( yes, organization, 1990 ) 
3 - We have an answer given (if the answer is like ' yes the CTZ organization is located in Tokyo since 1990 '  ) the system will fully grade because all keywords are found in the answer I am looking for some advice to take the problem with a better approach, (What I am worried about is if we got some bullshit as an answer which contains those keywords, the system will be completely wrong in my view. 

Thank you  
I look forward to your reply."
1020,NLP - Find keyword from the sentence,"['python', 'python-3.x', 'nlp', 'nltk']","I'm working on NLP project, from the below attached code. 
I could only match and find similarity score for the words given in the list.Can anyone help me how to find the similarity from paragraph?
Lets assume List A contains a paragraph & list B contains a keyword.
How to find the keyword in List A?"
1021,how to display the results of code on the flask,"['python', 'dataframe', 'flask', 'request', 'nltk']","i want to show the result from my code in flask, but when i try to call result variable the program doesn't show anything. how to show the result in result.html. I really need it. please someone help me to fix the problem.this is my code in app.py this is my result.htmli really need the solution to show the result, please somebody help this problem"
1022,Python NLP Text Tokenization based on custom regex,"['python', 'nlp', 'nltk', 'tokenize', 'spacy']","I am processing large amount of text for custom (NER) Named Entity Recognition using Spacy. For text pre-processing I am using nltk for tokenization..etc. I am able to process one of my custom entities which is based on simple strings. But the other custom entity is a combination of number and certain text (20 BBLs for example). 
The word_tokenize method from nltk.tokenize tokenizes 20 and 'BBLs' separately each as a separate token. What I want is to treat them (the number and the 'BBLs' string) as one token. I am able to extract all the occurrences of this using regex: Note: I am doing that because Spacy standard English NER model is mistakenly recognizing that as 'Money' or 'Cardinal' named entities. So I want it to re-train my custom model, so I need to feed it with this pattern (the number and the 'BBLs' string) as one token that indicates my custom entity. "
1023,How to identify the authorship of a text in python? (scikit-learn),"['python-3.x', 'machine-learning', 'scikit-learn', 'nltk', 'corpus']","If I have a given text written by the desired author, how might I compute whether or not another text is written by the same author. I understand I would need to vectorize the authors text such that it can be used to train some sort of method in scikit-learn and used to predict other texts. However, I am unfamiliar with scikit-learn and I am unsure how I might actually implement this. Any help is appreciated."
1024,How to extract sentences using sent_tokenize that are not seperated by delimiters?,"['python', 'nltk']","I have a blob of text containing many sentences like-Dogs love to chase and chew on bones
Cows feed on grass and give milk.
Unlike others, cats don't care about chasing moving objects
Goats are good pets too; their milk makes great cheese.I just need to grab a sentence based on the animal name like  'Dog','Cat','Goat' etc. My name token should recognize case-insensitive names and those in any part of the sentence. And grab the sentence for me even if there is/is not a presence of a sentence delimiter. I need to store these sentences later in a file/db. Tried this-
sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') print('\n-----\n'.join(sent_detector.tokenize(text.strip())))but output looks like this-
Dogs love to chase and chew on bones Unlike others, cats don't care about chasing moving objects
Goats are good pets too; their milk makes great cheese.When the sentence doesn't end with a period, 2 sentences are grabbed together. Is there way I can use new line charater as a sentence boundary to help me extract that particular sentence ? "
1025,Stemming Geographical words,"['python', 'text', 'nlp', 'nltk', 'text-processing']","What is the best stemming method to geographical entities? I want to convert the geographical entities gathered in a dataframe column to accurate region names like for example converting:toThe geographical words will be extracted from more than 50,000 news, so I'm looking for a function which could work critical situations."
1026,Porter and Lancaster stemming clarification,"['nlp', 'nltk', 'stemming', 'porter-stemmer', 'nltk-book']",I am doing stemming using Porter and Lancaster and I find these observations:My question are:I am not able to understand these concepts. Could you please help?
1027,nltk.download() Errno 61 and Proxy Issue,"['python', 'nlp', 'nltk']","I tried to download packages from nltk after I imported nltk in Python. It produces the following error:I then tried nltk.set_proxy because I am using proxies in China. However, the proxies I am using doesn't have username and password. So my code looks like nltk.set_proxy('http://proxy.example.com:3128') When I ran the nltk.download() again, there is the error messages it produced:Does anyone know what the problem might be? Thanks in advance!"
1028,Installed Matplotlib using terminal on mac but ModuleNotFoundError in Python,"['python', 'matplotlib', 'nltk']","I installed matplotlib using terminal on my mac and it was successfully installed. However, when I try to import matplotlib, it produces the ModuleNotFound error. The following are my codes and error messages.Terminal:Looking in indexes: https://pypi.doubanio.com/simple
Requirement already up-to-date: matplotlib in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (3.1.3)
Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from matplotlib) (2.8.1)
Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from matplotlib) (1.1.0)
Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from matplotlib) (2.4.6)
Requirement already satisfied, skipping upgrade: cycler>=0.10 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from matplotlib) (0.10.0)
Requirement already satisfied, skipping upgrade: numpy>=1.11 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from matplotlib) (1.18.1)
Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)
Requirement already satisfied, skipping upgrade: setuptools in /Users/zbao/.local/share/virtualenvs/myProject-opsBTjit/lib/python3.8/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0)IDLE:Traceback (most recent call last):
  File """", line 1, in 
    import matplotlib
ModuleNotFoundError: No module named 'matplotlib'Thanks for the help! If anything else is needed please let me know. Sorry I am still very new to Python."
1029,nltk comparison of two values,"['python', 'nltk']",Hi with this code I'm trying to get it to compare two values. However when it returns the tree it's just as one. What first order logic do I need for the start of the tree to be a comparirison?This is the current tree printed outBut I want the start of the tree to compare the two values and return true or false
1030,How to use the universal POS tags with nltk.pos_tag() function?,"['python', 'nlp', 'nltk', 'pos-tagger', 'universal-pos-tag']","I have a text and I want to find number of 'ADJs','PRONs', 'VERBs', 'NOUNs' etc.
I know that there is .pos_tag() function but it gives me different results , and I want to have results as 'ADJ','PRON', 'VERB', 'NOUN'.
This is my code:The code above works but I want to find a way to get this type of tags:I saw that there is a chapter here: https://www.nltk.org/book/ch05.htmlThat says:But I do not know how to apply that on my sample sentence.
Thanks for your help."
1031,How to find corpora with nltk?,"['python', 'nltk', 'corpus']","I found this code online:I can't figure out where the file 'tiger_release_aug07.corrected.16012013.conll09' is coming from. I mean: How can I know, that the param is specifically this one and not 'tiger_release_aug20.corrected.conll20' ? I want to have a similar list of all fileids provided as shown in the in https://www.nltk.org/book/ch02.html:-1) Where is the 'tiger_release_aug07.corrected.16012013.conll09' coming from and where is it stored?2) How can I see a list of all available fileids for the ConllCorpusReader3) Is there any page available that gives an overview of the latest (german) corpora?"
1032,Generating dictionaries to categorize tweets into pre-defined categories using NLTK,"['python', 'machine-learning', 'nlp', 'nltk', 'text-classification']","I have a list of twitter users (screen_names) and I need to categorise them into 7 pre-defined categories -  Education, Art, Sports, Business, Politics, Automobiles, Technology based on thier interest area.
I have extracted last 100 tweets of the users in Python and created a corpus for each user after cleaning the tweets.As mentioned here Tweet classification into multiple categories on (Unsupervised data/tweets) :
I am trying to generate dictionaries of common words under each category so that I can use it for classification. Is there a method to generate these dictionaries for a custom set of words automatically?Then I can use these for classifying the twitter data using a tf-idf classifier and get the degree of correspondence of the tweet to each of the categories. The highest value will give us the most probable category of the tweet. But since the categorisation is based on these pre-generated dictionaries, I am looking for a way to generate them automatically for a custom list of categories.Sample dictionaries :Example I/O:"
1033,Does NLTK provide a lib to measure vocabulary ordinary level?,"['nltk', 'vocabulary']","Does NLTK or any other NLP tools provide a lib to measure vocabulary ordinary level? By that ordinary level, I mean certain words are simple and more frequently used like ""and, age, yes, this, those, kind"", which any elementary school student must know. Similar to that Longman English Dictionary (usually for ESL) has defined a 3000-word basic vocabulary for explaining all the entries with. There could be a set of rare words that fall into the rare-used level, which only pedantic uses, like Agastopia, Impignorate, Gobbledygook, etc. There are for sure some levels in between of these 2 extremes. Certainly, this level definition is purely subjective and I expect different organizations or persons may have different views. At least it could vary region from region. My purpose is to measure the difficulty/complexity of some passages, well, currently naively, by just checking its vocabulary. ""Ordinary level' might not be the good description, but I am not able find a proper and formal expression :). I hope my explanation clarifies my purpose. "
1034,How do I transform a TF-IDF matrix into an overall dictionary of the top 10 words,"['python', 'python-3.x', 'nltk', 'tf-idf', 'corpus']","I am trying to get the overall tf-idf score of words over a few texts. I am following the manual method of calculating tf-idf seen here: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76I am using these sentences: ['the man went out for a walk','the children sat around the fire']The results can be seen in this pandas dataframe table:The dictionaries that are used to show the tf-idf result can be seen here:How can I transform this list of TF-IDF result dictionaries into one dictionary of the top tf-idf results overall, in order?"
1035,i have 200 text file in hindi. want to remove white space the special character and find the find bigram and trigram in python,"['python', 'nlp', 'nltk']","I am trying to remove special character fro all the 200 text file this is the code for bigram which I foundwant both codes in one program
thank you in advance
example my name is eshan.
output
my, name occurs 1
name,is  occurs 1
is, advance occurs 1 
occurance can be more then 1 according to text"
1036,store return of NLTK's .similar(),"['nlp', 'nltk']","Is there a way for me to store what the .similar() function in the NLTK library returns? Right now, if I have the line text.similar('woman') in my code, it prints it to the output and I can't store those similar words in a data structure such as a list or to a file. I'd like to be able to do that. "
1037,Spelling corrector with supplementary custom dictionary,"['python', 'nltk', 'spell-checking']","What is the best system for spell checking in python that enables the use of an external dictionary? I've seen packages that use an external dictionary to replace a default English dictionary for example. But I would like the external dictionary to supplement the existing spell check. For example, if I have weird abstract words (dsdfw, peloe, punj), I want the spell checker to be able to recognise them as English words for the purposes of spelling correction.This is na exapmle of a setence using pelloeShould becomeThis is an example of a sentence using peloe"
1038,Apply NLP WordNetLemmatizer on whole sentence show error with unknown pos,"['python', 'nlp', 'nltk']","I want to Apply NLP WordNetLemmatizer on whole sentence. The problem is that I get an error:Its like Im getting unknown 'pos' value, but I do not know why. I want to get base form of the words, but without 'pos' it does not work.
Can you tell me what am I doing wrong?"
1039,TF-IDF to Weight Sentence,"['python', 'nltk', 'tf-idf']","I have some sentence and key sentece like this.I'm trying to choose the sentence using the key with TF-IDF. I've preprocessed those sentence so the output would be like this.I'm using TfIdfVectorizer from NLTK with the code like thisSo that the ouput would be like thisWhat I gonna ask is from those number, what does those number means? I know it's obviously TF-IDF, but I don't understand what's the meaning of those number.
I want to choose from 2 sentence which is the weightest, but I don't understand how can I weight a sentence based on word's weight? Do I need using similarity, but I don't see why am I need using similarity as I want to find which sentence, not the distance?
Thank you:)"
1040,How to Embed your Dataframe using already trained model with Gensim (GoogleNews-vectors-negative300.bin),"['machine-learning', 'scikit-learn', 'nlp', 'nltk', 'gensim']","I am following this tutorial in which i have a following Dataset from Quora:Here i have already cleaned and tokenize the data in column q1_clean & q1_clean.Now i have trained the W2vModel by using GoogleNews pretrained model with the following code. Now i have to do the feature analysis, for that i have following function to get the average computed distance.Here i have computed the tfidf to use as a weights: and i am calling this get_pairwise_distance function like in the tutorial.For this function i need to pass the embedded version of q1_clean and q2_clean as X1 and X2 where weights are already computed using TFIDF. and i am getting no clue how to embed these two columns into vectors using pretrained model and pass it to the given function? "
1041,How can I fix an nltk.download [Win Errorr 10054] when trying to run the #nltk.download('stopwords')) code at my corporate computer?,"['python', 'download', 'nltk']","I am trying to use nltk and I need to download the nltk.download() data. I have tried a number of things on my work computer, but I'm not sure if it's our firewall, or if there is something else going on. I am doing this in Jupyter Notebook. I have tried running the following code (nltk.download()) and updating the the directory and server as shown below, but I still get an error. Which server index should I be using? 
I've also tried simply running an import statement followed by a download statement. Do we still need to do this to use stopwords? I've tried going through Anaconda Prompt and running the code below and I still get the same error. Lastly, I've tried going directly to the site (http://nltk.org/nltk_data/)to download the data and the website never opens and times out. Can anyone help direct me on how to fix this? I've seen something written on a proxy server. If that is the issue, how do I get around it? "
1042,Count total number of words in a corpus using NLTK's Conditional Frequency Distribution in Python (newbie),"['python', 'dataframe', 'nlp', 'nltk', 'frequency-distribution']","I need to count the number of words (word appearances) in some corpus using NLTK package.Here is my corpus: Here is how I try to get the total number of words for each document:(I split strings into words manually, somehow it works better then using corpus.words(), but the problem remains the same, so it's irrelevant). Generally, this does the same (wrong) job:This is what I get by typing cfd.appr.tabulate():But these are numbers of words of different length. What I need is just this (only one type of item (text) should be counted by number of words):I.e. the sum of all words of different length (or sum of columns that was composed using DataFrame(cfd_appr).transpose().sum(axis=1). (By the way, if there is some way to set up a name for this column that would also a solution, but .rename({None: 'W. appear.'}, axis='columns') is not working, and the solution would be generally not clear enough.So, what I need is:Would be grateful for help!"
1043,NLTK MLE model clarification trigrams and greater,"['python', 'nltk', 'mle', 'nlg']","I am learning NLTK and have a question about data preprocessing and the MLE model. Currently I am trying to generate words with the MLE model. The problem is that   when I pick an n>=3. My model will produce words completely fine until it gets to a period ('.'). Afterwards, it will only output end-of-sentence paddings.This is essentially what I am doing. I suspect that the answer to my problem lies in the way my n-grams are prepared for the model. So is there a way to format/prepare the data so that, for example, trigrams, are generated like   this --> ( . , </s>, <s> ) so that the model will try to start another sentence again and output more words ?Or is there another way to avoid my problem written above ?"
1044,Why do I get TypeError: unhashable type when using NLTK lemmatizer on sentence?,"['python', 'nltk', 'lemmatization']","I'm currently working on lemmantizing a sentence while also applying pos_tags. This is what I have so farHowever, when I input a sentence with this I get the errorI understand that lists are unhashable but am unsure of how to fix this. Do I change lists to a tuple or is there something I'm not understanding?"
1045,Can we create a simple thesaurus from a field in a dataframe?,"['python', 'python-3.x', 'dataframe', 'nlp', 'nltk']","I am trying to find synonyms and antonyms for one word, using strings from a field in a dataframe and not a standard wordnet.synsets lexical database.  I'm pretty sure this is possible, but I'm not sure how to feed in the appropriate data source (my specific field).For instance, the code below works fine.I tried to convert the field to an array, and use that...When I run that, I get this error:The field in the dataframe looks like this:These are the first 5 rows.  Maybe the issue is related to this thing (not sure):  itâ€™s "
1046,Unable to use nltk.LidstoneProbDist with NaiveBayesClassifier,"['python', 'nltk']","I can train a Naive Bayes classifier using the Laplace smoothing, but I cannot do it with Lidstone.With Laplace the classifier is trained:However, when I train with Lidstone:I get the error messageI used the this linesince it worked here. This is one of my many failed attempts, any help would be much appreciated."
1047,How to extract details from scanned reports using NLP/ML?,"['machine-learning', 'nltk', 'spacy']","I need to extract person health data from thousand of reports containing users health information.But the problem is that, all these reports are from different vendors and have data in completely different format. I tried, Form Recognizer and other OCR tools but because of the different format in each report, I am not getting the expected output.Now I want to write some custom python code where I want to achieve this using Spacy, NLTK and other library.Can anyone guide me the approach I should use."
1048,"Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words","['python', 'python-3.x', 'nlp', 'nltk', 'chatbot']","I am making a chatbot using Python. 
Code: It is running well but with every conversation it's giving this error:These are some conversations from CMD:ROBO: a chatbot is a piece of software that conducts a conversation via auditory or textual methods.what is indiaROBO: india's wildlife, which has traditionally been viewed with tolerance in india's culture, is supported among these forests, and elsewhere, in protected habitats.what is chatbotROBO: a chatbot is a piece of software that conducts a conversation via auditory or textual methods."
1049,Regular expression to match characters inside a word [duplicate],"['python', 'regex', 'python-3.x', 'nltk', 'nltk-book']","I am doing the Pig Latin text conversion:One of the requirements is putting qu together.What I did  re.findall(r'^qu', token)but it only accepts qu together if the word/token starts with quIf the word/token starts with aqu then it does not recognize qu together.What I want is to recognize qu no matter the placement, including if it starts with Qu.How do I fix this?"
1050,Getting chunk structures must contain tagged tokens or trees ERROR,"['regex', 'dictionary', 'parsing', 'nltk', 'grammar']","I am trying to create a dictionary in order to get quantities and type of food from a text in spanish. I have already tokenized, tagged the sentences and created a grammar with regexparser. My code follows:What I want to get is something like this for each sentence:Thanks in advance"
1051,Machine Learning: how to find missing info,"['python', 'jupyter-notebook', 'nltk', 'stop-words']",I am new to machine learning and can someone tell me what is causing this problem
1052,"I am getting TypeError: unhashable type: 'list', while trying to find the word frequency","['python', 'pandas', 'nlp', 'nltk']",I am getting when I am trying this piece of code:
1053,Issue in user input or text file data in sentiment analysis,"['python', 'nltk', 'sentiment-analysis', 'corpus']","I am new to Python-NLTK. I have written my code using movie reviews data set.
When I put hard coded sample text for sentiment analysis it is working fine but when I try to take user input or fetch the data from text file it shows alphabet level splitting.for e.g.
When sample text is hard coded like
[""Music was awesome"", ""Special effects are awesome""]
Then splitting is like a
Review : Music was awesome
Review : Special effects are awesome.But if I asked for user input or fetch the data from text file then it shows review as;
Review: M
Review: u
Review: S
Review: i
Review: c
Review: .#For text file Below is my sample code.#For user input Below is my sample code.plz guide.
Thanks!"
1054,"How to do corpus reading from .docx, .pptx, .pdf in Python?","['python', 'pdf', 'nltk', 'docx', 'corpus']","How to read files in such formats as .docx, .pptx, .pdf as a corpus? It's possible to read .txt with NLTK's PlaintextCorpusReader by doing something like:... to get a proper output.It's also possible to read single .docx and .pptx with textract (for instance) and .pdf with pdfminer (for instance), but it's impossible to read a number of files as a corpus.Is there an option to do so? How to assign process function of textract and extract_text of pdfminer to the CorpusReader?"
1055,How to search human names in a dataframe?,"['python', 'pandas', 'nltk', 'tokenize']",I am trying to search human name from the dataframe. I have a very large dataset where i need to tokenize everyword but human names should be full names. I am just creating this as an example.So i want the output like this:So far i am doing this:But i want full name when i do tokenizing
1056,Speech to Text recognition : Text Correction and Result Improvisation in Python,"['python', 'nlp', 'nltk', 'speech-recognition']","How can I achieve below result in Pythonreference text is : "" We wanted people to know that we've got something brand new and essentially this product is uh what we call disruptive changes the way that people interact with technology. ""Hypothesis text : "" We wanted people to know that how to me where I know and essentially this product is what we call scripted changes the way people are rapid technology. ""I have to consider this both text where reference text will act as ground truth and the hypothesis text is subjected to be modified based on the reference text.My final text should look like,"" We wanted people to know that how to me where I know we've got something brand new and essentially this product is uh what we call scripted disruptive changes the way that people are rapid interact technology.""I have used code sample box here to show that these texts are wrongly inserted and the Bold for the text to be inserted based on ground truth. But in a real scenario, I will use red color for the words which are wrongly inserted and green color for the words to be inserted based on ground truth reference.I tried to write logic by splitting the text into a list of elements and compare each element. but my logic seems not to work for all the sentences as there can be many possible combinations.Kindly suggest me if there are any libraries I can use to do this task.Thanks :)"
1057,Tokenizing the text and count in a dataframe based on other column,"['python', 'pandas', 'nltk']",I need to tokenize the data but seems really confusing. I have data like this:I need output like this:Is there any way to achieve output like this?
1058,StanfordDependencyParser does not consider numbers and punctuation (Python - NLTK),"['python', 'nltk', 'stanford-nlp', 'dependency-parsing']","I have a problem with the StanfordDependencyParser provided by NLTK.
If I write something like this:The output of this code does not take into account numbers and punctuation. I get as output:I noticed that there are some nodes in which the field ""word"" is None. How can I force the parser to consider also the numbers and the punctuation into the final dependency tree?"
1059,"splitting words by syllable with CMU Pronunciation Dictionary, NLTK, and Python3","['python-3.x', 'nlp', 'nltk']","I am working on a natural language processing project and am stuck on splitting words into syllables (using nltk and cmudict.dict() in python 3).I currently count syllables by looking a word in my corpus up in the CMU Pronunciation Dictionary and counting the number of stresses in its list of phonemes. This appears to work pretty well.What I am stuck on is how to use this information to split the accompanying grapheme after counting, as I do not understand how to either translate the phonemes back to the graphemes (seems error prone) or use the list of phonemes to somehow split the grapheme.Here is the function I wrote to do this (word tokenization happens elsewhere):Just as an example, my current output is:How can I split the word angry, for example, into an - gry?"
1060,unable to import ’NLTK',"['python', 'visual-studio-code', 'import', 'nltk', 'chatbot']","I’m currently working on my first chatbot and I need nltk for this bot to install. So I opened my terminal on my Mac and type pip install nltk and it successfully installs. So I opened VisualStudioCode and I type import nltk but it replies: ""Unable to import 'nltk‘ ""Why is this?Thanks a lot"
1061,Python NLTK not classify correctly even with correct informative features,"['python', 'nltk']","I'm use NLTK to analyze a little dataset (1000 rows) of data from my system. All works as expected, buy when I try classify data, same strange happens. Look my ""show_most_informative_features"" result:This is my data for test: I expected the result to be 1114, because I use top 5 words with best match with this classification. Buy my classifier only classify all as ""1790"". My code works with another dataset more simple, but using this dataset this no make sense.If I print prob_classify() this is a result:CODE:RESULT:And my qualificacao_id.value_counts():Any idea why this happens?"
1062,Counter to return null-value if Part of Speech tag not present,"['python', 'nltk', 'counter', 'part-of-speech']","Currently i am trying to count the instances a certain part of speech occurs in a given online review. While i am able to retrieve the specific tags corresponding to each word, and count these instances, i face difficulties in also capturing the null-values (if the tag is not present = 0). Ideally, i would have a list of all the tags with either the count of actual occurrences in the review, or if it is not present = 0.   I use NLTK's POS tagger. The following code will get me the specific tags per review, but thus only specifically to the tokens in the review:I tried to make a separate list with some specific tags (goal was to achieve all the verbs and nouns) but it still only returns only those with actual values (1 or more) and not those with 0 (not present in text). I could potentially insert all the available tags in there however it would thus only return actual values. For instance: So in the above example output would be:But i want Thanks for the help!Edit 2:
Code that worked in the end (thanks to manoj yadav):"
1063,nltk.tree fromstring() parses in wrong order,['nltk'],"I am trying to use nltk.tree.Tree with its fromstring() method. My use-case is that I only want tree which has 1 level of nesting, for example for the sentence:I want to parse trees of the structures:And so on.However, when I try to use nltk.tree.Tree.fromstring() on the above strings, the subtrees are pushed to the end incorrectly:How should I format the string so I can get the tree with the correct order of subtrees?"
1064,Python-2.7 NLTK saving and opening files with data from class type nltk.Text,"['python-2.7', 'nltk']","I am in the process of using NLTK to analyse some very large text files (ie 2GB of plain text) which will hopefully become part of my PhD thesis (not in computer science - I'm not a software engineer).
The workflow for this type of process is to convert from ASCII (str) to tokens (list) to nltk.Text (class).
Because the files are very large and take some time to process, I would like to be able to save the files at various stages of this workflow in order to not have to repeat the processing during subsequent sessions.
Converting and saving the ASCII and tokens is not a problem, and neither is saving the nltk.Text data:This enters each item in the text object onto a new line. The problem I have is reading back this data. I can instantiate an object of class nltk.Text thus:but if I then try to construct a loop to put the data back into the object:what ends up being entered into the text object is the number of lines in the testdata.txt file. As you can see I am addressing this class type as though it is a 1 dimensional array, but it isn't, and the reason I'm guessing is because the documentation is either poor or non-existent (well I can't find it anyway). I would be very grateful for any pointers, thanks."
1065,How can we do a sentiment analysis and create a 'sentiment' record next to each line of text?,"['python', 'python-3.x', 'nltk', 'sentiment-analysis']","I Googled for some solutions to do sentiment analysis, and write the results to a column next to the column of text that's being analyzed. This is what I came up with.This gives me:Now, I am trying to feed in my own column of text, from a dataframe.The sample code is from this site.https://programminghistorian.org/en/lessons/sentiment-analysisI have a field in a dataframe that consists of text, like this.These are 5 individual records from my dataframe. I'm trying to think of a way to assess each record as 'positive', 'negative', or 'neutral', and place each sentiment in a new field in the same row.In this example, I would think these 5 records have the following 5 sentiments (in a field next to each record):How can I do that?I came up with an alternative sample of code, as shown below.This ran for about 15 minutes, then I cancelled it, and I saw that it actually did nothing at all.  I want to add a field named 'sentiment' and populate it with 'positive' if scores[""compound""] >= 0.05, 'negative' if scores[""compound""] <= -0.05, and 'neutral' if scores[""compound""] >= -0.05 and scores[""compound""] <= 0.05."
1066,Python with NLTK shows error at sent_tokenize and word_tokenize,"['python', 'nltk']","I am using Google Colab to work on a script that I learn through video. Unfortunately, I get an error though following the video instructions. causes a problem. Both lines. I have tried each of it standing alone, in Python 3 (which I use mainly).
Here are the imported libraries:The error I get is:Frankly spoken, I don't understand the error.What's the problem that I don't see?--
Here's the whole code The solution:
adding nltk.download(""popular"") after import nltk"
1067,how do I parallelize a parts of speech tagging function in databricks (python/pyspark),"['python', 'pyspark', 'runtime', 'nltk', 'databricks']","Right now, I have ~5000 rows of substantial text data that I need to do POS tagging on. I first tried by using standard python code to do this, but the run time was over 2 hours (see code below) After this, I tried parallelizing it by using a pandas_udf:This second method took even longer (~3.5 hours) strictly due to the toPandas() call at the end.  Is there another way to do this with a significantly shorter runtime?"
1068,Error in loading NLTK resources: “Please use the NLTK Downloader to obtain the resource:\n\n”,"['python', 'nltk', 'tokenize', 'word2vec']","I adapted the following code from Susan Li's post, but incurred an error when the code tries to tokenize text using NLTK's resources (or, there could be something wrong with ""keyed vectors"" loaded from the web). The error occurred on the 5th code block (see below, might take a while to load from the web):Update on part 5 (per @luigigi's comments)This should work."
1069,Find the most frequent word pair from a list of messages in python,"['python', 'nltk']","i have a list of 100 messages. And i am able to find the most frequent words used in the list of messages.
But i want to find the pair of words which occur most frequently.
For example, key and board are being shown as most frequent words. But i need to find the number of occurrences where 'key board' are used as a pair in NLTK.
Here abstracts are the list of sentences and abstract words are list of words.Here i am able to plot the individual top 10 words. But need to plot combination of words which are most frequent.                                                        "
1070,"Python, Pandas, and NLTK Type Error 'int' object is not callable when calling a series","['python', 'python-3.x', 'pandas', 'dataframe', 'nltk']","I am trying to get the word frequencies for terms within each tweet contained in a dataframe. 
This is my code:And this is my error and traceback:Name: text, Length: 14640, dtype: object  Traceback (most recent call last):File """", line 1, in 
      runfile('C:/Users/leska/.spyder-py3/untitled1.py', wdir='C:/Users/leska/.spyder-py3')File
  ""C:\Users\leska\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"",
  line 827, in runfile
      execfile(filename, namespace)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"",
  line 110, in execfile
      exec(compile(f.read(), filename, 'exec'), namespace)File ""C:/Users/leska/.spyder-py3/untitled1.py"", line 30, in 
      data['words']= tweets.apply(nltk.FreqDist(tweets))File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\series.py"",
  line 4018, in apply
      return self.aggregate(func, *args, **kwds)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\series.py"",
  line 3883, in aggregate
      result, how = self._aggregate(func, *args, **kwargs)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\base.py"", line
  506, in _aggregate
      result = _agg(arg, _agg_1dim)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\base.py"", line
  456, in _agg
      result[fname] = func(fname, agg_how)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\base.py"", line
  440, in _agg_1dim
      return colg.aggregate(how, _level=(_level or 0) + 1)File
  ""C:\Users\leska\Anaconda3\lib\site-packages\pandas\core\series.py"",
  line 3902, in aggregate
      result = func(self, *args, **kwargs)TypeError: 'int' object is not callableI have verified that the type of data.text is a Pandas series.I had tried a solution earlier that I thought worked that used tokenizing and creating a wordlist to get the word counts, but it resulted in a frequency distribution for all the tweeets rather than each one. 
This was the code I had tried based on my earlier question:I really need to the term and counts for each tweet and I am stumped as to what I am doing wrong."
1071,Pandas NLTK tokenizing “unhashable type: 'list'”,"['python', 'pandas', 'nltk']","Following this example: Twitter data mining with Python and Gephi: Case synthetic biologyCSV to: df['Country', 'Responses']I can get through step 1 and 2, but get an error on step 3:I believe it's because I'm working in a dataframe and have made this (likely erronous) modification:Original example:My code:My full code:There are many questions on unhashable lists, but none that I understand to be quite the same.
Any suggestions? Thanks.TRACEBACK"
1072,How to retrieve the main intent of a sentence using spacy or nltk?,"['nlp', 'nltk', 'spacy', 'pos-tagger', 'dependency-parsing']","I have a use case where I want to extract main meaningful part of the sentence using spacy or nltk or any NLP libraries.Example sentence1: ""How Can I raise my voice against harassment""
Intent would be: ""raise voice against harassment""Example sentence2: ""Donald Duck is created by which cartoonist/which man/whom ?""
Intent would be: ""Donald duck is created by""Example sentence3: ""How to retrieve the main intent of a sentence using spacy or nltk"" ?
Intent: ""retrieve main intent of sentence using spacy nltk""I am new to dependency parsing and don't exactly know how to do this. Please help me."
1073,Incorporating additional numeric features into text classification model,"['python', 'nltk', 'logistic-regression', 'feature-engineering']","I modified some Python code from github to run logistic regression on a subset of consumer complaints data using the following code, the text vectorization and classification parts work smoothly. But I am wondering if it's possible to also include non-text, binary numerical indicators, such as timely_response and consumer_disputed. as features (alongside text vectors)?
However, when I did this, Python returns an error saying that I have input variables with inconsistent numbers of samples.which returns the following error messageWill be really grateful if someone could shed some lights on this."
1074,"Parse large text document, to keep only “account number”, and a specific keyword (“Market Value”)","['python', 'python-3.x', 'nltk']","I have a large textdocument (~20000 rows), The body of which looks something like this : Blocks like this are repeated thousands of times. 
Attempting Output: As I have never attempted something like this before, I have two questions:
1. How to identify and keep a sentence like this : Which has no fixed length?Is it wise to use nltk for this? Or can it be handled with regular expressions and string processing? "
1075,How to tokenize words and input them into another file?,"['python', 'nltk', 'tokenize']","I can only get stop words to implement into the document and then create a new file with the stop words removed. I cannot get word tokenize, porterstemmer, or sent tokenize to process.This is the part I cannot get to execute into the new txt file. This part create the new file"
1076,How to add missing word to nltk WordnetLemmatizer?,"['python-3.x', 'nlp', 'nltk', 'wordnet', 'plural']","I'm trying to do some basing textmining stuff on current research texts. I have an existing code base I used with previous researches, utilizing a WordNetLemmatizer from nltk to - among other things - get the singular form of plural nouns. This has been working pretty good so far.Now I found a noun WordNetLemmatizer doesn't know.The result is:'coronaviruses'The output I'd expect would be:'coronavirus'It works just fine with viruses:'virus'Can I somehow add coronaviruses / coronavirus to wordnet?"
1077,Using a text classifier model vectorized done using Tfidf with unigrams and bigrams,"['python', 'machine-learning', 'nlp', 'nltk', 'n-gram']","I have the following data frame or a text classification problem:clean_text does some of the basic cleaning required for this particular case like removal of special characters, converting numeric values in the text to buckets etc. As a next step - I create a pipeline as shown below:This model was tested using test data, and saved using pickle libraryUpon using this model on real world data, I use pickle again to load the model, and then for my real world text I apply cleaning using the same clean method as shown below.I have a concern regardin the next step: Can I code, as shown below, directly to do the prediction? I haven't created bigrams here though the tfidf vectorizer was given ngram_range(1,2), text_cleaned contains just the cleaned text. 
     text_clsf.predict([text_cleaned])Or is it that the model text_clsf by itself takes care of creating bigrams."
1078,How to stop Chatterbot training files downloading and displaying logs everytime?,"['python-3.x', 'nltk', 'chatbot', 'corpus', 'chatterbot']",Im building a chatbot. How to stop it from being trained everytime i run the program?My program :Im getting the following Log message everytime I run the above program.I can see that the nltk_data & conversations are being downloaded everytime I run the program. It takes 5-10 seconds to download and ask for user input. I understand the importance of keeping it up to data. But I dont want it to update everytime unless I wish to do manually. Is there anyway I can stop downloading the nltk_data and conversations.yml file everytime? (because my main program can't wait for 5-10 seconds for the chatbot to load)Is there anyway I can stop printing these logs on console?
1079,Understanding Hate-Speech classification with Python. Currently stuck with “tokenization” (TweetTokenizer),"['python', 'pandas', 'nltk', 'tokenize', 'sentiment-analysis']","i'm still very new to Python, and for a University project I am doing classification of hate speech on twitter. I have been following various guides on how to set up some basic sentiment analysis etc, but I am now going back through some working code, and beginning to build on top of it. I am currently trying to use NLTK's ""Tweet Tokenizer"" import to apply to my tweet dataset, as part of prepping my data to be modelled . However I am getting an error:My code as it stands looks like this (I know some of the imports aren't currently used, they have just been imported ready for me to use as I work through things, also the ""pipeline"" block of code with CountVectorizer etc is what I will be replacing once this tweet tokenizer works)If anyone could figure out how to get the tweet tokenizer to work with the code that I have got, as well as any other general advice going forward in regard to classifying hate speech/modelling the results that would be greatly appreciated.Edit*this is the line that I have attempted to tokenize withThank you a lot!"
1080,Python NLTK scores from 0 to 10,"['nltk', 'sentiment-analysis', 'textblob', 'vader']","I have .txt files IMDB text based movie review. I want to compare this review with star based review from IMDB.
I want to calculate the review's NLTK score to something like 0 to 10 scores (example: 7.5/10) through python NLTK (either textblob or vader). 
how can I calculate the score from NLTK to 0 to 10 score? "
1081,How can I fix this n-gram extractor in Python?,"['python', 'nlp', 'nltk', 'n-gram']","I've made an n-gram extractor that pulls organization's names from texts. However, the program only pulls the first letter of the first word and the last word. For example, if the phrase ""Sprint International Corporation"" appears in the text, the program will return ""s corporation"" as the n-gram. Do you know what I'm doing wrong? I've posted the code and output below. Thanks.This is the code for the n-gram extractor.Here is the text that I analyze and the program I use to analyze it.Here is the current output.This is what I want to output to look like."
1082,How to classify derived words that share meaning as the same tokens?,"['python', 'nlp', 'nltk', 'text-mining']","I would like to count unrelated words in an article but I have troubles with grouping words of the same meaning derived from one another.For instance, I would like gasoline and gas to be treated as the same token in sentences like The price of gasoline has risen. and ""Gas"" is a colloquial form of the word gasoline in North American English. Conversely, in BE the term would be ""petrol"". Therefore, if these two sentences comprised the entire article, the count for gas (or gasoline) would be 3 (petrol would not be counted).I have tried using NLTK's stemmers and lemmatizers but to no avail. Most seem to reproduce gas as gas and gasoline as gasolin which is not helpful for my purposes at all. I understand that this is the usual behaviour. I have checked out a thread that seems to be a little bit similar, however the answers there are not completely applicable to my case as I require the words to be derived from one another. How to treat derived words of the same meaning as same tokens in order to count them together?"
1083,how to tokenize a text by nltk python,"['nltk', 'tokenize', 'word']","i have a text like this:i tokenize this text with word_tokenize in python and output is:But as you can see, the second line outputs several words that are dotted together. How to separate these as a Word?!i use this python code:and In fact, I want all words to be separated like this:"
1084,"NLTK suddenly not working in anaconda, jupyter notebook","['python', 'import', 'jupyter-notebook', 'anaconda', 'nltk']","When I want to import nltk, this error happens.Be careful, it doesn't say the nltk module doesn't exist, it says it has no download attribute. it used to work just fine, I already uninstalled anaconda and reinstalled it. I tried uninstalling nltk package by using pip uninstall, but it takes ages, like hours, and I'm not sure it's actually working, because there are no results even after hours. I'm using windows. I'm new to programming, in the phase of learning. "
1085,Count words (even multiples) in a text with Python,"['python', 'python-3.x', 'nltk']","I have to write a function that counts how many times a word (or a series of words) appears in a given text.This is my function so far. What I noticed is that with a series of 3 words the functions works well, but not with 4 words and so on. "
1086,Error while running Anaconda3 :Can't find or load QT,"['qt', 'anaconda', 'nltk', 'spyder']","I installed Anaconda 3: ""Anaconda3-2019.10-Windows-x86_64"" , but when I try to run Spyder it shows me this: I've moved the file Anaconda3\Library\plugins\platforms to Anaconda3\platforms but nothing happend.
And when I tried conda update --all it shows me this:
""PS C:\Users\HP£\Anaconda3> conda update --all
Collecting package metadata (current_repodata.json): failedUnavailableInvalidChannel: The channel is not accessible or is invalid.
  channel name: nlkt*
  channel url: https://conda.anaconda.org/nlkt*
  error code: 404You will need to adjust your conda configuration to proceed.
Use conda config --show channels to view your configuration's current state,
and use conda config --show-sources to view config file locations.
""Can anyone tell me how to solve this? 
(excuse my bad english)"
1087,Text being written to single line when removing stopwords from columns,"['python', 'nlp', 'nltk', 'stop-words']","I'm trying to remove stopwords from a tab-delimited .txt file using the following code: The code executes successfully, but when I view the results all of rows have been re-written onto a single line. How can I maintain the columns while removing the stopwords? I found the following solution on a similar post: But inserting a new line simply created a new line after every word so that if I started with a row like this:the results looked like this: and I need the results in rows like so:I've been searching a while, but haven't found any solutions. "
1088,Parsing city of origin / destination city from a string,"['python', 'regex', 'pandas', 'nlp', 'nltk']","I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').The data:This should result in:Thus far I have tried:
A variety of NLTK methods, but what has gotten me closest is using the nltk.pos_tag method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks."
1089,How to find frequency of repeated sentence in a file,"['python', 'python-3.x', 'dataframe', 'nltk', 'word-frequency']","I have dataframe where I need to find the top 20 repeated sentence using Python, Please let me know how to go about itExpected OutputCode so farI need the result in a dataframe which can be written to CSV with ""Column A"" and ""Frequency"" columns in the final result"
1090,Stop sentence tokenizer from splitting sentence on “no.” abbreviation,"['python', 'nlp', 'nltk', 'gensim']","I am trying to tokenize the following sentence type: ""The item at issue is no. 3553.""Every tokenizer I've tried so far returns the following (including a Punkt tokenizer trained on my corpus):[[""the"", ""item"", ""at"", ""issue"", ""is"", ""no.""], [""3553.""]]Adding a ""no"" abbreviation to the tokenizer would be a problem for sentences ending in ""no."""
1091,Error when using string.punctuation to remove punctuation for a string,"['python', 'nltk', 'punctuation']","Quick question: I'm using string and nltk.stopwords to strip a block of text of all its punctuation and stopwords as part of data pre-processing before feeding it into some natural language processing algorithms. I've tested each component separately on a couple blocks of raw text because I'm still getting used to this process, and it seemed fine. However, when I apply this function to the text column of my dataframe – it's text from a bunch of Pitchfork reviews – I can see that the punctuation isn't actually being removed, although the stopwords are.Unprocessed: Processed: Any thoughts on what's going wrong here? I've looked through the documentation, and I haven't seen anyone who's struggling with this problem in the exact same manner, so I'd love some insight on how to tackle this. Thanks so much! "
1092,How to analyze specific string of text from PDF in python-3?,"['python', 'python-3.x', 'text', 'nltk', 'text-classification']","I am working on some code that identifies named entities (NERs) in PDF documents. My current code works in three steps. First, it turns the PDF into a text string. Second, it tokenizes the text. Third, it classifies the text.Right now, this code classifies every token (word) in the text string. However, I want the program to classify only a certain portion of the text. The portion is always located between the words ""Body"" and ""Classification"" (for those who are familiar with the format, I am analyzing LexisNexis documents). I am wondering if there is a way to tell the program to only classify the text between these two words? I've read several articles on this, but I have not been able to find an answer to my specific question.I have the feeling that I need to insert a line identifying the specific string between the ""tokenized_text"" and ""classified_text"" line, but I'm not sure what. Thanks for your help!Edit:Here is a simpler code to work from. Note, this program will not run unless you have the Stanford tagger (St) downloaded and set the file location."
1093,Python: Clustering Search Keywords,"['python-3.x', 'nlp', 'nltk', 'cluster-analysis']","I have a lot of ""Search Keywords"" for every product in the dataset. I try to cluster products according to their ""Search Keywords"".What I'm looking to do is cluster these keywords into clusters of ""similar meaning"", and create a hierarchy of the clusters (structured in order of summed total number of searches per cluster).An example cluster - ""women's clothing"" - would ideally contain keywords along these lines: women's clothing, 1000 ladies wear, 300 women's clothes, 50 ladies' clothing, 6 women wear, 2.I'm a beginner in NLP. Do you have any suggestions of NLP techniques for this task? Any help will be highly appreciated :-)"
1094,Using TfidfVectorizer with Punkt in Cloud Function,"['python', 'google-cloud-functions', 'nltk', 'tmp', 'tfidfvectorizer']","My current understanding of TfidfVectorizer is it requires nltk.download(""punkt"") to be run before transformation on input data, as all the default tokenizers are available in punkt. Currently, because I use TfidfVectorizer in my Cloud Function, I run nltk.download(""punkt"") inside of the Cloud Function container, which downloads punkt to /tmp. My issue with this is that I can't guarantee access to the same file system contents for each invocation of my Google Cloud Function because ""subsequent calls to the same function will sometimes execute in a different container, so they'll have different /tmp mounts. So you can't use /tmp to communicate between functions"" (from this SO question). This leads to punkt needing to be redownloaded anytime the container is switched, and this shows up in the logs of my Cloud Function.I tried creating a tokenizer deserialized from english.pickle that is part of punkt. Even when passing in this custom tokenizer's tokenize function as tokenizer to TfidVectorizer, the transformation of input data later on ends up failing because of the missing punkt download. Is there anyway to download punkt into Python's available memory, such that it doesn't get stored in the file system and get wiped when the container is switched? It seems like I need punkt to be downloaded into the file system regardless of whether I pass in a custom tokenizer or let TfidfVectorizer choose its own default tokenizer."
1095,Syntax error when lemmatizing column in pandas,"['python', 'pandas', 'nltk', 'lemmatization']","I am trying to lemmatize words in a particular column ('body') using pandas. I have tried the following code, that I found hereWhen I attempt to run the code, I get an error message that simply saysI have also tried the solution presented in this post but didn't have any luck. UPDATE: this is the full code so far"
1096,Boucle in python [duplicate],"['python', 'arrays', 'nltk']","I would like to know something in python. I'm searching to compare an array  with 20 elements. I want calculate the distance between the first word of my array with the 2nd and the 1st word  with the 3rd  and the first word with the 4th ... into 20th. For the moment I got it. But I want do more and compare the 2nd element with 3th and 2nd element with 4th  and 2nd  with 5th, and after the 3th with  4th and the 3th with 5th and 3th with 6th......This is my code for the moment:"
1097,spring boot call machine learning python script,"['java', 'python', 'spring', 'machine-learning', 'nltk']","I developed rest API project in java with spring framework 
and I also developed machine learning for spam detection in python with nltk library
it gets a string and detects if its spam or notnow I want to call my machine learning in my spring framework but I don't want that in each call the machine learning will train and validate itself from the.so I have two questions can I save my machine learning model (I saw something with pickle but didn't understand if this is what I need)and the second question is how can I call to my machine learning from my spring project"
1098,How do I determine which text in a corpus contains an error generated by the NLTK suite in Python?,"['python', 'error-handling', 'nltk', 'corpus']","I am trying to do some rudimentary corpus analysis with Python. I am getting the following error message(s):My assumption is that there is a UTF error in one of the 202 text files I am looking at.
Is there any way of telling, from the error messages, which file or files have the problem?"
1099,Choose one of topics for a sentence in python,"['python', 'nlp', 'nltk']","I have several topics to choose from, for example: Casinos, Museums, Nature, Nightlife, Spa. I would like to write a program, which colud choose the most matching of those topics for a given sentence. For example for a sentence ""I like art."" it should choose Museums. Is there any way to do that in Python?"
1100,Why doesn't work Polyglot that I installed with git or the wheel files?,"['python', 'module', 'nltk', 'detection', 'polyglot']","I tried install Polyglot module to determine the language like this on Python. But it doesn't work.Are there solution?Or is it possible not to work?My error message is;ModuleNotFoundError                       Traceback (most recent call last)
 in 
----> 1 import polyglotModuleNotFoundError: No module named 'polyglot'"
1101,Sklearn using natural language processing with numerical data,"['python', 'pandas', 'scikit-learn', 'nltk']","I'm using sklearn for a project and I have two columns to use to predict. One column is text, which is a series of articles, and the other is equal_cnts, which is a real number. I am trying to create a model that trains on both the text and the numbers using SVM, but I'm having trouble figuring out how to use both features.I'm currently trying to do the above, where the Pipeline is intended to process the text and the model is being testing for accuracy against df[""empirical""]."
1102,How to strip string from punctuation except apostrophes for NLP,"['python', 'nlp', 'nltk']","I am using the below ""fastest"" way of removing punctuation from a string:However, it removes all punctuation including apostrophes from tokens such as shouldn't turning it into shouldnt. The problem is I am using NLTK library for stopwords and the standard stopwords don't include such examples without apostrophes but instead have tokens that NLTK would generate if I used the NLTK tokenizer to split my text. For example for shouldnt the stopwords included are shouldn, shouldn't, t.I can either add the additional stopwords or remove the apostrophes from the NLTK stopwords. But both solutions don't seem ""correct"" in a way as I think the apostrophes should be left when doing punctuation cleaning.Is there a way I can leave the apostrophes when doing fast punctuation cleaning?"
1103,Out of index in NLP,"['python', 'nlp', 'nltk']","I am trying to import sentences and then select each word and compare them with a certain word (here, it is 'play')The code is:But the code gets error after 3rd iteration with 'out of index' error.Can anyone help?"
1104,NLTK draw tree in non-blocking way,"['python', 'tree', 'nltk', 'nonblocking']","NLTK provides a feature that allows you to ""draw"" tree structures, e.g. a dependency parse. In practice, when you call tree.draw(), a windows will pop up (on Windows at least) with the drawn tree. Even though this is nice functionality, it also blocks, meaning that the execution of a script is blocked when the tree is drawn until you close the window of the newly drawn tree.Is there any way to draw trees in a non-blocking way, i.e. without them stopping a script's execution? I have thought about starting a separate process in Python that is responsible for drawing the trees, but perhaps there is a more straightforward way."
1105,How to store 100k frequency word list from NLTK to a database,"['python', 'python-3.x', 'nltk', 'word-list']","I would like to store about 100k frequency word list from NLTK and have it on my database, i really appreciate your help.I have already tried some methods and searched too much on google but got nothing that useful "
1106,"API calls from NLTK, Gensim, Scikit Learn","['python', 'api', 'nlp', 'nltk', 'gensim']","I plan to use NLTK, Gensim and Scikit Learn for some NLP/text mining. But i will be using these libraries to work with my org data. The question is while using these libraries 'do they make API calls to process the data' or is the data taken out of the python shell to be processed. It is a security question, so was wondering if someone has any documentation for reference.Appreciate any help on this."
1107,Memory Error : Most frequent word using nltk or split function,"['python', 'python-3.x', 'sqlalchemy', 'nltk']","I am finding the most frequently used words from a list of reviews. I am using Amazon ec2 instance which had 1GB Ram. I am using Flask, SqlAlchemy, and NLTK for it. Here is my code which runs properly on my pc(8GB Ram) but not works on a server.It gives memory error reviewinstr = regex.sub('', soup.text.lower()). del variable was not first there, i had put it after the error. I had used the memory_profiler and found the following output.Should I increase a ram over a server? or There is a better way to fulfill my need. del variable helps me to decrease ram usage but it is not much effective."
1108,Expanding Vader’s lexicon with expressions/sentences,"['python', 'nltk', 'sentiment-analysis', 'vader']",I’m currently using Vader through the nltk library but following a few experiments I realize that I need to add some specific expressions (parts of sentences) with a precise score so that the compound score may obtain an appropriate value (appropriate meaning going in the direction I feel is best).Whenever I give an example sentence like “… raises the  target”  I obtain a 100% neutral result while “… cuts the target” leads to a negative result.I tried adding sentences in the lexicon as follows but the results remain unchanged:In the source code (https://www.nltk.org/_modules/nltk/sentiment/vader.html) I found details about what they call “special case idioms”… I wondered whether I could have my expressions added without modifying this part of the source code.Would you have any suggestion ?Thanks.
1109,"In Python searching multiple words(60+ words) in a data set, My data set having 10000+ Text Messages, Using Spacy","['python-3.x', 'nlp', 'dataset', 'nltk', 'spacy']",I have to search about 60+ words in dataset having 10000+ text messages and each text message consists of about 200 words. I want count of matching words in each text message. Trying Spacy to resolve issue.
1110,Phrase Rephraser,"['python', 'nlp', 'nltk']","I'm writing a bot and instead of phrasing user facing communication the same way, I want to implement a certain degree of change in the language, while still maintaining the original intent. To do this, instead of a standard dictionary key-value pairs, such as user_response [""GREET""] = ""Hello, how are you?"" is currently replaced such:user_response [""GREET""] = [""Hello, how are you?"", ""What's up?"", ""Hi, all good today?""]and a random string is picked from the choices available.
The problem is, it'll take much too long to do this manually, for all the strings in the system.Question: Is there a way NLTK or other library can rephrase a given phrase?
There is a question from 2010 that is similar in intent, but didn't have much by way of responses or follow ups."
1111,Error using a self created corpus with nltk in python,"['python', 'nltk', 'corpus']","I want to do a simple sentiment analysis myself. To do so, I have a folder on my computer called ""Test Data Model FR"", in which I have two subfolders: ""pos"" and ""neg"". The ""pos"" folder contains positive reviews, while tje ""neg"" folder contains negative reviews. I used the CategorizedPlaintextCorpusReader from ntlk to create a corpus based on that folder. This worked. Thereafter, I wanted to convert the corpus to a ""document"" which I could use for further analyses.
However, when I run the code, I get an error: TypeError: 'CategorizedPlaintextCorpusReader' object is not callable.This is my code:The error shows up when I run the 'documents = ...' part.
Can someone help me out? Why is the object not callable? Thank you."
1112,Python - getting all forms of a word,"['python', 'nltk']","I would like to get all forms of a word using python.For example, Africa would generate: ""Africa"", ""Africanism"", ""Africanist"", ""African"", ideally even ""afro-"" as a prefix.Is there any library which can accomplish this? I have tried both lemminflect and nltk WordNetLemmatizer and they aren't achieving what I want. I believe this is because these aren't technically inflections of ""Africa""."
1113,How to install NLTK modules when push app using IBM Cloud Foundry CLI?,"['ibm-cloud', 'nltk', 'ibm-cloud-functions']","My application requires some corpora to work, I added a nltk.txt file at the root of the application containing a corpora name per line. For instance:punkt
stopwords
wordsI have learning wiht this tutorial https://docs.cloudfoundry.org/buildpacks/python/index.html I got that ERROR:Logs:Thank you for help"
1114,How do I classify new text using a classification model built in a different project?,"['python', 'machine-learning', 'scikit-learn', 'nltk', 'text-classification']","In the first project I have trained and pickled a classification model that uses bag of words with 2500 features, but in this new project I want to actually classify new text.How do I classify new text?This is what I'm doing:This is the error generated:X has 8 features per sample; expecting 2500Indeedbut I want the original feature labels in the same order of when the model was created and trained.Should I pickle the original array of features and by hand rebuild a new bag of words table for the new text that I want to classify?"
1115,How to substitute gender pronouns in a large corpus of text?,"['python', 'nlp', 'nltk']","I am writing a novel and half-way through it I decided to change an important protagonist from male to female. I wrote some simple Python code thinking I could change the pronouns for that particular character easily, but it changed the pronouns of all the characters accidentally. Here's some example text from the novel:I wished to change Joe (male) to Jane (female) and wrote the following simple code:But the above code changes the pronouns for both Joe and John to female. I now realize that I'll have to use NLP to do this, but is there a module or an algorithm to do this?"
1116,NLTK data download hangs on MacOS in Anaconda environment,"['macos', 'download', 'nltk', 'macos-mojave']",Running the following in a fresh jupyter notebook session or straight from the commandline:freezes the MacOS login session and requires repeated login instead of popping up UI for choosing the data packages needed.Environment:How to workaround this?
1117,Execute nltk.stem.SnowballStemmer in pandas,"['python', 'pandas', 'nlp', 'nltk']","I have a four column DataFrame with two columns of tokenized words that have had stop words removed and converted to lower case and am now attempting to stem.  I'm not sure if the apply() method accesses the series plus its individual cells or if I need another way of stepping into each record so tried both (I think!)from nltk.stem import SnowballStemmer
stemmer = nltk.stem.SnowballStemmer('english')df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(item) for item in x)--------------------------------------------------------------------------- TypeError                                 Traceback (most recent call
  last)  in ()
  ----> 1 df_2['Headline__'] = df_2['Headline'].apply(lambda x: stemmer.stem(item) for item in x)~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], Series):pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()TypeError: 'generator' object is not callableI believe this TypeError is similar to the one that says 'List' object is not callable and fixed that one with the apply() method and out of ideas here.  df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
  ----> 1 df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], Series):pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer() in (x)
  ----> 1 df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\nltk\stem\snowball.py
  in stem(self, word)    1415     1416         """"""
  -> 1417         word = word.lower()    1418     1419         if word in self.stopwords or len(word) <= 2:AttributeError: 'list' object has no attribute 'lower'"
1118,fixing maximum match algorithm while tokenizing a low resouce languge using ntlk,"['python', 'nltk']","I am working to tokenize a low resource language, but while running the programme I am getting a return outside error.
 here is the code:The error is SyntaxError: ""return"" outside function"
1119,Adding Unknown words to Gensim dictionary and teaching the model,"['python', 'nltk', 'gensim', 'tf-idf']","I am trying some unknown word and it give out 0% by putting ""Polytechnic"", ""Diploma"" that dictionary does not even have and i try to find sources that are able to add words into dictionary that i find are not able to findHere is my function of code that i am callingSome function i added is to call nltk which is working.
And I am new to this gensim coding i really need help."
1120,python: getting all pairs of values from list,['python'],"I'd like a tuple of each pair of values from a list, e.g.[1,2,3,4]Would yield:This seems very one-line recipe ish but I can't quite get it to work."
1121,How to get the close words in WordNet in python,"['python', 'nlp', 'nltk', 'text-mining', 'wordnet']","I am using WordNet as follows to get the synsets using python.However, the word alzheimer does not seem to be in WordNet as I get an empty synsets list. Then, I tried different other variants such as alzheimer disease, alzheimer's disease, alzheimers, alzheimer's, alzhemimers disease.My question is; is it possible to get the word close to the word alzheimer in WordNet, so that I do not need to manually verify what is the term in WordNet to get the synsets.I am happy to provide more details if needed."
1122,find bigrams in pandas,"['python-3.x', 'pandas', 'nlp', 'nltk']","I have a DataFrame with 4 columns: 'Headline', 'Body_ID', 'Stance', 'articleBody', with 'Headline' and 'articleBody containing cleaned and tokenized words.  I want to find bi-grams using nltk and have this so far:I'm having trouble with the last step of applying the articleBody_biGram_finder with bigram_measures. I've tried multiple iterations of lambda with list comprehension but am getting nowhere.   my most recent attempts:df_2['articleBody_scored'] = score_ngrams(bigram_measures.raw_freq) for item in articleBody_biGram_finderdf_2['articleBody_scored'] = articleBody_biGram_finder.apply(lambda x: BigramCollocationFinder.score_ngrams(bigram_measures.raw_freq))"
1123,What is the idea or algorithm behind finding n-gram in NLTK?,"['python', 'nlp', 'nltk', 'n-gram']","I am using Python NLTK package to generate 2-gram and 3-gram from my corpus. But I can't find how NLTK can generate them from a corpus.I found this one here: An Introduction to N-grams: What Are They and Why Do We Need Them?, but I wonder if there are any other algorithms to find n-grams. And does NLTK use the algorithm in this article to find n-grams?And as always, thank you so much."
1124,How to perform Kneser-Ney smoothing in NLTK at word-level for tri-gram language model?,"['python', 'nlp', 'nltk', 'pytorch', 'trigram']","I am trying to train a tri-gram language model on a text corpus and want to perform KN smoothing. Apparently, the 'nltk.trigrams' does this at character-level. I was wondering how I would be able to do this at word-level and also perform KN smoothing. Here is a piece of code that I wrote and doesn't work: I get the error:"
1125,How to restore punctuation using Python? [closed],"['python', 'nlp', 'nltk', 'lstm']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 7 months ago.I would like to restore commas and full stops in text without punctuation. For example, let's take this sentence:And I would like to detect that there should be 1 commas and 1 full stop in the above example:Can anyone advise me on how to achieve this using Python and NLP concepts?"
1126,"With NLTK, is a bigram tagger work like an HMM tagger?","['python', 'nltk', 'n-gram', 'hmmlearn']","I have been trying to do a simple comparaison between bigram tagger and HMM tagger. And i get near the same result. I've read the documentation of the bigram tagger and it's like the description of an HMM tagger. Here is my code:It will be very kind if someone can explain me how bigram tagger work and why it get the same result as an HMM tagger.
Thank you in advance for your answers"
1127,Iterating Through Tuples Produced by NLTK Pos Tagger,"['python', 'nltk']","I have tagged sentences like this:I am passing one sentence from this list at a time (like process(sentence[1])) to this function:But getting this error:My main idea with this function is that for each sentence it takes, it should look at all tuples it has, if a tuple's tag (tup[1] in code) is 'NN' we store it in ac, and so on. Help appreciated."
1128,is there a more efficient way to iterate over a dataframe?,"['python', 'pandas', 'dataframe', 'nltk']","I am using the above code, in order to process all rows and extract keywords from each row from the column bookTitle and then insert them as a list into a new column named Keywords on the same row. The question is if there is a more efficient way to do this without iterating over all rows because it takes a lot of time. Any help would be appreciated. Thanks in advance !Solution by Changming:"
1129,Converting text from first person to second person with issues ignoring text within quotes,"['python', 'python-3.x', 'nltk', 'google-colaboratory']","I am trying to convert stories/sentences/words/etc from first person to second person grammer, but trying to not convert text within quotes "" "" or ' '.It reads a file from google drive and makes it a string. 'forms = {..}' is the list of words to convert. It converts the words, and ignores words within quotations. The output text is: 
The bottom line is that if you was going to tell anyone about the frog, it would be Soy . you decided that our walk home would be the most opportune time . “ Did you see anything outside today during math?”?” I asked Soy as we started walking . “ “ What do you mean? Like in the sky?”?” he asked, jumping over cracks in the sidewalk . “ “ I mean right outside the window . Like right up against it ,” ,” I answered . “ “ Like a person?”?” he asked, still hopping . Soy sat in the row farthest from the window, so it was possible, but unlikely, for someone to walk by without him noticing.Issue: So the code converts text from first person to second person, except for words within quotations. However after identifying quotations, it adds "" to the text when it sees "". It's also adding extra punctuation when it sees punctuation (? ! . , ). I need help with this issue. "
1130,"Issue converting text from first person to second person, while ignoring text within quotations “ ”","['python', 'python-3.x', 'nltk', 'google-colaboratory']","I am trying to convert stories/sentences/words/etc from first person to second person grammer, but trying to not convert text within quotes "" "" or ' '.This is being run in google colab, python 3 notebook. Code reads a file in my googledrive, reads the .txt file, converts the words from file through 'forms =' from first person to second person. There is also an issue where spaces are inserted before and after quotes ("" and ' are affected) after conversion takes place.The story I input:The bottom line is that if I was going to tell anyone about the frog,
  it would be Soy. I decided that our walk home would be the most
  opportune time. “Did you see anything outside today during math?” I
  asked Soy as we started walking. “What do you mean? Like in the sky?”
  he asked, jumping over cracks in the sidewalk. “I mean right outside
  the window. Like right up against it,” I answered. “Like a person?” he
  asked, still hopping. Soy sat in the row farthest from the window, so
  it was possible, but unlikely, for someone to walk by without him
  noticing.It converts to:The bottom line is that if you was going to tell anyone about the frog
  , it would be Soy . you decided that our walk home would be the most
  opportune time . “ Did I see anything outside today during math ?” you
  asked Soy as we started walking . “ What do I mean ? Like in the sky
  ?” he asked , jumping over cracks in the sidewalk . “ you mean right
  outside the window . Like right up against it ,” you answered . “ Like
  a person ?” he asked , still hopping . Soy sat in the row farthest
  from the window , so it was possible , but unlikely , for someone to
  walk by without him noticing .The issue is that the text within quotes should NOT be converted.
Ex: I tell her, ""You are boring"". ---> You tell her, ""You are boring"".Ignore any grammer mistakes besides the quote issue, I will fix it later."
1131,Having trouble importing nltk into Python3,"['python-3.x', 'nltk']","When I run the command:My terminal outputs:But when I load up python3, I get:This is driving me crazy, and I'm sure I'm missing something very basic, I just don't understand what!"
1132,"Converting text from first person to second person, and ignoring quotes","['python', 'nltk']","So I have this:What do I add to prevent converting words within quotes "" ""Example: example: 
input is:          'I go to the movies, I say ""What do you want to see?"" 
Conversion I want: 'You go to the movies, you say ""What do you want to see?""
 Conversion I don't want: 'You go the movies, you say ""what do I want to see?"""
1133,How to tokenize a word containing punctuation using NLTK,"['python', 'nltk', 'tokenize']","I have a PlainTextCorpusReader, and the text is job ads scraped from the web. I want to strip out the skills using NLTK.But I have failed at the first hurdle as the reader is tokenizing the word 'C#' as 'C','#'.I do want to filter out the noise such as stopwords and punctuation, so this is a problem.How can I get around this?"
1134,How to get more synonyms using NLTK Wordnet?,"['python', 'nltk', 'wordnet']","So I am using this code to get the synonyms.Now, if I use Get_Syn('recieve'), I recieve an empty list (no synonyms). But, if I use 
Get_Syn('get'), this is the list I get: As we see, recieve is a synonym of get, but get is not a synonym of recieve. So how can I get get when I search for recieve. Is there way to map the two together?"
1135,Python nltk shows error when i try to identify nouns and verbs,"['python', 'python-3.x', 'nltk']","I try to identify nouns and verbs in Python. I used the nltk package and it shows me a yellow color error with a long red lettering.
my code:my error:can you help me? or there are anothers packages to recognize "
1136,"Given a root word, how can i get variations of it in Python?","['python', 'python-3.x', 'nltk']","I want all the variations of a word, given a root word. For example, given the word - transfer, I would like to get an array like this back - [transfering, transferred] etc. How do I do this in python? I feel it is like the opposite of stemming. "
1137,Lemmatization of pandas column using Wordnet after POS,"['python', 'pandas', 'nltk', 'wordnet', 'lemmatization']","I have a pandas column df_travail[line_text] with text.I want to lemmatize each word of this column.First I Lowercase the text :Then, I tokenize it and apply POS (because of wordnet default configuration which consider every word as noun).Then I have the following : (extract of the entire df_travail['tok_and_tag']However, then, I'm lost about the lemmatization function to apply (with Wordnet), in order to take into account the fact that I applied POS ?Edit : The following link doesnt mention POS part of my question
  Lemmatization of all pandas cells"
1138,Word tokenizing gives different results at home than on Colaboratory,"['nltk', 'google-colaboratory']","Local:Output at home is:Same code on Colaboratory, result is:I have run non-NLTK code in both places and gotten identical output. Could it be local NLTK libraries that are different? Different versions of the NLTK?"
1139,while cicle in nltk in py3,"['python-3.x', 'while-loop', 'nltk']","I have this task:
""Find the distribution of hapax as the corpus increases for
incremental portions of 1000 tokens (1000 tokens, 2000 tokens, 3000 tokens, etc.)""
i tought to use a while cicle in this way:but when i run it the interpreter give me:
""UnboundLocalError: local variable 'i' referenced before assignment""i tryed in various ways but i can't find the right one. How can i do it?"
1140,Lemmatize tokenised column in pandas,"['pandas', 'nltk', 'lemmatization']","I'm trying to lemmatize tokenized column comments_tokenized I do:but haveWhat can I do to lemmatize a column with bag of words?And also how to avoid the problem with tokenization that divides [don't] to [do,n't]?"
1141,Changing K mean clustering distance metric to canberra distance or any other distance metric on python,"['python', 'scikit-learn', 'nltk', 'k-means']","How do I change the distance metric of k mean clustering to canberra distance or any other distance metric? From my understanding, sklearn only supports euclidean distance and nltk doesn't seem to support canberra distance but I may be wrong. Thank you!"
1142,Get inertia for nltk k means clustering using cosine_similarity,"['python', 'nltk', 'k-means']",I have used nltk for k mean clustering as I would like to change the distance metric. Does nltk k means have an inertia similar to that of sklearn? Can't seem to find in their documentation or online...The code below is how people usually find inertia using sklearn k means.
1143,Prevent nltk sentence tokenizer from discarding text chunks not ending in punctuation,['nltk'],"I am using the NLTK sentence tokenizer on text that includes paragraph chunks that don't always end in a punctuation, and such chunks seem to just be discarded.Thus a text like:is tokenized as:It thus seems that any text preceding two or more newlines that doesn't end with a punctuation is discarded. I would like such chunks just to get tokenized as sentences.  Thus I would expect instead something like:I'm not sure of the best way of doing this. Perhaps adding \n\n as possible closing punctuation?"
1144,I can't import a file.txt in py3,"['python', 'python-3.x', 'nltk']","I'm trying to write a program on py3. I have saved 2 raw texts in the same directory as ""programm.py"" but the program can't find the texts.
I'm using emacs, and I wrote: but when I run it in py3 it doesn't print text1 (I used it to see if it works)Instead, if I ask to print in py3 it can't find the fileWhat can I do?"
1145,How do I obtain individual centroids of K mean cluster using nltk (python),"['python', 'nltk', 'k-means']","I have used nltk to perform k mean clustering as I would like to change the distance metrics to cosine distance. However, how do I obtain the centroids of all the clusters?I am trying to perform the k mean clustering on a pandas dataframe, and would like to have the coordinates of the centroid of the cluster of each data point to be in the dataframe column 'centroid'.Thank you in advance!"
1146,Word tokeinizing from the list of words in python?,"['regex', 'python-3.x', 'machine-learning', 'nltk', 'python-textprocessing']","my program has list of words and amongst that i need few specific words to be tokenized as one word.
my program would split a string into words eg output will be now I want is to tokenize words such as 'red blood cell' as a single word. There are many such words in my list that has three or more words to be considered as one such as 'platelet count','white blood cell',etc. any suggestion for doing that. "
1147,Word Cloud python library displays an apostrophe at the end of every word,"['python', 'nltk', 'word-cloud']","I used nltk.tokenize to tokenize a txt file and it generated a new file, let's call it ""File_B"". Then I run:This is the result:https://i.stack.imgur.com/RnoJ7.pngThere's an apostrophe at the end of every word, even if they're not there in File_B. What am I missing?"
1148,I can't bigram a sentece with Python3,"['python-3.x', 'nltk', 'n-gram']","I'm using python3 and i'm traing to bigram a sentence but the interpreter gives me a problem that i can't understand.What does it means: ""generator object ngrams at 0x7ff1d81d2468""?
Why I can neither inspect nor print n-grams?"
1149,"I'm getting TypeError: expected string or bytes-like object ""","['python', 'nltk']",I am getting an error on this part
1150,Text Classification from Chatbot [closed],"['machine-learning', 'deep-learning', 'classification', 'nltk', 'text-classification']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed 7 months ago.I'm just getting started to be a Junior Data Analyst. I applied to a startup and they give me a test. I wonder if someone can give me a hint on how to solve it. The quest are:Given the random words below (extracted from chatbot):It belows are Unstructured Indonesian language:What algorithm do I need to complete those 3 tasks?"
1151,"Finding whether a sentence is positive, neutral or negative?","['python', 'machine-learning', 'nlp', 'nltk', 'sentiment-analysis']","I want to create a script that can find whether a sentence is positive or neutral or negative.i searched online found that through a medium article that it can be done using NLTK library.So, i have tried this code.and here is the output i gotThe issue I'm facing is that the dataset is very limited and hence the output accuracy is very low. Is there any better library or resource or anything else to check whether a statement is positive, neutral or negative?More specifically, I want to apply it on general day-to-day talk"
1152,I have a dataset on which I want to do Phrase extraction using NLP but I am unable to do so?,"['machine-learning', 'nlp', 'artificial-intelligence', 'nltk', 'word2vec']",How can I extract a phrase from a sentence using a dataset which has some set of the sentence and corresponding label in the form of I have tried using chunking with nltk but I am not able to use training data along with the chunks. 
1153,Similar sentences in two strings in Python,"['python', 'nlp', 'nltk', 'stemming', 'lemmatization']","I have to write functions that look for similar sentences in ""text1"" and in ""text2"" using the following methods:I already tokenized the sentences and the words separately, removing any punctuation and digit.
I know how to perform 'stopwords removal', 'stemming' and 'lemmatization'. I just don't manage to setup my program in an efficient way to reach my goal.I want to take every single word form one sentence in text1 and check if it's possibile to find it in a sentence in text2 and find the percentage of similar sentences for text..."
1154,How to find the category of sentences? List of categories are pre-specified,"['python', 'nltk', 'wordnet']","I have a list of sentences and list of category.My categories are like below: Auto,Games, News, Research, Business, Computing, Holiday, Places, Mispelling, URL, OtherMy sample senteces are like below:""holiday mansion houseboat""

""verizon wireless""

""select business services""

...

I have to categorize these sentences according to list of categories which are specified above.
I know I should use wordnet, but there are many examples however most of them use training and testing data set. But my data doesn't contain any class information. I should directly categorize sentences according to pre-specified categories.  
Anyone can help me or show me a simple example code which finds the category of a sentence?"
1155,“Fast” as an Adverb vs Adjective using NLTK pos_tag,"['python', 'nltk', 'pos-tagger']","Why nltk.pos_tag returns Fast once as an Adverb and another as an Adjective in the following examples?andIn both cases, it came before a noun. I noticed that it has a lot to do with Fast than the noun itself.Here is another example that will return the expected results:andI ran Fast followed by lots of names, and in most cases, it'll be returned as an RB (Adverb)."
1156,how to remove everything between round brackets using python [closed],"['python', 'nlp', 'nltk']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 7 months ago.for example I have a text like this:I want to see only this "
1157,"problems with invokings functions from module, using python3 and nltk","['python-3.x', 'nltk']","from the book: Natural Lenguage processing with Python, pg. 60.
I created a module with some functions (in EMACS) but when i run them with Python3 they give me problems.why it says that nltk is not defined?"
1158,TextBlob Naive Bayes classifier for neutral tweets,"['python', 'nltk', 'sentiment-analysis', 'naivebayes', 'textblob']","I am doing a small project on sentiment analysis using TextBlob. I understand there are are 2 ways to check the sentiment of tweet:My question is, using the Naive bayes classifier, can I also classify the tweet as 'neutral' ? In other words, can the 'sentiment polarity' defined in option 1 can somehow be used in option 2 ?"
1159,How to get top three words from the results tokenized in NLTK,"['python', 'nltk']","I'm trying to get the top three words in the results tokenized in NLTK. This is sorted by how often the words are used.results:dict_items([('everyone', 1), ('work', 15), ('ability', 7), ('determination', 3), ('ingredient', 2), ('understood', 1)])I want to get work, ability, and determination. How can I get these three words from the results?"
1160,Different plots are overlapped,"['python', 'matplotlib', 'nltk']",I'm using matplotlib and nltk to plot the freq dist of different authors but it seems two plots are being plotted in the plot. How can I fix this by creating another separate plot for author-name freq dist?Here is some of my code:
1161,How to extract meaningful and frequent words from concatenated strings in python?,"['python', 'python-3.x', 'nlp', 'nltk', 'spacy']",I have a list of concatenated string as given below which I wish to split into meaningful and frequent words. Code which I have created is giving me all sorts of un-frequent words as well.Expected output:My codeOutput:Is there any other way of doing this? Please help me in understand how to get the expected output.Thanks in advance
1162,“TclError: no display name and no $DISPLAY environment variable” Error in nltk,"['python', 'machine-learning', 'nlp', 'nltk', 'named-entity-recognition']","Though I got the output,at the same tcl error occurring.
"
1163,Keras LSTM for converting sentences to document context vector,"['python', 'keras', 'nltk', 'lstm', 'seq2seq']","I read the following blog post and tried to implement it via Keras:
https://andriymulyar.com/blog/bert-document-classificationNow, Im quite new to Keras and I do not understand how to use ""seq2seq neural networks"" to condens a sequence of subchunks (sentences) into a global context vector (document vector). - via LSTM..For example: I have 10 documents consisting of 100 sentences each and each sentence is represented by a 1x500 vector.
So the array would look like this:So I know I want to train my network and take the last hidden-layer cause this one represents my document vector/global context vector.However, the hardest part for me is to imagine the output vector.. do I just enumerate my documentsor do I have to use one-hot-encoded output vectors:or even something else..?As far as I understand, the final model should look something like this:"
1164,string manipulation with python pandas and replacement function,"['python', 'string', 'pandas', 'replace', 'nltk']","I'm trying to write a code that checks the sentences in a csv file and search for the words that are given from a second csv file and replace them,my code is as bellow it doesn't return any errors but it is not replacing any words for some reasons and printing  back the same sentences without and replacement.the sentences csv file looks likeand the change csv file looks like"
1165,NLTK CSV/TXT CLASSIFICATION,"['python-3.x', 'nltk']",Dou you guys know a simple way to get a csv/txt phrases classifications as same as  Puting a sing phase like below:teste='How Are You?'testesteming=[]stemmer = nltk.stem.RSLPStemmer()for (palavras) in teste.split():novo = extratorpalavras(testesteming)print(novo)     print(classificador.classify(novo))distribuicao = classificador.prob_classify(novo)for classe in distribuicao.samples():
1166,BERT-transformer: Where do the Masked Language Model perform mask on the input data,"['transformer', 'huggingface-transformers', 'bert', 'huggingface-tokenizers']","I am trying to pre-train a bert from scratch with my own word set using only the Masked Language Model.
I have a trouble finding where exactly the code masks 15% of the words and replaces it with 80% mask, 10% random, and 10% original.I noticed that the input ""labels"" kind of refers to the places where words are masked. Does it mean that when I preprocess the data, I need to masked it myself and then indicates the position of masks in the ""labels"" input? If so, is ""labels"" the only input that would be affect? Is there any other input variables, such as the ""masked_lm_positions"" and ""masked_lm_ids"" in google-bert that I need to take care of?"
1167,Where can I get the pretrained word embeddinngs for BERT?,"['embedding', 'huggingface-transformers', 'bert']","I know that BERT has total vocabulary size of 30522 which contains some words and subwords. I want to get the initial input embeddings of BERT. So, my requirement is to get the table of size [30522, 768] to which I can index by token id to get its embeddings. Where can I get this table?"
1168,BERT in excel not seen as adding,"['excel', 'bert']","I have a problem with installing BERT excel adding on my computer. Installation goes through well but when I open excel there is no BERT among addings. I have also tried with unabled addings, but BERT is not there. I have installed BERT on C - local disk.Thanks in advance,
Neža"
1169,Regression problem getting much better results when dividing values by 100,"['regression', 'scaling', 'loss-function', 'bert', 'mse']","I'm working on a regression problem in pytorch. My target values can be either between 0 to 100 or 0 to 1 (they represent % or % divided by 100).The data is unbalanced, I have much more data with lower targets.I've noticed that when I run the model with targets in the range 0-100, it doesn't learn - the validation loss doesn't improve, and the loss on the 25% large targets is very big, much bigger than the std in this group.However, when I run the model with targets in the range 0-1, it does learn and I get good results.If anyone can explain why this happens, and if using the ranges 0-1 is ""cheating"", that will be great.Also - should I scale the targets? (either if I use the larger or the smaller range).Some additional info - I'm trying to fine tune bert for a specific task. I use MSEloss.Thanks!"
1170,what is the meaning of Query-Key-value in Bert? [closed],"['deep-learning', 'transformer', 'bert']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 5 days ago.I am new in deep learning. Could anyone help me what Query-Key-value represent in Bert.
Thanks in advance."
1171,How to save and load Python Keras BERT model to serialize it?,"['python', 'keras', 'serialization', 'hdf5', 'bert']","I've just finished to train my Keras BERT model which treats about multilabel Text classification  (percentage unit) and I would like to be able to applied my train model on new (unlabeled) text.Here are main parts of my model :Right after this code part I've tried :But I've got :During my researches for this error I read that it's simple to only save model weights so I tried :Once modll_ weighs is conserved in the model I applied my model to a new text in order to obtain predication for it :But I got very bad classification (I've tried for many text), which is surprising because during training phase I had 0.94 accuracy. I think that I've done mistakes when I saved and loaded my model. Any idea about that ?"
1172,BERT server does not start,"['python', 'bert']","I successfully installed BERT server and client.
I tried to start server by typing following code in a Anaconda PromptIt supposed to print some responses if the server started properly as shown in https://github.com/hanxiao/bert-as-service#install.However, in my case, nothing shows in the prompt."
1173,Getting sequence output from BERT encoder (tensorflow),"['python', 'tensorflow', 'bert']","I am attempting to fine-tune BERT in tensorflow following this official guide with the goal of feeding the output further into LSTM/GRU. I am able to run the fine-tuning but the output shapes I am getting from the bert_encoder are [num_samples, hidden_units] and [num_samples, 1, 768]. I believe these are pooled and sequence outputs respectively but I am confused why the sequence output is not [num_samples, max_seq_length, hidden_units].Running this code after replacing bert_classifier with bert_encoder on compile and fit:produces:Since I am passing to sequence models, I need to get the sequence output but I keep getting only 1 shape for the sequence length. I've been trying to understand why but can't find anything on it. Any help and clarifications would be appreciated. Thanks!"
1174,ByteLevelBPETokenizer inconsistent behavior,"['huggingface-transformers', 'bert', 'huggingface-tokenizers', 'roberta']","I encountered a weird behavior of the ByteLevelBPETokenizer: this publicly available notebook is parameterized to run on two almost identical text files. The first one is a transliteration of the Hebrew Bible text while the second one is the same transliteration with 2 modifications - the ‘ and ’ characters are replaced by the Hebrew letters ע and א accordingly (the transliteration used these 2 types of apostrophes for denoting these Hebrew consonants). After training on the first file(tanach_translit_orig) the tokenizer is given a test sentence for encoding and results with 19 tokens but when running the same process on the second file and the same test sentence modified accordingly (by replacing the apostrophes by the Hebrew letters) the tokenizer results with 9 tokens. I assumed the ByteLevelBPETokenizer to be agnostic to the character meanings and I cannot understand why the results vary between the experiments. Can anyone shed some light please?
P.S. for toggling between the files all you need is to un-comment the appropriate line in the second step of the notebook:"
1175,how can i retrain huggingface fine tuned model,"['pytorch', 'reusability', 'bert']","thanks in advance.
I want to train first with more data, it means more label.
(its like pre training in this case)
int this example, label number is 19 and its done well.after that I choose some label only, and want to train again.
(only 10 label)But error happened during below codetokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)
model = RobertaForSequenceClassification.from_pretrained(pretrained_path, num_labels=10)error message is like below.
Error(s) in loading state_dict for RobertaForSequenceClassification:
size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([19, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).
size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([10]).how can I do it??"
1176,Bert Sequence Classifier returning same response for al sequence,"['pytorch', 'classification', 'bert']","I was doing fine tuning by BertForSequenceClassification from transformers using pytorch. But the problem is during evaluation, it returns the same output for every sequence. Can not figure out why?Tokenization:Generator:Bert Initialization:Save and Load FunctionsTrain:During train, the log:Evaluation:First print function above print the following, which means tokenization is okay.But the output is ambiguous:"
1177,i have an error while training BERT model,"['machine-learning', 'pytorch', 'bert']","I'm trying to train my model using GPUs.When I execute it I get this error below:This error occurs when i use single or multiple GPUs.How can i deal with this issue? Thank youHere is the code of the BERT model (no segment input, no pretraining)
c.f. When I use another model, this issue didn't occure."
1178,How to finetune distillbart for abstractive summarization using Gigaword or Cnn dailymail?,"['transformer', 'huggingface-transformers', 'summarization', 'bert']","I would like to ask about how to finetune distillbart on gigaword and cnn dailymail with the starting checkpoint distilbart-cnn-12-6.
I did use the gigaword dataset provided by tensorflow but it replaces numbers by this character: ""#"", as a result, my summaries have # instead of numbers, is it normal that it has those # ?
Also is it really possible to finetune distillbart from the checkpoint distilbart-cnn-12-6 with cnn daily mail?here is the link to gigaword: https://www.tensorflow.org/datasets/catalog/gigaword
and here is the link to cnn dailymail: https://www.tensorflow.org/datasets/catalog/cnn_dailymailFor the code I followed the instruction of fine tuning distillabart here:
https://github.com/Hildweig/transformers/tree/master/examples/seq2seqFor the outputs with gigawords I get something like this:
""foreign exchange rates in hong kong sept. ## #### ; china 's defense minister says he 's ready to work with yugoslavia 's foreign minister on iraq 's role in iraq with bc-me-gen iraq"""
1179,"Why TFBertForTokenClassification.from_pretrained(“allenai/scibert_scivocab_uncased”,from_pt=True,config=config) gives weights warning?","['pytorch', 'tensorflow2.0', 'huggingface-transformers', 'bert', 'allennlp']","I want to do Named Entity Recognition on scientific articles.But it gives following warning.Some weights of the PyTorch model were not used when initializing the
TF 2.0 model TFBertForTokenClassification: ['classifier.weight',
'classifier.bias'] - This IS expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model trained on another
task or with another architecture (e.g. initializing a
BertForSequenceClassification model from a TFBertForPretraining
model). - This IS NOT expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model that you expect to be
exactly identical (initializing a BertForSequenceClassification model
from a TFBertForSequenceClassification model). Some weights or buffers
of the PyTorch model TFBertForTokenClassification were not initialized
from the TF 2.0 model and are newly initialized:
['cls.predictions.transform.dense.weight',
'cls.predictions.decoder.bias',
'cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.LayerNorm.bias',
'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',
'cls.predictions.bias', 'cls.predictions.transform.dense.bias',
'cls.seq_relationship.bias'] You should probably TRAIN this model on a
down-stream task to be able to use it for predictions and inference.And when I try to fit the model, it gives follwing warning.WARNING:tensorflow:Gradients do not exist for variables
['tf_bert_for_token_classification/bert/pooler/dense/kernel:0',
'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when
minimizing the loss.Does anyone know why it is giving these warnings?"
1180,Why does DistilBERT huggingface forward function doesn't return pooler_output ike BERT does?,"['pytorch', 'huggingface-transformers', 'bert']","I'm working with huggingface in pytorch. I'm trying to use both BERT and distilBERT for text classification problem. I want to use BERT/distilBERT output as an input for a dropout layer and later linear layer.In BERT, I understood I have to use the pooler_output parameter that the forward function of BERT returns. However, distilBERT forward function doesn't return such a parameter, so I don't how I can use its output for the dropout layer and later linear layer.Can anyone pls help?
Thanks!"
1181,"OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index']","['python', 'tensorflow', 'pytorch', 'bert']","When I load the BERT pretrained model online I get this error OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory uncased_L-12_H-768_A-12 or 'from_tf' set to False what should I do?"
1182,PermissionError while loading model SentenceTransformer,"['python', 'python-3.x', 'virtualenv', 'transformer', 'bert']","I am trying to find the sentence similarity and I want to use the Sentence transformer BERT for thiswhen I am trying to run I got the following errorI understood the error is because of not able to make directory. Intially i tried changing the permissions of the directory, but it gave an error says that no such directory may be because its cache which is a temporary one.how to resolve the issue ? I am not sure if the issue is with Python or Virtualenv."
1183,Retraining existing base BERT model with additional data,"['tensorflow', 'bert', 'tensorflow-hub', 'finetunning']","I have generated new Base BERT model(dataset1_model_cased_L-12_H-768_A-12) using cased_L-12_H-768_A-12 as trained multi label classification from biobert-run_classifierI need to add more additional data as dataset2 and the model should be dataset2_model_cased_L-12_H-768_A-12Is tensorflow-hub help this to resolve my problem?Model training life cycle will be like this below,cased_L-12_H-768_A-12 => dataset1 => dataset1_model_cased_L-12_H-768_A-12dataset1_model_cased_L-12_H-768_A-12 => dataset2 =>
dataset2_model_cased_L-12_H-768_A-12"
1184,How to use a bert pretrained model somewhere else?,"['python', 'tensorflow', 'pytorch', 'pre-trained-model', 'bert']","I followed this course https://www.coursera.org/learn/sentiment-analysis-bert about building a pretrained model for sentiment analysis. During the trining, at each epoch they saved the model using torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model'). Now I want to use one of these models (the best one obviously) elsewhere, for example where a user can paste a tweet as an input and get the emotion of the writer. But I don't know how to load the model and predict, here's what I tried:I get this error: TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list"
1185,model.predict(dataset) give me “unsupported operand type(s) for *: 'int' and 'NoneType” error,"['python', 'tensorflow', 'keras', 'bert']","Now I'm editing Movie Reviews with bert-for-tf2 on TPU.ipynb to run Korean samples.
https://colab.research.google.com/drive/1BeG1n9IJmoxBZ2FicKKqWTdQ11jfYxlC?usp=sharingTo test the trained model with sample sentence, I use model.predict() function.I got the error message as follows:I just changed the original code to convert sentence into batched dataset.My code are as follows[Update] This error is relevant to the ""ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)"". It has the same shape, but internal data of this FetchedDataset becomes None... Is there anyone could explain why this code drop the internal data? If it is related to drop_remainder, I changed it to False, but its shape is not matched with model...
I'm really appreciate your any comments[Update]
When I changed 'ds.batch(batch_size, drop_remainder=True).prefetch(1)' into  'ds.batch(batch_size)' there is a data but the shape is become (1,512)
The error message is changed as follows:
InvalidArgumentError: {{function_node __inference_predict_function_238864}} Compilation failure: Input to reshape is a tensor with 786432 values, but the requested shape has 6291456
[[{{node sequential_1/bert/encoder/layer_0/attention/self/key/Tensordot/Reshape}}]]
TPU compilation failed
[[tpu_compile_succeeded_assert/_16383985289316826946/_4]]"
1186,Fail to run trainer.train() with huggingface transformer,"['tensorflow', 'huggingface-transformers', 'bert', 'finetunning', 'squad']","I am trying to set up a TensorFlow fine-tune framework for a question-answering project. Using hugging-face/transformer as the prototype, but cannot run through the trainer.The experiment is conducted at Databricks, the pre-trained model loaded is base-bert, train and dev sets are downloaded from hugging-face examples SQUAD 2.0 https://github.com/huggingface/transformers/tree/master/examples/question-answeringThe error log complains about the unexpected keyword argument 'is_impossible', which is a SQUAD 2 data format feature.TypeError                                 Traceback (most recent call
last)  in 
----> 1 trainer.train()/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in train(self)
410             if self.args.past_index >= 0:
411                 self._past = None
--> 412             for step, training_loss in enumerate(self._training_steps(train_ds, optimizer)):
413                 self.global_step = iterations.numpy()
414                 self.epoch_logging = epoch_iter - 1 + (step + 1) / steps_per_epoch/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in _training_steps(self, ds, optimizer)
457         Returns a generator over training steps (i.e. parameters update).
458         """"""
--> 459         for i, loss in enumerate(self._accumulate_next_gradients(ds)):
460             if i % self.args.gradient_accumulation_steps == 0:
461                 self._apply_gradients(optimizer)/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in _accumulate_next_gradients(self, ds)
490         while True:
491             try:
--> 492                 yield _accumulate_next()
493             except tf.errors.OutOfRangeError:
494                 break/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in call(self, *args, **kwds)
566         xla_context.Exit()
567     else:
--> 568       result = self._call(*args, **kwds)
569
570     if tracing_count == self._get_tracing_count():/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in _call(self, *args, **kwds)
613       # This is the first call of call, so we have to initialize.
614       initializers = []
--> 615       self._initialize(args, kwds, add_initializers_to=initializers)
616     finally:
617       # At this point we know that the initialization is complete (or less/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in _initialize(self, args, kwds, add_initializers_to)
495     self._concrete_stateful_fn = (
496         self._stateful_fn._get_concrete_function_internal_garbage_collected(--> 497             *args, **kwds))
498
499     def invalid_creator_scope(*unused_args, **unused_kwds):/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _get_concrete_function_internal_garbage_collected(self, *args,
**kwargs)
2387       args, kwargs = None, None
2388     with self._lock:
-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)
2390     return graph_function
2391/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _maybe_define_function(self, args, kwargs)
2701
2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)
2704       self._function_cache.primary[cache_key] = graph_function
2705       return graph_function, args, kwargs/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _create_graph_function(self, args, kwargs,
override_flat_arg_shapes)
2591             arg_names=arg_names,
2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),
2594         self._function_attributes,
2595         # Tell the ConcreteFunction to clean up its graph once it goes out of/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py
in func_graph_from_py_func(name, python_func, args, kwargs, signature,
func_graph, autograph, autograph_options, add_control_dependencies,
arg_names, op_return_value, collections, capture_by_value,
override_flat_arg_shapes)
976                                           converted_func)
977
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
979
980       # invariant: func_outputs contains only Tensors, CompositeTensors,/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in wrapped_fn(*args, **kwds)
437         # wrapped allows AutoGraph to swap in a converted function. We give
438         # the function a weak reference to itself to avoid a reference cycle.
--> 439         return weak_wrapped_fn().wrapped(*args, **kwds)
440     weak_wrapped_fn = weakref.ref(wrapped_fn)
441/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py
in wrapper(*args, **kwargs)
966           except Exception as e:  # pylint:disable=broad-except
967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
969             else:
970               raiseTypeError: in converted code:_accumulate_next  *
return self._accumulate_gradients(per_replica_features, per_replica_labels)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:498
_accumulate_gradients  *
per_replica_loss = self.args.strategy.experimental_run_v2(
/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/distribute/one_device_strategy.py:180 experimental_run_v2
return super(OneDeviceStrategy, self).experimental_run_v2(fn, args, kwargs)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:511
_forward  *
per_example_loss, _ = self._run_model(features, labels, True)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:532
_run_model  *
outputs = self.model(features, training=training, **labels)[:2]
/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778
call
outputs = call_fn(cast_inputs, *args, **kwargs)"
1187,HuggingFace for Japanese tokenizer,"['python', 'cjk', 'bert']","I recently tested on the below code based on the source:
https://github.com/cl-tohoku/bert-japanese/blob/master/masked_lm_example.ipynbwhen i try to encode it, I received error such as:Does anyone experienced this before? I tried a lot of different ways and refer to many posts but all using the same methods and no explanations, I just wanted to test multiple languages, other languages seem to work fine but not with japanese and I dont know why."
1188,some error in using Bert even though I redo all my modification to the correct version,"['python', 'bert']","I am newbie in NLP.
I am using Bert to do news classification, but this is not the issue.
the problem I met is very strange....I am using ""transformers"" package as followedand the token, dataloader and some function as followed:this code went well but got false result due to some typo in my function ""createBatch""but it would be:I think the shape of label_ids should be in the first form like thishowever, in the second form it's just like this:so I modified this line as followed:and got the same shape as the first kind:however, everything went wrong. after this modification, I trained my model with this function:and got this error messageso, I undo all my modification to the initial code.
but the code is broken....I can't use this code anymore.
this error was happened in another line as followed:I am so confused......
I totally can't understand what happened to this?
even though I close this terminal and reopen it, it still failed.
the only way to avoid this error is to load the previous version of this code.so my question is:thank you very much"
1189,Tensorflow BERT for token-classification - exclude pad-tokens from accuracy while training and testing,"['python', 'tensorflow', 'named-entity-recognition', 'huggingface-transformers', 'bert']","I'm doing token-based classification using the pre-trained BERT-model for tensorflow to automatically label cause and effects in sentences.To access BERT, I'm using the TFBertForTokenClassification-Interface from huggingface: https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassificationThe sentences I use to train are all converted to tokens (basically a mapping of words to numbers) according to the BERT-tokenizer and then padded to a certain length before training, so when one sentence has only 50 tokens and another one has only 30 the first one is filled up with 50 pad-tokens and the second one with 70 of them to get a universal input sentence-length of 100.I then train my model to predict on every token which label this token belongs to; whether it is part of the cause, the effect or none of them.However, during training and evaluation, my model does predictions on the PAD-tokens as well and they are also included in the accuracy of the model. As PAD-tokens are very easy to predict for the model (they always have the same token and they all have the ""none"" label which means they neither belong to the cause nor the effect of the sentence), they really distort my model's accuracy.For example, if you have a sentence which has 30 words -> 30 tokens and you pad all sentences to a  length of 100, then this sentence would get a score of 70% even if the model predicted none of the ""real"" tokens correctly.
This way i'm getting training and validation accuracy of 90+% really quick although the model performs poorly on the real pad-tokens.I thought that attention-mask is there to solve this problem but this doesn't seem to be the case.The input-datasets are created as follows:Model creation:And then I train it like this:Has anyone encountered this problem so far or does know how to exclude the predictions on pad-tokens from the model's accuracy during training and evaluation?"
1190,Additional training a recurrent neural network CamemBert,"['deep-learning', 'recurrent-neural-network', 'bert']","I started working with the Camembert deep learning model, an analogue of Roberta for the French language, and I have a question, how can I retrain such a model for a particular task? Specifically, the task is for the model to learn how to evaluate input sentences for correctness"
1191,BERT (Huggingface Transformer) - Get important features for class,"['text-classification', 'huggingface-transformers', 'bert']","My goal is to get the most important features for each class in a text classification task.
I created the model, learner and predictor like this:Is there any way to get the top features like this:(created for SVM)"
1192,“IndexError: index out of range in self” error during training of LSTM on top of BERT embeddings,"['lstm', 'embedding', 'multiclass-classification', 'index-error', 'bert']","I am new to nlp, so sorry if I am missing an obvious thing. I am trying to use LSTM for multiclass text classification on top of BERT embeddings. But during training, I have ""IndexError: index out of range in self"" error.Here is my LSTM class:The part of my training code which I am getting the index error:
for epoch in range(n_epochs):Here is the error I get:I have use dataloader, the shape of the batch I have used (batch_size=8): torch.Size([8, 512, 768]).Maybe I didn't get why we exactly use ""nn.Embedding"" in the LSTM class. Thank you for your help :)"
1193,ModuleNotFoundError: No module named 'bert' even after pip install bert-tensorflow and pip install bert-for-tf2,"['tensorflow', 'jupyter-notebook', 'bert']","I have tensorflow 1.9.0 and after successful installation of bert using !pip install bert-tensorflow, I cannot import bert in Jupyter notebook. I even ran !pip install bert-for-tf2. Still no success."
1194,BioBERT Inferencing Using TF_Record file and GRPC,"['python', 'tensorflow', 'grpc', 'tensorflow-serving', 'bert']","I have been trying to perform inference using bioBERT(https://github.com/dmis-lab/biobert)
Model and TF serve to perform QA task.I have succesfully exported the model: My serving function looks like this:serving_input_fn=tf.estimator.export.build_parsing_serving_input_receiver_fn(tf.feature_column.make_parse_example_spec(feature_columns))estimator._export_to_tpu = Falseestimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn, checkpoint_path)
##############################################I am also able to generate a TFrecord File and trying to utilize TFrecordIterator to iterate over tht tf records file and call the GRPC generated stub.
#record_path is the path to TF_record filw
Function below....The Error I am getting is as follows:Any help on this issue is appreciated."
1195,Why Bert transformer uses [CLS] token for classification instead of average over all tokens?,"['tensorflow', 'machine-learning', 'keras', 'deep-learning', 'bert']","I am doing experiments on bert architecture and found out that most of the fine-tuning task takes the final hidden layer as text representation and later they pass it to other models for the further downstream task.Bert's last layer looks like this :Where we take the [CLS] token of each sentence :Image sourceI went through many discussion on this huggingface issue,  datascience forum question,  github issue Most of the data scientist gives this explanation :BERT is bidirectional, the [CLS] is encoded including all
representative information of all tokens through the multi-layer
encoding procedure. The representation of [CLS] is individual in
different sentences.My question is, Why the author ignored the other information ( each token's vector ) and taking the average, max_pool or other methods to make use of all information rather than using [CLS] token for classification?How does this [CLS] token help compare to the average of all token vectors?"
1196,Generate percentage prediction label by using BERT multi-label classification,"['python', 'multilabel-classification', 'bert']","I'm currently working on multi-label classification task for text data. I have a dataframe with an ID column, text column and several columns which are text label containing only 1 or 0.I used an existing solution proposed on this website Kaggle Toxic Comment Classification using Bert which permits to express in percentage its degree of belonging to each label.Now, that I've trained my model I would like to use my model with new unlabeled text in order to obtain percentage of belonging to each label :I've found this solution on this website and more especially this part code that I want to add at the end of my Kaggle code :But this solution only permit me to obtain class prediction but not the percentage percentage prediction for each class.Do you have any idea how can I do to adapt this last pat of code to my Kaggle code and get percentage prediction ?"
1197,Saving and loading BERT model in R,"['r', 'keras', 'bert']","I used this tutorial to partially train a model (pretrained BERT) on some text data. Except from data, all other code is the same.I have problems saving and loading the trained model. If I use the following code:I get this error:How can I succesfully save and load the model?"
1198,evaluating MMR metrics for TREC-QA/Wiki-QA,"['python', 'tensorflow', 'metrics', 'huggingface-transformers', 'bert']",I am doing BERT project on dataset TREC-QA and Wiki-QA. I am having a problem while coding for metrics to evaluate the results.My manual code for MMR metrics is as follows:I also tried this but not able to understand how to use it because it was coded in C. If someone knows how can I use this C code to work in the python file.here is my colab notebook
1199,Apply trained BERT model for prediction deployment,"['python', 'multilabel-classification', 'bert']","I'm currently working on multi-label classification task for text data.
I have a dataframe with an ID column, text column and several columns which are text label containing only 1 or 0.I used an existing solution proposed on this website Kaggle Toxic Comment Classification using Bert which permits to express in percentage its degree of belonging to each label.Now, that I've train my model I would like to test it on few text extracts with no label in order to obtain percentage of belonging to each label :I've tried this solution :And I got :Any idea what I need to change to make the last part of my algorithm functional?"
1200,question about tf.data.Dataset.from_generator for bert,"['tensorflow', 'dataset', 'generator', 'bert']","I searched regarding this, but I cant find how to do.
usually, they use tf.constant or tf.data.TFrecord.I want to make bert input dataset api using generator from reading csv file.
I try several hours, but various error happened.
I confused hot to use output_types, and output_shapebelow is my code.
what is the problem??feed.py"
1201,Loss nan problem when using TFBertForSequenceClassification,"['tensorflow', 'machine-learning', 'keras', 'deep-learning', 'bert']","I have a problem when training a model for multi-label text classification.
I'm working at Colab as follows:The result is as follows:Epoch 1/2
739/14065 [>.............................] - ETA: 35:31 - loss: nan - accuracy: 0.0000e+00I have checked out my data: no nan or null or invalid values.I tried different optimizers, # of epochs, learning rate, but had the same problem.The number of labels is 52 and the distribution is as follows:I'm a beginner in this area. Please help me. Thanks in advance!"
1202,"BERT output should be text_A + text_B = some classification, but it's doing text_A = some classification and text_b = some classification","['python', 'tensorflow', 'bert']","I am using a code adapted from Predicting Movie Reviews with BERT on TF Hub.ipynb. I am trying to run a comparison between two sentences to retrieve a result out of them.Some previous code from ""Predicting Movie Reviews with BERT"" on TF Hub.ipynb is needed to run the code I am placing. I used small_bert_bert_uncased_L-4_H-768_A-12_1 as the model.And I think I took a little step towards the solution thanks to Matthew Viglione.The error states:I changed the code a little bit and found an interesting output.When I run:The error is:"
1203,"How to use word embeddings (i.e., Word2vec, GloVe or BERT) to calculate the most word similarity in a set of N words by “Python”?","['python', 'word2vec', 'cosine-similarity', 'bert', 'glove']","I am trying to calculate the semantic similarity by inputting the word list and output a word, which is the most word similarity in the list.E.g.If I pass in a list of wordsIt should output me something like this-"
1204,How can I get the hidden layers for each indiviual head?,"['python', 'huggingface-transformers', 'bert']","I know we can access the hidden layer from each layer by the following code. However, each hidden output we obtains is the result mixed from 12 head by a fully connected layers. Is there a way to obtain the outputs before they enter the fully connected layers?Thank you!"
1205,How can I convert the csv file into SQUAD 2.0 dataset format?,"['json', 'stanford-nlp', 'bert', 'question-answering', 'squad']","I have csv file. that contains colums as [paragraph, question, answer, is_impossible]. I want to convert it to json format as SQUAD 2.0 and train it on BERT. This is a new dataset I am creating for question answering. Thank you.Here is the csv format of my datasetwant to convert into this json format as SQUAD.tack.imgur.com/cc2X1.png"
1206,How to slice string depending on length of tokens,"['python', 'python-3.x', 'tokenize', 'huggingface-transformers', 'bert']","When I use (with a long test_text and short question):I get an error with the outputToken indices sequence length is longer than the specified maximum sequence length for this model (3 > 512). Running this sequence through the model will result in indexing errorsQuery has 1,244 tokens.How can I separate test_text into maximized length of chunks knowing that it won't exceed 512 tokens? And then ask the same question for each chunk of text, taking the best answer out of all of them, also going through the text twice with different slice points, in case the answer is cut during a slice."
1207,TFBertMainLayer gets less accuracy compared to TFBertModel,"['keras', 'transformer', 'bert']","I had a problem with saving weights of TFBertModel wrapped in Keras. the problem is described here in GitHub issue and here in Stack Overflow.The solution proposed in both cases is to use instead of The problem is that when I change my model to the former code, the accuracy decreases by 10 percent.While the parameters count in both cases are the same. I wonder what is the reason and how can be prevented?"
1208,"CamemBERT,'charmap' codec can't encode character '\u2260' with BertLMDataBunch.from_raw_corpus","['python', 'encoding', 'bert']","Hello I'm trying in implement Cammebert on french texts, these texts are long, sometimes complicated because they can contain mathematics formulas, url,....
they are encoded in utf8.
to implement BertLMDataBunch.from_raw_corpus, I need to pass texts information in a list of strings."
1209,How to load a fine-tuned BERT model,"['keras', 'bert']",I have fine tuned a BERT model on my data and saved the model using model.save()now am trying to load using the belowbut I keep getting the below error
1210,error while training the bert model on tensorflow 2.0,"['tensorflow', 'bert']","I was trying to train a bert model in tensorflow 2.0 and encounter an error (when I hit model.fit) . Anyone knows what this kind of error is!? Thanks in advance.UnboundLocalError                         Traceback (most recent call last)
 in 
3     validation_split=0.3,
4     epochs=3,
----> 5     batch_size=12
6 )
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
64   def _method_wrapper(self, *args, **kwargs):
65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
67
68     # Running inside run_distribute_coordinator already.
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
854               logs = tmp_logs  # No error, now safe to assign to logs.
855               callbacks.on_train_batch_end(step, logs)
--> 856         epoch_logs = copy.copy(logs)
857
858         # Run validation.UnboundLocalError: local variable 'logs' referenced before assignment**"
1211,making sense of the “download bert during first training without gpu” instruction,"['deep-learning', 'bert']","I am looking into this repository for a summarization system using bert.https://github.com/nlpyang/PreSumm/issuesIt comes with the instruction saying that I should start training with gpu=-1(No gpu) in order to allow bert to be downloaded and then I can kill the training process and restart with gpu=1. This instruction is causing me trouble as there is no indication of the bert model being downloaded. If using gpu=-1 during training, which takes ages and using gpu=1 during testing, it results in an error, not to mention the extra time it takes to train. Would it matter if I just start training with gpu=1 from the very beginning? Can anyone explain the download bert during first training part? If I cannot do gpu=1 during the first training, can anyone enlighten me on just manually downloading bert first. Many thanks. "
1212,Implementing HuggingFace BERT using tensorflow fro sentence classification,"['tensorflow', 'text-classification', 'huggingface-transformers', 'bert']",I am trying to train a model for real disaster tweets prediction(Kaggle Competition) using the Hugging face bert model for classification of the tweets.I have followed many tutorials and have used many models of bert but none could run in COlab and thros the errorMy Code is: On running the above code in colab I get the following error:
1213,CUDA out of memory even though there is free memory when using the simple-transformers pytorch library,"['pytorch', 'bert']",Ive been trying to follow this tutorial for the simple transformers library but I keep running into cuda errors that I dont quite understand.RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 697.82 MiB already allocated; 3.99 GiB free; 744.00 MiB reserved in total by PyTorch)I dont understand why its failing to allocate memory when it says there is 4GB free. Im also seeing this error in the stack traceOSERROR: [WinError 1455] The paging file is too small for this operation to completeoccasionally this one will also pop up.OSERROR: [WinError 1114] A dynamic link library (DLL) initialization routine failedDont really know where to go from here.
1214,How to mask custom words instead of random words in BERT pretraining?,"['huggingface-transformers', 'bert']","How to mask custom words instead of random words in BERT pretraining?
Usecase is to attend certain words more than others while learning. Custom words are usually topics of a sentence.
I could not find anything in huggingface library."
1215,Subprocess.CalledProcessError of baidu SKEP,"['python', 'bert', 'baidu', 'paddle-paddle']",When I try the SKEP(https://github.com/baidu/Senta)sh ./script/run_train.sh ./config/ernie_1.0_skep_large_ch.Chnsenticorp.cls.jsonI have encountered the following program errors
1216,Tensor conversion requested dtype string for Tensor with dtype float32:,"['parsing', 'exception', 'bert']","I am training a Bert model using a google cloud storage bucket with the given data set. For some reason, at the final step, I get this error,
Tensor conversion requested dtype string for Tensor with dtype float32.
At first, I thought it was a problem with my dataset, but after some alterations with it, it did not turn out to be that. Does anyone have any ideas on why it is giving this exception?2020-06-13 02:50:45,710 :  Calling model_fn.
2020-06-13 02:50:45,723 :  Error recorded from training_loop: in converted code:
    relative to /usr/local/lib/python3.6/dist-packages/tensorflow_core/python:2020-06-13 02:50:45,724 :  training_loop marked as finishedValueError                                Traceback (most recent call last)
 in ()
----> 1 estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)27 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--> 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raiseValueError: in converted code:
    relative to /usr/local/lib/python3.6/dist-packages/tensorflow_core/python:"
1217,"cannot import clean_up_tokenization , bert tokenizer","['pytorch', 'transformer', 'bert']","I am trying to do machine translation with fairseq and used bert as tokenizer. I installed pytorch-transformers from source. But i am not able to import a function clean_up_tokeinization.from pytorch_transformers.tokenization_utils import clean_up_tokenizationAlthough, i can import pytorch_transformers.tokenization_utils and tokenization_utils has a function clean_up_tokenization. I cannot import it.Could anyone help me with importing this function? Thanks in advance."
1218,How to apply “last_hidden_states = (model(input_ids))” in batches of “input_ids” for BERT?,"['python', 'bert']","I'm trying to apply BERT on my data, but it is very large and I'm getting memory error. Therefore, I want to do the following code in batches ""input_ids""?whole code is as follows:I tried the following code but the resultant tensor differs in shape of original tensor (if not done in batches):Any idea?"
1219,How can I plot ROC curves for BERT implemented with tensorflow?,"['tensorflow', 'roc', 'multiclass-classification', 'bert']",I'm using pretrained bert to train a multiclass classifier. I wanted to evaluate the performance of my model using ROC curves. I tried implementing my own one-vs-rest classifier but I'm having some messy results.I want to know if there is a way to visualize the curves without using one-vs-rest technique.ref: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=FnH-AnOQ9KKW
1220,Bert Text Classification Loss is Nan,"['python', 'tensorflow', 'sentiment-analysis', 'text-classification', 'bert']","I'm try to make an model that classify the text in 3 categories.(Negative,Neural,Positive)I have csv file that contain comments on different apps with their rating.First I import all the necessary librariesThen i'll get my csv fileConverting scores to sentimentCreating Helper Methods to fit the data into modelCreating ModelIf i change the count of the sentiment and make it just positive and negative then it works.
But with 3 or more labels creates this problem."
1221,"TensorFlow1.15, the inner logic of Estimator's input_fn? Or the inner logic of MirroredStrategy?","['tensorflow', 'deep-learning', 'tensorflow-datasets', 'tensorflow-estimator', 'bert']","I am pretraining BERT in 1 machine with 4 GPU, not 1 GPU.For each training step, I am wondering whether the input_fn give 1 GPU 1 batch or give 4 GPU 1 batch.The mirrow strategy code:The input_fn code:Other code:If input_fn give 1 GPU 1 batch, then train_batch_size should be max_batchsize_per_gpu.If input_fn give 4 GPU 1 batch, then train_batch_size should be max_batchsize_per_gpu*4."
1222,BERT weight calculation,"['nlp', 'ner', 'bert']","I am trying to understand the BERT weight calculation. Please suggest me some article which can help me to understand the internal workings of BERT. I have read articles from Medium.I am doing a small project to understand the Bert pretraining and fine-tuning from different sources. My idea is to calculate the weights of each token in their own sources and find avg of all weights to get a global model. Then this global model can be used to fine-tune in different sources. Also, note that I am trying to use Tensorflow version of the Bert implementation and planning to fine-tune for the NER task."
1223,"Bert + Resnet joint learning, pytorch model is empty after instantiation","['pytorch', 'resnet', 'bert', 'torchvision']","I'm writing a simple joint model, which has two branches, one branch is a resnet50 another one is a bert. I concatenate the two outputs and pass that to a simple linear layer with 2 output neurons.I implemented the following model :But when I instantiate, I get an empty model:Out:
BertResNet()list(bert_resnet.parameters()) also returns []"
1224,"Huggingface Bert, Which Bert flavor is the fastest to train for debugging?","['machine-learning', 'nlp', 'huggingface-transformers', 'bert']","I am working with Bert and the library https://huggingface.co/models hugginface.
I was wondering which of the models available you would choose for debugging?In other words which models trains/loads the fast on my GPU, to get runs as fast as possible?
Albert, distillbert or?"
1225,CUDA Runtime Error: Which Cuda version is compatible to run NER task using BERT-NER,"['pytorch', 'ner', 'huggingface-transformers', 'bert']","I have setup all the requirement packages installed on my VM and i found no nvidia GPU driver  installed, In the requirements doesn't have nvidia GPU driver installation instructions, I want to know which cuda version and it compatible nvidia driver which needs too resolve the below error.Github link: githubError logs:After installing latest cuda version from the following link,
cuda I got the following error,"
1226,Fine tuning BERT with my own entities/labels,"['neural-network', 'transformer', 'bert']","i would like to fine tune A BERT model with my own labels, like [COLOR, MATERIAL] and not the normal ""NAME"", ""ORG"".I'm following this Colab: https://colab.research.google.com/drive/14rYdqGAXJhwVzslXT4XIwNFBwkmBWdVVI prepared train.txt, eval.txt, test.txt like this:But whene i execute this commandi get this errorDid i create wrongly train.txt file?"
1227,How to perform Multi output regression using RoBERTa?,"['pytorch', 'regression', 'huggingface-transformers', 'bert']","I have a problem statement where I want to predict multiple continuous outputs using a text input. I tried using 'robertaforsequenceclassification' from HuggingFace library. But the documentation states that when the number of outputs in the final layer is more than 1, a cross entropy loss is used automatically as mentioned here: https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification.
But I want to use an RMSE loss in a regression setting with two classes in the final layer. How would one go about modifying it?"
1228,Error with BERTEmbedding() in package kashgari.embeddings,"['python', 'machine-learning', 'nlp', 'bert']","The function of BERTEmbedding() in package kashgari.embeddings is returning an error.this is my code:error:I set the sequence_length is 200 witch type is 'int',but the AttributeError said it's 'str'"
1229,huggingface transformers bert model without classification layer,"['pytorch', 'huggingface-transformers', 'bert']","I want to do a joint-embedding from vgg16 and bert for classification.The thing with huggingface transformers bert is that it has the classification layer which has num_labels dimension.But, I want the output from BertPooler (768 dimensions) which I will use as a text-embedding for an extended model.This gives the following model:How can I get rid of the classifier layer?"
1230,Low accuracy when fine-tuning BERT for question answering,"['python', 'pytorch', 'bert', 'question-answering']","I'm trying to fine-tune CamemBERT (french version of Roberta) for question answering.At the first I'm using CamemBERT model to generate the input embedding of question and text and a output linear layer to output the start and end logits that corresponds to the start and the end of the answer.In the official results in the paper the performance for question answering is (88 %, 77%) of (F1 score, EM) but the results I get are (71%, 46%).My question is why the results are not close enough ?This is a part of the script that I'm using to train and evaluate the model on FQuAD dataset with the same hyper-parameters as the official model:"
1231,PCA on BERT word embeddings,"['python', 'pca', 'bert']","I am trying to take a set of sentences that use multiple meanings of the word ""duck"", and compute the word embeddings of each ""duck"" using BERT. Each word embedding is a vector of around 780 elements, so I am using PCA to reduce the dimensions to a 2 dimensional point. I expect that words that have the same meaning of ""duck"" will be clustered together in the graph, but instead there is no recognizable clusters. I am not sure if I am doing something wrong in obtaining the word embeddings or performing PCA on them. My method of getting the word embeddings:We are using the last layer of the 12 hidden layers to get the embedding. For PCA, we're using sklearn.decomposition and calling pca.fit_transform(). Is there a recommended way of normalizing the data (our word embeddings) before calling the function? "
1232,I encountered this problem while training BERT：(tensorflow read less bytes than requested),['bert'],"I encountered this problem while training BERT：(tensorflow read less bytes than requested),Do any kind people answer me？"
1233,cannot import name 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP' from 'transformers.modeling_distilbert',"['python', 'neural-network', 'bert', 'question-answering', 'distilbert']","I am trying to train the distil BERT model for Question Answering purpose. 
I have installed simple transformers and everything but when I try to run the following command:I am getting the error - >please help!"
1234,How to fine-tune a model that is already fine-tuned?,['bert'],"I would like to finetune a bert model with dataset1 first, after this training is done, I would like to train the model again with dataset2. But I don't know how to recover the model in my second training. What should I do to fine-tune the fine-tuned model? Thanks!"
1235,how to use ktrain for NER Offline?,"['python', 'tensorflow', 'offline', 'ner', 'bert']","I have trained my English model following this notebook (https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb). I  am able to save my pretrained model and run it with no problem. However, I need to run it again but OFFLINE and it is not working, I  understand that I need to download the file and do something similar to what is done here. https://github.com/huggingface/transformers/issues/136However, I  am not able to understand where do  I need to change the settings of ktrain.I  run this:and this is  the error I get: "
1236,"TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?","['tensorflow', 'tensorflow-datasets', 'tensorflow-estimator', 'transformer', 'bert']","I am pretraining BERT in 1 machine with 4 GPU.The input function code:The mirrow strategy code:The problem is that I have 4 GPU. Each GPU could run 8 batchsize at most.I set train_batch_size = 8 not 32. Is OK but I don't know each GPU get different data in one training step.If I set train_batch_size = 32, it will out of memory (OOM).Is my code right now? Will the data be distributed to 4 GPU and each GPU get different data?"
1237,Bert Tokenizer is not working despite importing all packages. Is there a new syntax change to this?,"['python', 'tokenize', 'sentiment-analysis', 'bert']","Trying to run the tokenizer for Bert but I keep getting errors. Can anyone help where I am going wrong.Error: AttributeError Traceback (most recent call last) in () ----> 1 FullTokenizer = bert.bert_tokenization.FullTokenizer 2 bert_layer = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1"", 3 trainable=False) 4 vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() 5 do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()AttributeError: module 'bert' has no attribute 'bert_tokenization'All the below have been imported for reference."
1238,What does merge.txt file mean in BERT-based models in HuggingFace library?,"['nlp', 'tokenize', 'huggingface-transformers', 'bert']","I am trying to understand what merge.txt file infers in tokenizers for RoBERTa model in HuggingFace library. However, nothing is said about it on their website. Any help is appreciated."
1239,Is there a way to convert a pytorch_model.bin to pytorch_model.pt and vice versa -— BERT vs SBERT,"['python', 'pytorch', 'bert']","I have been working on BERT for a while. I used Nvidia BERT which uses a model.pt checkpoint whereas while using SBERT/sentence_BERT, it uses model.bin with a bunch of files (i.e. model.bin, vocab.txt, config.json). What is the difference between these two. And, is there a script that can be used to convert one into another (and vice versa), so both can be used in place of each other. i.e. Re-using pretrained model from SBERT into Nvidia_BERT scripts and the reverse. Thank You!"
1240,German Chatbot or conversational AI,"['nlp', 'dataset', 'chatbot', 'bert']",I want to build a chatbot mostly BERT(Transformer) based in the German Language. But I do not find any German chatbot data set!So does it make sense to use google translator API to translate the English dataset to German and then train the model on it?Any idea where I can find German datasets or solve this issue?
1241,"RuntimeError: Given groups=3, weight of size 12 64 3 768, expected input[32, 12, 30, 768] to have 192 channels, but got 12 channels instead","['python-3.x', 'pytorch', 'conv-neural-network', 'bert']","I started working with Pytorch recently so my understanding of it isn't quite strong. I previously had a 1 layer CNN but wanted to extend it to 2 layers, but the input and output channels have been throwing errors I can seem to decipher. Why does it expect 192 channels? Can someone give me a pointer to help me understand this better? I have seen several related problems on here, but I don't understand those solutions either. Here's a snippet of data https://github.com/Kosisochi/DataSnippet"
1242,"ValueError: not enough values to unpack (expected 4, got 1): Pytorch","['python', 'deep-learning', 'nlp', 'pytorch', 'bert']",I am getting this error while running the eval report. I trained my model with bert-base-german-cased for my custom dataset.The code is as follows:When i run the do_eval function the following error happend. How do I fix this error?
1243,How to get embedding from bert finetuned model?,"['pytorch', 'transformer', 'bert']","I have finedtuned 'bert-base-uncased' model using transformer and torch which gave me pytorch_model.bin, vocab.txt and other files as output.
After loading the model how to I get embedding for complete vocab, like a matrix which maps every word to its embedding vector "
1244,How to stop BERT from breaking apart specific words into word-piece,"['python', 'text', 'nlp', 'tokenize', 'bert']","I am using a pre-trained BERT model to tokenize a text into meaningful tokens. However, the text has many specific words and I don't want BERT model to break them into word-pieces. Is there any solution to it?
For example:Create tokens like this:However, I want to keep the whole words as one token, like this:"
1245,Pytorch: IndexError: index out of range in self. How to solve?,"['python', 'pytorch', 'bert']","The error is as follows: 
    While running the training for text classification using bert models came across the follow. How do i fix it?"
1246,Python “Can't pickle local object” exception during BertModel training,"['python', 'nlp', 'text-classification', 'transformer', 'bert']","I am using simpletransformers.classification to train a Bert moder to classify some text inputs. Here is my code.Everything looks okay and it starts to training. But at the end of the training it gives an error like below.Sounds like this problem is related with worker_count because it runs using multithreading. But I could not find any solution.Operating System : Windows 10
RAM : 16 Gb"
1247,What's wrong with my NER transformers camembert model?,"['named-entity-recognition', 'ner', 'huggingface-transformers', 'bert']","I was trying to use transformers for NER in french language using 'camembert-base' model. I came across to this code from : https://huggingface.co/transformers/usage.html. Unfortunately, the prediction results of my short sentence is not satisfactory and I can't understand if there is something wrong with my code.Prediction's output :"
1248,Why we need the init_weight function in BERT pretrained model in Huggingface Transformers?,"['python', 'huggingface-transformers', 'bert']","In the code by Hugginface transformers, there are many fine-tuning models have the function init_weight. 
For example(here), there is a init_weight function at last.As I know, it will call the following codeMy question is If we are loading the pre-trained model, why do we need to initialize the weight for every module?I guess I must be misunderstanding something here."
1249,simpletransformers model trained on Colab doesn't work locally,"['python', 'google-colaboratory', 'bert', 'simpletransformers']","I've followed the instructions in this link and trained a BERT model on Google Colab. I then downloaded it and tried to use it locally on my computer. But something went wrong.Here is my code for training the model (on Google Colab):It worked completely fine on Colab but I need it locally, so I downloaded the archive that was created using the following code:I then unpacked it and placed in the directory with the file with the following code for testing the model:So the only difference in how I create models is use_cuda=False (because I don't have a GPU locally).When I executed the code the following output appeared:And it freezes on this moment. Nothing happens despite this progress bar.
What should I do to make it work locally? Maybe there is another way?Thanks in advance for any help!"
1250,Low accuracy during training for text summarization,"['tensorflow', 'keras', 'neural-network', 'training-data', 'bert']","I am trying to implement an extractive text summarization model. I am using keras and tensorflow. I have used bert sentence embeddings and the output of the embeddings are fed into an LSTM layer and then to a Dense layer with sigmoid activation function. I have used adam optimizer and binary crossentropy as the loss function.The training y labels is a 2d-array i.e [array_of_documents[array_of_biniary_labels_foreach_sentence]]The problem is that during training, I am getting the training accuracy of around 0.22.How can I improve my accuracy for the model?"
1251,How to download pre-trained models from Gluonnlp through a proxy?,"['proxy', 'mxnet', 'bert']",I am behind a corporate proxy. I am trying to download BERT's vocabulary. How can provide my proxy data to gluonnlp in my Jupyter Notebook?
1252,"Multiple answer spans in context, BERT question answering","['python', 'pytorch', 'bert', 'question-answering', 'squad']","I am writing a Question Answering system using pre-trained BERT with a linear layer and a softmax layer on top. When following the templates available on the net the labels of one example usually only consists of one answer_start_index and one answer_end_index. For example, from Huggingface when instantiating a SQUADFeatures object:However, in my own dataset I have examples where the answer word is found at several locations in the context, i.e. there may be several correct spans constituting the answer. My problem is I don't know how to manage such examples? In the templates available on the net labels are usually in a list, say:In my case this may look like:In other words, I do not have a list containing one label per example, but a list containing either single-labels or a list of ""labels"" for an example, i.e. a list consisting of lists. When following other templates the next step in the process is:However this of course (?) raises an error as my span_start lists and span_end lists does not contain only single-items but sometimes a list within the list.Anyone have an idea on how I can tackle this problem? Should I only use examples where there's only one span constituting the answer present in the context?If I work around the torch-error, will the backpropagation / evaluation/ computation of loss still work?Thank You! /B"
1253,Unable to pip install -U sentence-transformers,"['transformer', 'sentence', 'bert']","I am unable to do: pip install -U sentence-transformers. I get this message on Anaconda Prompt:
ERROR: Could not find a version that satisfies the requirement torch>=1.0.1 (from sentence-transformers) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch>=1.0.1 (from sentence-transformers)
Can someone help?"
1254,BERT embeddings for abstractive text summarisation in Keras using encoder-decoder model,"['python', 'keras', 'nlp', 'seq2seq', 'bert']",I am working on a text summarization task using encoder-decoder architecture in Keras. I would like to test the model's performance using different word embeddings such as GloVe and BERT. I already tested it out with GloVe embeddings but could not find an appropriate example for BERT embeddings in seq2seq models using Keras. This is an excerpt of my code:How to add BERT word embeddings to such a model? I tried this implementation on my data frame before tokenization but I ran into an error:I could not find a solution to it. Are there any other ways how to simply add these word embeddings as GloVe? 
1255,huggingface bert showing poor accuracy / f1 score [pytorch],"['pytorch', 'huggingface-transformers', 'bert']","I am trying BertForSequenceClassification for a simple article classification task.No matter how I train it (freeze all layers but the classification layer, all layers trainable, last k layers trainable), I always get an almost randomized accuracy score. My model doesn't go above 24-26% training accuracy (I only have 5 classes in my dataset).I'm not sure what did I do wrong while designing/training the model. I tried the model with multiple datasets, every time it gives the same random baseline accuracy.Dataset I used: BBC Articles (5 classes) https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbcConsists of 2225 documents from the BBC news website corresponding to
  stories in five topical areas from 2004-2005. Natural Classes: 5
  (business, entertainment, politics, sport, tech)I added the model part and the training part which are the most important portion (to avoid any irrelevant details). I added the full source-code + data too if that's useful for reproducibility.My guess is there is something wrong with the I way I designed the network or the way I'm passing the attention_masks/ labels to the model. Also, the token length 512 should not be a problem as most of the texts has length < 512 (the mean length is < 300).Model code:Training code:Full source-code with the dataset is available here: https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynbUpdate:After observing the prediction, it seems model almost always predicts 0:Actually, the model is always predicting the same output [0.2270, 0.1855, 0.2131, 0.1877, 0.1867] for any input, it's like it didn't learn anything at all.It's weird because my dataset is not imbalanced."
1256,Spacy's BERT model doesn't learn,"['python', 'spacy', 'text-classification', 'multiclass-classification', 'bert']","I've been trying to use spaCy's pretrained BERT model de_trf_bertbasecased_lg to increase accuracy in my classification project. I used to build a model from scratch using de_core_news_sm and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in this article as an example.Here is my code:Function get_data() opens files with different categories, creates a tuple like this one (text, {'cats' : {'category1': 0, 'category2':1, ...}}), gathers all these tuples into one array, which is then being returned to the main function.Function test(nlp) opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.Again, everything worked just fine with de_core_news_sm, so I'm pretty sure that functions get_data() and test(nlp) are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.Thanks in advance for any help!UPDATETrying to understand the above problem I decided to try the model with only a few examples (just like it is advised here). Here is the code:And the output was:Not only the model performs bad, the loss is not getting smaller and scores for all the test sentences are almost the same. And most importantly: it didn't even get those questions correct, that happened to be in the train data. So my question is: does the model even learn? And what am I doing wrong?Any thoughts?"
1257,How to print the output weights for the output layer in BERT?,"['nlp', 'stanford-nlp', 'bert']","I would like to print the output vector/tensor in BERT an wasn't sure how to do it.  I've been using the following example to walk myself through it: https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcXIts a simple classification problem, but I want to be able to get the output vector before we classify the training examples.  Can someone point to where in the code I can do this and how?"
1258,handling class imbalance in neural networks,"['tensorflow', 'classification', 'bert']","I am doing a multiclass classification problem. The class i am interested has 15% samples and the other two classes have 60% and 25% samples.
(1) Is there a way to handle class imbalance in BERT using weight parameters?
(2) If oversampling or undersampling were done, would we do it on the the training set only or both training and validation sets. I know that we have to keep the test set real.
Any ideas would be appreciated.
Thanks"
1259,How to convert Following Bert Code from tf1.x to tf2.x?,"['python-3.x', 'nlp', 'tensorflow2.0', 'tf.keras', 'bert']","While trying to use Bert model with tensorflow 2.x version for text classification, I am getting this error. File ""C:\Users\saran\bert\modeling.py"", line 671, in attention_layer
  query_layer = tf.layers.dense(
  AttributeError: module 'tensorflow' has no attribute 'layers'The error is around the Code lines(671-692) below taken from modeling.py in https://github.com/google-research/bert: 
''''''
To solve this, I found tf.layers.dense does not work with tensorflow 2.x and I should use tf.keras.layers.Dense. I made the following changes in the code, now it is giving me the following error:
''''''File ""C:\Users\saran\bert\modeling.py"", line 676, in attention_layer
      kernel_initializer=create_initializer(initializer_range))
  TypeError: init() got multiple values for argument 'activation' Can someone please help me migrate this code from tf1.x to tf2.x with minimal changes and without compromising the way it is supposed to work in Bert?"
1260,Adding a pretrained model to my layers to get embeddings,"['tensorflow', 'neural-network', 'embedding', 'pre-trained-model', 'bert']",I want to use a pretrained model found in [BERT Embeddings] https://github.com/UKPLab/sentence-transformers and I want to add a layer to get the sentence embeddings from the model and pass on to the next layer. How do I approach this?The inputs would be an array of documents and each document containing an array of sentences.The input to the model itself is a list of sentences where it will return a list of embeddings.This is what I've tried but couldn't solve the errors:the error message
1261,How to get Sentence embedding using pre-trained SciBERT weights?,"['keras', 'tensorflow2.0', 'bert']",How can I get Sentence embedding using pre-trained SciBERT in keras(tensorflow)
1262,Cased VS uncased BERT models in spacy and train data,"['python', 'spacy', 'bert']","I want to use spacy's pretrained BERT model for text classification but I'm a little confused about cased/uncased models. I read somewhere that cased models should only be used when there is a chance that letter casing will be helpful for the task. In my specific case: I am working with German texts. And in German all nouns start with the capital letter. So, I think, (correct me if I'm wrong) that this is the exact situation where cased model must be used. (There is also no uncased model available for German in spacy). But what must be done with data in this situation?
Should I (while preprocessing train data) leave it as it is (by that I mean not using the .lower() function) or it doesn't make any difference?"
1263,Cannot load German BERT model in spaCy,"['python', 'spacy', 'transformer', 'bert']","Here is my problem: I am working on the German text classification project. I use spacy for that and decided to fine-tune its pretrained BERT model to get better results. However, when I try to load it to the code, it shows me errors.Here is what I've done:import spacy
 nlp = spacy.load('de_trf_bertbasecased_lg')And the output was:If I run the same code in PyCharm, it also shows me these two lines before all of those above:If I got it right, these two lines complain that I don't have a GPU. However, according to the docs, I should be able to use BERT even without GPU.So I am really stuck right now and looking for your help. I should also mention, that I used de_core_news_sm model before and it worked fine.I have also already tried several solutions, but none of them worked. I tried:
this and this. I have also tried to uninstall all spacy-related libraries and installed them again. Didn't help either.I am working with:Windows 10 HomePython: 3.7.2Spacy: 2.2.4Spacy-transformers: 0.5.1Would appreciate any help or advice!"
1264,What are some techniques to improve contextual accuracy of semantic search engine using BERT?,"['semantic-web', 'word-embedding', 'language-model', 'bert']","I am implementing a semantic search engine using BERT (using cosine distance) To a certain extend the method is able to find out sentences in a high level context. However when it comes narrowed down context of the sentence, it gives several issues.Example:
If my search term is ""Wrong Product"", the search engine might match with sentences like ""I bought a washing machine but it was defective"".I understand that the fixes to these is always finding some equilibrium and live with minor errors.However if there are any rules, techniques which has improved your semantic search implementation accuracy please do share. "
1265,"Deploying on heroku bert pytorch model using flask: ERROR: _pickle.UnpicklingError: invalid load key, 'v'","['python', 'git', 'flask', 'heroku', 'bert']","Trying to deploy bert model on Heroku. ERRORMODEL.load_state_dict(torch.load(""weight.bin""))2020-05-18T06:32:32.134536+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.7/site-packages/torch/serialization.py"", line 593, in load2020-05-18T06:32:32.134536+00:00 app[web.1]: return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)2020-05-18T06:32:32.134536+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.7/site-packages/torch/serialization.py"", line 763, in _legacy_load2020-05-18T06:32:32.134537+00:00 app[web.1]: magic_number = pickle_module.load(f, **pickle_load_args)2020-05-18T06:32:32.134537+00:00 app[web.1]: _pickle.UnpicklingError: invalid load key, 'v'."
1266,How to fine tune BERT on Squad2.0,"['python', 'nlp', 'bert', 'squad']","I am really new to BERT and I would like to fine tune BERT base model on Google Colab. Basically I set up using GPU, downloaded data and try to call python run_squad.pyThe code above basically make sure that I have a GPU set up, get required dependencies and get Squad2.0 data downloaded. Next is to call run_squad.py and this is where I am lost. Here is my file location. I ran the cell and got the error: python3. I thought I configured the path correctly, why is it still missing bert_config.json?"
1267,"Unable to import TensorFlow_hub, getting “AttributeError: module 'tensorflow' has no attribute 'flags'” message","['python', 'tensorflow', 'bert']","I am trying to import TensorFlow hub in my local jupyter notebook but unable to do so. I have created a local conda environment installed all packages. Current tf version: Tensorflow 2.0 and local tf hub version : tensorflow-hub 0.1.1. when I run the ""import tensorflow_hub as hub"" code i get the below error."
1268,BERT encoding layer produces same output for all inputs during evaluation (PyTorch),"['python', 'nlp', 'pytorch', 'pre-trained-model', 'bert']","I don't understand why my BERT model returns the same output during evaluation. The output of my model during training seems correct, as the values were different, but is totally the same during evaluation.
Here is my BERT model class My dataset classMy evaluation functionAnd my training functionThanks!"
1269,Freezing of BERT layers while using tfhub module,"['tensorflow', 'tensorflow-hub', 'bert', 'tf-hub']","In this link click here the author says that:If user wishes to fine-tune/modify the weights of the model, this parameter has to be set as True.
So my doubt is if I set this to false does it mean that I am freezing all the layers of the BERT which is my intension too. I want to know if my approach is right."
1270,How to use tensorflow.estimator to load only part of layer parameters of the pre training model,"['tensorflow', 'bert']","I want to add some additional layers behind the Bert model, when loading the pre_training model, delete these layers from the list that needs to be loaded, but it fails, and if still reports that cannot find the parameter. use tensorflow 1.14.0
Can someone tell me how to do it? Thanks very much!
enter image description here
enter image description hereI got the following error"
1271,How to get the probability of a particular token(word) in a sentence given the context,"['nlp', 'pytorch', 'huggingface-transformers', 'bert']","I'm trying to calculate the probability or any type of score for words in a sentence using NLP. I've tried this approach with GPT2 model using Huggingface Transformers library, but, I couldn't get satisfactory results due to the model's unidirectional nature which for me didn't seem to predict within context. So I was wondering whether there is a way, to calculate the above said using BERT since it's Bidirectional.I've found this post relatable, which I randomly saw the other day but didn't see any answer which would be useful for me as well.Hope I will be able to receive ideas or a solution for this. Any help is appreciated. Thank you. "
1272,Understanding the Hugging face transformers,"['pre-trained-model', 'huggingface-transformers', 'bert', 'question-answering', 'squad']","I am new to the Transformers concept and I am going through some tutorials and writing my own code to understand the Squad 2.0 dataset Question Answering using the transformer models. In the hugging face website, I came across 2 different linksI want to know the difference between these 2 websites. Does one link have just a pre-trained model and the other have a pre-trained and fine-tuned model? Now if I want to use, let's say an Albert Model For Question Answering and train with my Squad 2.0 training dataset on that and evaluate the model, to which of the link should I further?"
1273,How to handle text classification model that gives few results with higher confidence to wrong category?,"['python', 'machine-learning', 'text-classification', 'false-positive', 'bert']","I had a dataset of 15k records. I trained the model using a k-train package and 'bert' model with 5k samples. The train-test split is 70-30% and test results gave me accuracy and f1 scores as 93-94%. I felt the model is well trained, But on evaluating with the remaining 10k records of my dataset to the trained model. Around 10% of result samples are getting predicted to the wrong label with higher confidence. I want to process the text to the next level if the confidence is high. If it's mapping to the wrong label with higher confidence there is no way to say this model is good. How to handle or what kind of techniques to applied to bring correct predictions here?"
1274,OperatorNotAllowedIngraphError tensorflow problem,"['python', 'tensorflow', 'neural-network', 'bert']",Error: OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.I'm trying to implement BERT sentence encoding followed by neural network construction for a text similarity project. The two input columns consists of quora questions in the form of strings. Can you please help me correct the code ?[code]
1275,Bert pre-trained model giving random output each time,"['python-3.x', 'pytorch', 'huggingface-transformers', 'bert']","I was trying to add an additional layer after huggingface bert transformer, so I used BertForSequenceClassification inside my nn.Module Network. But, I see the model is giving me random outputs when compared to loading the model directly.Model 1:Out:Model 2:They should be the same model, right. I found a similar issue here but no reasonable explanation https://github.com/huggingface/transformers/issues/2770Does Bert has some ranomized parameter if so how to get reproducible output?Why the two models give me different outputs? Is there something I'm doing wrong?"
1276,DISTILBERT_BASE_UNCASED failed to load - Hugging face transformers,"['huggingface-transformers', 'bert']","I am trying to execute :I am getting the following error:ktrain version 0.14.6
transformers version 2.8.0Libraries was installed using pip install.Any help would be appreciated."
1277,Pytorch import failing,"['dll', 'bert']","I have been trying to search for similar errors but found none. Below is Process log for my programI am working on BERT code ...got this error while importing Torch,also getting the same kind of error while importing BertConfig, BertForSequenceClassification, BertTokenizer.
My Python version is 3.7.6 
torch = 0.6.0"
1278,How to use an AutoModelWithLMHead for Question Answering without use something like BertForQuestionAnswering.from_pretrained?,"['nlp', 'transformer', 'huggingface-transformers', 'bert', 'question-answering']","I have a model pretrained from a BERT:And my dataset have this format:Result in :The question is, what should I do inside my model on the forward to transfor a simple BERT model for question answering:"
1279,Finetuning BERT on Custom data using Colab,['bert'],"I am running run_lm_finetuning on colab, to fine tune CamemBERT on custom vocabulary.I am using the following parameters:However, I am getting the following error:Anyone has an idea about this error?"
1280,Getting embedding lookup result from BERT,"['python', 'tensorflow', 'nlp', 'huggingface-transformers', 'bert']","Prior to passing my tokens through BERT, I would like to perform some processing on their embeddings, (the result of the embedding lookup layer). The HuggingFace BERT TensorFlow implementation allows us to access the output of embedding lookup using:Subsequently, one can process inputs_embeds and then send this in as an input to the same model using:where output now contains the output of BERT for the modified input. However, this requires two full passes through BERT. Instead of running BERT all the way through just to perform embedding lookup, I would like to just get the output of the embedding lookup layer. Is this possible, and if so, how?"
1281,HuggingFace BERT `inputs_embeds` giving unexpected result,"['python', 'tensorflow', 'nlp', 'huggingface-transformers', 'bert']","The HuggingFace BERT TensorFlow implementation allows us to feed in a precomputed embedding in place of the embedding lookup that is native to BERT. This is done using the model's call method's optional parameter inputs_embeds (in place of input_ids). To test this out, I wanted to make sure that if I did feed in BERT's embedding lookup, I would get the same result as having fed in the input_ids themselves.The result of BERT's embedding lookup can be obtained by setting the BERT configuration parameter output_hidden_states to True and extracting the first tensor from the last output of the call method. (The remaining 12 outputs correspond to each of the 12 hidden layers.)Thus, I wrote the following code to test my hypothesis:Again, the output of the call method is a tuple. The first element of this tuple is the output of the last layer of BERT. Thus, I expected result[0] and result2[0] to match. Why is this not the case?I am using Python 3.6.10 with tensorflow version 2.1.0 and transformers version 2.5.1.EDIT: Looking at some of the HuggingFace code, it seems that the raw embeddings that are looked up when input_ids is given or assigned when inputs_embeds is given are added to the positional embeddings and token type embeddings before being fed into subsequent layers. If this is the case, then it may be possible that what I'm getting from result[-1][0] is the raw embedding plus the positional and token type embeddings. This would mean that they are erroneously getting added in again when I feed result[-1][0] as inputs_embeds in order to calculate result2.Could someone please tell me if this is the case and if so, please explain how to get the positional and token type embeddings, so I can subtract them out? Below is what I came up with for positional embeddings based on the equations given here (but according to the BERT paper, the positional embeddings may actually be learned, so I'm not sure if these are valid):"
1282,PermissionDeniedError: From /job:worker/replica:0/task:0,"['google-cloud-storage', 'google-colaboratory', 'bert']","I am using google colaboratory to pretrain bert.My train_data is on google cloud storage,my code is on colabThe flowing is error message:This is my colab cordI think maybe I need to set permission for my train_data on cloud storage 
How can I get permission from google cloud storage"
1283,how to fine tune google bert to get better predictions on stack-overflow tags dataset,"['machine-learning', 'data-science', 'multiclass-classification', 'bert']","Im using google bert to classify text from here, it is a multi class classifier:
https://github.com/google-research/bert
I changed it to use it for my own requirements, I removed the TPU estimator
and use it to predict stack-overflow tags.
but unfortunately I get very poor results from bert, I think I maybe need to change the parametersany idea which parameter in the above run_bert() function should I fine tune in order to see some improvement?                           "
1284,Why do we need “ state_dict = state_dict.copy() ”,"['pytorch', 'bert']","In my task I want to load the weights from the already pretrained model to my local model. But I don’t understand why we need this line "" state_dict = state_dict.copy() "" if the two have the same name "" state_dict""Note: the above code is from huggingface, (line: 669)
https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_utils.py"
1285,How can i use BERT fo machine Translation?,"['jupyter-notebook', 'machine-translation', 'bert', 'sequence-to-sequence']","I got a big problem. For my bachelor thesis I have to make a machine tranlation model with BERT. 
But I am not getting anywhere right now. 
Do you know a documentation or something that can help me here? 
I have read some papers in that direction but maybe there is a documentation or tutorial that can help me.For my bachelor thesis I have to translate from a summary of a text into a title.
I hope someone can help me."
1286,Wordpiece Tokenization Model,"['nlp', 'tokenize', 'huggingface-transformers', 'bert', 'bert-language-model']","Can somebody tell me how exactly the wordpiece model work ? I am having some hard time trying to understand how exactly the wordpiece model is working. I understand the BPE that it is based on merging according the highest frequency pairs. After digging for hours on the internet and reading the paper. It is mentioned that in wordpiece we make the final merge according to what maximize the likelihood of the Language model we created.
How is this language model is created ? Is it created by Probability of Pair equal to Count of Pair / Total Count of Pairs or what ?
What i understand is that we want to measure which token pair minus separate tokens is the largest , like if we have ""de"" = 9 , ""d"" = 15 ""e"" = 12 and ""th"" = 10 , ""t"" = 12 ""h""= 12 , then we choose to merge token ""t"" and ""h"" as its 10-24 > 9-27. Am i right ? Please somebody correct me"
1287,Question about Fine Tuning Bert for MaskedLM,"['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert']","I'm working with BERT for incomplete sentences. Specifically, sentences in Spanish without articles, like:Abandonó ___ universidad el primer año. The predicted word should be the article ""la"".I want to improve my accuracy. I have been reading a lot about fine tuning but I'm still a bit lost about it. I was wondering if I could fine tune BertforMaskedLM focusing just on this type of words, for example masking just articles in purpose or something like that.Thanks in advance."
1288,How does masked_lm_labels argument work in BertForMaskedLM?,"['nlp', 'language-model', 'bert', 'bert-language-model', 'mlmodel']","This code is from huggingface transformers page. https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlmI cannot understand the masked_lm_labels=input_ids argument in model. 
How does it work? Does it means that it will automatically mask some of the text when input_ids is passed?"
1289,Get the attention vector from the last layers of BERT,"['text', 'nlp', 'bert']","Is there any way to get the attention vector with normalizing values (0-1) from the last layer of BERT? I'm interested in getting the attention value that BERT assigns to each word in a sentence. I'm working on emotion classification. I want to extract the relevant words associated with emotions. For example:I feel wonderful today. The words feel and wonderful are the more relevant words in the sentence for the classifier, so I want to get the attention scores that BERT assigns to each of them. Thanks in advance"
1290,How to prevent non-deterministic output behavior in BERT with Tensorflow (in the tutorial mentioned in the body post),"['python', 'tensorflow', 'sentiment-analysis', 'bert']","While fine-tuning BERT (with a Tensorflow implementation) for text classifications purposes, I noticed that at inference time, the results are not deterministic.The issue can be replicated  by running the tutorial by Google which shows an example on how to tune BERT for sentiment analysis: at inference time, when predicting the sentiment of the sentences, if you run getPrediction(pred_sentences) multiple times, you can see that the probability outputted are always changing.Using the tutorial code as reference, how can one prevent the non-deterministic output behavior?"
1291,"Removing commas after processing lists of strings, when ' '.join(x) does not work","['python', 'nlp', 'spacy', 'bert', 're']","So I fed in a dataframe of sentences for token prediction in BERT, and I received as output along with the predictions, the sentences split into words. 
  Now i want to revert my dataframe of the split/tokenized sentences and predictions back to the original sentence.(of course i have the original sentence, but i need to do this process so that the predictions are in harmony with the sentence tokens)I identified three processes necessary.
1. Remove quote marks 2. removes the CLS ,SEP and their extra quote marks and commas, 3. remove the commas separating the words and merge them.The first two functions largely worked correctly, but the last one did not. instead, i got a result that looked like this.The commas didnt go away, and the text got distorted instead. I am definitely missing something. what could that be?"
1292,BERT Certainty (iOS),"['ios', 'machine-learning', 'bert']","I am currently integrating the BERT model listed on https://developer.apple.com/machine-learning/models/#text into an iOS application and have had difficulty removing answers that have low certainty.I have used the sample code found at the link above but because I wanted to answer questions based on larger volumes of text, I loop over an array of paragraphs and predict an answer for each one. However, the model does not return nil or ""No Answer"" if an answer is not found and instead returns a (seemingly) random substring. I suppose what I am trying to ask is: is it possible to access the certainty of BERT's response to filter out unlikely results? Or is there another way to get BERT to only return results above a set certainty threshold?"
1293,Huggingface's BERT tokenizer not adding pad token,"['tokenize', 'huggingface-transformers', 'bert']","It's not entirely clear from the documentation, but I can see that BertTokenizer is initialised with pad_token='[PAD]', so I assume when you encode with add_special_tokens=True then it would automatically pad it. Given that pad_token_id=0, I can't see any 0s in the token_ids however:Output:"
1294,BERT pre-training loss not decreasing,"['nlp', 'loss', 'bert']","I'm pre-training BERT with Bulgarian dataset on a single Cloud TPU v2 8 using the original parameters (learning rate = 5e-5, training batch size = 32, number of training steps = 100000).
The problem is that it finishes training very fast (3 hours) and the loss doesn't go below 3. My training data is 40 GB and I'm using tensorflow 1.15 enter image description hereDo you have any idea what the problem might be?"
1295,Pretraining a language model on a small custom corpus,"['deep-learning', 'transfer-learning', 'huggingface-transformers', 'language-model', 'bert']","I was curious if it is possible to use transfer learning in text generation, and re-train/pre-train it on a specific kind of text.   For example, having a pre-trained BERT model and a small corpus of medical (or any ""type"") text, make a language model that is able to generate medical text. The assumption is that you do not have a huge amount of ""medical texts"" and that is why you have to use transfer learning.Putting it as a pipeline, I would describe this as:  Does this sound familiar? Is it possible with hugging-face?"
1296,tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested [[{{node checkpoint_initializer_196}}]],"['python', 'tensorflow', 'bert']",I used BERT released from Google research to classify my own data set. I created a new class in the source code to specify my own classification tasks and I run my code on Colab. The error occurs when run the run_classifier.py in the file. Here is my codecreate my own taskrun
1297,What can be the cause of the validation loss increasing and the accuracy remaining constant to zero while the train loss decreases?,"['pytorch', 'text-classification', 'multiclass-classification', 'bert', 'skorch']","I am trying to solve a multiclass text classification problem. Due to specific requirements from my project I am trying to use skorch (https://skorch.readthedocs.io/en/stable/index.html) to wrap pytorch for the sklearn pipeline. What I am trying to do is fine-tune a pretrained version of BERT from Huggingface (https://huggingface.co) with my dataset. I have tried, in the best of my knowledge, to follow the instructions from skorch on how I should input my data, structure the model etc. Still during the training the train loss decreases until the 8th epoch where it starts fluctuating, all while the validation loss increases from the beginning and the validation accuracy remains constant to zero. My pipeline setup isin which I am using a tokenizer class to preprocess my dataset, tokenizing it for BERT and creating the attention masks. It looks like thisthen I initialize the model I want to fine-tune as I initialize the model and create the classifier with skorch as followsand I use fit like thatin which my training samples are lists of strings and my labels are the an array containing the indexes of each class, as pytorch requires.This is a sample of what happenstraining historyI have tried to keep train only the fully connected layer and not BERT but I have the same issue again. I also tested the train accuracy after the training process and it was only 0,16%. I would be grateful for any advice or insight on how to solve my problem! I am pretty new with skorch and not so comfortable with pytorch yet and I believe that I am missing something really simple. Thank you very much in advance!"
1298,Is it possible to fine-tune BERT to do retweet prediction?,"['machine-learning', 'nlp', 'bert']","I want to build a classifier that predicts if user i will retweet tweet j. The dataset is huge, it contains 160 million tweets. Each tweet comes along with some metadata(e.g. does the retweeter follow the user of the tweet).the text tokens for a single tweet is an ordered list of BERT ids. To get the embedding of the tweet, you just use the ids (So it is not text)Is it possible to fine-tune BERT to do the prediction? if yes, what do courses/sources do you recommend to learn how to fine-tune? (I'm a beginner)I should add that the prediction should be a probability.If it's not possible, I'm thinking of converting the embeddings back to text then using some arbitrary classifier that I'm going to train. "
1299,bert + text and structur data,"['text', 'tabular', 'bert']","For each instance, I have the text and tabular data. I was wondering if there is any way I can use Bert_classification and combine the result to classify the whole dataset without overfitting. Is there any way to make two different classifications for text and tabular data and combine them together?  "
1300,Gradient of the loss of DistilBERT for measuring token importance,"['pytorch', 'transformer', 'attention-model', 'huggingface-transformers', 'bert']","I am trying to access the gradient of the loss in DistilBERT with respect to each attention weight in the first layer. I could access the computed gradient value of the output weight matrix via the following code when requires_grad=True where model is the loaded distilbert model.
My question is how to get the gradient with respect to [SEP] or [CLS] or other tokens' attention? I need it to reproduce the figure about the ""Gradient-based feature importance estimates for attention to [SEP]"" in the following link: 
https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062A similar question for the same purpose has been asked in the following, but it is not my issue:
BERT token importance measuring issue. Grad is none "
1301,Token indices sequence length error when using encode_plus method,"['nlp', 'tokenize', 'huggingface-transformers', 'bert']","I got a strange error when trying to encode question-answer pairs for BERT using the encode_plus method provided in the Transformers library.I am using data from this Kaggle competition. Given a question title, question body and answer, the model must predict 30 values (regression problem). My goal is to get the following encoding as input to BERT:[CLS] question_title question_body [SEP] answer [SEP]However, when I try to useand encode only the second input from train.csv as follows:I get the following error:It says that the length of the token indices is longer than the specified maximum sequence length, but this is not true (as you can see, 46 is not > 512).This happens for several of the rows in df_train. Am I doing something wrong here?"
1302,Error importing BERT: module 'tensorflow._api.v2.train' has no attribute 'Optimizer',"['python', 'tensorflow', 'classification', 'bert']","I tried to use bert-tensorflow in Google Colab, but I got the following error:--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
        1 import  bert
  ----> 2 from bert import run_classifier_with_tfhub # run_classifier
        3 from bert import optimization
        4 from bert import tokenization1 frames /usr/local/lib/python3.6/dist-packages/bert/optimization.py
  in ()
       85 
       86 
  ---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):
       88   """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""
       89 AttributeError: module 'tensorflow._api.v2.train' has no attribute
  'Optimizer'Here is the code I tried:!pip install --upgrade --force-reinstall tensorflow
!pip install --upgrade --force-reinstall tensorflow-gpu
!pip install tensorflow_hub
!pip install sentencepiece
!pip install bert-tensorflowfrom sklearn.model_selection import train_test_split
import pandas as pd
from datetime import datetime
from tensorflow.keras import optimizers
import  bert
from bert import run_classifier
from bert import optimization
from bert import tokenization
I've also tried 
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()But got the same error."
1303,Which pre-trained model do I need to use for long text classification in BERT?,"['nlp', 'bert']","We know that bert has a max length limit of tokens = 512, So if an acticle has a length of much bigger than 512, such as 10000 tokens in text. In this case, How can I use BERT?"
1304,Is there any solution to run tensorflow on multiple gpus on windows?,"['multi-gpu', 'bert']","I am trying the bert-multi-gpu example from github, on Windows2019 Server, with 4 NVIDIA TeslaM10 GPU cores. It seems I need to install NCCL for running on multiple gpus.However I cannot install NCCL on Windows and nvidia-docker on windows server. So Is there any solutions to run bert-multi-gpu on multiple gpus  on windows?
Thanks in advance"
1305,How to get Embedded table size for BERT in Tensorflow,"['tensorflow', 'tensorflow-estimator', 'bert']","I have a pre-trained BERT model and fine-tuning on my local machine. The code is obtained from Google AI github repo. The code is implemented with Tensorflow Estimator. How to get the size of the embedded table for a given data set?Thanks,
K"
1306,How to add pooling layer to BERT QA for large text,"['python', 'pytorch', 'bert', 'question-answering', 'max-pooling']","I'm trying to implement a Question answering system that deal with large input text: so the idea is to split the large input text into subsequences of 510 tokens, after I will generate the representation of each sequence independently and using a pooling layer to generate the final representation of the input sequence.I using the CamemBERT model for French language.I have tried the following code:Since I'm a beginner with pyTorch, I'm not sure about if the code should be like that.Please if you have any advice or if you need any further information contact me."
1307,how to do avg pool on the output of bert model for each sentence?,"['tensorflow', 'pooling', 'bert']","for classification, we usually use [CLS] to predict labels. but now i have another request to do avg-pooling on the output of each sentence in bert model. it seems a little bit hard for me? 
sentence is split by [SEP] but lengh of each sentence in each sample of a batch is not equal, so tf.split is not fit for this problem?  an example as follows(batch_size=2), how to get the avg-pooling of each sentences?[CLS] w1 w2 w3 [sep] w4 w5 [sep][CLS] x1 x2 [sep] x3 w4 x5 [sep]"
1308,Testing BERT for large sequences,"['python', 'nlp', 'pytorch', 'bert']","I'm a beginner with pytorch and I want just to test the pretrained model CamemBERT for Question Answering task with long input text sequences (over than 512 tokens).
So, I'm trying to implement it using the following code:To deal with text length sequence limitation I want to split the input sequence into subsequences and generate the text representation of each one of them and then using a max pooling layer to generate the final text representation (510 tokens).So, can any one help me in order to adapt this script to attend this objective ?Thank you,"
1309,kaggle kernel: AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer',"['python-3.x', 'tensorflow', 'kaggle', 'tensorflow-hub', 'bert']","I have installed the following python libraries in a kaggle kernel:  tensorflow==1.15, bert-tensorflow==1.0.1, tensorflow-hub==0.8.0 I am trying to import the bert module. However I am getting this error. It as follows. As per some stack overflow threads, I believe I have installed the correct and compatible libraries. Would anyone be able to help me in this regards. Thanks & Best RegardsMichael"
1310,How to get input text representation form BERT model using pytorch,"['pytorch', 'embedding', 'bert']","Any one help me to get the representation vector of an input text using the pretrained BERT model using the pytorch framework.Or in other words, how can I access to the output of a specific layer of BERT model ? Thank you"
1311,Evaluation of the performance of BERT model on a document,"['nlp', 'evaluation', 'bert', 'question-answering']","I'm using the CamemBERT model pretrained on FQuAD dataset for question answering task and I want to use this model to answer some questions for another domain from a very large input document (over than 512 tokens).Since the CamemBERT is pretrained on a dataset that not share too much semantics with the target document and also the document is very large, so my question is if there is any automatic method that can help me to evaluate the performance of this model to answering question from the input large document ?Thank you."
1312,what is the classifier used in BERT for sequence classification?,"['classification', 'extractor', 'bert']","I read BERT description in GitHub and other articles, but I am still not understanding wich classifier used by BERT?is it logistic regression, SVM, Neural network, or specific Deep model?another question, can I describe BERT model as a feature extraction model OR classification model OR BOTH for sequence classification task??
thank you in advance"
1313,[Tensorflow - Hub library) How do i export a model,"['tensorflow', 'export', 'transfer', 'bert']","I'm working with the BERT tensorflow model. I want to take a part of the model, a tokenizer, and make a Module out of it for later use.Basically I open and edit the tokenizer with the following code:Then i register the module for exportAfter this, i have to export the module. Looking at the LastestModuleExporter documentation (https://www.tensorflow.org/hub/api_docs/python/hub/LatestModuleExporter), it seems like i need an estimator as a parameter, for the ""export"" method. Can't find any examples of this online, so i'd be happy if someone could point me towards the right direction.Thanks!"
1314,How to convert tweets to .pickle file in the same way as ATIS datatest,"['python-3.x', 'pickle', 'bert']","I am following tutorial given at the link below:https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03The tutorial uses the Atis dataset that is available at the following link:https://www.kaggle.com/siddhadev/ms-cntk-atisThis data set contains 4978 train and 893 test spoken utterances (text) classified into one of 26 intents. Each token in a query utterance is aligned with slot filling IOB labelAccording to the tutorial, the code to load ATIS dataset in Google Colab notebook is:I want to replace 'atis.training.pkl' and 'atis.test.pkl' with my training and test dataset. Now the problem is to format my data in a similar way as 'atis.test.pkl' and 'atis.train.pkl'   I used the following code to read the contents of atis.test.pkl:I could not understand the output. How to format my data like ATIS dataset? Few Samples from my dataset as follows:@FansMotorola When will moto g4 plus oreo update get?@FansMotorola Haven't seen the update. It is already February@FansMotorola Received in Moto G6 too today."
1315,Text classification using BERT - how to handle misspelled words,"['pytorch', 'text-classification', 'huggingface-transformers', 'bert', 'misspelling']","I am not sure if this is the best place to submit that kind of question, perhaps CrossValdation would be a better place. I am working on a text multiclass classification problem.
I built a model based on BERT concept implemented in PyTorch (huggingface transformer library). The model performs pretty well, except when the input sentence has an OCR error or equivalently it is misspelled. For instance, if the input is ""NALIBU DRINK"" the Bert tokenizer generates ['na', '##lib', '##u', 'drink'] and model's prediction is completely wrong. On the other hand, if I correct the first character, so my input is ""MALIBU DRINK"", the Bert tokenizer generates two tokens ['malibu', 'drink'] and the model makes a correct prediction with very high confidence. Is there any way to enhance Bert tokenizer to be able to work with misspelled words? "
1316,Load bert model in java,"['java', 'python', 'named-entity-recognition', 'bert']","I have bert model for named entity recognition.(config.json, model.bin, vocab.txt).
I can load model and get named entities from text with model in pythonHow can i load this model in Java and get named entities"
1317,Training a Bert word embedding model in tensorflow,"['python', 'tensorflow', 'nlp', 'bert']","I have my own corpus of plain text. I want to train a Bert model in TensorFlow, similar to gensim's word2vec to get the embedding vectors for each word.What I have found is that all the examples are related to any downstream NLP tasks like classification. But, I want to train a Bert model with my custom corpus after which I can get the embedding vectors for a given word.Any lead will be helpful."
1318,Issue when preprocessing text with Ktrain and DistilBERT,"['python', 'keras', 'transformer', 'bert', 'distilbert']","Following the example notebook here:https://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroup-distilbert.ipynbAt STEP 1: Preprocess Data, I run into the errors listed below. When I do exactly the same in a Colab notebook, it works. What am I missing on my machine? I am able to run this with BERT, DistilBERT causes problems.causes:Any ideas what's wrong here?"
1319,Using BERT embeddings for Seq2Seq model building,"['tensorflow', 'word-embedding', 'seq2seq', 'bert', 'glove']","Earlier I've used Glove embedding to build the seq2seq model for text summarization, Now I want to change the Glove with BERT to see the performance of the model. For this, I used the bert-as-service feature from https://github.com/hanxiao/bert-as-service But giving the input to the model the same as Glove failing. How to code this part?"
1320,CUDA out of memory,"['nlp', 'pytorch', 'ner', 'bert']","I am getting error when trying to run BERT model for NER task.
""CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.82 GiB total capacity; 2.58 GiB already allocated; 25.38 MiB free; 6.33 MiB cached)I have also tried reducing batch size to 1cAttached is the output of nvidia-smi:-nvidia-smi"
1321,Does BertForSequenceClassification classify on the CLS vector?,"['python', 'machine-learning', 'pytorch', 'bert', 'huggingface-transformers']","I'm using the Huggingface Transformer package and BERT with PyTorch. I'm trying to do 4-way sentiment classification and am using BertForSequenceClassification to build a model that eventually leads to a 4-way softmax at the end.My understanding from reading the BERT paper is that the final dense vector for the input CLS token serves as a representation of the whole text string:The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.So, does BertForSequenceClassification actually train and use this vector to perform the final classification?The reason I ask is because when I print(model), it is not obvious to me that the CLS vector is being used. Here is the bottom of the output:I see that there is a pooling layer BertPooler leading to a Dropout leading to a Linear which presumably performs the final 4-way softmax. However, the use of the BertPooler is not clear to me. Is it operating on only the hidden state of CLS, or is it doing some kind of pooling over hidden states of all the input tokens?Thanks for any help."
1322,"using colab , flair:RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED","['bert', 'flair']","--> 559                               self.dropout, self.training, self.bidirectional, self.batch_first)
    560         else:
    561             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILEDI don't know how to fix this problem, could you help me?"
1323,How to keep a Google Bert model running in production?,"['nlp', 'bert']","I would like to put my Bert relation extraction model into production. However, it takes about 20 minutes even just to run a prediction of 200 sentences, which is too long for production use. I think the 20 minutes include spinning up the resources, etc. Any suggestions I can keep it always running and to cut down the time?Thank you very much!"
1324,Can I use non-fine-tuned BERT model from TF HUB to serve it with TF serving?,"['python', 'tensorflow', 'nlp', 'bert']","I'm a new to TF serving and currently I have such kind of problem. I run server part using bert_en_uncased from TF HUB, but I don't understand how to implement client side correctly. I faced with a couple of articles but each of them assumes that I have a ready-made fine-tuned model with pre-assigned handlers for requests. Can anyone share some tutors or maybe API references to facilitate my task?Some of articles I have read:PS. I'm not trying to create QA model or something like that, I just need BERT embeddings from this particular model."
1325,Access the output of several layers of pretrained DistilBERT model,"['python', 'nlp', 'pytorch', 'bert', 'huggingface-transformers']","I am trying to access the output embeddings from several different layers of the pretrained ""DistilBERT"" model. (""distilbert-base-uncased"")The bert_output seems to return only the embedding values of the last layer for the input tokens."
1326,tf.Session().run() throws tensorflow iterator error,"['tensorflow', 'iterator', 'tensorflow-estimator', 'bert']",i am attempting to write a tensorflow1 version of Google-Bert without using tf.Estimator as there is less flexibility to add custom layers in it. i am getting the following error:getting this error when a run tf.Session() on tensorflow op.the dataset is extracted from a tf.record file.the way the model_function is called is:
1327,Have you encountered the similar problem like loss jitter during training?,"['deep-learning', 'pytorch', 'tensorboard', 'loss', 'bert']","Background: It's about loss jittering which generates at the beginning stage of every training epoch. When the dataloader loads the first batch data to feed into the network, the loss value always rises suddenly, then returns to normal from the second batch and continues to decline. The curve is so strange. I need your help!"
1328,BERT get sentence level embedding after fine tuning,"['python', 'tensorflow', 'keras', 'classification', 'bert']","I came across this page1) I would like to get sentence level embedding (embedding given by [CLS] token) after the fine tuning is done. How could I do it?2) I also noticed that the code on that page takes a lot of time to return results on the test data. Why is that? When i trained the model it took less time as compared to when i tried to get test predictions. 
From the code on that page, I didnt use below blocks of the codeRather I just used below function on my entire test data3) how could i get probability of prediction. is there a way to use keras predict method?question 2 update -
could you test on 20000 training examples using getPrediction function?....it takes much longer time for me..even more than the time took to train model on 20000 examples."
1329,How to use BERT for Q&A system?,"['machine-learning', 'nlp', 'bert']","I would like to build a system that responses to close-domain questions (linear algebra). The knowledgebase is Wikipedia. How I can use BERT for this purpose? I didn't find any docs to BERT, only code on git-hub.Please, give me some roadmap and keywords for this problem.Thank you."
1330,AttributeError: module 'bert.tokenization' has no attribute 'printable_text',"['python-3.x', 'neural-network', 'tensorflow2.0', 'bert']","I'm using tensorflow 2.0 and imported run_classifier
from bert import run_classifier 
Now I'm getting error response train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)
    File ""/root/mj/python/ai/venv/lib/python3.7/site-packages/bert/run_classifier.py"", line 779, in convert_examples_to_features
      max_seq_length, tokenizer)
    File ""/root/mj/python/ai/venv/lib/python3.7/site-packages/bert/run_classifier.py"", line 466, in convert_single_example
      [tokenization.printable_text(x) for x in tokens]))
    File ""/root/mj/python/ai/venv/lib/python3.7/site-packages/bert/run_classifier.py"", line 466, in 
      [tokenization.printable_text(x) for x in tokens]))
  AttributeError: module 'bert.tokenization' has no attribute 'printable_text'"
1331,BertPunc (punctuation restoration with BERT),"['nlp', 'transformer', 'bert']","I've found the script https://github.com/nkrnrnk/BertPunc for punctuation restoration. And I have one question about this method.I will briefly explain the logic of the author. One of four tokens is assigned for each word: Other (0), PERIOD (1), COMMA (2), QUESTION (3). Further, all words are converted to BERT tokens. Here is an example:Next, we do padding. We set a segment (e.g. eight words) and for each word we take two words before a token and four words after a token. Also, we add a padding token after each word. For the very first word, there are no words before. Therefore, a word are taken from the end. Similarly, for the last word, there are no words after and therefore a word are taken from the beginning.
Here is an example of it. The first column contains tokens, and the second column contains punctuation symbols. In first column, ""0"" corresponds to a padding. Next we do TensorDataset, and than DataLoader. In the second column, '0' corresponds to ""other"", and '2' corresponds to a ""period"". Finaly we train a model:The algorithm works well, but I do not understand the following.
What's the point of putting padding in the middle? Maybe we can do punctuation restoration with BERT in a more simple way?"
1332,BERT with 256 hidden embeddings,"['python', 'tensorflow', 'pytorch', 'bert']","I am trying to use BERT to get word embeddings from different data sets for my NLP task. I have used the 'bert_base_uncased' with 768 word embeddings but it runs out of memory. The versions with 256 word embeddings has been released? Or is there any way that I can compress the 768 hidden embeddins?
Thank you!"
1333,Why should I call a BERT module instance rather than the forward method?,"['bert', 'huggingface-transformers']","I'm trying to extract vector-representations of text using BERT in the transformers libray, and have stumbled on the following part of the documentation for the ""BERTModel"" class:Can anybody explain this in more detail? A forward-pass makes intuitive sense to me (am trying to get final hidden states after all), and I can't find any additional information on what ""pre and post processing"" means in this context.Thanks up front!"
1334,Fine-Tuning Pretrained BERT with CNN. How to Disable Masking,"['python', 'keras', 'conv-neural-network', 'text-classification', 'bert']","Has anyone used CNN in Keras to fine-tune a pre-trained BERT? 
I have been trying to design this but the pre-trained model comes with Masks (i think in the embedding layer) and when the fine-tuning architecture is built on the output of one the encoder layers it gives this errorLayer conv1d_1 does not support masking, but was passed an input_maskSo far I have tried some suggested workarounds such as using keras_trans_mask to remove the mask before the CNN and add it back later. But that leads to other errors too. Is it possible to disable Masking in the pre-trained model or is there a workaround for it? EDIT: This is the code I'm working withSo layer_output contains a mask and it cannot be applied to conv1d"
1335,Why OOM happens while fine-tuning?,"['python', 'tensorflow', 'bert']","When I tried to do fine-tuning, I got this error.
I have enough RAM and GPU for running.What's wrong with it? please let me knowResourceExhaustedError (see above for traceback): OOM when allocating
  tensor with shape[8192,3072] and type float on
  /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
           [[node bert/encoder/layer_6/intermediate/dense/truediv (defined at
  C:\Users\Namgon\Documents\kwbert\make_bert_model\modeling.py:277)  =
  Mul[T=DT_FLOAT,
  _device=""/job:localhost/replica:0/task:0/device:GPU:0""](bert/encoder/layer_6/intermediate/dense/BiasAdd,
  ConstantFolding/gradients/bert/encoder/layer_0/intermediate/dense/truediv_grad/RealDiv_recip)]]
  Hint: If you want to see a list of allocated tensors when OOM happens,
  add report_tensor_allocations_upon_oom to RunOptions for current
  allocation info.Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info."
1336,"NameError: name 'BertModel' is not defined, error while importing Bert extractive summarizer","['python', 'deep-learning', 'nlp', 'bert']",I have freshly installed Anaconda module and installed Bert extractive summarizer package. The following error is being encountered by me while importing the module:Please help me in resolving this issue. Thanks in advance.
1337,"BERT with train, dev, test, predicion mode","['classification', 'bert']","I am doing a text classification task with BERT. I am basically using run_classifier.py.
This code uses train.tsv and dev.tsv (with labels) to fine-tune BERT and a test.tsv (without labels) to make predictions. However, I need to use train-dev-test splits to train the model (train set), calculate the hyperparameters and loss function (dev set), and evaluate the performance (test set). As regular train-dev-test splits all include labels. 
I also have a forth unlabeled dataset to make the prediction on. Do you know of any repository that implements BERT with 4 modes (train-dev-test-prediction)?  "
1338,content search engine using mobile bert - python,"['python', 'bert']",considering i having n rows having string related to medical filed enter image description hereinput : US datasetoutput: ID 1 with the textWhich model i have to use 
1339,How to pre-train the unified language model (UniLm),"['python', 'nlp', 'transfer-learning', 'bert', 'nlg']","I'm working on a non-English NLP project and for that, I need to re-train the UniLm on the bert-base-multilingual-cased, but they did not specify how to in their GitHub repository. Any help is appreciated! This is the link for the GitHub repository for Microsoft's UniLm"
1340,BertForSequenceClassification vs. BertForMultipleChoice for sentence multi-class classification,"['python', 'machine-learning', 'pytorch', 'bert', 'huggingface-transformers']","I'm working on a text classification problem (e.g. sentiment analysis), where I need to classify a text string into one of five classes.I just started using the Huggingface Transformer package and BERT with PyTorch. What I need is a classifier with a softmax layer on top so that I can do 5-way classification. Confusingly, there seem to be two relevant options in the Transformer package: BertForSequenceClassification and BertForMultipleChoice.Which one should I use for my 5-way classification task? What are the appropriate use cases for them?The documentation for BertForSequenceClassification doesn't mention softmax at all, although it does mention cross-entropy. I am not sure if this class is only for 2-class classification (i.e. logistic regression).Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.The documentation for BertForMultipleChoice mentions softmax, but the way the labels are described, it sound like this class is for multi-label classification (that is, a binary classification for multiple labels).Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.Thank you for any help."
1341,Error: AttributeError: module 'transformers' has no attribute 'TFBertModel',"['nlp', 'tensorflow', 'pytorch', 'bert']","I am applying transfer learning with the python framework (PyTorch). I am getting the below error, when loading a PyTorch pre-trained model in Google Colab. After changing the code 1 to be as code 2, I got the same error.I tried to search the internet, but I didn't find any useful content."
1342,Python bert-as-a-service issues with Tensorflow version,"['python', 'tensorflow', 'pip', 'bert']","I'm trying to use the bert-as-a-service package, but am having some issues with tensorflow.For reference:
https://bert-as-service.readthedocs.io/en/latest/section/get-start.html#installationI've tried using the latest version of Tensorflow, which doesn't seem to be compatible (problems with the logger etc). Since the version should be >1.10 according to the requirements I tried a few others (1.13, 1.14, 1.15) but get the message ""Illegal instruction (core dumped)"". A quick google suggest using Tensorflow 1.5, but it doesn't seem to be available for installation with pip anymore. I'm using Python 3.7.6.Any suggestions would be much appreciated."
1343,"RuntimeError, working on IA tryna use a pre-trained BERT model","['python', 'tensorflow', 'classification', 'pytorch', 'bert']","Hi here is a part of my code to use a pre-trained bert model for classification: ...but then I receive this error message:RuntimeError: Expected tensor for argument #1 'indices' to have scalar
  type Long;but got torch.IntTensor instead (while checking arguments for embedding)
So I think I should transform my b_input_ids to tensor but don't know how to do it.
Thanks a lot in advance for your help everyone !"
1344,Calculating semantic coherence in a given speech transcript,"['python', 'cosine-similarity', 'word-embedding', 'lsa', 'bert']","I am trying to calculate the semantic coherence in a given paragraph/transcript, ie. if somebody goes off track while talking about a thing or topic - more specifically describing a picture (the picture might have many sub details). For example - Transcript 1: I like to play sports. There are so many sports fans in the world.Transcript 2: I like to play sports. There is a deadly virus spreading across the world.Semantic coherence should be high for Transcript 1 and low for Transcript 2. I am using BERT (bert-as-service) to generate sentence embeddings for the sentences. I then try to compare sentence i and i+1 in a given transcript by calculating the cosine similarity between the sentence embedding vectors. I have also tried using a sliding window, with and without overlap to calculate cosine similarity. The problem I am running into is, that the cosine similarities are very close for two sentences, for example the examples above whereas I would expect a greater difference between the two.I am thinking of using an LSA Model trained on Wikipedia data next to see if I can see better differentiation. Is there a better method of doing this? "
1345,Why can Bert's three embeddings be added?,"['vector', 'nlp', 'embedding', 'transformer', 'bert']","I already know the meaning of Token Embedding, Segment Embedding, and Position Embedding. But why can these three vectors be added together? The Size and direction of vectors will change after the addition, and the semantics of the word will also change. (It's the same question for the Transformer model which has two Embeddings named Input Embedding and Position Embedding.)"
1346,Differences between BERT sentence embeddings and LSA embeddings,"['word-embedding', 'lsa', 'bert']","BERT as a service (https://github.com/hanxiao/bert-as-service) allows to extract sentence level embeddings. Assuming I have a pre-trained LSA model which gives me a 300 dimensional word vector, I am trying to understand in which scenario would an LSA model perform better than BERT when I am trying to compare two sentences for semantic coherence? I cannot think of a reason why LSA would be better for this use case - since LSA is just a compression of a big bag of words matrix. "
1347,Predicting NER with BertForTokenClassification model,"['neural-network', 'nlp', 'prediction', 'bert']","I have built my model using this tutorial on NER with bert:https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/#resourcesHowever, I could not figure out how to parse in a input data into the model to predict its ner value.The following links are some of the resources I have looked throughHow should properly formatted data for NER in BERT look like?https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification"
1348,Need to Fine Tune a BERT Model to Predict Missing Words,"['python', 'nlp', 'bert']","I'm aware that BERT has a capability in predicting a missing word within a sentence, which can be syntactically correct and semantically coherent. Below is a sample code:Can someone explain to me, do I need to fine Tune a BERT Model to predict missing words or just use the pre-trained BERT model? Thanks."
1349,Training TFBertForSequenceClassification with custom X and Y data,"['nlp', 'pytorch', 'tensorflow2.0', 'bert', 'huggingface-transformers']","I am working on a TextClassification problem, for which I am trying to traing my model on TFBertForSequenceClassification given in huggingface-transformers library.I followed the example given on their github page, I am able to run the sample code with given sample data using tensorflow_datasets.load('glue/mrpc').
However, I am unable to find an example on how to load my own custom data and pass it in 
model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=valid_dataset, validation_steps=7). How can I define my own X, do tokenization of my X and prepare train_dataset with my X and Y. Where X represents my input text and Y represents classification category of given X.Sample Training dataframe : "
1350,How run BERT in production?,"['docker', 'apache-spark', 'hadoop', 'bert']","I've created a BERT model. What are the ways to do the deployment of this model? Is it possible to use it with Spark, Hadoop or Docker?"
1351,No module named: bert_embedding. Python,"['python', 'module', 'bert']",So I am using Colab and I have a problem importing bert_embedding...I use:Error: No module named bert_embeddingBut on the documentation it clearly says that is how I should call it: https://pypi.org/project/bert-embedding/Any ideas on how to fix this? Thanx
1352,BERT prediction function,"['python', 'tensorflow', 'nlp', 'bert']","I am surprized to see that return statement returns prediction['probabilities'], prediction['labels'] and so on where as the line of code above that assigns predictions = estimator.predict and not to prediction. Then how are the probabilities and labels being accessed ?
For further reference I have the estimator.predict function below:"
1353,"If BERT's [CLS] can be retrained for a variety of sentence classification objectives, what about [SEP]?","['transformer', 'bert', 'huggingface-transformers']","In BERT pretraining, the [CLS] token is embedded into the input of a classifier tasked with the Next Sentence Prediction task (or, in some BERT variants, with other tasks, such as ALBERT's Sentence Order Prediction); this helps in the pretraining of the entire transformer, and it also helps to make the [CLS] position readily available for retraining to other ""sentence scale"" tasks.I wonder whether [SEP] could also be retrained in the same manner.
While [CLS] will probably be easier to retrain as the transformer is already trained to imbue its embedding with meaning from across the sentence, while [SEP] does not have these ""connections"" (one would assume), this might still work with sufficient fine-tuning. With this one could retrain the same model for two different classification tasks,   one using [CLS] and one using [SEP].Am I missing anything? 
Is there a reason why this would not work? "
1354,How is the semantic similarity score calculated in STS Benchmark dataset?,"['python', 'nlp', 'cosine-similarity', 'bert', 'finetunning']","This is the GitHub repo : https://github.com/brmson/dataset-stsThe STS Benchmark dataset contains about 4000 pairs of similar and dissimilar  sentences along with their semantic similarity scores.Task that I'm trying to do: 
I have another custom dataset which also has pairs of similar and dissimilar sentences. ( with just 200 pairs) I want to combine these two datasets (STS & my custom dataset) and use that for fine tuning a Bert model. 
(Bert sentence transformer: https://github.com/UKPLab/sentence-transformers)But the model needs the semantic similarity score of all the pairs of sentences. How do I compute that score for the sentences that I have in my custom dataset? It has to be computed in the same way as how it was computed for the pairs of sentences in the STS Benchmark dataset.This thread is very similar but it didn't quite answer the question that I am looking for : Bert fine-tuned for semantic similarity"
1355,Colab crashed while encoding bert,"['python-3.x', 'tensorflow', 'keras', 'bert', 'tf-hub']","I am trying to use BERT for training
Here is the code for creating masksWhen i run itMy google colab crashes becuase ram runs out. Any solution"
1356,How to feed the output of a finetuned bert model as inpunt to another finetuned bert model?,"['pytorch', 'pre-trained-model', 'bert', 'huggingface-transformers']","I finetuned two separate bert model (bert-base-uncased) on sentiment analysis and pos tagging tasks. Now, I want to feed the output of the pos tagger (batch, seqlength, hiddensize) as input to the sentiment model.The original bert-base-uncased model is in 'bertModel/' folder which contains 'model.bin' and 'config.json'. Here is my code:But I get the below error. I appreciate it if someone could help me. Thanks!"
1357,How to create iob tags for a sentence?,"['tagging', 'ner', 'bert']","I have a dataset for NER in which I have to do POS tagging and IOB tagging, but I don't understand the concept or method of how iob tags are created. Even CoNLL is pretagged. "
1358,BERT error: Cannot squeeze a dimension whose value is not 1,"['pytorch', 'coreml', 'bert']","I have an mlmodel based on pytorch-pretrained-BERT, exported via ONNX to CoreML. That process was pretty smooth, so now I'm trying to do some (very) basic testing—i.e., just to make some kind of prediction, and get a rough idea of what performance problems we might encounter.Howver, when I try to run prediction, I get the following error:Is this error indicating a problem with the model itself (i.e., from model conversion), or is there something in Swift/CoreML that I'm doing wrong? My prediction function looks like this:I'm not trying to do anything meaningful, at this stage, only to get it running.I used the pytorch-pretrained-BERT repo because I was able to find a ground-up pretraining example for that. I have since noticed that HuggingFace has released a ""from scratch"" training option, but there are still some issues with the tutorial that are being sorted out. So I would like to at least understand what might be going wrong with my current model/approach. However, if the problem is definitely in the PyTorch->ONNX->CoreML conversion, then I don't really want to fight that fight and will just dig into what HuggingFace is offering.Any thoughts appreciated.UPDATE: On Matthijs' advice, I'm trying to predict from the model in python: I admit I haven't run an mlmodel from python before, but I think the inputs are correct. The spec indicates:for my inputs.For this case, I don't get the cannot squeeze message, or the error code (-5), but it does fail with Error computing NN outputs. So something is definitely wrong. I'm just not at all sure how to go about debugging it.J.UPDATE: For comparison I've trained/converted the HuggingFace BERT Model (actually, DistilBert — I've updated the code above accordingly) and have the same error. Looking at the log from onnx, I see that there is one Squeeze added (also clear from the onnx-coreml log, of course), but the only squeeze in the PyTorch code is in BertForQuestionAnswering, not BertForMaskedLM. Perhaps onnx is building the question answering model, not the mlm one (or the question answering one is being saved in the checkpoint)? Looking at the swift-coreml-transformers sample code I can see that distilBert's input is just let input_ids = MLMultiArray.from(allTokens, dims: 2), which is exactly how I've defined it. So I'm at a dead-end, I think. Has anybody managed to run MLM with Bert/DistilBert in CoreML (via onnx)? If so, an example would be super helpful.I'm training using the latest run_language_modeling.py from HuggingFace, btw.Looking at the mlmodel in Netron, I can see the offending squeeze. I confess that I don't know what that branch of the input is for, but I'm guessing it's a mask (and a further guess might be that it has to do with question answering). Could I just remove that somehow?"
1359,"Where do the input_ids, input_mask, and segment_ids variables come from in a BERT model?","['tensorflow', 'nlp', 'bert']","I'm trying to work through the Google BERT tutorial in Google Colab, and am having a hard time following some of its steps.  Specifically, there is a function called create_model which reads like this:This would be fine, but when I look for where the create_model function is called in the notebook, the origin of the input_ids, input_mask and segment_ids arguments is not clear to me.  This is where the function is referenced later on in the notebook:The problem here is that the features argument is not listed as an argument in the parent function, and it's not defined anywhere else in the notebook.  So I'm not sure why it works, and even more importantly, I'm not sure what things like features['input_ids'] are supposed to represent.  Without this being clear to me, it'll be difficult to make sense of what BERT actually does.Thank you for your help."
1360,How to update vocabulary of pre-trained bert model while doing my own training task?,"['machine-learning', 'neural-network', 'nlp', 'bert']","I am now working on a task of predicting masked word using BERT model. Unlike others, the answer needs to be chosen from specific options.For instance:I use hugging face's BertForMaskedLM to do this task. This model will give me a probability matrix which representing every word's probability of appearing in the [MASK] and I just need to compare the probability of word in options to select the answser. But the problem is:
    If the options are not in the “bert-vocabulary.txt”, the above method is not going to work since the output matrix does not give their probability. The same problem will also appear if the option is not a single word. Should I update the vocabulary and how to do that? Or how to train the model 
 to add new words on the basis of pre-training?"
1361,Training with BERT for classification using Keras gives Nan as validation loss,"['tensorflow', 'keras', 'nan', 'bert']","I am trying to use BERT for classification task using the code available here
https://github.com/tensorflow/models/tree/master/official/nlp/bertI could successfully train the model using low-level API. However, if I use Keras API, I get validation loss as nan at the end of every epoch. The model training loss is decreasing after every epoch in Keras API as well and I get reasonable accuracy numbers on the dev set, indicating that model training is working as expected.
Is there a way to determine in Keras why validation loss is nan? "
1362,How should properly formatted data for NER in BERT look like?,"['python', 'nlp', 'format', 'bert', 'huggingface-transformers']",I am using Huggingface's transformers library and want to perform NER using BERT. I tried to find an explicit example of how to properly format the data for NER using BERT. It is not entirely clear to me from the paper and the comments I've found.Let's say we have a following sentence and labels:Would data that we input to the model be something like this:?Thank you!
1363,Question-Answering with SQuAD: Reading text-files as content,"['user-data', 'question-answering', 'bert']","I am trying to implement the available code by hugging face trying to add only a few lines of code where the user can enter the file he wants to use as content. The plan is that the user can ask questions based on this context.However, it gives me an error message -- probably because I used more than 120 characters. I used this context as an example:Harry Potter is a series of fantasy novels written by British author
  J. K. Rowling. The novels chronicle the lives of a young wizard, Harry
  Potter, and his friends Hermione Granger and Ron Weasley, all of whom
  are students at Hogwarts School of Witchcraft and Wizardry. The main
  story arc concerns Harry's struggle against Lord Voldemort, a dark
  wizard who intends to become immortal, overthrow the wizard governing
  body known as the Ministry of Magic and subjugate all wizards and
  Muggles (non-magical people).Since the release of the first novel,
  Harry Potter and the Philosopher's Stone, on 26 June 1997, the books
  have found immense popularity, critical acclaim and commercial success
  worldwide. They have attracted a wide adult audience as well as
  younger readers and are often considered cornerstones of modern young
  adult literature.[2] As of February 2018, the books have sold more
  than 500 million copies worldwide, making them the best-selling book
  series in history, and have been translated into eighty languages.[3]
  The last four books consecutively set records as the fastest-selling
  books in history, with the final instalment selling roughly eleven
  million copies in the United States within twenty-four hours of its
  release.The series was originally published in English by two major
  publishers, Bloomsbury in the United Kingdom and Scholastic Press in
  the United States. A play, Harry Potter and the Cursed Child, based on
  a story co-written by Rowling, premiered in London on 30 July 2016 at
  the Palace Theatre, and its script was published by Little, Brown. The
  original seven books were adapted into an eight-part namesake film
  series by Warner Bros. Pictures, which is the third highest-grossing
  film series of all time as of February 2018. In 2016, the total value
  of the Harry Potter franchise was estimated at $25 billion,[4] making
  Harry Potter one of the highest-grossing media franchises of all
  time.A series of many genres, including fantasy, drama, coming of age,
  and the British school story (which includes elements of mystery,
  thriller, adventure, horror, and romance), the world of Harry Potter
  explores numerous themes and includes many cultural meanings and
  references.[5] According to Rowling, the main theme is death.[6] Other
  major themes in the series include prejudice, corruption, and
  madness.[7]The success of the books and films has allowed the Harry
  Potter franchise to expand with numerous derivative works, a
  travelling exhibition that premiered in Chicago in 2009, a studio tour
  in London that opened in 2012, a digital platform on which J.K.
  Rowling updates the series with new information and insight, and a
  pentalogy of spin-off films premiering in November 2016 with Fantastic
  Beasts and Where to Find Them, among many other developments. Most
  recently, themed attractions, collectively known as The Wizarding
  World of Harry Potter, have been built at several Universal Parks &
  Resorts amusement parks around the world.How can I fix the problem? Is there a way to implement BERT with more than 120 characters? "
1364,Get the value of '[UNK]' in BERT,"['python-3.x', 'pytorch', 'bert', 'huggingface-transformers']","I have designed a model based on BERT to solve NER task. I am using transformers library with the ""dccuchile/bert-base-spanish-wwm-cased"" pre-trained model. The problem comes when my model detect an entity but the token is '[UNK]'. How could I know which is the string behind that token?I know that an unknown token can't be reverted to the original one, but I would like to at least capture that values before passing the inputs to the model. The code is really simple:As you see is really simple, just tokenize, padding or truncating, creating attentionMask and calling to the model.I have tried using regex, trying to find the two tokens that are around it and things like that, but I can't solve it properly."
1365,Bert service python getting word embeddings efficiently,"['python', 'performance', 'nlp', 'word-embedding', 'bert']","I have a list of 5000 words and I need to get their word embeddings using bert service.I use the command 
bert-serving-start -model_dir /Users/cem/PycharmProjects/bert-wikipedia/temp2 -num_worker=1 to start the server.Then I connect using bc = BertClient(check_length=False)I get each element's embedding in the list using the codefor j in range(0,len(wordlist) : bc.encode([wordlist[j]]However it takes very very long time to finish. Is there anything that I can do to reduce the execution time ?  I tried to change the number of workers when starting the server however it did not effect much. "
1366,How do I use my trained BERT NER (named entity recognition) model to predict a new example?,"['ner', 'bert', 'huggingface-transformers']",I trained my own BERT NER following this Medium post: https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73I saved my model to the disc and successfully loaded it. model.eval() works:I am new to BERT and the transformer lib. I would expect something similar to would show me recognised entities.I also tried:where output gives me a big tensor and I do not know what to do with it.How can I use the model now to predict the entities in an example sentence? What am I supposed to do with the output?Thanks!
1367,Input shape mismatch while drawing inference from a saved and loaded BERT model,"['python', 'tensorflow', 'numpy-ndarray', 'bert']","I am running the BERT model for Binary sentence classification. I want to draw inference from my trained model which I was able to export using serving_input_receiver_fn() as below:OUTPUT: Input example shape:(?,)I was able to load the model using the predictor.from_saved_model(export_dir) from the predictor class in tensorflow as below:I have also taken care of changing the shape of the input sentences for prediction to the shape as required by the function above as follows:OUTPUT: prediction sentence shape:(24,)However, when I try to draw inference from the predict_fn function, I am facing an array mismatch error. OUTPUT: This error might possibly be because of the [] around pred_sentences_array in predict_fn({'Sentence': [pred_sentences_array]}) but if I remove those brackets, the following error occurs:OUTPUTI tried searching stackoverflow but couldn't find a possible solution. I might have missed out on them but it's highly unlikely. 
Hope the explanation is optimum for understanding my problem."
1368,Is possible multiples GPUs work as one with more memory?,"['docker', 'tensorflow', 'nvidia-docker', 'bert']",I have a deep learning workstation where there are 4 GPUs with 6 GB of memory each. Would it be possible to make a docker container see the 4 GPUs as one but with 24 GB?Thank you.
1369,"Pre-training BERT/RoBERTa language model using domain text, how long it gonna take estimately? which is faster?","['language-model', 'bert', 'huggingface-transformers', 'bert-language-model']","I want to pre-train BERT and RoBERTa MLM using domain corpus (sentiment-related text). How long it gonna take for using 50k~100k words. Since RoBERTa is not trained on predicting the next sentence objective, one training objective less than BERT and with larger mini-batches and learning rates, I assume RoBERTa will be much faster?"
1370,What does BERT's special characters appearance in SQuAD's QA answers mean?,"['question-answering', 'bert', 'huggingface-transformers', 'squad']","I'm running a fine-tuned model of BERT and ALBERT for Questing Answering. And, I'm evaluating the performance of these models on a subset of questions from SQuAD v2.0. I use SQuAD's official evaluation script for evaluation. I use Huggingface transformers and in the following you can find an actual code and example I'm running (might be also helpful for some folks who are trying to run fine-tuned model of ALBERT on SQuAD v2.0):And the output is like the following:As you can see there are BERT's special tokens in the answer including [CLS] and [SEP].I understand that in cases where the answer is just [CLS] (having two tensor(0) for start_scores and end_scores) it basically means model thinks there's no answer to the question in context which makes sense. And in these cases I just simply set the answer to that question to a null string when running the evaluation script.But I wonder in cases like the example above, should I again assume that model could not find an answer and set the answer to empty string? or should I just leave the answer like that when I'm evaluating the model performance?I'm asking this question because as far as I understand, the performance calculated using the evaluation script can change (correct me if I'm wrong) if I have such cases as answers and I may not get a realistic sense of the performance of these models. "
1371,Cosine similarities between a sample sentence and a knowledge base,"['nlp', 'bert']",I have python code which encodes a text using BERT model. The code then does a cosine comparison of this text to a set of text from a knowledge base. The knowledge base in dev is only about a 100 sentences. The steps listed below:running on a cpu my code is taking about 27 seconds for a 100 question question-bank. I ran the encoding in a multi threaded (running 3 threads) form and could only cut the execution time to 10 seconds. any idea how to speed up this processing.
1372,Transformers PreTrainedTokenizer add_tokens Functionality,"['pytorch', 'bert', 'huggingface-transformers']","Referring to the documentation of the awesome Transformers library from Huggingface, I came across the add_tokens functions.I tried the above by adding previously absent words in the default vocabulary. However, keeping all else constant, I noticed a decrease in accuracy of the fine tuned classifier making use of this updated tokenizer. I was able to replicate similar behavior even when just 10% of the previously absent words were added.My questionsAny help would be appreciated.Thanks in advance."
1373,"is sentiment analysis better with tf-idf , word2vec or bert?","['deep-learning', 'nlp', 'sentiment-analysis', 'doc2vec', 'bert']","right now I'm trying to do the sentiment analysis 
dataframe looks like this in this case I used the doc2vec to represent each label
but is bert work for sentiment analysis? what is main difference doc2vec and bert?"
1374,Get output of the last 4 layers of BERT (Tensorflow),"['tensorflow', 'tensorflow2.0', 'tpu', 'bert', 'tf-hub']","I was going through Andreas Poyiatzis' article posted on towardsdatascience.com. Below is the link for the article.https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b#--responsesThe embeddings generated are through the use of TPU. But I want to run it on my local machine, which does not support TPU. Can someone let me know how can I run the same code on the CPU? Your help will be appreciated. Thanks!"
1375,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 512], but got 891 [Op:Slice]","['python', 'tensorflow', 'bert']","I am reading text files stored in my database and they all have different sizes. When I run my code it suddenly stops and giving this error. Not finding any relevant answers anywhere.
I have tried changing the max_seq_embeddings but still does not work.
As soon as I encounter a file of length 3619 it raised an error.Here is the link to all the file used.
https://github.com/kamalkraj/BERT-NER-TF"
1376,Where is detailed explanation about a model of BERT?,['bert'],"I'd like to implement BERT in C++.Could anyone tell me
detailed explanation about a model for pre-training,
about target and response tensors,
and about an algorithm of calculating loss using these two tensors.Thank you."
1377,Making Predictions using a Trained BERT model,"['text-classification', 'predict', 'bert']",I trained the BertForSequenceClassification model from this repo using the train.tsv and dev.tsv datasets. Now I need to use this trained model to make predictions on an unlabeled dataset. Do you guys have any idea how to make predictions using bert model? I'm looking for something similar to Keras predict() function to get unlabeled data as input and predict the labels. 
1378,getting word-level encodings from sub-word tokens encodings,"['nlp', 'tokenize', 'bert', 'huggingface-transformers']","I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?"
1379,"Saving, Loading and Predicting using the bert tensorflow estimator","['python', 'tensorflow', 'tensorflow-serving', 'tensorflow-estimator', 'bert']","I tried following the answer on this link for saving the model.Saving and doing Inference with Tensorflow BERT modelThough I am able to save and load back the model. I am not able to make predictions using the loaded model. The code I used for loading the model is below.I am not sure about the format that needs to be used for the predictions, i.e. the format in which the predict_fn should receive the arguments, or if the code that I am using is incorrect.Can someone please help with this.The IMDB movie review example, that is similar to my use case.(For reference)
https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=ERkTE8-7oQLZ"
1380,BERT OSS implementation in Angular,"['angular', 'single-page-application', 'bert']","Does anyone have an experience in implementing the open source of BERT NLP method in an angular SPA? If so, are there any specific docs that guide on this?"
1381,R Forecasting in Excel using the BERT console,"['r', 'excel', 'forecasting', 'bert']","Does anyone know how I can use the ""forecast"" function from R in Excel through the BERT console? I tried writing the following function:}but i got the error below.Error in ets(object, lambda = lambda, biasadj = biasadj, allow.multiplicative.trend = allow.multiplicative.trend,  :
  y should be a univariate time series

Thanks, (-Alex)"
1382,Bert model giving CUDA out of memory error on google colab,"['python', 'tensorflow', 'pytorch', 'google-colaboratory', 'bert']","I am using the following tutorial here to train and test a Bertsequenceclassifier model on a dataset of documents of varying lengths (small(0-280), medium(280-10000), large(10000 plus)) on the google collab platform using a GPU. I have been able to successfully train and test the small dataset using a maximum sequence length of 380 on 140,000 entries. When I try to train the medium dataset on the maximum sequence length which is 512 for only 1 epoch and only on 10000 entries I get a CUDA out of memory error. When I change the maximum sequence length to 400 it is able to train the model though.I guess that obviously the length of the documents is causing this but can anyone explain exactly why this is happening and if there is any way to fix this. Thank youRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 15.15 GiB already allocated; 7.88 MiB free; 44.38 MiB cached)"
1383,Bert Embedding element #308,"['python', 'pytorch', 'word-embedding', 'bert']","I've been getting some practice with Bert Embedding. Specifically, I'm using BERT-Base Uncased model with the PyTorch librariesI noticed that for pretty much every word I've looked at the element #308 in the bert embedding 768-long vector is a negative outlier with the values below -2, perhaps half of the time below -4.It is really weird. I tried to google something about 'bert embedding 308' but couldn't find anything.I wonder if there is any explanation for this 'phenomenon'. Here is my routine to extract the embeddings:"
1384,What is the difference between Sentence Encodings and Contextualized Word Embeddings?,"['nlp', 'word-embedding', 'elmo', 'bert']",I have seen both terms used while reading papers about BERT and ELMo so I wonder if there is a difference between them.
1385,ALBERT for creating embedding's of dimension of 128,"['neural-network', 'bert']","How do I use ALBERT model to create embedding's of dimension 128,I tried both SentenceTransformer and HuggingFace,both of which generate embedding's of size 768,and got a suggestion to add an other Dense layer to the existing model with 128 nodes,but this doesn't feel right to me as , as by definition ALBERT should be able to produce this,any inputs?Below is the code used I used to produce 128 sized embeddings."
1386,How to find the closest word to a vector using BERT,"['nlp', 'word-embedding', 'bert-language-model', 'bert']","I am trying to get textual representation(or the closest word) of given word embedding using BERT. Basically I am trying to get similar functionality as in gensim:So far, I have been able to generate contextual word embedding using bert-as-service but can't figure out how to get closest words to this embedding. I have used pre-trained bert model (uncased_L-12_H-768_A-12) and haven't done any fine tuning."
1387,How to use a custom model with Tensorflow Hub?,"['tensorflow', 'google-colaboratory', 'tensorflow-hub', 'bert', 'tf-hub']",My goal is to test out Google's BERT algorithm in Google Colab. I'd like to use a pre-trained custom model for Finnish (https://github.com/TurkuNLP/FinBERT). The model can not be found on TFHub library. I have not found a way to load model with Tensorflow Hub.Is there a neat way to load and use a custom model with Tensorflow Hub?
1388,Trying to simplify BERT architecture,"['machine-learning', 'nlp', 'bert']","I have an interesting question about BERT.Can I simplify the architecture of the model by saying that the similarity of two words in different context will depend on the similarity of input embeddings making up different contexts? For example, can I say that the similarity of the embeddings of GLASS in the context DRINK_GLASS and WINE in the context LOVE_WINE will depend on the similarity of the input embeddings GLASS and WINE (last position) and DRINK and LOVE (first position)? Or should I also take into account the similarity between DRINK (first context, first position) and WINE (second context, second position) and LOVE and GLASS (viceversa)?Thanks for your help, for now it is really difficult for me to understand exactly the architecture of Bert, but I'm trying to make experiments so I need to understand some basics. "
1389,RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at ..\aten\src\THC\THCGeneral.cpp:50,"['deep-learning', 'gpu', 'pytorch', 'bert']","I was trying to run the extractive summarizer of the BERTSUM program(https://github.com/nlpyang/PreSumm/tree/master/src) in test mode with the following command:python train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path C:\Users\hp\Downloads\PreSumm-master\PreSumm-master\bert_data -log_file ../logs/val_abs_bert_cnndm -model_path C:\Users\hp\Downloads\bertext_cnndm_transformer -test_from C:\Users\hp\Downloads\bertext_cnndm_transformer\model_1.pt -sep_optim true -use_interval true -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/abs_bert_cnndmHere is the error log: I am sure that I have a CUDA-enabled GPU, I made sure by checking the list on NVIDIA. This is an NVIDIA GeForce GTX 950M. I have also used my GPU for deep learning projects with CUDA before. 
I have installed CUDA and cudNN following these instructions, thinking that could be the problem:https://www.easy-tensorflow.com/tf-tutorials/install/cuda-cudnn(latest versions, CUDA 10.2). I also tried adding os.environ['CUDA_VISIBLE_DEVICES']='0' in train.py(as this worked for people facing the same kind of error from help posts online). But still the error persists.I'd really appreciate if someone could help me figure this out. "
1390,Removing SEP token in Bert for text classification,"['python', 'bert']","Given a sentiment classification dataset, I want to fine-tune Bert.As you know that BERT created to predict the next sentence given the current sentence. Thus, to make the network aware of this, they inserted a [CLS] token in the beginning of the first sentence then they add [SEP] token to separate the first from the second sentence and finally another [SEP] at the end of the second sentence (it's not clear to me why they append another token at the end).Anyway, for text classification, what I noticed in some of the examples online (see BERT in Keras with Tensorflow hub) is that they add [CLS] token and then the sentence and at the end another [SEP] token.Where in other research works (e.g. Enriching Pre-trained Language Model with Entity Information for Relation Classification) they remove the last [SEP] token.Why is it/not beneficial to add the [SEP] token at the end of the input text when my task uses only single sentence?"
1391,BERT tokenizer & model download,"['python', 'github', 'pytorch', 'huggingface-transformers', 'bert']","I`m beginner.. I'm working with Bert. However, due to the security of the company network, the following code does not receive the bert model directly.So I think I have to download these files and enter the location manually.
But I'm new to this, and I'm wondering if it's simple to download a format like .py from github and put it in a location.I'm currently using the bert model implemented by hugging face's pytorch, and the address of the source file I found is:https://github.com/huggingface/transformersPlease let me know if the method I thought is correct, and if so, what file to get.Thanks in advance for the comment."
1392,How to set the max number of CPU/cores to run BERT as service?,"['nlp', 'numbers', 'cpu', 'bert']","I can install and run BERT model as described in https://github.com/hanxiao/bert-as-serviceIt will take all the CPUs on the Linux machine when it runs, but I'd like to have it run on only 50 CPUs of the Linux server. Can set I the max number of CPU/cores to run BERT model?"
1393,Sequence labeling with BERT for words position,"['python', 'tensorflow', 'machine-learning', 'neural-network', 'bert']","If I have a set of sentences and in these sentences there are some dependencies between words.
I want to train BERT to predict which words have dependencies with others.Example, If I have this sentence:We were moving around in Paris, which is the capital of France.0------1-------2-------3------4----5------6-----7---8-----9----10---11  (words indices)I want BERT to predict, for word Paris, the position of France. So, to shape the task as sequence labeling task.where the label for a word could be -1, if there is no relation between this word and any other words in the sentence, or the index of the other word; for our example above, Paris word should have 11 as the index of word France.Is it a right way to place the indices as labels?"
1394,How to get the vocab file for Bert tokenizer from TF Hub,"['tensorflow', 'tokenize', 'tensorflow2.0', 'bert']","I'm trying to use Bert from TensorFlow Hub and build a tokenizer, this is what I'm doing:But now when I check the vocab file in the resolved objects I get an empty tensorWhat is the proper way to get this vocab file?"
1395,Out of Memory; BERT,"['nlp', 'bert']","I am new in this area and trying to learn through the below Github link.
However, I have encountered a runtime error. Despite tweaking the batch size and gradient accumulation to smaller values, and clearing the cache, the runtime error persists.Can anyone share any insight on this? Thanks very much.https://github.com/nlptown/nlp-notebooks/blob/master/Text%20classification%20with%20BERT%20in%20PyTorch.ipynbRuntimeError: CUDA out of memory. Tried to allocate 1024.00 KiB (GPU 0; 4.00 GiB total capacity; 3.09 GiB already allocated; 528.00 KiB free; 32.48 MiB cached)"
1396,Multilingual Bert sentence vector captures language used more than meaning - working as interned?,"['python', 'deep-learning', 'pytorch', 'multilingual', 'bert']","Playing around with BERT, I downloaded the Huggingface Multilingual Bert and entered three sentences, saving their sentence vectors (the embedding of [CLS]), then translated them via Google Translate, passed them through the model and saved their sentence vectors.I then compared the results using cosine similarity.I was surprised to see that each sentence vector was pretty far from the one generated from the sentence translated from it (0.15-0.27 cosine distance) while different sentences from the same language were quite close indeed (0.02-0.04 cosine distance).So instead of having sentences of similar meaning (but different languages) grouped together (in 768 dimensional space ;) ), dissimilar sentences of the same language are closer. To my understanding the whole point of Multilingual Bert is inter-language transfer learning - for example training a model (say, and FC net) on representations in one language and having that model be readily used in other languages.  How can that work if sentences (of different languages) of the exact meaning are mapped to be more apart than dissimilar sentences of the same language?  My code:P.S.At least the Heb1 was closer to Heb3 than to Heb2.
This was also observed for the English equivalents, but less so. "
1397,How can I get hidden_states from BertForSequenceClassification?,"['python', 'pytorch', 'bert']","I read the official tutorial(https://huggingface.co/transformers/model_doc/bert.html) and tried to set config, but it doesn't work."
1398,estimator.train throws ValueError: model_fn should return an EstimatorSpec,"['tensorflow', 'tensorflow-estimator', 'bert']","Here's the code I'm using...I've got a breakpoint installed at what is for me line 304...estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)Has anyone seen this?  I'm certain I have the correct versions of TensorFlow and BERT installed.The complete stack trace is as follows....This code is my attempt to run some Google colab code from here - https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=t6Nukby2EB6-"
1399,Running BERT - run_squad.py from another python script with sub process,"['python', 'python-3.x', 'tensorflow', 'subprocess', 'bert']","Team,I am trying to invoke Bert module - run_squad.py from another Python code. I have tried to invoke the same by importing it, but I am unable to make it work.The output is:WARNING:tensorflow:From .\bert\optimization.py:87: The name
  tf.train.Optimizer is deprecated. Please use
  tf.compat.v1.train.Optimizer instead.Traceback (most recent call last):   File """", line 2, in
   TypeError: main() got an unexpected keyword argument
  'vocab_file'The output is: Traceback (most recent call last):   File """",
  line 2, in  TypeError: 'module' object is not callableNow, I am trying to run this as sub process.The output is:print(process.communicate()[0]) b''
        print(process.stdout) <_io.BufferedReader name=3>I do not know what am I missing? Can you pl advise?Regards"
1400,How to replace the embedding layer with a custom function in a Keras model?,"['python', 'tensorflow', 'keras', 'bert']","I want to replace the Keras Embedding layer with a custom function - f(list of o strings)- that takes a list of variant lengths strings and returns a tensor with the shape of (batch_size, max_len, feature_len). This function returns the embedding vector of each token produced by BioBERT with the length of feature_len for each token in each string.More information:
function f() works very well without any problem. It's adapted version of the extract_feature.py inside the BioBert package. Considering the following embedding function i want to use it in my model:Input for the embedding() should be a list of untokenized sentencesNow, when I try to create a model like:I am getting the following errors. Any help would be appreciated.Traceback (most recent call last):   File
  ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1356, in _do_call
      return fn(*args)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1341, in _run_fn
      options, feed_dict, fetch_list, target_list, run_metadata)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1429, in _call_tf_sessionrun
      run_metadata) tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root
  error(s) found.   (0) Invalid argument: You must feed a value for
  placeholder tensor 'input_1' with dtype string and shape [?,1]
  [[{{node input_1}}]]   (1) Invalid argument: You must feed a value for
  placeholder tensor 'input_1' with dtype string and shape [?,1]
  [[{{node input_1}}]]   [[input_1/_1]] 0 successful operations. 0
  derived errors ignored.During handling of the above exception, another exception occurred:Traceback (most recent call last):   File
  ""/home/akkasi/PycharmProjects/BioBERT/BioBERT_Embedding.py"", line 35,
  in 
      model = Lambda(embeding, output_shape=(max_seq_length, 768))(Inputs)   File
  ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/engine/base_layer.py"",
  line 457, in call
      output = self.call(inputs, **kwargs)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/layers/core.py"",
  line 687, in call
      return self.function(inputs, **arguments)   File ""/home/akkasi/PycharmProjects/BioBERT/BioBERT_Embedding.py"", line 9,
  in embeding
      listOfSentences = list(listOfSentences.eval())   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"",
  line 731, in eval
      return _eval_using_default_session(self, feed_dict, self.graph, session)   File
  ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"",
  line 5579, in _eval_using_default_session
      return session.run(tensors, feed_dict)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 950, in run
      run_metadata_ptr)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1173, in _run
      feed_dict_tensor, options, run_metadata)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1350, in _do_run
      run_metadata)   File ""/home/akkasi/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/client/session.py"",
  line 1370, in _do_call
      raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root
  error(s) found.   (0) Invalid argument: You must feed a value for
  placeholder tensor 'input_1' with dtype string and shape [?,1]
  [[node input_1 (defined at
  /anaconda3/envs/BERT/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517)
  ]]   (1) Invalid argument: You must feed a value for placeholder
  tensor 'input_1' with dtype string and shape [?,1]     [[node input_1
  (defined at
  /anaconda3/envs/BERT/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517)
  ]]     [[input_1/_1]] 0 successful operations. 0 derived errors ignored.Original stack trace for 'input_1':   File
  ""/PycharmProjects/BioBERT/BioBERT_Embedding.py"", line 34, in 
      Inputs = Input(shape=(1,), dtype=tf.string)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/engine/input_layer.py"",
  line 178, in Input
      input_tensor=tensor)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/legacy/interfaces.py"",
  line 91, in wrapper
      return func(*args, **kwargs)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/engine/input_layer.py"",
  line 87, in init
      name=self.name)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"",
  line 517, in placeholder
      x = tf.placeholder(dtype, shape=shape, name=name)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"",
  line 2143, in placeholder
      return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)   File
  ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"",
  line 6262, in placeholder
      ""Placeholder"", dtype=dtype, shape=shape, name=name)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"",
  line 788, in _apply_op_helper
      op_def=op_def)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"",
  line 507, in new_func
      return func(*args, **kwargs)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"",
  line 3616, in create_op
      op_def=op_def)   File ""/anaconda3/envs/BERT/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"",
  line 2005, in init
      self._traceback = tf_stack.extract_stack()Process finished with exit code 1"
1401,keras LSTM get hidden-state (converting sentece-sequence to document context vectors),"['python', 'keras', 'lstm', 'embedding', 'bert']","Im trying to create document context vectors from sentence-vectors via LSTM using keras (so each document consist of a sequence of sentence vectors).My goal is to replicate the following blog post using keras: https://andriymulyar.com/blog/bert-document-classificationI have a (toy-)tensor, that looks like this: X = np.array(features).reshape(5, 200, 768) So 5 documents with each having a 200 sequence of sentence vectors - each sentence vector having 768 features.So to get an embedding from my sentence vectors, I encoded my documents as one-hot-vectors to learn an LSTM:Until now, my code looks like thisWhen I print states_h I get a tensor of shape=(?,5) and I dont really know how to access the vectors inside the tensor, which should represent my documents.Or am I doing something wrong? To my understanding there should be 5 document vectors e.g. doc1=[...] ; ...; doc5=[...] so that I can reuse the document vectors for a classification task."
1402,BERT binary Textclassification get different results every run,"['python-3.x', 'text-classification', 'transformer', 'bert']","I do binary text classification with BERT from the Simpletransformer.I work in Colab with GPU runtime type. I have generated train and test set with the sklearn StratifiedKFold Method. I have two files with the dictionaries containing my folds.I run my classification in the following while loop:And i get different Results Running this code for the same Folds:Here for example the F1 Scores for two runs:How can they be that diffeerent for the same folds?What i tried already is give a fixed Random seed right before my loop starts:I came up with approach of having the Model initialized in the loop because, when its outside the loop, it somehow remembers what it has learned - that means after the 2nd fold I get f1 score of almost one - despite the fact that i delete the cache.."
1403,Huggingface Transformers - AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map',"['transformer', 'bert', 'huggingface-transformers']","When using Hugginface Transformers on GLUE task, I've got the error AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map'I suspect a problem of compatibility."
1404,BERT Multi-class text classification in Google Colab,"['python', 'pytorch', 'data-science', 'google-colaboratory', 'bert']","I'm working on a data set of social media comments(including youtube links) as input features and the Myers-Biggs Personality Profile as the target label: but from what I've found, BERT wants DataFrame's to be in this format:The resulting output must be a prediction on a test set of comments split into four columns, one for each Personality Profile where, for example, 'Mind' = 1 is the label for Extrovert. Basically splitting a type like INFJ into 'Mind','Energy','Nature','Tactics', like such:I've installed pytorch-pretrained-bert using:I've imported the models and tried to tokenize the 'posts' column using:but receive this error:I tried this based off the pytorch-pretrained-bert GitHub Repo and a Youtube vidoe. I am a Data Science intern with no Deep Learning experience at all. I simply want to experiment with the BERT model in the most simplest way to predict the multi-class classified output so I can compare the results to simpler text-classification models we are currently working on. I am working in Google Colab and the resulting output should be a .csv file.I understand that this is a complicated model and all documentation and examples surrounding the model are complex (fine tuning layers etc.) but any help for a simple implementation(if in fact there is such a thing) for a beginner Data Scientist with minimal Software Engineering experience, would be much appreciated."
1405,Bert-multilingual in pytorch,"['python', 'pytorch', 'multilingual', 'bert-language-model', 'bert']","I am using bert embedding for french text data. and I have problem with loading model and vocabulary.I used the following code for tokenization that works well, but to get the vocabulary, it gives me Chinese words!!I expected french words in vocabulary but i get chinese words, should i specify the language somewhere in the code?"
1406,Tensorflow 2 Glove could not broadcast input array Can't prepare the embedding matrix but not +1,"['python', 'tensorflow', 'keras', 'stanford-nlp', 'word-embedding']","I get a ValueError: could not broadcast input array from shape (50) into shape (100) preparing embedding matrix I have loaded glove and made the word to vec Found 400000 word vectors.I did look at a bunch of similar questions but they all seem to deal with forgetting to add the +1 in the max number words, I think I have that covered but still have the issue. Any help deeply appreciated.I also triedUsing pre-trained word embeddings in a keras model?I also tried this one as wellKeras word embeddings Glove: can't prepare the embedding matrixbut also was the +1 issueFYI: Extreme newbie at this 1st time doing Seq to seq to due to the translating Tagalog into EnglishThe Error that is receivedCode"
1407,What is a good substitute for averaging vectors generated from Word2vec,"['vector', 'pca', 'gensim', 'word2vec', 'word-embedding']","My dataset is in the following format where for each disease I am generating a 2D vector using word2vec.(Showing 2D vectors for example but in practice, vectors are in 100D )From here I am generating a 1D array for each disease/disease combination by taking the average of the vectors. The issue with averaging word vectors is the fact that the combination of 2 or more diseases can have the same average vector as a totally different disease which is not at all relevant but the average vectors get matched. This makes the concept of averaging vectors flawed. To counter this, the understanding is with an increase in dimension of the vectors, this should be even less of a possibility.So, couple of questions in all:Is there a better way than averaging the output from word2vec vectors to generate a 1D array?These generated vectors will be treated as features to a classifier model that I am trying to build for each disease/disease combination so, if I generate a 100D feature vector from word2vec, shall I use something like a PCA on it to reduce the dimension or shall I just consider the 100D feature vector as 100 features to my classifier."
1408,"Error when checking input: expected input_39 to have 2 dimensions, but got array with shape (100, 50, 780)","['python', 'keras', 'deep-learning', 'lstm', 'word-embedding']","I am trying to put a word embedding on my Encoder-Decoder, but there seems to be some dimension issue.Up till here, there are no errors. And the model summary isHowever when I try to fit the model,I get Error when checking input: expected input_39 to have 2 dimensions, but got array with shape (100, 50, 780)I tried tweaking around things the whole day but got nowhere.  The only way I could do it is if I put Input(shape=(None, nEncoderToken)) and remove the Embedding Layer altogether."
1409,Adding a Pre-Trained Word Embedding into an Encoder Decoder,"['python', 'deep-learning', 'lstm', 'word-embedding', 'encoder-decoder']",I would like to add a Pretrained Word Embedding to my Encoder-Decoder.  Below is my code:I tried many ways but I just couldn't get it.
1410,How can I use word embedding result as an input in SVM? [closed],"['python', 'deep-learning', 'svm', 'text-classification', 'word-embedding']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 3 days ago.I am currently working on a project on how to detect hate speech using deep learning.
So my question is :can I use a word embedding matrix as an input in SVM for text classification? if yes, can you send me a link to show me how (Python)."
1411,fine tune spacy word vectors or training my own ones?,"['spacy', 'word-embedding']","This question is of a more conceptual type.
I was using the pre-trained word-vectors of spacy (the de_core_news_md model).
The problem is that I have a lot of domain specific words which all get a 0-vector assignet and overall the results are in gerneral not too good.
I was wondering how one should proceed now.
should I try to fine tune the existing vectors? If so, how would one approach that?
Or, should I just not use the pre-trained word vectors of spacy and create my own?Thank you in advance for any input!"
1412,Word Embeddings with FastText [closed],"['python', 'machine-learning', 'gensim', 'word-embedding', 'fasttext']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 6 days ago.I used fastText with the gensim package in order to create, for every word, a vector (an embedding). My question is: How can I use these embeddings? I tried to make the mean of all the vectors of every sentence in order to have just one vector for every observation and then use an ML classifier to train them, but the result was really bad on the test set.
Are there some other techniques that I can use in order to deal with all of these embeddings?"
1413,extracting numpy value from tensorflow object during transformation,"['python', 'tensorflow', 'word-embedding', 'tfrecord', 'tf.data.dataset']","i am trying to get word embeddings using tensorflow, and i have created adjacent work lists using my corpus.Number of unique words in my vocab are 8000 and number of adjacent word lists are around 1.6 millionWord Lists sample photoSince the data is very large i am trying to write the word lists in batches to TFRecords file.##############################"
1414,Using tensorflow.keras.layers.concatenate for concatenating character and word embeddings gives CUDNN_STATUS_BAD_PARAM error,"['machine-learning', 'lstm', 'embedding', 'word-embedding']",I am trying to use both pretrained word embeddings and character embeddings with my Bi-LSTM model as defined below. The base code has been taken from the tutorial here.The problem I am facing is that when I try to concatenate both the embeddings using the lineI get the error as shown belowNote: I get this error only when I am trying to convert the LSTM layers to cuDNN LSTM as described here. It works fine with simple LSTM layers.Can anyone help me fix this?
1415,How to Input a token(int) and a 2d sequence in Keras?,"['python', 'tensorflow', 'keras', 'lstm', 'word-embedding']",I am trying to train a model on input data which consists of a token (an id) and a 2d sequence data of fixed width(number of channels) and length(length of the time series window). I want the token to go to an embedding layer and the timeseries data to go to a LSTM later. My question is how do I input such a data in Keras/Tensorflow and then later inside the model how do I split it?
1416,Download pre-trained BERT model locally,"['python-3.x', 'word-embedding']","I am using the SentenceTransformers library (here: https://pypi.org/project/sentence-transformers/#pretrained-models) for creating embeddings of sentences using the pretrained model bert-base-nli-mean-tokens. I have an application that will be deployed to a device that does not have internet access. How can I save this model locally so that when I call it, it loads the model locally, rather than attempting to download from the internet? As the library maintainers make clear, the method SentenceTransformer downloads the model from the internet (see here: https://pypi.org/project/sentence-transformers/#pretrained-models) and I cannot find a method for saving the model locally."
1417,Vectorize/Support Batching a PyTorch Model,"['pytorch', 'word-embedding']","I have the following PyTorch model for CBOW embedding:It works well when I send it a tensor of the dimension (2*context_radius), but not when I send (N, 2*context_radius) where N > 1. For example, if my context radius is 2 (hence context is 4), I can send the input [36, 30, 12, 7] and I get an output of the shape (1, vocab_size). But I would like to be able to send a batch of contexts and retrieve a batch of outputs."
1418,(tf.)keras loading saved model weights with trainable word embeddings,"['python', 'tensorflow', 'keras', 'word-embedding']","I'm having a trouble with loading model weights in (tf.)Keras.My model is just a simple LSTM model with a pre-trained word embedding, but I left the word embedding to be trainable while training.I saved model weights with the following code:I checked that there exists the hdf5 file at the filepath, with a size of around 18MB.Later, I tried to load the weights with the following code:However, model.load_weights(filepath) returns NoneQuestion1. Is there any problem with these codes? If not is this possibly because I left the word embedding to be trainable?Question2. = In this case, where is the modified word embedding saved? Is it saved with other parameters in the hdf5 file? If this is the case how can I load this fine-tuned word embedding?"
1419,"ValueError: Error when checking input: expected embedding_1_input to have shape (150,) but got array with shape (74,)","['machine-learning', 'word-embedding', 'cnn', 'fasttext']",I am using a pretrained word vector (fasttext) and then running a CNN model. I seem to be getting a shape mismatch for the embedding input and output layer. I checked out this similar question but still cannot figure out how to resolve it.Following is my CNN architecture:Output:Error because of model.fit on mismatched input and output embedding layerError :Some additional info:
1420,What is the best way in text minig if word corps contain one words instance of large word sentances,"['keras', 'deep-learning', 'word-embedding', 'glove']","I need to train a deep learning model with different features. One of feature is the location and it contains USA states. And others all features are numeric values. For now, I used embedding technique to the text convert to dense vector. (texts are USA states names). Here is the example of JSON file.""location"":""Arizona"", ""location"":""Kentucky"", ""location"":""Texas"".... Is there any best way to text convert into vectors other than embedding technique?"
1421,WARNING:tensorflow:Model was constructed with shape … but it was re-called on a Tensor with incompatible shape,"['python', 'tensorflow', 'word-embedding']","I have the following modell:It takes an input of shape (1,1). However, I now feed an input of shape (1,8) in.But, instead of a crash, I only get the warning:WARNING:tensorflow:Model was constructed with shape Tensor(""embedding_1_input:0"", shape=(1, 1), dtype=float32) for input (1, 1), but it was re-called on a Tensor with incompatible shape (1, 8).Hence my question:"
1422,Incompatible shapes error - ELMO model from tensorflow hub,"['python', 'tensorflow', 'word-embedding', 'elmo', 'tf-hub']","I am using ELMO model from tensorflow hub. I have saved the model and using it in production environment.Every now and then, I can see errors in my production log:Line 33 in elmo_model.py isOther than this for most of the requests it has no issue. If there would have been any mistake, it shouldn't have worked for any requests."
1423,Word2Vec in short text clustering,"['cluster-analysis', 'word2vec', 'word-embedding']","I am trying to use word2vec in short text document clustering. However, I am not sure how the approach should be.This is what I have currently done.Because word2vec converts each word into a vector, how can I use the output of word2vec to represent a document which is then used as the input of the clustering algorithm?Does taking the average of the list of vectors (of the words) in a document works?"
1424,Using BioBERT with Longformer to obtain vectors on long sentences,"['python-3.x', 'vectorization', 'word-embedding', 'huggingface-transformers', 'sentence']","Based on this StackOverflow topic Pytorch error ""RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows"", I want to use Longformer to be able to compute vectorization on long sentences (more than 512 tokens that is the BioBERT vectorization limit). My problem is that when I look to Longformer documentation, I don't really see how to integrate Longformer vectorization to my use case.Please have a look at the SO topic mentioned above to have the context of this present question."
1425,How to encode multiple setence using transformers.BertTokenizer?,"['word-embedding', 'huggingface-transformers', 'huggingface-tokenizers']",I would like to create a minibatch by encoding multiple sentences using transform.BertTokenizer. It seems working for a single sentence. How to make it work for several sentences?
1426,How to train a model in sklearn with a feature column containing vectors instead of scalars?,"['machine-learning', 'scikit-learn', 'word-embedding']","Say I had a dataset containing days of the week: sat, sun, mon, etc.I used the technique of replacing categories with their embeddings. Whereas before I would have transformed the “days of week” feature using 1-hot encoding, entity embeddings get me a vector for each day.How can I use these vectors in my model?Similar question applies to an NLP classification task. Say before I would have one hot encoded all the words in my vocabulary, so each column corresponds to a word. Now I have vectors representing each word. How does this change a model? Do I need to feed a 3D tensor to, say, logistic regression?"
1427,Word2Vec embedding with pyspark: missing word embedding in lookup table,"['python', 'pyspark', 'word2vec', 'word-embedding', 'lookup-tables']","I have the dataframe below:
I want to embed with word2vec the ""words"" (categorical feature values) contained in the sentence column.It contains a lot of U, but it is not embedding the U. Why? Does pyspark internally consider it as a stopword?"
1428,Search Engine - rank the output by a weighted mechanism,"['python', 'tensorflow', 'elasticsearch', 'word-embedding', 'sentence-similarity']","I am trying to build a semantic search FAQ system using Elastic 7.7.0 and Universal Sentence Encoder (USE4) word embeddings, so far i have indexed a set of question and answers, which i am able to search. I am doing 2 searches whenever there is input :Now i want to combine both to give the robust output, because sometimes results are off from these individual algorithms. Any good suggestions on how can i combine them? use the weighted mechanism to give more weight to Semantic search, and/or be able to match them again. Question is how do i get best of both. Please advise.My output looks like this:i want one output which is based on both of these, where we can get more accurate results and rank them accordingly too. Please advise or redirect where i can refer some good practices around this. Thanks in Advance."
1429,Is the size in a Word2Vec model equal to the max input size of a sentence?,"['python', 'gensim', 'word2vec', 'word-embedding']","This might seem like a trivial question to many but its not clear for me.So i have a model that looks like thisI also tokenize my x_train and x_test like thisMy embedding layer looks like thisSo I get that the 300 in the model is the dimension size and the 300 in the pad_sequences is the max sequence length aka max sentence lengthI also get that the 300 in the embedding_matrix definition is the w2v dimension size again and that the first 300 in the embedding_layer is referring to the w2v dimension sizeand the second 300 would be the max input length aka the same as the max length in the pad_sequences but what I don't get is.If my dimensional size of my w2v model equals to let's say 240, does this mean my pad_sequences maxlen should also be 240 and also the input_length in my embedding_layer?For example, the w2v size is 300 but my sentence length that I want to predict is 360 and my pad_sequences maxlen would be 500. Would my neural net be able to correctly predict this sentence of length 360?"
1430,"can we convert weights of pretrained embedding layer of shape (x, y) to (x, z) where z < y, by compressing or something?","['tensorflow', 'pytorch', 'word-embedding', 'transformer']","suppose i want to use pre-trained bert embeddings in my custom project, but BERT embeddings are of shape (30522, 768), but my project expects embeddings as (30522, 256).Is it possible to convert these weights by compression or something?"
1431,Embedding layers trained on Amazon Reviews,"['sentiment-analysis', 'word-embedding']","I am working on research to perform sentiment analysis on Amazon reviews. My data is not labelled so I am now using Lexicon based sentiment analysis such as Vader. I wonder is that possible to use embedding layers to perform sentiment analysis in an unsupervised manner?
Are there any pre-trained embedding layers on Amazon reviews that I can use? Thank you very much in advance"
1432,Pytorch error “RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows”,"['python-3.x', 'pytorch', 'vectorization', 'word-embedding', 'huggingface-transformers']","I have sentences that I vectorize using sentence_vector() method of BiobertEmbedding python module (https://pypi.org/project/biobert-embedding/). For some group of sentences I have no problem but for some others I have the following error message :File
""/home/nobunaga/.local/lib/python3.6/site-packages/biobert_embedding/embedding.py"",
line 133, in sentence_vector
encoded_layers = self.eval_fwdprop_biobert(tokenized_text)   File ""/home/nobunaga/.local/lib/python3.6/site-packages/biobert_embedding/embedding.py"",
line 82, in eval_fwdprop_biobert
encoded_layers, _ = self.model(tokens_tensor, segments_tensors)   File
""/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"",
line 547, in __call__
result = self.forward(*input, **kwargs)   File ""/home/nobunaga/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py"",
line 730, in forward
embedding_output = self.embeddings(input_ids, token_type_ids)   File
""/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"",
line 547, in __call__
result = self.forward(*input, **kwargs)   File ""/home/nobunaga/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py"",
line 268, in forward
position_embeddings = self.position_embeddings(position_ids)   File
""/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"",
line 547, in __call__
result = self.forward(*input, **kwargs)   File ""/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/sparse.py"",
line 114, in forward
self.norm_type, self.scale_grad_by_freq, self.sparse)   File ""/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/functional.py"",
line 1467, in embedding
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: index out of range: Tried to
access index 512 out of table with 511 rows. at
/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237I discovered that for some group of sentences, the problem was related to tags like <tb> for instance. But for others, even when tags are removed, the error message is still there.
(Unfortunately I can't share the code for confidentiality reasons)Do you have any ideas of what could be the problem?Thank you by advanceEDIT : you are right cronoik, it will be better with an example.Example :This last line of code is what caused the error message in my opinion."
1433,Why pytorch transformer src_mask doesn't block positions from attending?,"['pytorch', 'word-embedding', 'transformer']",I am trying to train word embedding with transformer encoder by masking the word itself with diagonal src_mask:After training the model predicts exactly the same sentence from the input. If I change any word in the input - it predict the new word. So the model doesn't block according to the mask.Why is it ?I understand that there is a mistake in my logic because BERT would probably be much simpler if it worked. But where am I wrong ?Edit:I am using the a sequence of word indices as input. Output is the same sequence as input.
1434,How there is one representation for one word in word2vec embeddings whereas there are n-dimensions to represent?,['word-embedding'],"A paragraph from KDnuggets - ""It is for this reason that traditional word embeddings (word2vec, GloVe, fastText) fall short. They only have one representation per word, therefore they cannot capture how the meaning of each word can change based on surrounding context."""
1435,Pad vectors in tf.keras for LSTM,"['keras', 'lstm', 'padding', 'tf.keras', 'word-embedding']","Keras has a preprocessing util to pad sequences, but it assumes that the sequences are integer numbers.My sequences are vectors (my own embeddings, I do not want to use Keras embeddings), is there any way in which I can pad them to use in a LSTM?Sequences can be made equal in Python, but the padding methods in Keras provide additional metainformation for layers like LSTM to consider for masking."
1436,Why does 'dimension' mean several different things in the machine-learning world? [closed],"['tensorflow', 'deep-learning', 'neural-network', 'pytorch', 'word-embedding']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed last month.I've noticed that AI community refers to various tensors as 512-d, meaning 512 dimensional tensor, where the term 'dimension' seems to mean 512 different float values in the representation for a single datapoint. e.g. in 512-d word-embeddings means 512 length vector of floats used to represent 1 english-word e.g. https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6But it isn't 512 different dimensions, it's only 1 dimensional vector? Why is the term dimension used in such a different manner than usual? When we use the term conv1d or conv2d which are convolutions over 1-dimension and 2-dimensions, a dimension is used in the typical way it's used in math/sciences but in the word-embedding context, a 1-d vector is said to be a 512-d vector, or am I missing something?Why is this overloaded use of the term dimension? What context determines what dimension means in machine-learning as the term seems overloaded?"
1437,GloVe - Subscript out of bounds error in R,"['r', 'search', 'word-embedding', 'glove']","I am attempting to write a feature that checks for each query what the maximum score is. The query is the search term, and the glove word vectors are trained using product descriptions. The code I use for training the word vectors and the feature itself (all.gloveterms) are listed below. 
I am receiving the error: Error in word_vectors[query, , drop = FALSE] : subscript out of bounds, after attempting to execute the command: all.gloveterms(query_product$search_term[2]), where the search term should return ""angle bracket"". I am having a hard time finding out how to fix this error, and was wondering if anyone can help."
1438,Glove implementation with PySpark,"['apache-spark', 'pyspark', 'word-embedding', 'glove']","I'm working with word embeddings using PySpark, in particular I'm working with Word2Vec. Now, I'd like to try Glove instead of Word2Vec but, apparently, it doesn't exist an implementation of Glove with PySpark, but only with Scala language. Is there any way to use Glove with PySpark? Can I use the Scala implementation into PySpark, or am I obliged to implement from scratch the algorithm? Thanks"
1439,How to reduce vocabulary using tf.keras.preprocessing.text.Tokenizer?,"['tensorflow', 'keras', 'nlp', 'tokenize', 'word-embedding']","I'm using tf.keras.preprocessing.text.Tokenizer to build a vocabulary for my corpus (5 million documents). The tokenizer finds 145K tokens. The issue is that the embedding layer has far too many parameters. What's a simple way to force the tokenizer to only consider the top N most common words? Then anything outside of that would get a .As indicated by @MarcoCerliani in the comment, you can simply change the num_words parameter in the tokenizer. That won't change the tokenizer's internal data - it'll instead return OOV tokens for words outside the range specified by num_words."
1440,HDF5 dataset stores nothing but 0/empty values,"['python', 'csv', 'hdf5', 'h5py', 'word-embedding']","I am writing a CSV file into an HDF5 file to load information in a better way without filling up the memory. My CSV file contains indices that I am converting to their corresponding values through a dictionary.The CSV file is very big (4 GB worth of indices) and the corresponding values are 512 sized arrays. To create the dataset, I first define the datasets in the H5 file, and then chunk-wise read the CSV file so that it uses an appropriate amount of RAMI map the indices to their corresponding values using the df.map function. After that, I get the embedding (or the 512 sized arrays as I described earlier). After that, I append them to their corresponding datasets.However, to test, I print an embedding from the H5 file, using this:And I get an array of zeros (of size 512) as an output.Can anyone guide me where I am going wrong here? As per my estimate, it should be where I am ""appending"" the embedding to the datasets. In my opinion, I am not appending the values and doing something wrong there.Also, when I test the labels, they are coming out to be correct. So, I guess, it mainly highlights my issue to this line:dset1[i:i+chunksize, num_features:] = paragraph_embeddings"
1441,Combining a word array and a vector array to make a Gensim W2V model,"['gensim', 'word2vec', 'embedding', 'word-embedding']","I have a word array from a pickle file, and a corresponding vector array from an npy file, how do I combine them to make a Gensim W2V model?"
1442,"Word embedding with gensim and FastText, training on pretrained vectors","['python', 'gensim', 'word-embedding', 'fasttext']","I am trying to load the pretrained vec file of Facebook fasttext crawl-300d-2M.vec with the next code:But it fails with the next error:It is not possible to load this vector?If it is possible, afterwards can I train it with my own sentences?Thanks in advance.Whole error trace:"
1443,Word embedding of new word using similarity,"['vector', 'cosine-similarity', 'word-embedding']","I used Word2Vec to get the vectors of a sample textual data.
Assuming for each word (W), W1, W2, W3, ... , Wn we have a vector. 
And also we have a similarity matrix that shows cosine similarity between words.Now the question is for a new word (X) that is in the similarity matrix, can I get the vector?
Lets say the similarity between W1 and X is 0.33, ans similarity between W2 and X is 0.12. Can I compute the vector X by multiply the vector of each word(W) to the similarity and then sum the values?I read about it and still I'm not sure if it's a correct approach.
For example the link below does't answer my question, since I don't have data related to the new word, only similarity matrix is available.
https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"
1444,PyTorch: Loading word vectors into Field vocabulary vs. Embedding layer,"['python', 'machine-learning', 'pytorch', 'word-embedding']","I'm coming from Keras to PyTorch. I would like to create a PyTorch Embedding layer (a matrix of size V x D, where V is over vocabulary word indices and D is the embedding vector dimension) with GloVe vectors but am confused by the needed steps.In Keras, you can load the GloVe vectors by having the Embedding layer constructor take a weights argument:When looking at PyTorch and the TorchText library, I see that the embeddings should be loaded twice, once in a Field and then again in an Embedding layer. Here is sample code that I found:Specifically:Here are other StackOverflow questions that did not answer my questions:PyTorch / Gensim - How to load pre-trained word embeddingsEmbedding in pytorchPyTorch LSTM - using word embeddings instead of nn.Embedding()Thanks for any help."
1445,how to calculate mean of words' glove embedding in a sentence,"['python', 'word-embedding', 'glove']","I have downloaded the glove trained matrix and used it in a Keras layer. however, I need the sentence embedding for another task.I want to calculate the mean of all the word embeddings that are in that sentence.what is the most efficient way to do that since there are about 25000 sentences?also, I don't want to use a Lambda layer in Keras to get the mean of them."
1446,How can I consider word dependence along with the semantic information in information retrieval?,"['deep-learning', 'nlp', 'text-mining', 'information-retrieval', 'word-embedding']","I am working on a project that text retrieval is an important part of it. There is a reference collection (D), and users can enter queries (Q). Therefore, like a search engine, the goal is to retrieve the most related documents to each query.   I used pre-trained word embeddings to extract semantic knowledge about each word within a text. I then aggregated the continuous vectors of words to represent each text as a vector (using mean/sum aggregate function). Next, I indexed the source vectors and extracted the most similar vectors to the query vector. However, the result was not acceptable. I also tested the traditional approaches like the BOW technique. While these approaches work very well in some situations, they do not consider semantic and syntactic information (that made them not good for some queries).Based on my investigation, considering word dependence (for example, co-occurring the words in the same sentence) along with the semantic information (obtained using the pre-trained word embeddings) can be very useful. However, I do not know how to combine them to be applicable in IR. It should be noted that:I'm not looking for paragraph2vec or doc2vec; those require training on a large data corpus, and I don't have a large data corpus. Instead, I want to use an existing word embeddings.I'm not looking for a re-ranking technique like learning to rank approach. Instead, I'm looking for a way to take advantage of both syntactic and semantic information in the representation step i.e. mapping the text or query to a feature vector. Any help would be appreciated. "
1447,`King - Man + Woman = Queen` cannot be validated using spaCy word embedding calculations,"['python', 'machine-learning', 'nlp', 'spacy', 'word-embedding']","Word embeddings are supposed to make calculations using words possible as explained in this article. However, when using spaCy's pretained word embedding, this can not be reproduced, i.e. the difference between King - Man + Woman and Queen is not close to zero.The result is:What might be wrong?"
1448,Keras nn multiple output types,"['python', 'tensorflow', 'keras', 'word-embedding']","IN SUMMARY:I haven't found how to implement the following:Much of the online documentation/examples I've found (linked) are close, but I haven't found how to adjust them to attain even the first item in that list.IN ENTIRETY:I have a dataset with 3 inputs (2 continuous, 1 categorical (one-hot)) and 2 outputs (1 cont. 1 cat. (embedded)) that I'd like to classify on.I'm not sure how to formulate my TF model to handle this, and the docs on outputs don't quite cover this case.TensorFlow does a great job describing here how to structure each feature column, but not label columns (which would be important if I wanted to bucketize, use embedding, etc.). A very different approach (structurally) is taken here but the output uses one-hot encoding and the categorical output in question would have a relatively large domain of possibilities, so I would definitely like to avoid that.Lastly, if I followed a structure similar to the tensorflow article above or using an embedded layer as they describe in their word embeddings documentation, could I even mix categorical and numerical outputs like this? In a more custom environment, I obviously could have different neurons in the output layer with different activation functions, but in this case I'd need both a softmax (cat.) and some other (relu,tanh,etc.) in the output layer for the continuous outputs.Data Format:Note: It seems a lot of articles stylistically differ from the linked TF docs by directly splitting up input and output data, then using some one-hot encoding utils (LabelBinarizer, to_categorical(), etc.) to handle categorical data as opposed to the feature_column utils TF uses in their own docs. Firstly, That seems odd. Secondly, I haven't found a function similar to to_categorical() that uses a word embedding, rather than a one-hot implementation."
1449,Keras: merging Embedding layers in a model with two inputs,"['python', 'keras', 'neural-network', 'word-embedding']","I'm trying to build a keras model with two inputs. Inputs are sequences of shape 5 where each element of this sequence is the index of a vectorized word in an embedding matrix.Currently I tried something like that:When I try to run it, I get the following errorThis is something that I'm currently not able to understand. Why concatenate is not spotting something with 3-dimensions when inputs are correctly set? What I'm doing wrong?Thanks in advance.Note: To reproduce this error, please, generate inputs with the following code:"
1450,How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"['nlp', 'text-mining', 'gensim', 'word2vec', 'word-embedding']","Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). E.G. ""I love you."" ==> [(I,love), (I, you)]May I ask what is the word pair when the sentence contains only one word? Is it  ""Happy!"" ==> [(happy,happy)] ?I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.===============UPDATE===============================As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights."
1451,I want to use pre-trained Word2Vec model but not sure how to use it,"['model', 'nlp', 'word2vec', 'word-embedding']","I want to use this pre-trained Word2Vec model but not sure how to use itI tried to load the zip file and then use it but getting this error:
Not a gzipped file (b've')"
1452,Weights are not updated in Pytorch nn.Embedding,"['pytorch', 'word-embedding']","I loaded the PyTorch's nn.Embedding module with a pre-trained embedding matrix. I set it to trainable as follows. I processed this output using Bidirectional Gated Recurrent Units network. 
After the model is trained, I checked whether the weights of nn.Embedding are updated. The weights were not updated. model.embedding_layer.weight and 'self.embedding' weights are the same. 
I checked the gradients of model.embedding_layer. They are all zeros. Could you please help me? Thank you."
1453,How to “expand” an English sentence,"['algorithm', 'tensorflow', 'deep-learning', 'nlp', 'word-embedding']","I would like to get some steps and resources about, how to build program to expand a sentence in English.For example, if an input sentence is ""My father is coaching and refereeing soccer and basketball.""Then the program would output four sentences:""My father is coaching soccer.""""My father is coaching basketball.""""My father is refereeing soccer.""""My father is refereeing basketball.""The generated sentences must be valid English sentences and their meaning must follow from the meaning of the input sentence."
1454,How to measure how distinct a document is based on predefined linguistic categories?,"['nlp', 'data-science', 'topic-modeling', 'cosine-similarity', 'word-embedding']","I have 3 categories of words that correspond to different types of psychological drives (need-for-power, need-for-achievement, and need-for-affiliation). Currently, for every document in my sample (n=100,000), I am using a tool to count the number of words in each category, and calculating a proportion score for each category by converting the raw word counts into a percentage based on total words used in the text. For each document, I would like to get a measure of distinctiveness that indicates the degree to which the content of a document on the three psychological categories differs from the average content of all documents (i.e., the prototypical document in my sample). Is there a way to do this?"
1455,From numpy array of sentences to array of embedding,"['tensorflow', 'keras', 'nlp', 'embedding', 'word-embedding']","I'm learning to use tensorflow and trying to classify text. I have a dataset where each text is associated with a label 0 or 1. My goal is to use some sentence embedding to do the classification. First I've created an embedding from the whole text using the Gnews precompile embedding:Now I'd like to try something else (similar to this method http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) and I wanted to:I'm able to separate the texts in sentences. Each text is an array of sentences saved as:Now when I try to create an embedding from one element of the array it works. Here is my embedding function:But if I try to input a batch of data (as will be done later in the pipeline, the program crashesThe error states that it's not possible to convert a Numpy array to a tensor. I've tried changing the input_shape parameter of the KerasLayer to no avail. The only solution I see is to calculate the embedding for each text by looping through all of them one by one before finding the result to the rest of the network but that seems highly inefficient (and requires too much memory for my laptop). Examples I see with word embedding, do it this way however.What is the correct way to go about getting a list of embedding from multiple sentences?"
1456,Keras 'one_hot' for text processing,"['tensorflow', 'keras', 'nlp', 'word-embedding', 'python-textprocessing']","Suppose I have sequence of word/ sentence: ""I like food""If I enconde with keras one_hot:it gives me the following values:[10, 39, 17]However, suppose I have a sequence of entry like this: ['Add more', 'Add less', 'Do little more']
Here each of the entries is like a word in a sentence. Therefore, I want to encode 'Add more', 'Add less', and 'Do little more' as a single word.   All of the entries consist of full sequence or sentence.How can I encode this like one_hot in keras. Using one_hot it shows error:'list' object has no attribute 'lower'"
1457,How to visualize the SpaCy word embedding as scatter plot?,"['matplotlib', 'machine-learning', 'nlp', 'spacy', 'word-embedding']",Each word in SpaCy is represented by a vector of length 300. How can I plot these words on a scatter plot to get a visual perspective on how close any 2 words are?
1458,Tensorflow feature column sequence_categorical_column_with_vocabulary_list for variable list of values,"['python', 'tensorflow', 'tensorflow2.0', 'word-embedding']","From the TensorFlow docs it's clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input a list of sequence and outputs a dense Matrix. For exampleMy question is related to having three lists of strings as features. For example, if every feature list have same length, I can get the desired result, but when my features list are variab length, for example features = {'letter': [['A'], ['C','D'], ['E','F','A']]}variable_columns_layer will throw ValueError: Can't convert non-rectangular Python sequence to Tensor.How could I get the correct result from the variable_columns_layer for variable list input?"
1459,"Using Dropout on output of embedding layer changes array values, Why?","['python', 'python-3.x', 'keras', 'deep-learning', 'word-embedding']","Observing the outputs of embedding layer with and without dropout shows that values in the arrays are replaced with 0. But along with this why other values of array changed ?Following is my model:-Building  model2 from trained model , with input as the input layer and output as the output of Dropout(0.2) . -Printing the first array of both dropout and no dropout:  Output--Output-Some values are replaced with 0 , agreed, but why are other values changed ?
As the embedding outputs have a meaning and are unique for each of the words, if these are changed by applying dropout, then is it correct to apply dropout after embedding layer ?Note- I have used ""learning_phase"" as 0 and 1 for testing(nodropout)
  and training(droput) respectively."
1460,"Trying to get all word embeddings, but Tensorflow StaticVocabularyTable object is not iterable?","['python', 'tensorflow', 'lookup', 'word-embedding', 'oov']","I encoded the top 100 most common words in a dataset of tweets. Those were initialized with tf.lookup.KeyValueTensorInitializer with IDs 0-99. Now I am using tf.lookup.StaticVocabularyTable to leave room for 10 out-of-vocabulary (OOV) buckets.I know how to use StaticVocabularyTable to lookup the ID of a specific word, but what I really need is a full list of the 110 words used (the 100 common words plus the 10 OOV words) and their respective ID numbers after I encode the training set.I thought I could iterate over StaticVocabularyTable as key/value pairs, but it's not iterable. How can I get the full table of 110 IDs and words?I also tried model.layers[0].get_weights()[0], but that doesn’t give me the words."
1461,Glove6b50d parsing: could not convert string to float: '-',"['python', 'text', 'gensim', 'word-embedding', 'glove']","I am trying to parse the Glove6b50d data from Kaggle in via Google Colab, then run it through the word2vec process (apologies for the huge URL - it's the fastest link I've found). However, I'm hitting a bug where '-' tokens are not parsed correctly, resulting in the above error. I have attempted to handle this in a few ways. I've also looked into the load_word2vec_format method itself and tried to ignore errors, however it doesn't seem to make a difference. I've tried a map method on line two, following combinations of advice from these links: [a] and [b]. This hasn't fixed or changed the error message received (i.e. removing it changes nothing in the text). Per the comment below, the exact error I'm getting is as follows:The system works fine using a text file containing only: ""test -1.0 1.526 -2.55"" or ""- -1.0 1.526 -2.55"". Additionally, searching the source text file (glove.6B.50d.txt) for occurrences of "" - "" comes up with no results. I'm on Windows, so I have done so by executing: Calling print(gloveFile) both pre- and post-map call provide the following output. Note that I've kept the mapping call in for completeness of my efforts, not for its effect. If I print the first ten lines of the glove6b50d_word2vec.txt file, I get the following text, which matches the word2vec format. Additionally, if I count the occurrences of the string "" - "" in the document, I find none. My search methods are evidently thusfar ineffective. Would really appreciate some help. "
1462,Why does my LSTM layer keeps throwing errors?,"['python', 'arrays', 'lstm', 'recurrent-neural-network', 'word-embedding']","I have a rnn and want to feed in a sentence with a length of 50, and have the output be the same length. (For a chatbot). Does anyone know why this error:keeps appearing? Here is the code:The shape of both arrays are 5000, 50....5000 sentences with 50 words each. They are already encoded. I first though it was because I was flattening...but this is the error before flattening:##BTW the vocab_size is 12097##"
1463,Generating data for a CBOW model in tensorflow,"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']","I cant figure out how to build a tensorflow word embedding CBOW model.
I have trouble building the generate data function.
This is my preprocessing func:I already have Skipgram implemented like so:How should I approach creating a generate_data_CBOW function, similar to the one of the generate skipgram.
I tried looking for something similar but all the results are from 2016 and pretty outdatedEDIT:
This is the model I'm running my skipgram model through, if its any help"
1464,How to compare cosine similarities across three pretrained models?,"['nlp', 'gensim', 'word2vec', 'word-embedding', 'glove']","I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?"
1465,Is there a way to explicitly set colors for embeddings in Tensorboard Embedding Projector?,"['tensorflow2.0', 'tensorboard', 'word-embedding']","I have embeddings that represent a hierarchical group structure.  I'd like to be able to set my own color scheme for visualization in the embedding projector (rather than letting the embedding projector choose colors for me).I want to capture the hierarchical structure of my data. For example, I'd like class A1, A2, A3 to all be shades of one color and class B1, B2, B3 to be shades of another color."
1466,"WARNING: WARNING:tensorflow:Model was constructed with shape (None, 150) , but it was called on an input with incompatible shape (None, 1)","['python', 'tensorflow', 'keras', 'reshape', 'word-embedding']","So I'm trying to build a word embedding model but I keep getting this error.
During training, the accuracy does not change and the val_loss remains ""nan""The raw shape of the data isThen I reshape it so:Then I run it through my modelAfter training, I get a constant accuracy and a val_loss nan for every epochI think it has to do whit the input/output shape but I'm not certain. I tried modifying the model in various ways, adding layers/ removing layers/ different optimizers/ different batch sizes and nothing worked so far."
1467,Keras NLP validation loss increases while training accuracy increases,"['tensorflow', 'keras', 'nlp', 'word-embedding', 'hyperparameters']","I have looked at other posts with similar problems and it seems that my model is overfitting. However, I've tried regularization, dropout, reducing parameters, decreasing the learning rate and changing the loss function, but nothing seems to help.Here is my model:And my training output:My validation loss seems to always increases no matter what. I am trying to predict political affiliation from tweets. The dataset I am using has worked well on other models, so perhaps there is something wrong with my data preprocessing instead?I am really stumped. Any help is appreciated."
1468,Tensorflow Embedding Layer Vocabulary Size,"['python', 'tensorflow', 'word-embedding']",I am learning Tensorflow and have come across the Embedding layer in tensorflow used to learn one's own word embeddings. The layer takes the following parameters:The 'input dim' should be the same size as the vocabulary i.e. unique words. If I wanted to limit the vocabulary to only the first 25000 most frequent words - how should I do this? Can I simply change 'input_dim' to 25000 or would I have to go through my corpus and replace any word that is outside the top 25000 words with an  token for example?
1469,Size of input and output layers in Keras implementation of an RNN Language Model,"['tensorflow', 'keras', 'neural-network', 'word-embedding', 'language-model']","As part of my thesis, I am trying to build a recurrent Neural Network Language Model. From theory, I know that the input layer should be a one-hot vector layer with a number of neurons equal to the number of words of our Vocabulary, followed by an Embedding layer, which, in Keras, it apparently translates to a single Embedding layer in a Sequential model. I also know that the output layer should also be the size of our vocabulary so that each output value maps 1-1 to each vocabulary word.However, in both the Keras documentation for the Embedding layer (https://keras.io/layers/embeddings/) and in this article (https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#comment-533252), the vocabulary size is arbitrarily augmented by one for both the input and the output layers! Jason gives an explenation that this is due to the implementation of the Embedding layer in Keras but that doesn't explain why we would also use +1 neuron in the output layer. I am at the point of wanting to order the possible next words based on their probabilities and I have one probability too many that I do not know to which word to map it too.Does anyone know what is the correct way of acheiving the desired result? Did Jason just forget to subtrack one from the output layer and the Embedding layer just needs a +1 for implementation reasons (I mean it's stated in the official API)?Any help on the subject would be appreciated (why is Keras API documentation so laconic?).Edit:This post Keras embedding layer masking. Why does input_dim need to be |vocabulary| + 2? made me think that Jason does in fact have it wrong and that the size of the Vocabulary should not be incremented by one when our word indices are: 0, 1, ..., n-1.However, when using Keras's Tokenizer our word indices are: 1, 2, ..., n. In this case, the correct approach is to:Set mask_zero=True, to treat 0 differently, as there is never a
0 (integer) index input in the Embedding layer and keep the
vocabulary size the same as the number of vocabulary words (n)?Set mask_zero=True but augment the vocabulary size by one?Not set mask_zero=True and keep the vocabulary size the same as the
number of vocabulary words?"
1470,How does Word Embeddings in Deep Learning works?,"['tensorflow', 'deep-learning', 'word-embedding']","I have a very basic doubt in Word Embeddings. I have an understanding that word embeddings are used to represent text data in a numeric format without losing the context, which is very helpful in training deep models.Now my question is, does the word embedding algorithm need to learn all the data once and then represent each record in numeric format? Or else, each record will be represented individually with knowing what other records.Tensorflow code:This is an experiment I did with sample code where embeddings independently reframe the data into the specified dimension.Is my understanding correct?"
1471,Speed up embedding of 2M sentences with RoBERTa,"['python', 'nlp', 'word-embedding', 'transformer']","I have roughly 2 million sentences that I want to turn into vectors using Facebook AI's RoBERTa-large,fine-tuned on NLI and STSB for sentence similarity (using the awesome sentence-transformers package).I already have a dataframe with two columns: ""utterance"" containing each sentence from the corpus, and ""report"" containing, for each sentence, the title of the document from which it is from.From there, my code is the following:Right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than I can spare.Is there any way I could speed this up by changing my code? Is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (I suspect not).Many thanks in advance!"
1472,Gensim's `model.wv.most_similar` returns phonologically similar words,"['python', 'data-science', 'gensim', 'embedding', 'word-embedding']",gensim's wv.most_similar returns phonologically close words (similar sounds) instead of semantically similar ones. Is this normal? Why might this happen? Here's the documentation on most_similar: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similarThe training data contains academic papers and this was my training script: 
1473,"ValueError: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (3306, 67, 1)","['tensorflow', 'keras', 'neural-network', 'nlp', 'word-embedding']","I am trying to train a neural network on Semantic Role Labeling task (text classification task). The dataset consist of sentences on which the neural network has to be trained to predict a class for each word. Apart from using the embedding matrix, I am also using other features (meta_data_features). The number of classes in Y_train are 61. The number 3306 represents the number of sentences in my dataset (size of my dataset). MAX_LEN = 67. The code for the architecture is:The snapshot of model summary is:The function call is:X_train constitutes [padded_sentences, meta_data_features] and Y_train is padded_verbs. Their shapes are:padded_sentences - (3306, 67)meta_data_features - (3306, 67)padded_verbs - (3306, 67, 1)When I try to fit the model, I get the error, ""ValueError: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (3306, 67, 1)""It would be great if somebody can help me in resolving the error. Thanks!"
1474,How to get pure google news articles for training own word vectors?,"['word-embedding', 'fasttext', 'google-news']",Is it possible to get a latest dump of google news for specific NLP purposes? I want to get ideally the latest google news (from this year) to train word embeddings with fasttext. The reason why is to get latest news (mostly Corona related).Are there any easily accessible dumps like for wikipedia?
1475,"How to go about creating embeddings (especially, token to Id mapping) for categorical columns in tensorflow 2.0+?","['tensorflow', 'tensorflow2.0', 'tensorflow-datasets', 'word-embedding']",I have a csv with both categorical and float dtypes. I want to do the following:I dont know how to go about doing it currently. A very inelegant solution i can see is using tensorflow_datasets:This is a very basic thing that i think tensorflow would be able to do relatively easily. Please guide me to the right solution
1476,word2vecf feature embedding file formats,"['word2vec', 'word-embedding']","I'd like to experiment with the Word2Vecf embedding functionality in the DeepLearning4j project.
Word2Vecf allows embedding features as well as words into the embeddings.
I haven't found a good simple explaination of the file format for the input files.
Word2vecfTrainExample.java requires a training file, a word vocabulary file, and a word context file.
What is the format for these 3 files?I want to add POS, NER and DEPREL symbolic features from the Stanford parser to the embeddings. I also have some numeric features as well.
Can anyone point me to some small sample files, or a detailed description of the input format?"
1477,Text2Vec: Using Jaccard / Cosine Similarity instead of Relaxed Word Mover's Distance for a Document Similarity Task,"['r', 'nlp', 'word-embedding', 'text2vec', 'glove']","I am comparing several methods for a document similarity task. 
In one method I represent my text using Glove WordEmbeddings and compute document similarity using the Relaxed Word Mover's(RWM) distance, see the code below from the text2vec package. I would like to test if I can increase performance by using cosine and/or jaccard distance instead of the RWM distance to compare the similarity of my documents. Is there a way to implement that? "
1478,Pearson coefficient and Dimensions,"['python', 'nlp', 'word-embedding', 'pearson']","How should higher correlation value for lower dimensions should be interpreted?
For example 0.7 correlation for dimension 10"
1479,Check CPU utilisation of command in Jupyter notebook,"['python', 'jupyter-notebook', 'word-embedding']","Im trying to apply infersent embed-dings on 400k records and the kernel dies every-time i run. i wanted to understand the CPU utilisation, but not able to find any commands which can do it in Jupyter notebook"
1480,Training PCA on BERT word embedding: entire training dataset or each document?,"['python', 'scikit-learn', 'pca', 'word-embedding']","I want to reduce the dimensionality of the BERT word embedding to, let's say, 50 dimensions. I am trying with PCA. I will use that for the document classification task. Now for training  PCA, should I train on the entire dataset by using all the word vectors from the entire data set at once that is:or word vectors per document, that is:"
1481,Why there is a difference between tf.nn.embedding_lookup_sparse and tf.nn.embedding_lookup when dealing with the embedded results?,"['tensorflow', 'sparse-matrix', 'tensorflow2.0', 'embedding', 'word-embedding']","tf.nn.embedding_lookup_sparse has a parameter called combiner, specifying the reduction operation. However, tf.nn.embedding_lookup does not have this, which makes the two functions return different results when dealing with the corresponded parameters. Why there is a difference between the two functions? Any reason from the perspective of the designing purpose? "
1482,TypeError: unhashable type: 'list' in training word2vec,"['python', 'typeerror', 'text-mining', 'word2vec', 'word-embedding']",I wrote this function and got the TypeError: unhashable type: 'list'. how can I fix it?TypeError: unhashable type: 'list'
1483,use spacy word vectors in torchtext,"['python', 'spacy', 'word-embedding', 'torchtext']","spacy provides german word vectors spacy model de, so I want to make use of them with pytorch and torchtext.However when i try to build TEXT.build_vocab I get the following error message:So i looked  Vector up in the torchtext documentation torchtext vectors and it seems I need to load them first. Is there a solution where I do not have to save the spacy word vectors to a file first? And if not, what is the correct way to save spacy word vectors that later I can load them ?"
1484,Gensim word2vec downsampling sample=0,"['python', 'math', 'gensim', 'word-embedding', 'subsampling']","Does sample= 0 in Gensim word2vec mean that no downsampling is being used during my training? The documentation says just that ""useful range is (0, 1e-5)""However putting the threshold to 0 would cause P(wi) to be equal to 1, meaning that no word would be discarded, am I understanding it right or not? I'm working on a relatively small dataset of 7597 Facebook posts (18945 words) and my embeddings perform far better using sample= 0rather than anything else within the recommended range. Is there any particular reason? Text size? "
1485,Text classification using word embeddings,"['machine-learning', 'text-classification', 'word-embedding', 'unsupervised-learning', 'supervised-learning']","I've got a dataset of positive and negative content. So let's assume it's a spam project. I need to build a model, which can categorize the content in pos/neg. So I am doing a supervised learning task, because I've got a labeled dataset. The best choice therefore must be using a SVC model.So far so good.Now the complicated part comes.I want to solve the same task by using Keras LSTM model. So my question:Is it still supervised or is it unsupervised , because I am using word embeddings for this task and referring to this post here, word embedding is used for unsupervised tasks: https://www.quora.com/Is-deep-learning-supervised-unsupervised-or-something-elseThere it says: Deep learning can be Unsupervised : Word embedding, image encoding
  into lower or higher dimensional etc.So - is it now unsupervised or supervised (because my dataset is labeled) ? And is deep learning another technique like unsupervised and supervised learning or how is the relation between these topics? Is deep learning using supervised and unsupervised techniques? Or do one have to choose between deep learning, unsupervised and supervised learning?It's so confusing! Please help! Especially for the LSTM task. I need to know where it's supervised (because of the labeled dataset) or unsupervised (because of the usage of word embeddings)Thanks in advance guys!"
1486,Dependency-Based Word Embeddings by Lavy and Goldberg : How to run code,"['nlp', 'word2vec', 'word-embedding']",I recently read a paper by Levy and Goldberg about dependency aware word embeddings (which is an extension of word2vec with arbitrary context instead of BoW context) and found a link to the code. Here is the link of the readme file in the code. I am following these instructions but unable to run.Did anybody go through it?
1487,How to measure similarity between words or very short text,"['elasticsearch', 'nlp', 'word2vec', 'nearest-neighbor', 'word-embedding']","I work on the problem of finding the nearest document in a list of documents. Each document is a word or a very short sentence (e.g. ""jeans"" or ""machine tool"" or ""biological tomatoes""). By closest I mean close in a semantical way.I have tried to use word2vec embeddings (from Mikolov article) but the closest words or more contextually linked than semanticaly linked (""jeans"" is linked to ""shoes"" and not ""trousers"" as expected).I have tried to use Bert encoding (https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#32-understanding-the-output) using last layers but it faces the same issues.I have tried elastic search, but it doesn't find semantical similarities.
(The task needs to be solved in French but maybe solving it in English is a good first step)"
1488,How can I get RoBERTa word embeddings?,"['encoding', 'nlp', 'word-embedding']","Given a sentence of the type 'Roberta is a heavily optimized version of BERT.', I need to get the embeddings for each of the words in this sentence with RoBERTa. I have tried to look at the sample codes online, failing to find a definite answer. My take is the following:where embedding[:,1:n,:] is used to extract only the embeddings for the words in the sentence, without the start and end tokens. Is it correct?"
1489,Text Classification with Spacy : going beyond the basics to improve performance,"['nlp', 'spacy', 'text-classification', 'word-embedding', 'spacy-pytorch-transformers']","I'm trying to train a text categorizer on a training dataset of texts (Reddit posts) with two exclusive classes (1 and 0) regarding a feature of the authors of the posts, and not the posts themselves.  
Classes are unbalanced: approximately 75:25, which means that 75% of authors are ""0"", while 25% are ""1"".
The whole dataset is composed of 3 columns: the first one representing the author of the post, the second the subreddit the post belongs to, and the third the actual post.  The dataset looks like this:Where postI_J is the J-th post of the I-th author. Notice that in this dataset the same author may appear more than once, if she/he has posted more than once. In a separate dataset I have the class each author belongs to. The first thing I did was to group by author:There are a total of 5000 authors, 4000 used for training and 1000 for validation (roc_auc score).
And here is the spaCy code that I'm using (train_texts is the subset of train_data_full_agg.body.tolist() I use for training, while test_texts the one I use for validation).I get poor performance (measured with the ROC on a test dataset i don't have the labels of) also if compared to simpler algorithms one could write using scikit learn (like tfidf, bow, word embeddings etc)I've tried to get better performance with the following procedures: Using this logistic regression, I get ROC = 0.89 over the test dataset. If I remove any of these steps and use an intermediate model, the ROC lowers.I've also tried the following steps, which again just lowered the ROC:Q: What is the ROC of baseline models like Multinomial Naive Bayes or SVM ? 
I have easy access to ROCs evaluated just on texts(no subreddits or vectors). An svm set up like so: 
svm= svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, max_iter=-1) 
Would give roc (using CountVectorizer bow) = 0.53 (same with rbf kernel, but rbf + class_weight = None or ""balanced"" gives 0.63 (same without the constraint on cache_size)).Anyway, an XGBregressor with parameters set with gridsearch would give roc = 0.88. The same XGB but also with CountVectorizer subreddits and scikit Doc2Vec vectors (combined with an lr like above) gives about 93. The ensemble code you see above, only on texts would give around 83. With subreddts and vectors (treated as above) gives 89 Q: Have you tried not concatenating? 
If i don't concatenate, performance (just on not concatenated texts, so again no vectors/subreddits) is similar to the case in which I concatenate, but I wouldn't then know how to combine multiple predictions for the same author into one prediction. Because remember that I have more comments from each author, and I have to predict a binary feature regarding the authors.Any suggestion is highly appreciated. 
Please be as explicit as possible in terms of code / explanations / references as I am new to NLP."
1490,Doc2Vec Pre training and Inferring vectors,"['python', 'nlp', 'word-embedding', 'doc2vec', 'pre-trained-model']","Suppose I have trained the doc2vec model with 50000 documents and I want to infer vectors for a separate dataset containing 36000 documents. In this case will the inferred vectors be effective for the downstream task of classification, becasue my assumption is that the inferred vectors depend on the size of documents with which the model is trained. Note: Both dataset i.e one used for training doc2vec and other for inferring vectors are unique but from the same domain of US supreme court.Please correct me if I am wrong with valid reason."
1491,How do I use the TensorFlow Word Embedding feature?,"['tensorflow', 'word-embedding']","I am new to TensorFlow.  My objective is to upload a corpus and use the projector.tensorflow feature but, I don't even know where to begin.  I am sort of staring at the ""Loading text tutorial"", which I got to from the ""Word Embeddings"" tab in Google's Colab.  Any help would be appreciated, however, I am afraid I might be beyond help.  Thanks"
1492,NLP Sentiment analysis - basic guidelines,"['machine-learning', 'nlp', 'sentiment-analysis', 'word-embedding']","I am doing my first project in the NLP domain which is sentiment analysis of a dataset with ~250 tagged english data points/sentences. The dataset is reviews of a pharmaceutical product having positive, negative or neutral tags. I have worked with numeric data in supervised learning for 3 years but NLP is unchartered territory for me. So I want to know the best pre-processing techniques and the steps that I need to do that are best suited to my problem. A guideline from an NLP expert would be much appreciated!"
1493,How can I train an anomaly detection algorithm with dataset of system logs having numbers and texts?,"['python-3.x', 'machine-learning', 'nlp', 'word-embedding']","I am trying to work on an unsupervised anomaly detection algorithm where my dataset comprises of only normal log data. I have used the payload column from the log to identify patterns in the data using Drain algorithm. So now I have the following data available. For example, consider the following three identified patterns. The bold part represents the key with a value to be replaced in <*>. The italic part represents the value to be replaced with each <*>.instance: <> <> limit not specified defaulting to unlimited, ['34ae441714e744fab7a7a154811ee7bc memory']<> GET <> HTTP/<>.<> status: <> len: <> time: <>.<>, ""['172.17.0.2', '/v2.1/f09ddef028834df19337502ece1490c5/servers/34ae441714e744fab7a7a154811ee7bc', '1.1', '200', '1783', '0.1045711']""instance: <> Attempting claim on node <> memory <> MB disk <> GB vcpus <> CPU,""['dc3b53790ca14b86abaa003c226412dd', 'wally117:', '4096', '40', '2']""*There are many more such identified key-value pairs. Also, the patterns are repeated and have different values too.To start with, I have used CountVectorizer to encode the values from the payload column. The shape for ngram(1,1) is close to 40000, 21000. I am now unsure about my next step towards anomaly detection having just one type of data, i.e. normal. No anomalous data is present in the dataset.This problem also happens to be a time-series related to anomaly detection.It would be really helpful if someone can guide me on the approach that I should take or some relevant posts.Thanks!!!"
1494,[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.I'm using the function Word2Vec from the python library gensim.My problem is that I can't run my model on every word of my corpus as long as I set the parameter min_count greater than one. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an error saying word 'blabla' is not in the vocabulary, whereas this is exactly what I want ( I want this word to be out of the vocabulary).I can imagine this is not very clear, then find below a reproducible example:You can find Google's model there for example, but feel free to use any model or just do without, this is not the point of my post.As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter min_count to 1, everything works fine.I hope it is clear enough.Thanks."
1495,Subword vectors to a word vector tokenized by Sentencepiece,"['nlp', 'word-embedding']","There are some embedding models that have used the Sentencepiece model for tokenization. So they give subword vectors for unknown words that are not in the vocabulary. But I want to get word vector for each word like Word2vec, fastText.
Should I average subword vectors to represent a word vector? "
1496,Issues while loading a trained fasttext model using gensim,"['python', 'python-3.x', 'gensim', 'word-embedding', 'fasttext']","I am trying to load a trained fasttext model using gensim. The model has been trained on some data. Earlier, I have used model.save() with a extension of .bin to use it later. After the training process and saving the model using model.save in .bin format, generates 3 files respectively. They are:1) .bin  2) bin.trainable vectors_ngrams_lockf3) bin.wv.vectors_ngrams Now I am unable to load the trained binary file (.bin).  But I don't understand why I am getting a error named:raise NotImplementedError(""Supervised fastText models are not supported"")
  NotImplementedError: Supervised fastText models are not supportedAfter going through many blogs, peoples have suggested that gensim does not supports supervised training. It's fine. My question is how can I be able to load the trained binary model. Shall I need to train the model differently.Any help is appreciated.What I have tried after the training process: "
1497,General usefulness of Dense layers for different identification tasks,"['tensorflow', 'keras', 'conv-neural-network', 'cluster-analysis', 'word-embedding']","I'd like to ask, is it practical to use embeddings and similarity metrics to any form of identification task? If I had a neural network trained to find different objects in a photo, would extracting the fully-connected layers/Dense layers and clustering them be useful?I've recently found that there is an embeddings projector tool from tensorflow that is very cool and useful. I know that there has been some work in word embeddings and how similar words cluster together. This is the case for faces as well.Having said that, I want to follow the same methods into analyzing geological sites; can I train a model to create embeddings of the features of a site and use clustering methods to classify?"
1498,Keras - Concatenating embeddings of different dimensions,"['python', 'keras', 'concatenation', 'word-embedding']","I am trying to create an autoencoder in Keras. So far this is my code for the encoder which is based on the code2vec architecture:Now my input is a representation of source code based on the code2vec as well, but different in a way. The source code is represented as a context_path, which is a source_node connected by a path to a target_node. Now in simple terms, both source and target_node are just strings of two words each, e.g. '1 variable', '37 function'. This is then encoded using keras one_hot encoding function, so it is an int array of length 2. Now the source and target_node are connected by a path which is exactly the same, but has a length of 27. Now each source code file is represented by 500 such triplets --> context_paths. I want to create an embedding for each of the triplets and then combine them to a single vector using the concatenate layer. I am not sure if I should try to prepocess my data in a different way or if there is some clever way to workaround this problem. My question is given what I described, should I look for an alternative or is there a general solution where the dimensions of input to the concatenate layer do not match?"
1499,How does window affect accuracy of skip-gram?,"['neural-network', 'word2vec', 'word-embedding']",I would like to know how the window_size of skip-gram model affect the accuracy of predicting similar words in word embedding. Under what cases can accuracy drop and rise.Thanks. 
1500,Freeze only some lines of a torch.nn.Embedding object,"['python', 'pytorch', 'gradient-descent', 'embedding', 'word-embedding']","I am quite a newbie to Pytorch, and I am trying to implement a sort of ""post-training"" procedure on embeddings. I have a vocabulary with a set of items, and I have learned one vector for each of them. 
I keep the learned vectors in a nn.Embedding object. 
What I'd like to do now is to add a new item to the vocabulary without updating the already learned vectors. The embedding for the new item would be initialized randomly, and then trained while keeping all the other embeddings frozen.I know that in order to prevent a nn.Embedding to be trained, I need to set to False its requires_grad variable. I have also found this other question that is similar to mine. The best answer proposes to either store the frozen vectors and the vector to train in different nn.Embedding objects, the former with requires_grad = False and the latter with requires_grad = Trueor store the frozen vectors and the new one in the same nn.Embedding object, computing the gradient on all vectors, but descending it is only on the dimensions of the vector of of the new item. This, however, leads to a relevant degradation in performances (which I want to avoid, of course).My problem is that I really need to store the vector for the new item in the same nn.Embedding object as the frozen vectors of the old items. The reason for this constraint is the following: when building my loss function with the embeddings of the items (old and new), I need to lookup the vectors based on the ids of the items, and for performances reasons I need to use Python slicing. In other words, given a list of item ids item_ids, I need to do something like vecs = embedding[item_ids]. If I used two different nn.Embedding items for the old items and the and new one I would need to use an explicit for-loop with if-else conditions, which would lead to worse performances.  Is there any way I can do this?"
1501,How to train bert (or similar models) to learn embeddings for a words from a specific domain?,"['nlp', 'word-embedding']","I have a dataset of posts from a website about specific domain. I want to learn embeddings for the words in that dataset. But, as far as I understand, fine-tuning of BERT is for fine tuning pre-trained embeddings for a specific task like classification. Is there a way to learn the embeddings of the new words in my dataset using models like BERT (either training from scratch or fine tuning)?"
1502,Building Logistic Regression with Keras,"['python-3.x', 'tensorflow', 'keras', 'deep-learning', 'word-embedding']","I am trying to create a word embedding, but when I fit the data to the model (which is just a logistic regression model with an embedding layer) I get the following errorError when checking target: expected dense_29 to have 3 dimensions, but got array with shape (59568180, 1)I don't understand why the last densor unit is expecting 3 dimensions in output when I just want to predict the probability.Input array X is of shape (59568180, 1, 2) whilst the output Y is of shape (59568180,)Here is my code"
1503,How to give Word2Vec vector to classifier in python?,"['python', 'classification', 'word2vec', 'word-embedding']","my code in python is for multi-label classifying; using TF-IDF vectorizer for a bunch of tweets. i just put the corresponding part of the code below. my vocab is a 14182 words lexicon and the train_array.shape is (6838,14182). also the train_labels.shape is (6838, 11):the code works well. now i want to use Word2Vec as vectorizer. i changed the code to:then i get this error:then i find out that train_array isn't a array. i mean i find out that for getting trained vectors of Word2Vec you should use vector_maker.wv.vectors. but first i tried this to see the vectors dimension:but i get (30, 50). shouldn't i get (6838,50)? or what? actually i don't know so much about how Word2Vec works. i read many but didn't get so much. 
can you guys tell me what should i do to use created vectors for classifying?"
1504,How to convert the text into vector using word2vec embedding?,"['python-3.x', 'machine-learning', 'nlp', 'word2vec', 'word-embedding']",Suppose I have a dataframe shown below:|Text|Storm in RI worse than last hurricane|Green Line derailment in Chicago|MEG issues Hazardous Weather Outlook I created word2vec model using below code:now how I will convert the text present in the 'Text' column to vectors using this word2vec model?
1505,Generating synonyms or similar words using BERT word embeddings,"['nlp', 'word-embedding']","I want to generate synonyms or similar words using BERT words embeddings. 
I started to do this using BERT.
For later software integration, it has to be done in JAVA, so I went for easy-bert
(https://github.com/robrua/easy-bert). It appears I can get word embeddings this way:Do you know how I could get similars words from these word embeddings ?Thanks for your help !"
1506,Properly Inverse_transform categorial labels that were used to create Embedding layer (entity encoding),"['python', 'keras', 'word-embedding']","I'm using the following code in order to create a vector representation for a categorical variable (entity encoding):My main concern is that the recovery using inverse_transform of the LabelEncoder categories
did not preserve the right order, and each original_cat is mistakenly associated with a different vector.How can I know that the process of embedding following by inverse_transform did preserve the order of the categories?"
1507,"Invalid argument: indices[0,0] = -4 is not in [0, 40405)","['keras', 'word-embedding', 'implementation']","I have a model that was kinda working on some data. I've added in some tokenized word data in the dataset (somewhat truncated for brevity):Now, I get this error:I think this must be coming from my comment_text column since that is the only thing I added.Here is what comment_text looks like before I make the substitution:
And here is after:
My full code (before I made the change) is here:
https://colab.research.google.com/drive/1y8Lhxa_DROZg0at3VR98fi5WCcunUhyc#scrollTo=hpEoqR4ne9TO"
1508,Starspace: What is the interpretation of the labelDoc fileFormat?,"['facebook', 'nlp', 'word-embedding']","The starspace documentation is unclear on the parameter 'fileFormat' which takes the value 'labelDoc' or 'fastText'.
I would like to understand intuitively what material difference setting this paramter would have.Currently, my best guess is that if you set fileFormat to 'fastText' then all tokens in the training file that do not have the prefix '__label__' will be broken down into character-level n-grams as in fastText.
Alternatively, if you set fileFormat to 'labelDoc' then starspace will assume that all tokens are actually labels, and you do not need to prepend '__label__' to the tokens, because they will be recognized as labels anyway.Is my thinking correct?"
1509,How to get and visualize the original dictionary of a pre-trained ELMO model?,"['word-embedding', 'pre-trained-model', 'elmo']","I have a few questions regarding the ELMO pre-trained model which I found here:
https://tfhub.dev/google/elmo/3First, some background info to what I want to achieve. I have a corpus in which some misspellings exist. 
As far as I understood, ELMO can cope with misspellings (so, it can actually map a misspelled word to the ""original/correct"" one in the space using the context). For this reason, I want to take a pre-trained model and just map some words from my noisy corpus to the nearest words which have appeared in the corpus the model was trained on. I would also like to visualize the whole (for instance using PCA). 
So, to my questions:
1) can I somehow see the dictionary of the pretrained model?
2) If there is no dcitionary, how could I achieve the mapping?
3) without a dictionary, what steps should I take to achieve the mapping?Thank you!"
1510,Keras Embedding Layer-should the embedding matrix consist of vocabulary from the entire set or just the train data?,"['keras', 'neural-network', 'text-classification', 'word-embedding', 'transfer-learning']","While using pretrained W2V embeddings in an embedding layer in a Keras sequential model, should my embedding matrix consist of vocabulary from the entire set or jus the train data?I am using a Keras sequential model to do a binary classification on text data. My train data has unique vocab of about 6000 words and the test data has about 2000 unique words. When I use the entire 8000 words in the embedding matrix and train the network, I get good prediction scores whereas using only the 6000 odd vocab from the train data makes the model very bad. What am I doing wrong here? Do I need to add more data such that almost all the unique words in test must be covered in my training set?
I am currently using a sequential model with 1 embedding layer, 20 GRU and a sigmoid activation for the prediction. I am setting the trainable parameter to false in the embedding layer. "
1511,Memory crash Tensorflow Tokenizer texts_to_matrix,"['python', 'tensorflow', 'nlp', 'tokenize', 'word-embedding']",I'm working on word Embeddings for arabic dialect -like slang of a region- while preprocessing the data:what i want from step 7 is to make onehot encoding so i can feed it to the networkafter that i apply the processing on the json file nd do all the steps 
1512,No module named 'gensim' but already installed it,"['python', 'machine-learning', 'jupyter-notebook', 'gensim', 'word-embedding']","i'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command !pip install gensim before i import it, but it still can not be imported, and the error said ModuleNotFoundError: No module named 'gensim'Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention "
1513,Embedding Layer in Keras: Vocab Size +1,"['r', 'keras', 'tensorflow2.0', 'word-embedding']","From a number of examples I have seen, when we use text_tokenizer from keras, when specifying the input size for the input layer, we use vocab size +1.  This naturally yields an embedding space with +1 'rows'.  For example, I fit a simple model to estimate the embedding vectors for a vocab of size 3 = I like turtles.  The embedding space has length 5 per word in our vocabulary.The embedding weights are:My question:  I assume that the first ""row"" in our matrix is the 0-based vector, such that rows 2, 3, and 4 would be associated with ""I"", ""like"", and ""turtles"" respectively.   Is this the case?  I want to ensure that I align my vocabulary properly, but I haven't been able to pin down any documentation to confirm this assumption."
1514,Which Starspace training mode to use for multi-level embeddings,"['embedding', 'word-embedding']","I am using the StarSpace embedding framework for the first time and am unclear on the ""modes"" that it provides for training and the differences between them.The options are:Let's say I have a dataset that looks like this:(In reality, my dataset is not a language data and looks nothing like this, but this example demonstrates the point well enough.)I would like to simultaneously create embeddings from scratch (not using pre-existing embeddings at any point) for each of the following:Even after reading the coumentation, it doesn't seem clear which 'mode' of starspace training I should be using.If anyone could help me understand how to interpret the modes to help select the appropriate one, that would be much appreciated.I would also like to know if there are conditions under which the embeddings generated using one of the modes above, would in some way be equivalent to the embeddings built using a different mode (ignoring the fact that the embeddings would be different because of the non-determinstic nature of the process.)Thank you"
1515,how to concatenate pre trained embedding layer and Input layer,"['python', 'tensorflow', 'keras', 'concatenation', 'word-embedding']","My idea was to create an input with 256 dimension and pass it to a dense layer.I got the following error.ValueError: Layer concatenate_10 was called with an input that isn't a symbolic tensor. Received type: . Full input: [, ]. All inputs to the layer should be tensors.Please help me how to do this."
1516,Why Fasttext train_unsupervised freezing after training for hours?,"['python-3.x', 'word-embedding', 'fasttext']","I have 0.9 Million text files in a folder ""raw_data"" each containing Bangla language text(Unicode characters). I am training a fasttext model with the following self explanatory codeIt trained for 2 days showing progress like as followsBut after 2 days of training it freezes without any error just showing that the intended file has 1 word. But it actually has more than 40 words. I have checked for RAM and disk space availability but it seems that's not the issue as I have enough space available both in RAM and memory. I don't understand if it's an issue with my code. "
1517,How to use VGG16 to do a text classification?,"['deep-learning', 'conv-neural-network', 'text-classification', 'word-embedding', 'vgg-net']",I was trying to do a text classification using VGG16 but  it ended up with a bad result. Is it possible way to do such thing?
1518,reshaping vectors into tensors for embedding layer in keras LSTM mini-batch training,"['python', 'tensorflow', 'keras', 'deep-learning', 'word-embedding']","I'm trying to train an LSTM topic model (many-to-one problem) on text using an embedding layer and mini-batch training in keras with tensorflow backend in Python. I am struggling with formatting my inputs and outputs in a way that is compatible with the embedding layer format. My input consists of batches of count-vectorized text tokens, post-padded to 50 cells. My output is a vector of iteger labels corresponding to one of 4 classes, likewise post-padded to 50 cells.This is an example input vector:And this is the corresponding output vector:As a whole, my inputs and outputs consist of a list of padded arrays each. Next, I initialize my model architecture as follows:My embedding layer has three parameters: (1) input dimension of 20000, corresponding to the size of my vocabulary, (2) output dimension of 100, which is the arbitrary dimension of the dense embedding, (3) inout length of 50, which is the maximum length of my post-padded vectors.To simplify the batching problem, I train my model in a for-loop, passing one batch at a time. So my input is now correctly specified to be in 2D as required by the embedding layer (as per an earlier error stating expected embedding_1_input to have 2 dimensions after I tried to reshape to 3D.When I try to fit the model, I get this error: This is really puzzling, because when I double-check the dimensions of my input they are indeed (50,)!Transposing a 1 dimensional vector does not make any difference, so I'm not sure how to proceed at this point. Any advice?I also noticed this post has a similar question, but so far no one has attempted to answer it. Thanks in advance!"
1519,Why aren't all bigrams created in gensim's `Phrases` tool?,"['python', 'nlp', 'gensim', 'n-gram', 'word-embedding']",I have created a bigram model using gensim and the try to get the bigram sentences but it's not picking all bigram sentences why?Can anyone explain how to get all bigrams.
1520,What does each element in an embedding mean?,"['python', 'word2vec', 'feature-extraction', 'embedding', 'word-embedding']","I've been working with facial embeddings but I think Word2Vec is a more common example.Each entry in that matrix is a number that came from some prediction program/algorithm, but what are they? Are they learned features?"
1521,Understanding usage of glove vectors,"['python', 'nlp', 'word2vec', 'word-embedding', 'glove']",I used the following code to using glove vectors for word embeddingsI understand this chunk of code is for using glove pretrained vectors for your word embeddings. But I am not sure of what is happening in each line. Why to convert glove to word2vec format ? What does KeyedVectors.load_word2vec_format does exactly ?
1522,How to make part of the embedding matrix trainable and rest part as not trainable in pytorch?,"['python', 'nlp', 'pytorch', 'word-embedding']","Codes are in Pytorch.I want part of my embedding matrix to be trainable and I want rest part to freeze weights as they are pretrained vectors.My src_weight_matrix is the pretrained embedding matrix of dimension : [vocab_size + 4 special_tokens] X [emb_dim] = [49996 + 4] X [300]first 49996 rows are for word in vocab and last 4 words in the vocabulary are special token i.e. [UNK], [PAD], [START], [STOP]. I have randomly initialized the embedding vectors for these 4 words.So I want to train these 4 embedding weights and let other embedding have their own weights.The code is as follow where all embedding weights are frozen except last 4 which is correct or not I don't know.Any lead is greatly appreciated. Thanks in advance"
1523,What is the best approach for comparing two texts after a previous NER step using deep learning?,"['tensorflow', 'machine-learning', 'nlp', 'word-embedding', 'recommender-systems']","What I am trying to do is a resume-job description match that gives a score to a given resume for a given job description. I have developed a NER model that extract the most important entities from resumes and job description (e.g. years of experience, university degree, programming languages etc). How can I compare the features extracted from the resume with the features extracted from the job description in order to get a score that describe how well the resume fit a given job description? I was thinking of something like word embedding ELMO, BERT etc in order to get a vectorized representation of my text but I don't know if this approach is valid even with a previous NER step. "
1524,How to load a pretrained word embedding in Keras (not a dictionary),"['keras', 'word-embedding']","I would like to load a pretrained model to perform word embedding in Keras (as an intermediate step before further processing). However the pretrained models I found, such as Word2Vec or GloVe, are all available only as dictionaries of word-vector pairs.Is there a reason why the models themselves are not available? Is it less efficient than a dictionary? If not so, where could one find pretrained models?"
1525,Is there any way to represent text in a 2d 3 channel array?,"['conv-neural-network', 'word-embedding']",I am trying to use VGG16 to do spam emails detection.I was wondering if there is a good way to treat text like a rgb image.
1526,Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?,"['deep-learning', 'gensim', 'word2vec', 'one-hot-encoding', 'word-embedding']","I need to combine Word2Vec with my CNN model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (a.k.a. target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of Word2Vec as I need both of them in order to train my deep neural network?p.s. I'm using Gensim implementation of Word2Vec.p.s. My corpus has 6,925 sentences, and Word2Vec produces 5,260 vectors.Edit: More detail regarding my corpus (as requested):The structure of the corpus is as follows:sentences (label: positive) -- A Python listsentences (label: negative) -- A Python listThen all the sentences were given as the input to Word2Vec.I'll feed my CNN with the extracted features (which is the vocabulary in this case) and the targets of sentences. So, I need these labels of the sentences as well."
1527,Invalid argument error during keras training,"['python', 'keras', 'neural-network', 'word-embedding']","Invalid argument error during word2vec training in keras 
though vocab size is index+1Please see below network architecture summary:this is the part of the code:Getting following errorDelete the underlying status object from memory otherwise it stays
  alive  as there is a reference to status from this from the traceback
  due to InvalidArgumentError: indices[7,0] = 3795 is not in [0, 3795)"
1528,How to input sentence embedding as a feature in Neural Networks?,"['python', 'pandas', 'keras', 'neural-network', 'word-embedding']","After preprocessing, I have the pandas dataframe like in the screenshot attached. I want to train a Keras neural network model to classify the vectors into intents. The vectors here are LASER Embeddings of sentences, and are in numpy.ndarray format. While trying to input these features into the model, I get errors regarding shape of the data. I think I have to work with either the Embedding Layer, or the input_shape or create an embedding matrix before implementing the model. Since I do not have much knowledge in Keras Neural Networks I find it difficult to understand what exactly I have to do here. Can anyone show me how I can shape this data and use to train a Keras Sequential model like in here..gives the belowI tried the below and the error was - ValueError: Error when checking input: expected embedding_1_input to have shape (50,) but got array with shape (1,)"
1529,Is there any solution for semantic similarity score for organization names?,"['python', 'word-embedding', 'sentence-similarity']","currently we use Multi-lingual Universal Sentence Encoder (MUSE) for similarity score. We have a chatbot system where we can define intents with a list of samples of each intent. Then we compare semantic similarity of user's utterance with each of intent samples to decide if that utterance belongs to any intent.But I don't know how we can deal with organization names (or any name entities), for example if we have intent ask_info, we want sentences like ""Tell me info about Google"" or ""Tell me info about Samsung"" belong to that intent. But if we only put 1 sentence as sample, such as ""Tell me info about Google"", if user types ""Tell me about XXX"", the similarity score from MUSE is very low, only around 0.4 ~ 0.5 which cannot pass our threshold. So anyone has idea to deal with this situation? Thank u very much."
1530,where can i download a pretrained word2vec map?,"['python', 'nlp', 'word2vec', 'word-embedding']","I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. "
1531,Cost/Loss function when using word embedding matrix,"['deep-learning', 'nlp', 'recurrent-neural-network', 'word-embedding']","Would appreciate some help if someone can clarify this point.when building a language model using RNN, if I were to use word embedding matrix in my model. Is my Y_predicted supposed to compare with one hot vector of actual Y output or the word embedding vector for my actual Y output for computation of loss/cost function?"
1532,Why I get a very low accuracy with LSTM and pretrained word2vec?,"['python', 'deep-learning', 'lstm', 'tf.keras', 'word-embedding']",I'm working on a reviews classification model with only two categories 0 (negative) and 1 (positive). I'm using pre-trained word2vec from google with LSTM. The problem is I get an accuracy of around 50% where it should be around 83% according to this paper. I tried many different hyperparameters combination and still gets a horrible accuracy. I also tried to change the data preprocessing techniques and tried stemming but it hasn't resolved the problemhere's my codeI also tried other optimizers like AdaMax and MSLE loss function and no matter how much I increase the epoch or change the batch size the accuracy never gets better. I'm just so confused if the problem isn't with the model and preprocessing where could it be? Thanks
1533,unsupervised binary classify text using deep learning,"['deep-learning', 'text-classification', 'word-embedding', 'unsupervised-learning']","So I am working on a project and I'm kind of confused on how to analyse the problem.The task is as follow:I have two text documents from two different periods of time. both documents has target words(like 4 specific words) my task is to see if each of these 4 words has changed its meaning or use over time-either has got a new use or has lost a meaning or use-for example in the old document the target word 'cell' for example had two meanings either biological cell or a cell chamber and no other uses for cell other than those two, while in the second recent text document the word cell has got a new meaning or use that is cellphone , and by this I would say the 'cell' word has changed meaning over time. on the other hand if the target word's use has remained the same over time I would classify it as unchanged.So, all I have now are two text documents, 4 target words and I need to use deep neural network binary classify those target words to either changed (1) or not changed(0).
I am a total newbie to deep learning, and think I can do it with a regular python code that would work like this:1- spot the target word in the document2- collect all the adjacent words to that word in an array or any other data structure3- repeat that for the second document4-compare the two arrays for each word and see if they are different and based on that I would5- classify the word as changed or not changedSo my question is how would Deep learning make a positive contribution here, am I getting the whole idea wrongly? is it not that easy to classify them upon change on adjacent words?I would appreciate a light guiding me through this road.till this point I have learned about tokenization and embedding layer in Keras, and how they are important to transform my text into numbers so the algorithms can work with it. but what is next? how to do the classification thing?
I would say I can tokenize the text to words and then give a label to each distinct word in the document as 0 initially and then input the second document and update the label based on the word pairing in the second document but it feels like immature idea.what do you think?"
1534,Keras word embedding matrix has first row of zeros,"['keras', 'word-embedding', 'glove']","I am looking at the Keras Glove word embedding example and it is not clear why the first row of the embedding matrix is populated with zeros.First, the embedding index is created where words are associated with arrays.Then the embedding matrix is created by looking at words from the index created by tokenizer. Since the loop will start with i=1, then the first row will contain only zeros and random numbers if the matrix is initialized differently. Is there a reason for skipping the first row?"
1535,"People name embedding from name, commas and spaces to keys","['regex', 'string', 'word-embedding']","I'm trying to figure out a good algorithm for embedding name as such.
space = 0, word = 1, comma = 2, double quotations = 3  So ""Bob Dylan"" should embed as ""101""
While ""Brown, Millie Bobby"" should embed as ""120101""
and ""Dwayne ""The Rock"" Johnson"" should embed as ""103101301"""
1536,"Extract Keras concatenated layer of 3 embedding layers, but it's an empty list","['python', 'tensorflow', 'keras', 'nlp', 'word-embedding']","I am constructing a Keras Classification model with Multiple Inputs (3 actually) to predict one single output. Specifically, my 3 inputs are:Output:Python Code (create the multiple input keras)Model's StructureMy problem:After successfully fitting and training the model on some training data, I would like to extract the embeddings of this model for later use. My main approach before using a multiple input keras model, was to train 3 different keras models and extract 3 different embedding layers of shape 100. Now that I have the multiple input keras model, I want to extract the concatenated embedding layer with output shape (None, 300).Although, when I try to use this python command:or I get either an empty list (1st code sample) either an IndenError: list index out of range (2nd code sample).Thank you in advance for any advice or help on this matter. Feel free to ask on the comments any additional information that I may have missed, to make this question more complete.Note: Python code and model's structure have been also presented to this previously answered question"
1537,"How to deciding number of units in the Embedding, LSTM, layers in the deep learning","['machine-learning', 'keras', 'deep-learning', 'lstm', 'word-embedding']","So this is the part of the code of a deep learning model which I am building using keras function API.
Can You Please tell me how to decide Number of Units in each layer.How to decide the values of no_of_output_dim in the Embedding layer and no_of_lstm_units given that all other values which are hard coded are constant."
1538,Universal sentence encoder for big document similarity,"['machine-learning', 'nlp', 'cosine-similarity', 'word-embedding']","I need to create a 'search engine' experience : from a short query (few words), I need to find the relevant documents in a corpus of thousands documents.After analyzing few approaches, I got very good results with the Universal Sentence Encoder from Google.
The problem is that my documents can be very long. For these very long texts it looks like the performance are decreasing so my idea was to cut the text in sentences/paragraph.So I ended up with getting a list of vectors for each document (representing each part of the document).My question is : is there a state-of-the-art algorithm/methodology to compute a scoring from a list of vector ? I don't really want to merge them into one as it would create the same effect than before (the relevant part would be diluted in the document). Any scoring algorithms to sum up the multiple cosine similarities between the query and the different parts of the text ?important information : I can have short and long text. So I can have 1 up to 10 vectors for a document."
1539,Evaluating word2vec using SimLex-999 and wordsim - key error,"['nlp', 'word2vec', 'word-embedding', 'evaluate']","I have a strange problem while trying to evaluate my Word2Vec model in Italian. 
I'm using this data http://www.leviants.com/ira.leviant/MultilingualVSMdata.htmlI'm getting this error `Now the strange part is that the word ""di"" is neither in my vocabulary nor in my txt file. What is wrong here? I thought it was a separator problem, tried tsv, tried to give the argument =""\t"", nothing changed. Any ideas to solve this? "
1540,AttributeError: 'Word2Vec' object has no attribute 'endswith',"['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'word-embedding']","When I run my .py file containing the following code The following error is generatedTraceback (most recent call last):   File ""assignment.py"", line 35, in
  
      model11 = gensim.models.keyedvectors.KeyedVectors.load(model1)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 1540, in load
      model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 228, in load
      return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 424, in load
      compress, subname = SaveLoad._adapt_by_suffix(fname)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 513, in _adapt_by_suffix
      compress, suffix = (True, 'npz') if fname.endswith('.gz') or fname.endswith('.bz2') else (False, 'npy') AttributeError: 'Word2Vec'
  object has no attribute 'endswith'"
1541,BERT sentence embeddings: how to obtain sentence embeddings vector,"['keras', 'nlp', 'embedding', 'word-embedding', 'sentence']","I'm using the module bert-for-tf2 in order to wrap BERT model as Keras layer in Tensorflow 2.0 I've followed your guide for implementing BERT model as Keras layer.
I'm trying to extract embeddings from a sentence; in my case, the sentence is ""Hello""I have a question about the output of the model prediction; I've written this model:Then I want to extract the embeddings for the sentence written above: the object predict contains 4 arrays of 768 elements each:I know that each array of that 4 represents my original sentence, but I want to obtain one array as the embeddings of my original sentence.
So, my question is: How can I obtain the embeddings for a sentence?In BERT source code I read this:For classification tasks, the first vector (corresponding to [CLS]) is used as the ""sentence vector."" Note that this only makes sense because the entire model is fine-tuned.So I have to take the first array from the prediction output since it represents my sentence vector?Thank you for your support"
1542,Best Python GloVe word embedding package,"['python-3.x', 'word-embedding', 'glove']","What is the best Python GloVe word embedding package that I can use? I want a package that can help modify the co-occurrence matrix weights. If someone can provide an example, I would really appreciate that.Thanks,
Mohammed"
1543,"Word2Vec - How to rid of “TypeError: unhashable type: 'list'” and “AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found”?","['gensim', 'word2vec', 'word-embedding']","Getting TypeError: unhashable type: 'list' and AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found errors when I create my model based on Word2Vec implementation of the gensim module.Each entry has three parts which are presented within a list. And, the model contains three entries for the sake of demonstration.Here is what I have tried:The value of the features is: Edit: The error stack trace after correcting the form of the feature vector according to the comment of @gojomo."
1544,Can anybody explain what a t-SNE visualization or graph on a word2vec model signifies?,"['data-visualization', 'data-science', 'word2vec', 'word-embedding']",Can we take the stand that the words around the target word have an impact on it in a positive or negative way or is there any other kind of interpretation.A t-SNE graph is given at the end of the page in the link provided for reference. Thank you.https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial?source=post_page-----a38bf1906483----------------------#Getting-Started 
1545,Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,"['python', 'nlp', 'gensim', 'word-embedding', 'fasttext']","I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:https://fasttext.cc/docs/en/crawl-vectors.htmlIn particular, I would like to load the following word embeddings: Gensim offers the following two options for loading fasttext files:gensim.models.fasttext.load_facebook_model(path, encoding='utf-8') gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')Source Gensim documentation: 
https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_modelSince my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).Is there an option to load these large models from disk more memory efficient?"
1546,Text preprocessing in python: the case of scientific papers,"['nlp', 'nltk', 'data-cleaning', 'word-embedding']","I'm trying to preprocess some scientific papers to apply word embedding. Of course, I do not need editorial information, the number of pages nor bibliography. Is there any easy way to clean that information?
Thanks!"
1547,News category classifier using pytorch and word embedding,"['text', 'classification', 'pytorch', 'lstm', 'word-embedding']","I am working on text classification using pytorch and word embeddingI am trying to develop news category classifier using LSTM from pre-trained word-embedding data.My question would be if I trained 5000 corpus and predict some news article with new words that aren't included in train data set, Do I need to train data again with new words?If I embed words, Do I need to keep this embedded word inforamtion? ( what words have been embedded that I call it for testing ) or there is a different way to do itOr there is an elegant way to achieve it?Thank you in advance"
1548,Natural Language Processing techniques for understanding contextual words,"['machine-learning', 'deep-learning', 'nlp', 'word-embedding', 'linguistics']","Take the following sentence:The meaning of change means replace, as in someone is going to replace the light bulb. This could easily be solved by using a dictionary api or something similar. However, the following sentencesThe first sentence does not mean replace anymore, it means Exchangeand the second sentence, change means adjust.If you were trying to understand the meaning of change in this situation, what techniques would someone use to extract the correct definition based off of the context of the sentence? What is what I'm trying to do called?Keep in mind, the input would only be one sentence. So something like:Is not what I'm trying to solve, because you can use the previous sentence to set the context. Also this would be for lots of different words, not just the word change.Appreciate the suggestions.Edit: I'm aware that various embedding models can help gain insight on this problem. If this is your answer, how do you interpret the word embedding that is returned? These arrays can be upwards of 500+ in length which isn't practical to interpret. "
1549,Should .lower() be applied in Word Embeddings (in particular German)?,"['nlp', 'spacy', 'word-embedding', 'fasttext']","I noticed that in pretrained embeddings such as fastText and spaCy words vectors are different depending on whether I leave the first letter capitalized or not. Does this mean the capitalization is considered in these embeddings or is it still best to .lower() everything before processing? In particular, I am interested in models such as the German model where capitalization plays a further role unlike in English were mostly all not NER words are not capitalized."
1550,"Given a sentence in english with a blank space, how can I estmate in python the probability of a specific word to fit into that blank space?","['python', 'nlp', 'data-science', 'word-embedding']","Let's say we have a sentence in python3 with a blank space like the following:sentence = ""Tomorrow I want to go _______.""and we want to find out which word is more likely to appear in the blank space from a pool of words:pool_of_words = ['eating', 'playing', 'thinking', 'jogging']Question: How can I estimate the probability of each word from the pool to appear in that blank space?Example:
I guess the solution could be in the shape of a probability function that would give something like the examples below. How can I build such a function? probability('jogging') = 0.98probability('eating') = 0.81probability('thinking') = 0.2Thanks a lot in advance."
1551,Source of Spacy's pretrained word embeddings,"['spacy', 'word-embedding']","Spacy comes with the pretrained word embedding vectors. Does anyone know where they are from? Are they the the pre-trained Glove or Word2vec vectors which are publicly available? Or, did the Spacy team train on their own and release it? I tried to find the information but couldn't. "
1552,Is it possible to have a context tensor in an encoder-decoder model with attention mechanism?,"['word-embedding', 'attention-model', 'encoder-decoder']","I have read here about the encoder-decoder model with the attention mechanism. Based on the link, the size of the context mechanism is a 2D matrix(# of target words * # of source words). I am wondering is it possible to have a 3D tensor context array? I want to vectorize the cells of a column, each of which has multiple tokens. How should I use attention?"
1553,ValueError:Layer conv1d was called with an input that isn't a symbolic tensor.All inputs to the layer should be tensors,"['keras', 'conv-neural-network', 'sequential', 'word-embedding', 'tf.keras']",I built this model and it was working fine. I recently reran it and I'm getting an error I have checked similar post here but none is very similar to mine. I have tried their suggestions like adding the axis to Concatenate() or using concatenate instead but nothing changed. 
1554,Vec2Word neural network scheme,"['matlab', 'word-embedding']","Could someone describe the probable deep network scheme for the function vec2word in Matlab? I'm just wondering which type of neural net should be used in order to convert a vector to a word? Or in other words, reverse word embedding. Any help highly appreciated."
1555,NLP Transformers: Best way to get a fixed sentence embedding-vector shape?,"['machine-learning', 'deep-learning', 'nlp', 'pytorch', 'word-embedding']","I'm loading a language model from torch hub (CamemBERT a French RoBERTa-based model) and using it do embed some french sentences:  Imagine now in order to do some semantic search, I want to calculate the cosine distance between the vectors (tensors in our case) u and v : I'm asking what is the best method to use in order to always get the same embedding shape for a sentence regardless the count of its tokens?=> The first solution I'm thinking of is calculating the mean on axis=1 (embedding of a sentence is the mean embedding its tokens) since axis=0 and axis=2 have always the same size:But, I'm afraid that I'm hurting the embedding of the sentence when calculating the mean since it gives the same weight for each token (maybe multiplying by TF-IDF?).=> The second solution is to pad shorter sentences out. That means:  What are your thoughts?
What other techniques would you use and why?Thanks in advance!"
1556,Can not train from text file in fasttext. Getting ValueError: Empty vocabulary,"['python-3.x', 'word-embedding', 'fasttext']","I am trying to create a fasttext word embedding using the following code.here train.txt contains a single line - ""Have a nice day.""After running the code, I get the following error:I don't understand the reason of it and how to fix it. Additional information:
I am running- 
Ubuntu 18.04,
Python 3.6.8,
fasttext 0.9.1"
1557,How to remove duplicate values in tensor in Tensorflow?,"['python', 'tensorflow', 'duplicates', 'tensor', 'word-embedding']","I'm dealing with word embedding recently, and here is the problem I face right now.
I will have duplicated value in tensor. Suppose I have the following 2D tensor in shape(?, 5)inputexpected outputI do some survey in the community. I still cannot address this problem specificly. Is there any method I can solve the duplicated issue in tensorflow?"
1558,Test whether a Set of Vectors are in normal distribution,"['word-embedding', 'python-embedding', 'multivariate-testing']",I am working with word embeddings. I can gather all the embeddings i.e. a vector of the same dimension for the corresponding words. I want to check whether these vectors are following a normal distribution. How can I implement it with python?
1559,Gensim most_similar method coefficients are very low,"['nlp', 'gensim', 'word2vec', 'word-embedding']","I trained word embedding word2vec model using gensim and then used most_similar method to find the most associated words.The result is below:I wonder why the coefficient is very low, even the top word is less than 0.25.Thank you!"
1560,Keras: Eager execution of tf.constant with unsupported shape,"['python', 'tensorflow', 'keras', 'word-embedding']","I am trying to run the following code in google compute engine:I get the following error at the last statement:TypeError: Eager execution of tf.constant with unsupported shape (value has 1441550 elements, shape is (31815, 50) with 1590750 elements). However, I can run the code without any problem in Colab.
How can I fix this problem?"
1561,Fastest way to retrieve word vectors of a sequence and fed into model?,"['python', 'dictionary', 'nlp', 'pytorch', 'word-embedding']","For training, I have to feed the model sequence of word vector. Each sequence has on average 40 words. So, if I use a dictionary of pre-trained word embedding (like Glove), For each sequence have to hit the embedding dictionary around 40 times and for each batch, it will be around batch_size*40 times. The dataset is divided into many batches and the whole dataset has to be iterate (epoch) several times also. So, you can imagine how many times the dictionary will get hit.
This is the approach I have done already and it is taking really a lot of time. To solve this, I tried to make a dictionary of sequence to vector. This dictionary should contain a sequence as key and a 2d python list (each row is a word vector) as a value of the key. The hope is to, I just have to look for the sequence and get the values. This should decrease the time a lot but the dictionary would be very big (I estimated the size by saved the data (sequence->vectors) in mongodb and exported it and the file is 23gb). A dictionary of size 23gb should not be problem because my I am using shared server where I can allocate as much as 100gb memory. But the program gets killed while loading the dictionary. So this is not working. Another approach I am thinking about is to copy the word embedding vector into pytorch's nn.Embedding().Here the numbers are indices of the word. Regarding this approach,pytorch embedding uses numpy matrix as lookup table. So, my concern is, isn’t for executing the previous code, there will be 7 hits on the numpy matrix? Or it will be retrieved parallelly? Even it runs parallelly, there should be another dictionary to convert word to indices. That also needs 7 hits on the word2indices dictionary.So, what do you think, what is the fastest and efficient way to retrieve word vectors of a sequence and fed into model?"
1562,"In Fasttext skipgram training, what will happen if some sentences in the corpus have just one word?","['neural-network', 'word2vec', 'word-embedding', 'fasttext']","Imagine that you have a corpus in which some lines have just one word, so there is no context around some of the words. In this situation how does Fasttext perform to provide embeddings for these single words? Note that the frequency of some of these words are one and there is no cut-off to get rid of them. "
1563,Is there any way to understand the output features of the word2vec?,"['machine-learning', 'word2vec', 'word-embedding']","I want to understand what each dimension means in the output of word2vec.For example if I make a decision tree with one hot encoded variables, I can tell exactly which category in a categorical variable is responsible for the split. If I use embeddings however, I can't explain the reason for these splits.I am aware of the famous example of Embedding(King) - Embedding(Man) + Embedding(Woman) = Embedding(Queen). From this example, we can say that the characteristic of ""royalty"" has been understood.I guess a way would be to cluster similar data points based on cosine similarity to get some context about what the output features are. For example, if I get the nearest 3 neighbors of a data point to be ""Kilometer"", ""Inch"" and ""Mile"". I could infer that the ""Length"" is the could be responsible for the split in a decision tree. However, I was wondering if there was a another way."
1564,using a Spanish pretrained model with Gensim causes raise KeyError(“word '%s' not in vocabulary” % word),"['python', 'deep-learning', 'gensim', 'word-embedding', 'keyerror']","I am struggling with the following problem:I downloaded a pre-trained word embedding model for Spanish (over 1 million words 300-dimensional word vectors for Spanish)
I loaded it successfully and I even managed to undertake a couple of experiments, such as most similar words and basic analogies in Spanish (A is to B as C is to what), but when I try the following:It raises the error:I already checked that the word is actually in the word embedding. I have also eliminated the possibility of an encoding error. Based on my research, this type of error is produced when the token/word is supposed to be wrapped in a list [], however I don’t know how to apply that to this specific problem. Besides, this block of code is the same code used in “Deep Learning Cookbook” in chapter 3 (Word2vecMath)This is the complete script:Thank you from your support"
1565,LASER embeddings,"['word-embedding', 'toolkit', 'python-embedding']","I am trying to use LASER toolkit by Facebook to embed Japanese text. The tool supports 100 languages but does not provide the code for the languages. For example I am trying to run./embed.sh jp_cards.txt LANGUAGE jp_card_embedding.rawand in the LANGUAGE variable I set jp or ja and I still get the following error: WARNING: No known abbreviations for language 'ja', attempting fall-back to English version...Any ideas on what the language code for Japanese is? "
1566,Question pairs (ground truth) datasets for Word2Vec model testing?,"['machine-learning', 'nlp', 'word2vec', 'word-embedding']",I'm looking for test datasets to optimize my Word2Vec model. I have found a good one from gensim:gensim/test/test_data/questions-words.txt Does anyone know other similar datasets?Thank you!
1567,How to use word embedding as features for CRF (sklearn-crfsuite) model training,"['python', 'word-embedding', 'crfsuite', 'python-crfsuite']","I want to develop an NER model where I want to use word-embedding features to train CRF model. Code perfectly working without word-embedding features but when I insert embedding as features for CRF training, got error messages. Here is the part of snippet of my code: When I want to train the CRF model I got this error messages:TypeError: only size-1 arrays can be converted to Python scalarsCan anyone suggest me how to use word embedding vectors to train CRF model ?"
1568,Can anyone please share example code to make sentence2vector (sen2vec) from Fast.ai ULMFIT model.?,"['deep-learning', 'word-embedding', 'transfer-learning', 'fast-ai', 'sentence-similarity']","I want to find sentences which semantically and contextual similar. Earlier i was using Fasttext, which is word2vec and doesn't give contextual vectors. So I want to change the model to state of the art models like ULMFIT or GPT2 . I prefer implementation through fast.ai library because they provide good methods for retraining for custom data. SO can anyone please share some example code to achive this?I used fasttext word2vec and re-trained it on custom data set and did cosine similarity to get similar sentences. Here instead of next word i want vector of input text."
1569,How to construct PPMI matrix from a text corpus?,"['python', 'nlp', 'word-embedding']","I am trying to use an SVD model for word embedding on the Brown corpus. For this, I want to first generate a word-word co-occurence matrix and then convert to PPMI matrix for the SVD matrix multiplication process.I have tried to create a co-occurence using SkLearn CountVectorizer But:(1) Am not sure how I can control the context window with this method? I want to experiment with various context sizes and see how the impact the process.(2) How do I then compute the PPMI properly assuming that 
PMI(a, b) = log p(a, b)/p(a)p(b)Any help on the thought process and implementation would be greatly appreciated!Thanks (-:"
1570,Transforming a Keras sequential API layer for the functional API,"['python', 'tensorflow', 'keras', 'embedding', 'word-embedding']","I am struggling with transforming a Tensorflow Keras layer made for the sequential API to use it in the functional API. I have already seen many errors today so I'll try to keep the description short and concise:Thanks for your time and help! :-)Best,
TobiasTested on Google Colab.
Keras Version: 2.2.5
Tensorflow Version: 1.15.0This is the Python code for the DynamicMetaEmbedding Keras layer:This is an example code to call the Keras layer later on in a model."
1571,fastText - Throws exception without any reasons,"['python-3.x', 'gensim', 'word-embedding', 'fasttext']","I'm using fastText implementation of the module gensim. Despite getting no reasons, my program throws an exception.Here is the code:The end of the output:"
1572,word2vec - KeyError: “word X not in vocabulary”,"['gensim', 'word2vec', 'word-embedding']","Using the Word2Vec implementation of the module gensim in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word happy is defined in the vocabulary, getting the error KeyError: ""word 'happy' not in vocabulary"". Tried to apply the given the answers to a similar question, but did not work. Hence, posted my own question.Here is the code:"
1573,What is “dont” and “isnt” in the pertained GloVe vector files (e.g. glove.6B.50d.txt)?,"['neural-network', 'nlp', 'word-embedding', 'glove']","I found these 2 words ""dont"" and ""isnt"" in the vector file glove.6B.50d.txt downloaded from https://nlp.stanford.edu/projects/glove/. I wonder if they were originally ""don't"" and ""isn't"". This will likely depend on the sentence_to_word parsing algorithms they used. If someone is familiar, please confirm if this is the case. A secondary question is if this is a common way to deal with apostrophe for words like ""don't"", ""isn't"", ""hasn't"" and so on. i.e. just filter replace that apostrophe with an empty string such that ""don"" and ""t"" becomes one word. Finally, I am also not sure if GloVe comes with API to do sentence_to_word parsing so you can be consistent with what the researchers have done originally."
1574,Finding both target and center word2vec matrices,"['nlp', 'word2vec', 'word-embedding']","I've read and heard(In the CS224 of Stanford) that the Word2Vec algorithm actually trains two matrices(that is, two sets of vectors.) These two are the U and the V set, one for words being a target and one for words being the context. The final output is the average of these two.
I have two questions in mind. one is that:  Why do we get an average of two vectors? Why it makes sense? Don't we lose some information?  The second question is, using pre-trained word2vec models, how can I get access to both matrices? Is there any downloadable word2vec with both sets of vectors? I don't have enough resources to train a new one.Thanks"
1575,How to save self-trained word2vec to a txt file with format like 'word2vec-google-news' or 'glove.6b.50d',"['machine-learning', 'nlp', 'word2vec', 'word-embedding']","I wonder that how can I save a self-trained word2vec to txt file with the format like 'word2vec-google-news' or 'glove.6b.50d' which has the tokens followed by matched vectors.I export my self-trained vectors to txt file which only has vectors but no tokens in the front of those vectors.
My code for training my own word2vec:"
1576,"Calculate Cross-Lingual Phrase Similarity (using e.g., MUSE and Gensim)","['python', 'nlp', 'multilingual', 'gensim', 'word-embedding']","I am new to NLP and Word Embeddings and still need to learn many concepts within these topics, so any pointers would be appreciated. This question is related to this and this, and I think there may have been developments since these questions had been asked. Facebook MUSE provides aligned, supervised word embeddings for 30 languages, and it can be used to calculate word similarity across different languages. As far as I understand, The embeddings provided by MUSE satisfy the requirement of coordinate space compatibilty. It seems that it is possible to load these embeddings into libraries such as Gensim, but I wonder: *e.g., ""ÖPNV"" in German vs ""Trasporto pubblico locale"" in Italian for the English term ""Public Transport"". I am open o any implementation (libraries/languages/embeddings) though I may need some time to learn this topic. Thank you in advance."
1577,Why are distances in text2vec's RWMD module between 1 and -1?,"['r', 'word-embedding', 'text2vec']","From what I understand, the dist2 RWMD feature of the great text2vec package calculates distances between matrixes as cosine distances. Wouldn't that mean 1 - (cosine similarity)? If cosine similarity runs between 0 and 1, then shouldn't that result in values between 0 and 1, too? I am not sure how to interpret negative distances in this case, and how are they different from positive distances. Thanks!"
1578,How to properly use BERT in keras for classification,"['keras', 'deep-learning', 'word-embedding']","I am having an issue with using BERT for classification of text within my database. Previously, I have used GLoVE and ELMo that work quite ok. Also Random forests give me quite good F1-scores (over 0.85), however, when using BERT, I am stuck around 0.55. I was trying to modify learning rate for Adam optimizer, used anything between 0.001 to 0.000001, but nothing really helps. This is my code: https://github.com/EuropeanSocialInnovationDatabase/ESID-main/blob/development/TextMining/Classifiers/DatabaseWithKickStarter/NNClassifierTest2.pyIf anyone can pin the problem down, I would be really grateful. "
1579,Input Shape Error Adding Embedding Layers to LSTM,"['python', 'keras', 'lstm', 'word-embedding']","I'm trying to add an embedding layer to my LSTM that predicts characters.I've tried adding an embedding layer in this format, However, keras throws this error I'm confused because there is no place in the embedding layer to set a variable for the number of examples in the dataset. And I'm not sure how to reshape this dataset to make it work with the embedding layer.Here is my full code."
1580,Wrong implementation of embeddings?,"['nlp', 'word-embedding']","I am new to NLP and studying from the book Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning. The associated github repo is here.I believe the Embedding implementation in this notebook has an issue, and would like to confirm if I am correct. The make_embedding_matrix function's helper doc says that it should be fed in a list of words in the dataset. However, for the embedding matrix to return the correct embedding of a word from pretrained embeddings, the word list should be fed in the same order as in the vocabulary. Furthermore, there should be no gaps in the word indices in the vocabulary. These are big assumptions.I think the correct way to construct the embedding matrix is to pass the vocab to the make_embedding_function, and use the token_to_idx method in the vocab to find which rows of the embedding matrix should be populated.Is this right? Let me know if there is a better reference."
1581,How to build a Validation Set to evaluate domain specific Word Embeddings?,"['word2vec', 'similarity', 'evaluation', 'word-embedding']","Problem Statement:I am building Word Embeddings for a specific domain and I would like to evaluate those embeddings with some test set for their goodness (Intrinsic Evaluation). I could see that most of the open source validation datasets are built to validate around the language models. Can someone please shed a light on the process of building a validation set which is similar to WS-353 or any other equivalent dataset that can be used for Word Embeddings evaluation for a specific domain.Many thanks in advance.My Analysis:When I analysed the WS-353 dataset I could see that most of the words present in the dataset are Nouns and in other datasets I could see Verbs, Nouns, Adverbs etc. Is there any concrete process around building such validation sets? What is the strategy followed to build such datasets?PS: I am aware that the some of these datasets are developed for a specific purpose, but if we just want to build a validation set which is similar to wordsim-353. Is it advised to follow the similar process that the creators of wordsim-353 followed?https://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/Publicly available datasets to validate Word Embeddings:Word Similarity datasetsWord Analogy DatasetsConcept Categorization DatasetsOutlier Detection DatasetsReferences:"
1582,Visualize the cosine similarity scores calculated using pretrained word embeddding in SpaCy,"['python-3.x', 'nlp', 'spacy', 'word-embedding']","I have used SpaCy's pretrained model 'en_core_web_lg' to find the cosine distance between a group of values and attributes. I wanted to visualize the relationship of how close a word is from the other word, very much similar to clustering.Here is the link to the table which contains similarity scores for each value vs attribute Here the columns are the attributes for which i am trying to find the similarity score, while the row are the values for which i am trying to find what attribute it is most likely to be classifiedThis is the output i am trying to achieve. Please take a look at it"
1583,Is there an equivalent of word2vec for images?,"['image-processing', 'deep-learning', 'word2vec', 'word-embedding']","I'm wondering if it would be possible to create dense vector representations for an image, similar to how you might create a word embedding with an algorithm like Word2Vec?I understand that there are some big differences between text and image data – specifically the fact that word2vec uses the word's context to train – but I'm hoping to find a similar counterpart for images.If a simplistic example of w2v (from Allison Parrish's GitHub Gist) is:And another example being king - man + woman = queenIs there some analog (or way of creating some type of analog) for images where you might get something generally along these lines (with some made-up numbers):(and just to clarify, know the actual vectors created by an algorithm like Word2Vec wouldn't cleanly fit into human-interpretable categories, but I just wanted to give an analogy to the Word2Vec example.)or (starry night) - (landscape) + (man) = (van Gogh self portrait) or = (abstract self portrait) or something generally along those lines.Those might not be the best examples but just to recap, I'm looking for some sort of algorithm for creating an abstract n-dimensional learned representation for an image that can be grouped or compared with vectors representing other images.Thanks for your help!"
1584,Distill bert and svm for text classification,"['nlp', 'torch', 'word-embedding']","I am using DistillBert from hugging face to get the vector embedding of the words. The output for the example in hugging face will be a torch tensor with three dimensions. how can I feed these embeddings to an SVM?
I did detach the vectors and then reshape them to be 2 dimensional as an input for SVM. I was wondering if it is right?
So here, I have a list of sentences. I will make a for loop and get the vectors.
Code:
''''''"
1585,How to use Google's Universal Sentence Encoder to find the most similar document based on several documents?,"['python', 'nlp', 'recommendation-engine', 'word-embedding']","I'm trying to build a simple recommendation system which uses Google's Universal Sentence Encoder to transform the description of different products into vector space. I am pre-computing the embeddings for all the different products. Currently I can give the top N recommendations based upon a single product by calculating the cosine distance between the chosen product and all the other products.But if a user would say that ""I like these 10 products"", what is the best way to take that into consideration when making a recommendation? I do not have any data regarding which companies the user does NOT like. Is it to take the average vector of all those 10 products and then find the closest vectors to that? Or treat them as a cluster and find the centroid and make a recommendation based on that?Does anyone know any good practices for this? "
1586,How to implement SciBERT with pytorch; error while loading,"['error-handling', 'neural-network', 'nlp', 'tar', 'word-embedding']","I am trying to use SciBERT pre-trained model, namely: scibert-scivocab-uncased the following way: And I get the following error: I downloaded the file from the website (https://github.com/allenai/scibert)I converted it from ""tar"" to gzip Nothing worked. Any hint on how to approach this? Thank you!"
1587,Interpreting training.log in Flair (Zalando Research),"['python', 'word-embedding']","I was playing with the Flair library in order to see if there is a big difference (in terms of results) between fine-tuning (implemented separately) and embedding projection. The problem that I'm facing involves reading the results (in this case, the experiment was done by using BERT embeddings).
In the training.log I get this:My test dataset contains 2365 instances for a binary text classification task. What do the last 2 lines mean? The 0 and 1 followed by the true positives, precision, recall and so on? What is 0? And what is 1?
I also loaded separately the best model and tested on my test dataset and I obtained different results.Any help would be greatly appreciated."
1588,How to save fasttext model in vec format?,"['python', 'word-embedding', 'fasttext']",I trained my unsupervised model using fasttext.train_unsupervised() function in python. I want to save it as vec file since I will use this file for pretrainedVectors parameter in fasttext.train_supervised () function. pretrainedVectors only accepts vec file but I am having troubles to creating this vec file. Can someone help me?Ps. I am able to save it in bin format. It would be also helpful if you suggest me a way to convert bin file to vec file.
1589,error while loading semantic similarity with BERT model,"['error-handling', 'neural-network', 'nlp', 'lstm', 'word-embedding']","I want to calculate semantic similarity between sentences using BERT. I found this code on github for an already fine-tuned BERT for semantic similarity: It downloads 100% and gives me the following error (this is the final line):I tried to read about this error, but I don't understand where is the 2 positional arguments that where given instead of one. I'd appreciate any hint on where to look. Thanks!-------------------------EDIT QUESTION------------------------------ This is the whole error:"
1590,Querying a MongoDB database that has NER entities,"['mongodb', 'nlp', 'spacy', 'word-embedding', 'ner']",We have a MongoDB collection that stores documents similar to these:(entities are the result of applying SpaCy's NER.)We have a search component that would ideally query for 500.00 and USD and then give the relevant results according to some relevance score.A suggestion was that the search component should generate an in-memory (possibly Redis) representation of MongoDB NERs in word vectors and try to find the closest match.I have seen something similar here: https://blog.acolyer.org/2017/07/06/using-word-embedding-to-enable-semantic-queries-on-relational-databases/But it relates to SQL databases. What is the typical/preferred approach in such cases?
1591,Embedding in TensorFlow Functional API with 200.000 different words dictionary,"['python', 'tensorflow', 'keras', 'neural-network', 'word-embedding']","I have checked several questions in Stakoverflow and tutorials about Keras and TensorFlow embedding but I have found no answer that works for me. I explain.I have 200.000 words dictionary. With 10376 unique ""words"". They represent Cellular device ID. IMEI. In this particular instance, I want to process them using Keras Functional API and then merge with the numerical data eventually when I solve this.But I can pass the first level which part which is the embedding.Here the codeI check similar questions and answers but I can't find something that works for me.If someone can help me will be greatly appreciated Thank you very much indeed."
1592,Word embeddings with multiple categorial features for a single word,"['python', 'python-3.x', 'pytorch', 'word-embedding']","I'm looking for a method to implement word embedding network with LSTM layers in Pytorch such that the input to the nn.Embedding layer has a different form than vectors of words IDs.Each word in my case has a corresponding vector and the sentence in my corpus is consequently a vector of vectors. So, for example, I may have the word ""King"" with vector [500, 3, 18] where 500 is the Word ID, 3 is the word color, and 18 is the font size, etc. The embedding layer role here is to do some automatic feature reduction/extraction.How can I feed the embedding layer with such form data? Or do you have any better suggestions?"
1593,Classification using word embeddings,"['python', 'python-3.x', 'word-embedding']","I'm trying to do Classification using word embeddings, but I face typeError problem.expected and actual results:Error: "
1594,Word Embedding Model,"['machine-learning', 'deep-learning', 'word2vec', 'word-embedding', 'fasttext']","I have been searching and attempting to implement a word embedding model to predict similarity between words. I have a dataset made up 3,550 company names, the idea is that the user can provide a new word (which would not be in the vocabulary) and calculate the similarity between the new name and existing ones.During preprocessing I got rid of stop words and punctuation (hyphens, dots, commas, etc). In addition, I applied stemming and separated prefixes with the hope to get more precision. Then words such as BIOCHEMICAL ended up as BIO CHEMIC which is the word divided in two (prefix and stem word)The average company name length is made up 3 words with the following frequency:The tokens that are the result of preprocessing are sent to word2vec:After the model included all the words in the vocab , the average sentence vector is calculated for each company name:
    df['avg_vector']=df2.apply(lambda row : avg_sentence_vector(row, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word)).tolist())Then, the vector is saved for further lookups:If a new company name is not included in the vocab after preprocessing (removing stop words and punctuation), then I proceed to create the model again and calculate the average sentence vector and save it again.I have found this model is not working as expected. As an example, calculating the most similar words pet is getting the following results:In the dataset, I have words such as paws or petcare, but other words are creating relationships with pet word.This is the distribution of the nearer words for pet:On the other hand, when I used the GoogleNews-vectors-negative300.bin.gz, I could not add new words to the vocab, but the similarity between pet and words around was as expected:This is the distribution of the nearest words:I would like to get your advice about the following:Thanks"
1595,Bert sentence embeddings,"['nlp', 'word-embedding', 'sentence']","Im trying to obtain sentence embeddings for Bert but Im not quite sure if Im doing it properly... and yes Im aware that exist such tools already such as bert-as-service but I want to do it myself and understand how it works.Lets say I want to extract a sentence embedding from word embeddings from the following sentence ""I am."". As I understood Bert outputs in the form of (12, seq_lenght, 768). I extracted each word embedding from the last encoder layer in the form of (1, 768). My doubt now lies in extracting the sentence from these two word vectors. If I have (2,768) should I sum the dim=1 and obtain a vector of (1,768)? Or maybe concatenate the two words (1, 1536) and applying a (mean) pooling and get the sentence vector in shape of (1, 768). Im not sure what is the right approach is to obtain the sentence vector for this given example is. "
1596,Train LSTM with custom padding in each batch size,"['keras', 'lstm', 'word-embedding', 'zero-padding']","I am training a RNN for text classification. I want to train the model with a batch size X. However, for each batch, I want to create a zero padding where word_padding_length = maximum string length in each batch - current_word_length. I have tried search but was not able to find anything related to this. This should happen during when I fit the model."
1597,How can I recover the likelihood of a certain word appearing in a given context from word embeddings?,"['nlp', 'word-embedding', 'word-sense-disambiguation']","I know that some methods of generating word embeddings (e.g. CBOW) are based on predicting the likelihood of a given word appearing in a given context. I'm working with polish language, which is sometimes ambiguous with respect to segmentation, e.g. 'Coś' can be either treated as one word, or two words which have been conjoined ('Co' + '-ś') depending on the context. What I want to do, is create a tokenizer which is context sensitive. Assuming that I have the vector representation of the preceding context, and all possible segmentations, could I somehow calculate, or approximate the likelihood of particular words appearing in this context?"
1598,How to do link prediction with node embeddings?,"['machine-learning', 'graph', 'recommendation-engine', 'word-embedding']","I am currently working on an item embedding task in recommendation system and I want to evaluate the performance of the new embedding algorithm with the old ones. I have read some papers about graph embedding and almost every paper mentioned a normal method to evaluate the embeddings which is link prediction. But none of these papers described exactly how you do it. So my question is how to evaluate the embeddings using link prediction?The algorithm I am trying to apply is:
First a directed graph is built on user click sequences, each node in the graph represents an item, and if a user once clicked item A then clicked B, there should be two nodes A and B and an edge A-B with weight of 1. When another user clicked A then clicked B, the weight of edge A-B is added by 1.
Then a new sequence dataset is generated by random walking the graph, using the outbound weights as the teleport probabilities.
Finally SkipGram is performed on the new sequences to generate the node embeddings.As many papers mentioned, I removed a certain proportion of the edges in the graph as the positive samples of test set(e.g. 0.25) and randomly generated some fake edges as the negative ones. So what's next? Should I simply generate fake edges for the real edges in the training set, concatenate the embeddings of the two nodes on each edge, and build a common classifier such as logistic regression and test it on the test set? Or should I calculate the AUC on test set with cosine similarity of the two nodes and a label of 0/1 indicating if the two nodes are really connected? Or should I calculate the AUC with the sigmoided dot product of the embeddings of two nodes and a label of 0/1 indicating if the two nodes are really connected, since this is how you compute the probability at last layer?I have tried the three method in several tests, and they are not always telling the same thing. Some parameter combinations perform better with testA() and bad in other metrics, some the opposite..etc. Sadly there is no such a parameter combination that out performs others in all three metrics...The question is which metric should I use?"
1599,Gensim find vectors/words in ball of radius r,"['python', 'gensim', 'word-embedding']","I would like take word ""book"" (for example) get its vector representation, call it v_1 and find all words whose vector representation is within ball of radius r of v_1 i.e. ||v_1 - v_i||<=r, for some real number r.I know gensim has most_similar function, which allows to state number of top vectors to return, but it is not quite what I need. I surely can use brute force search and get the answer, but it will be to slow. "
1600,How are the TokenEmbeddings in BERT created?,"['machine-learning', 'nlp', 'word-embedding']","In the paper describing BERT, there is this paragraph about WordPiece Embeddings. We use WordPiece embeddings (Wu et al.,
  2016) with a 30,000 token vocabulary. The first
  token of every sequence is always a special classification
  token ([CLS]). The final hidden state
  corresponding to this token is used as the aggregate
  sequence representation for classification
  tasks. Sentence pairs are packed together into a
  single sequence. We differentiate the sentences in
  two ways. First, we separate them with a special
  token ([SEP]). Second, we add a learned embedding
  to every token indicating whether it belongs
  to sentence A or sentence B. As shown in Figure 1,
  we denote input embedding as E, the final hidden
  vector of the special [CLS] token as C 2 RH,
  and the final hidden vector for the ith input token
  as Ti 2 RH.
  For a given token, its input representation is
  constructed by summing the corresponding token,
  segment, and position embeddings. A visualization
  of this construction can be seen in Figure 2.
  As I understand, WordPiece splits Words into wordpieces like #I #like #swim #ing, but it does not generate Embeddings. But I did not find anything in the paper and on other sources how those Token Embeddings are generated. Are they pretrained before the actual Pre-training? How? Or are they randomly initialized? "
1601,"Get similarity score between 2 words using Pre trained Bert, Elmo","['nlp', 'gensim', 'word2vec', 'word-embedding', 'elmo']","I am trying to compare Glove, Fasttext, Bert ,Elmo on basis on similarity between 2 words using pre-trained models of Wiki. Glove and Fasttext had pretrained models which could easily be used with gensim word2vec in python. Does Elmo and Bert have any such models ?"
1602,Gensim's FastText KeyedVector out of vocab,"['gensim', 'word-embedding', 'fasttext']","I want to use the read-only version of Gensim's FastText Embedding to save some RAM compared to the full model.After loading the KeyVectors version, I get the following Error when fetching a vector:IndexError: index 878080 is out of bounds for axis 0 with size 761210The error occurs when using words that should be out-of-vocabulary e.g. ""lawyerxy"" instead of ""lawyer"". The full model returns a vector for both. So, my assumption is that the KeyedVectors do not offer FastText's out of vacabulary function - a key feature for my usecase. This limitation is not given in the documentation:
https://radimrehurek.com/gensim/models/word2vec.htmlCan anyone prove that assumption and/or name a fix to allow vectors for ""lawyerxy"" etc. ?"
1603,Is it possible to use Google BERT to calculate similarity between two textual documents?,"['python', 'text', 'scikit-learn', 'nlp', 'word-embedding']",Is it possible to use Google BERT for calculating similarity between two textual documents? As I understand BERT's input is supposed to be a limited size sentences. Some works use BERT for similarity calculation for sentences like:https://github.com/AndriyMulyar/semantic-text-similarityhttps://github.com/beekbin/bert-cosine-simIs there an implementation of BERT done to use it for large documents instead of sentences as inputs ( Documents with thousands of words)?
1604,How to write adversarial and virtual adversarial custom loss function in keras,"['keras', 'nlp', 'text-classification', 'word-embedding']","I am trying to write a keras implementation of this paper https://arxiv.org/abs/1605.07725 on using adversarial and virtual adversarial loss fn to improve the performance on text classification.But my custom loss function is not working.grad, = tf.gradients(cls_loss,embedding) returns None.How do i implement both the cost functions in keras"
1605,how to get one number after calculation of distance between 2D matrices,"['python-3.x', 'matrix', 'vector', 'scikit-learn', 'word-embedding']","I'd like to calculate document similarity by using word embedding models (w2v, glove) so one document can be represented 257*300 matrix 
 ( 257(max number of document) * 300(pretrained embedding model dimension)) And now I try to calculate distance between all document.When I use cosine similarity, euclidean or other vector calculation methods in scikit-learn.But these methods return similarity matrix. Is there any method to get one number after matrix distance calculation?Or should I calculate average of all vectors in similarity matrix ? (I think this is not proper way to solve this problem..) "
1606,Any bugs on FastText.build_vocab?,"['python', 'gensim', 'word-embedding', 'fasttext', 'incremental-build']","I cannot update the training of my gensim fasttext model with the command : model.build_vocabI think the key is ""AttributeError: 'FastText' object has no attribute 'syn1neg'""Please give me some suggestion. Thanks a lotprint('load fasttext pretrain model ')
pretrained_model=FastText_gensim.load(pretrained_model_file)sent=token_df['token'].values.tolist()   pretrained_model.build_vocab(sent,update=True)Traceback (most recent call last):
File ""C:/Users/marcus/PycharmProjects/DIVA_CWS/FastText_pretrain.py"", line 313, in 
pretrained_model.build_vocab(sent,update=True)
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 712, in build_vocab
self.finalize_vocab(update=update)  # build tables & arrays
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 953, in finalize_vocab
self.update_weights()
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 1373, in update_weights
self.syn1neg = vstack([self.syn1neg, zeros((gained_vocab, self.layer1_size), dtype=REAL)])
AttributeError: 'FastText' object has no attribute 'syn1neg'"
1607,Should the vocabulary be restricted to the training-set vocabulary when training an NN model with pretrained word2vec like GLOVE?,"['keras', 'neural-network', 'word-embedding', 'glove']","I wanted to use word embeddings for the embedding Layer in my neural network using pre-trained vectors from GLOVE. Do I need to restrict the vocabulary to the training-set when constructing the word2index dictionary? 
Wouldn't that lead to a limited non-generalizable model? 
Is considering all the vocabulary of GLOVE a recommended practice?"
1608,Preprecessing for Embedding: transforming word tokens to integer vectors,"['python', 'tensorflow', 'keras', 'nlp', 'word-embedding']","I am trying to preprocess a corpus of text for input to a word embedding layer, which takes padded vectors of integers. I know Keras/Tensorflow already has a list of functions that can do this (e.g., https://keras.io/preprocessing/text/#one_hot). But I would like to do my custom tokenization. This will be useful if the language is not English, for example.Does anyone know of examples of code that can take in a list of tokens, and transform them to vectors of integers? I would think this is a fairly common NLP task, so I wanted to check before reinventing the wheel."
1609,How to implement word embedding on dataset which have numerical as well as text data,"['python', 'machine-learning', 'word-embedding']",I am working on word embedding for recommendation but i am able to convert the text to vectors but don't have any idea about the how to deal with numeric data.
1610,Doc2vec matrix representation,"['python', 'word-embedding', 'doc2vec']","Using Doc2vec, I would like to see the impact of each word in the generated matrices.Is there a way to see the detail representation of a matrix i.e.
the content of the matrix and mostly  what is represented by each row and each column? For example this way I can see the matrix representation but not the column and row description:"
1611,BERT for Text Summarization,"['tensorflow', 'keras', 'deep-learning', 'word-embedding', 'seq2seq']","I'm trying to build a text summarization model using seq2seq architecture in Keras. I've followed this tutorial https://keras.io/examples/lstm_seq2seq/ and implemented it with Embeddings layer, which works fine. But now I want to use BERT. Can pretrained BERT embeddings be used in such a task, usually I see text classifiation, but not the encoder-decoder architecture used with BERT.I access BERT model from TF Hub, and have a Layer class implemented from this tutorial https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb, I also tokenize accordingly with BERT tokenizer, below is my modelThe model builds and after a couple of epochs accuracy rises to 1, and loss drops below 0.5, but the predictions are awful. Since I'm working on a dev set comprised of 5 samples, with max 30 WordPiece tokens and predicting on the same data, I only get the first or maybe two tokens right, then it just repeats the last seen token, or [PAD] token."
1612,"Should I train embeddings using data from both training,validating and testing corpus?","['nlp', 'word-embedding']","I am in a case that I don't have any pre-trained words embedding for my domain (Vietnamese food reviews). so I got a though of embedding from the general and specific corpus.And the point here is can I use the dataset of training, test and validating (did preprocess) as a source for creating my own word embeddings. If don't, hope you can give your experience.Based on my intuition, and some experiments a wide corpus appears to be better, but I'd like to know if there's relevant research or other relevant results."
1613,RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index',"['pytorch', 'word-embedding']","I'm working with the project 'lda2vec-pytorch' on Google CoLab,
runnin pytorch 1.1.0https://github.com/TropComplique/lda2vec-pytorchI'm getting an exception in the forward method adding 'noise' in my 
class negative_sampling_loss(nn.Module):Here's the stack trace:Any ideas?"
1614,Apply Word-Embedding on Security books. What will be the process or what output should i expect?,"['security', 'word-embedding']","I was asked to to word-embedding on some books related to security. 1) What process i should follow as i am a beginner in this topic?
2) What output will i get?Thank you."
1615,Tensoflow error while using BERT-as-Service,"['python', 'tensorflow', 'word-embedding']","I'm trying to use BERT-as-service as provided in this tutorial. I'm trying to use the server through the python env LINK. It throws an error :C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:526:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   _np_qint8 = np.dtype([(""qint8"", np.int8,
  1)])
  C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:527:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   _np_quint8 = np.dtype([(""quint8"",
  np.uint8, 1)])
  C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:528:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   _np_qint16 = np.dtype([(""qint16"",
  np.int16, 1)])
  C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:529:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   _np_quint16 = np.dtype([(""quint16"",
  np.uint16, 1)])
  C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:530:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   _np_qint32 = np.dtype([(""qint32"",
  np.int32, 1)])
  C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\framework\dtypes.py:535:
  FutureWarning: Passing (type, 1) or '1type' as a synonym of type is
  deprecated; in a future version of numpy, it will be understood as
  (type, (1,)) / '(1,)type'.   np_resource = np.dtype([(""resource"",
  np.ubyte, 1)]) Traceback (most recent call last):   File
  ""C:/Users/UserABCD/PycharmProjects/projectABC/BERTasService.py"", line
  1, in 
      from bert_serving.server.helper import get_args_parser   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\bert_serving\server__init__.py"",
  line 29, in 
      _tf_ver_ = check_tf_version()   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\bert_serving\server\helper.py"",
  line 164, in check_tf_version
      import tensorflow as tf   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow__init__.py"",
  line 24, in 
      from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import   File
  ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python__init__.py"",
  line 82, in 
      from tensorflow.python import keras   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras__init__.py"", line 24, in 
      from tensorflow.python.keras import activations   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\activations.py"",
  line 24, in 
      from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object   File
  ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\utils__init__.py"",
  line 38, in 
      from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model   File
  ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\utils\multi_gpu_utils.py"",
  line 22, in 
      from tensorflow.python.keras.engine.training import Model   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\engine\training.py"",
  line 42, in 
      from tensorflow.python.keras.engine.network import Network   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\engine\network.py"",
  line 40, in 
      from tensorflow.python.keras.engine import saving   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\tensorflow\python\keras\engine\saving.py"",
  line 38, in 
      import h5py   File ""C:\Users\UserABCD\Anaconda_Envs\user_env\lib\site-packages\h5py__init__.py"",
  line 36, in 
      from ._conv import register_converters as _register_converters   File ""h5py\h5r.pxd"", line 21, in init h5py._conv   File
  ""h5py\h5r.pyx"", line 145, in init h5py.h5r AttributeError: type object
  'h5py.h5r.Reference' has no attribute 'reduce_cython'I currently have tensorflow 1.3 installed via conda install -c conda-forge tensorflow. I also did the pip install, which installs the tensorflow 1.4Any pointers as to where the error might be ?"
1616,Universal sentence encoding embedding digits very similar,"['tensorflow', 'nlp', 'word-embedding']","I have task of sentence similarity where i calculate the cosine of two sentence to decide how similar they are . It seems that for sentence with digits the similarity is not affected no matter how ""far"" the numbers are . For an example:a = generate_embedding('issue 845')b = generate_embedding('issue 11')cosine_sim(a,b) = 0.9307is there a way to distance the hashing of numbers or any other hack to handle that issue?"
1617,How to train fastText for a completely new language dataset?,"['python', 'nlp', 'word-embedding', 'fasttext']",I am working with a regional language for which I need its word embeddings. I want to generate the word embeddings by training the FastText model for my dataset. But could not find the code to train rather I could find only the pre-trained models for different languages which does not include the regional language which I am working on. Could anyone please help me with the problem. Thank you in advance.
1618,How to improve the accuracy on google analogy task while training word2vec models?,"['nlp', 'word2vec', 'word-embedding']","I have implemented a word2vec model(skipgram+negative sampling) only using Numpy, and I trained it on 300M English copera, and the model achieved an mediocre wordsim353 score about 0.5, but the accuracy on google analogy task is very bad, smaller that 0.1,how can I improve that?As I observed, with more and more epochs, the wordsim353 score will improve abount 0.05 to 0.1 for each epoch, but the google analogy score only improves abount 0.01, I think it's weired.....Is the embedding initialization presented in the paper matter? I initialized the embedding matrix by filling the matrix with the data sampled from a gassian(mean 0,variance 1) distribution.... The codes are written with Numpy.I expected a high score but the actual output is 0.0864  0.0758"
1619,"How to build a seq2seq model for ASR, using mfcc vectors and corresponding word embedding vectors of the transcripts as the input and output data?","['keras', 'speech-recognition', 'word-embedding', 'mfcc', 'attention-model']",I am trying to build a voice to text model without using existing speech recognition libraries. I am using common-voice dataset from mozilla. I have done the data preprocessing where I extracted mfcc features from the input audio files and also used word embeddings to get vectors for the transcripts.I am stuck in building a seq2seq model for this. Can anyone help how to build a seq2seq model for this use case.
1620,Failed to load tensorflow BERT pre-trained model,"['python', 'tensorflow', 'word-embedding', 'pre-trained-model']","I tried to load a BERT pre-trained model to do NER task. But the system cannot find the pretrained model file.I used the following code in terminal, the folder contains model.ckpt-1000000, model.ckpt-1000000.index,model.ckpt-1000000.meta files. The error message is Note: The original names of the model files are model.ckpt-1000000.data-00000-of-00001, model.ckpt-1000000.index and model.ckpt-1000000.meta. I also triedThen the error would be"
1621,Word shows up more than once in TSNE plot,"['matplotlib', 'scikit-learn', 'nlp', 'word2vec', 'word-embedding']","When plotting word embedding TSNE results, words show up more than once.I am reducing dimensionality of a Word2Vec word embedding, but when I plot the results for a subset of the most similar words (manually enter several words for which I want the most similar ones), the same words show up more than once:Is this a normal behavior for PCA and TSNE word similarity dimensionality reduction, or is there something off with my code? Is it possible that the plot is treating each of the similar words subsets as independent from each other?"
1622,3D input for Embedding - LSTM Network,"['python', 'keras', 'lstm', 'recurrent-neural-network', 'word-embedding']","I'm building a generative music model using LSTM neural networks and I have stumbled into an issue regarding embedding layers, which expect a 2D input. The idea is to predict, at a given timestep, which notes should be active for a single instrument, based on an arbitrary number of previous timesteps (in this case 75). Standard RNN stuff. The  shape of the Labels (y) is an N length list of 4 numbers which indicate active notes at this timestep. example final shape = (300, 4)The shape of the  data (x) is an N length list of 2D arrays, with the 2D array being of shape (75, 4) indicating the previous 75 timesteps which led to the current label (1, 4). example final shape = (300, 75, 4)This is the part of the code which is causing the issue:This is the error I get:My main question is: is it possible to feed a 3D input to an embedding layer, or must I reduce the dimensions of the data before it can be processed into a word vector?"
1623,Word embeddings in Tensorflow2,"['nlp', 'gensim', 'word2vec', 'word-embedding', 'tensorflow2.0']","I am trying to understand how to use Tensorflow2 to train word embeddings without the preset labels.In the Tensorflow2 tutorial (https://www.tensorflow.org/beta/tutorials/text/word_embeddings) it shows how to train word embeddings using pre-structured dataset with labels.However, I wonder how to train - with Tensorflow2 - the embeddings on the non-labeled text, similar to what can be done with Gensim's Word2Vec?"
1624,How to combine 3D token embeddings into 2D vectors?,"['python', 'tokenize', 'gensim', 'word2vec', 'word-embedding']","I have a set of strings that I am tokenizing. I am sending each string into the word2vec model in gensim. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (list of list in python). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  I am sending this 3D into a model in Tensorflow library. I am doing the following,model.add(Embedding(max_features, 128, input_length=maxlen))Here max_features is the size of the token vector i.e. 100 and input_length is also the same. But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model."
1625,How can i use XLNet to generate word embeddings,['word-embedding'],"I want to know how i can use XLNet to generate word embeddingsI am currently using a word embedding model, but I want to compare its performance with XLNet"
1626,I want to know how can we give a categorical variable as an input to an embedding layer in keras and train that embedding layer?,"['keras', 'nlp', 'lstm', 'categorical-data', 'word-embedding']","let's say we have a data frame where we have a categorical column which has 7 categories - Monday, Tuesday, Wednesday, Thursday, Friday, Saturday and Sunday. Let's say we have 100 data points and we want to give the categorical data as an input to the embedding layer and train the embedding layer using Keras. How do we actually achieve it? Can you share some intuition with code examples? I have tried this code but it gives me an error which says ""ValueError: ""input_length"" is 1, but received input has shape (None, 26)"". I have referred to this blog https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9, but I didn't get how to use it for my particular case.I want to know in case of 7 categories, what will be the shape of input_layer2? What should be the vocab size, output dim and input_length? Can anyone explain, or correct my code? Your insights will be really helpful."
1627,word2vec with different grammar,"['word2vec', 'word-embedding']","what is the effect of word2vec if implemented on different language and different grammar? I mean word2vec is implemented on english corpus for the first time, is there any affect if we used another language corpus?"
1628,Interepretation of word2vec evaluation result,"['word2vec', 'evaluation', 'word-embedding']",I have created word embeddings (Word2vec) using my own dataset. I have used Gensim module to create word embeddings. I want to evaluate my word embeddings.I have used Wordsim353 dataset to evaluate word embeddings. The following code Shows the result of Evaluation. Code:Result:How can I interprete the result?Please help me to interprete the results.
1629,Cosine similarities and totally different results using same source,"['python', 'machine-learning', 'cosine-similarity', 'word-embedding', 'machine-translation']","I am learning word embeddings and cosine similarity. My data is composed of two sets of same words but in 2 different languages. I did two tests:Should I expect to obtain quite the same results? I noticed that sometimes I have two opposite results. Since I am new on this, I am trying to figure out if I did something wrong or if there is an explanation behind. According to what I have been reading, soft cosine similarity should be more accurate than the usual cosine similarity.Now, it's time for some data to show you. Unfortunately I can't post a part of my data (the words themselves), but I will try my best to give you the max of information I can give you.Some other details before:(1-distance.cosine(data['LANG1_AVG'].iloc[i],data['LANG2_AVG'].iloc[i]))For the usual cosine similarity I am using the Fast Vector cosine similarity from FastText Multilingual, defined in this way:@classmethod
def cosine_similarity(cls, vec_a, vec_b):
    """"""Compute cosine similarity between vec_a and vec_b""""""
    return np.dot(vec_a, vec_b) / \
        (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))As you will see from the image here, for some words I obtained the same results or quite similar using the two methods. For others I obtained two totally different results. How can I explain this? "
1630,Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker,"['amazon-web-services', 'tensorflow', 'word-embedding', 'amazon-sagemaker', 'tensorflow-hub']",I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get Failed precondition: Table not initialized. as an error. I have included the part where I save my model below:I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders
1631,understanding FastText multilingual,"['python', 'text-alignment', 'word-embedding', 'fasttext']","I am working with this modified version of FastText (fastText_multilingual) that will let me align words in two languages.I am trying to understand their fasttext.py and especially the Fast Vector class. 
In the example file align_your_own.ipynbthe authors show how to measure similarity between two words. I would like to iterate the process for the whole set of words, instead of measuring similarity every time for a single word. To do this I need to understand how to access to these FastVector objects. 
That's why I am trying to understand the Fast vector class. I am stuck here:I have never created a class in python, so this make things more difficult for me. These are the lines that I can't understand in depth:These are my questions:"
1632,"ValueError: cannot reshape array of size 3800 into shape (1,200)","['python', 'deep-learning', 'tokenize', 'word2vec', 'word-embedding']","I am trying to apply word embedding on tweets. I was trying to create a vector for each tweet by taking the average of the vectors of the words present in the tweet as follow:Next, when I try to Prepare word2vec feature set as follow:I get the following error inside the loop:I checked all the available posts in stackOverflow but non of them really helped me.I tried reshaping the array and it still give me the same error. My model is:any suggestions please? "
1633,Is Google word2vec pertrained model CBOW or skipgram,"['python-3.x', 'word2vec', 'word-embedding']",Is Google's pretrained word2vec model CBO or skipgram.We load pretrained model by:How can we specifically load pretrained CBOW or skipgram model ?
1634,Where should I pass pre trained word embedding in a encoder-decoder architecture?,"['machine-learning', 'keras', 'nlp', 'word-embedding', 'machine-translation']",I have  pre-trained word embeddings from two different languages using MUSE. Now suppose I have a encoder-decoder architecture. And I created a embedding layer from one of this embedding. But where do I pass it in the model?The model is trying to translate from one language to another. I have created a embedding_layer. Where do I pass it in in the below code?
1635,Entity Embedding of Categorical within Time Series Data and LSTM,"['lstm', 'word-embedding', 'tf.keras']","I'm trying to solve a time series problem. In short, for each customer and material (SKU code), I have different orders placed in the past. I need to build a model that predict the number of days before the next order for each customer and material.What I'm trying to do is to build an LSTM model in Keras, where for each customer and material I have a 50 max padded timesteps of history, and I'm using a mix of numeric (# of days since previous order, AVG days between orders in last 60 days etc...) and categorical features (SKU code, customer code, type of SKU etc...).For the categorical, I'm trying to use the popular entity embedding technique. I started from an example published on Github, that was not using LSTM (it was embedding using input_lengh = 1) and generalized it to work with higher input emebdding that I could feed to LSTM.Below my code.What I observed it that:
    -the model show good performance with numeric features only
    -adding categorical does nothing to improve performances (I would at least expect the model to overfit by producing very specific rules, like client X ordered material Y in week Z after 5 days), but this never happensMy question is, is there something conceptually wrong in using entity embedding in LSTM like this? Should I change something?Thanks a lot in advance"
1636,How to input a competed word instead of single word to fasttext model,"['python', 'similarity', 'word-embedding', 'fasttext']","I am using fasttext for word similarity purpose. I input a txt file which has a word in each line. The text format is string.ex:
iphone 8 \n
apple hkd \n
billie jean jh audio \n
anafi fcc mode \n
smartone promotion \n
zenfone 5 price \n
brands of smartphones \nBut when I enter ""iphone"" to trained model in order to get the similarity results, it returns a singe word such as:('買iphone', 0.7245190143585205), 
('""iphone', 0.6674467325210571), 
('iphone.', 0.6550431251525879), 
('piphone', 0.6026722192764282),
.....
('xs', 0.38547301292419434), 
('iph0ne', 0.37973055243492126), 
('hybrid', 0.3791016936302185), 
('cellphone', 0.37475910782814026), 
('xr', 0.3736417889595032)I expect the output should have completed word like ""iphone 8"", ""iphone xr""."
1637,How to train a word embedding representation with gensim fasttext wrapper?,"['machine-learning', 'nlp', 'gensim', 'word-embedding', 'fasttext']","I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:In:Out:In:Out:However, when I try to look in a vocabulary words:I get False, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:What is the correct way of using gensim's fasttext wrapper?"
1638,"After creating an embedding layer using a tensorflow placeholder, how is the tf.nn.embedding_lookup() function used with it?","['tensorflow', 'nlp', 'word-embedding']","I'm trying to add a pre-trained word2vec word embedding to my tensorflow code. Now after creating the embedding matrix, one way to add this layer is by creating a tensorflow variable, but this leads to repetition of calculation and hence, not efficient. 
The other way is to create a placeholder and pass data through that. Now I want to be able to pass data to that embedding by calling the tf.nn.embedding_lookup() function, what tensor exactly do I pass into the lookup function?"
1639,Why does the TensorBoard display the wrong cosine distance?,"['python', 'tensorboard', 'word-embedding']","i want to visualize word embeddings in the Projector from TensorBoard, but the cosine distances doesnt seem right.If i compute the cosine distances via sklearn i get different results.Am i using the TensorBoard Projector wrong?TensorBoard:
https://i.imgur.com/2hRtXym.pngSklearn:
https://i.imgur.com/49OaiEU.png "
1640,How do I convert BERT embeddings into a tensor for feeding into an LSTM?,"['keras', 'deep-learning', 'nlp', 'lstm', 'word-embedding']","I am trying to replace Word2Vec word embeddings by sentence embeddings by BERT in a siamese LSTM network (https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb). However my BERT embeddings are (1,768) shaped matrix and not tensors that can be fed to a keras layer. I wanted to know if it would be possible to convert it.I have found a way to replace word embeddings by Universal sentence embeddings (http://hunterheidenreich.com/blog/google-universal-sentence-encoder-in-keras/) I tried to modify the code of the LSTM to use BERT sentence embeddings from the following service (https://github.com/hanxiao/bert-as-service#what-is-it).I am getting the following error message TypeError: ""Tensor(""lambda_3/Squeeze:0"", dtype=string)"" must be , but received class 'tensorflow.python.framework.ops.Tensor'"
1641,How to check the performance of word embedding,"['python', 'word2vec', 'word-embedding']","I have used the gensim Word2Vec model and applied it in my list of documents. Well , the word embedding is getting created. I want to know if Word2Vec is performing well on my list of documents. Is there any metrics to measure that? How will I understand if Word2Vec has really worked well on my document corpus or should I try some different embedding?
Below is the code I have used from gensim."
1642,How to encode categorical data that have variable length so could be fetched to nn.Embedding in PyTorch,"['machine-learning', 'pytorch', 'data-mining', 'encode', 'word-embedding']","Let's say i have a data field named movie_genre for each sample movie, it is selected from the following genres:And for each movie, it might contain multiple genres:which means, the movie's genres is a variable list.If i use one hot vector to encode the genre, Action can be encoded as (1, 0, 0, 0), Adventure can be encoded as(0, 1, 0, 0), and so on.So movie with mid1 can be encoded as (1, 1, 0, 0), mid2's genre can be encoded as (0, 0, 1, 0), and so on.However, the pytorch embedding layer nn.Embedding takes tensor containing the indices as input, but not one-hot vector. So how should i encode the data so that it can be fetched into the embedding layer?"
1643,Can Keras embedding layer give random vector for a certain index (e.g: -1) instead of a fixed vector,"['python', 'python-3.x', 'keras', 'keras-layer', 'word-embedding']","I have a problem where I have texts ( that can be very long max ~9000 words) that I need to embed with Keras Layer. I choose the fixed size 5000 for every text and I need to pad each sequence to get to the right shape. The classical way is to use Keras' pad_sequence that take as input list of lists of indexes and pad with zeros or cut the lists of indexes to 5000. 
For my downstream task, I use a sort of convnet inspired by Kim's Paper (https://arxiv.org/abs/1408.5882). My concern is that the network learns in a certain sense the Wordcount by detecting the pattern of vectors that embed the 0 I used to pad the sequences. I am not saying that this feature is not important but I would like to force the network to learn other features in preferences. I was thinking about two things, first using an additional task (like an adversarial task) that take the latent representation created by the model before the output and use a branch of the model to predict the size of the text or a cluster of size, for example :Then use the output to encourage the network to map other information in the latent space by adding an adversarial loss to the main loss term. My other idea was instead of using zeros' vectors to pad the embed the zero paddings, we could use random vectors, generated on the fly while training. (every time the network sees a particular index, for example -1, it knows it has to generate a random vector). I was thinking that it breaks the symmetry introduced by using zeros vectors and helps the model to generalize better as it introduces noise in the training process.As I didn't find any papers on this task of padding with something else than zeros, I turn to the community. What do you think? I went through the Embedding layer implementation and I am pretty sure that the implementation of the second idea is pretty straightforward in keras by changing the K.gather() by a flag for the right indexes (It would be longer execution time though).Thanks in advance for your feedback and your ressources !"
1644,how to save BERT word embedding as .vec similar to word2vec,"['vector', 'word2vec', 'word-embedding', 'glove', 'torchtext']","I want to use the generated BERT word embedding as a vector for building the vocab in Torchtext 
I can load vectors such as GloVe or word2vec
but I didn't know how to save the word embedding from BERT to a format acceptable by Torchtext vocabwhen I try this codeI get this error:"
1645,Variable length input to embedding layer Keras,"['python', 'keras', 'lstm', 'embedding', 'word-embedding']","I have a variable size text corpus. I am trying to feed my texts to a LSTM model using the Embedding layer in keras. My code looks something like this:I have genereted a sample input using numpy random number generator:Output:When I try to predict the get only the embeddings using model.predict(input_array), i get the following errorI know I can just pad the sequence, but won't LSTM layer will return only the last hidden state for padded sequence. I want the hidden state from the last hidden state of the actual sequence and not the padded one i.e. if my sequence length is 15 and the maximum sequence length is 200, I want the hidden state vector from 15th state and not 200th state"
1646,Formal meaning of 'weight-tying' expression in literature,"['deep-learning', 'autoencoder', 'word-embedding']","I recently came across the term Weight-tying regarding auto-encoders and words-embeddings as well - yet couldn't find a clear definition. My guess is that it means concatenating multiple outputs of ,say, FC layers and feeding to a single layer, thus 'tying' the weights to be influenced by the same 'trigger'."
1647,Can't map a certain word to vector,"['nlp', 'word-embedding', 'glove']","I'm having trouble with implementing word-to-vector mapping with GloVe. My code seems to be working fine, but there is a weird problem: I'm getting error when trying to map one particular word - 'the', to it's vector representation. I have no idea why this is happening. This is my code for reading the GloVe file:As you can see the function above returns variable 'word_to_vec_map' which is supposed to map the words from training set to the their GloVe representations.Here's a snippet from the training set:It appears as though I am able to map the words using word_to_vec_map:But then:Does anyone has any idea why this is happening? Why can't I map this particular word?"
1648,Create a matrix from a dict of dicts for calculating similarities between docs,"['matrix', 'nlp', 'similarity', 'tf-idf', 'word-embedding']","Here is my problem:I have a dataframe like this:column 'id' represent the ids of the docs and 'tfidf_weights' the tfidf weight for each word of each docs.from this dataframe, i can obtain a dict with the following structure:what i want to do is, from this dictionary, obtain a matrix like this:Thank you for your help !"
1649,How and why we use CNN layer wrapped with time distributed layer?,"['python', 'machine-learning', 'nlp', 'word-embedding', 'faster-rcnn']","I need to know how this code works.  It's taking Embedding then it sends it into this model. model1 is CNN and moel2 is Time distributed layer. Why wrapping is done in this code, i didn't find article on this. then  it merges and getting the output. I don't understand the computation behind this."
1650,BERT - Extract values from all layers,"['python', 'tensorflow', 'tensorflow-estimator', 'word-embedding', 'bert-language-model']","i'm trying to get the ""Word Embedding"" from BERT.
I have already fine tuned the model for ""Sentiment classification"" and the model predict if a sentence is positive or negative.
But i need to extract values from all layers to get the featurized representation.i have tried something like thisall_layers = model.get_all_encoder_layers()but doesn't worksi have to change the model_fn to get values from all layers of BERT?"
1651,Keras Embedding Layer: keep zero-padded values as zeros,"['machine-learning', 'keras', 'text-classification', 'word-embedding', 'zero-padding']","I've been thinking about 0-padding of word sequence and how that 0-padding is then converted to the Embedding layer. At first glance, one would think that you want to keep the embeddings = 0.0 as well. However, Embedding layer in keras generates random values for any input token, and there is no way to force it to generate 0.0's. Note, mask_zero does something different, I've already checked.One might ask, why worry about this, the code seems to be working even when the embeddings are not 0.0's, as long as they are the same. So I came up with an example, albeit somewhat contrived, where setting the embeddings to 0.0's for the 0 padded token makes a difference. I used the 20 News Groups data set from sklearn.datasets import fetch_20newsgroups. I do some minimal preprocessing: removal of punctuation, stopwords and numbers. I use from keras.preprocessing.sequence import pad_sequences for 0-padding. I split the ~18K posts into the training and validation set with the proportion of training/validation = 4/1. 
I create a simple 1 dense hidden layer network with the input being the flattened sequence of embeddings:The model has about 14M trainable parameters (this example is a bit contrived, as I've already mentioned). 
When I train it it looks like for 4 epochs the algorithm is struggling to find its way out of the 'randomness':It ends up with the accuracy of ~0.87However, when I explicitly set the embeddings for the padded 0's to 0.0the model with the same number of parameters immediately finds its way out of the 'randomness':and ends up with a better accuracy of ~0.9.Again, this is a somewhat contrived example, but still it shows that keeping those 'padded' embeddings at 0.0 can be beneficial. Am I missing something here? And if I'm not missing anything, then, what is the reason Keras doesn't provide this functionality out-of-the-box?UPDATE@DanielMöller I tried your suggestion:Unfortunately, the network was stuck in the 'randomness':I also tried without the NonNeg() constraint, the same result."
1652,Accuracy of fine-tuning BERT varied significantly based on epochs for intent classification task,"['nlp', 'word-embedding', 'finetunning', 'bert-language-model']","I used Bert base uncased as embedding and doing simple cosine similarity for intent classification in my dataset (around 400 classes and 2200 utterances, train:test=80:20). The base BERT model performs 60% accuracy in the test dataset, but different epochs of fine-tuning gave me quite unpredictable results.This is my setting:These are my experiments:I don't understand while it behaved like this. I expect that any epochs of fine-tuning shouldn't be worse than the base model because I fine-tuned and inferred on the same dataset. Is there anything I misunderstand or should care about?"
1653,Understanding Dense layer after Embedding Layer in Keras,"['python', 'keras', 'neural-network', 'nlp', 'word-embedding']","I am having some problems to understand the functioning of a Dense layer handling text sequences. Let's imagine this simple case: I have two sentences and I assign integers to the words:Then we take the input (the integer sequences) and add them to a Embedding layer (random numbers in 2 dimensions):Next step is to Flatten the sequences with the embeddings in order to make it 1D:Now we can use those arrays as input for the Dense layer, something that would look like this:Dense layer of 3 units with input length of sequence. Dot product of the input sequence with the weights matrix of the dense layer. Dense layerThe problem here is that every feature corresponds to one part of the sequence, X5 and X6 belong to the vector of the last word. Normally, with Bag of Words input every X is assigned to a word, and if the word is present then it gets a weight (following TF-IDF for example). Also with numerical data, Xi represents a feature (price, temperature, GDP...) and it is always that case. Here, however, X is not assigned to a word and depends on the order. See X5 and X6 how they change because the words are different. The weights in the Weight Matrix of the Dense layer are assigned to a feature (Xi), and then they are optimised.My question:How does it work if the order of words changes all the time and Xi is referring to different words?I understand LSTMs and other recurrent networks can handle dynamic ordering, but Dense layers seemed to me that could not work with sequential text and that the input should be fixed by One Hot vector or TF-IDF for example. Still I have seen examples of models with Sentences to Sequences of Integers, Embedding, Flatten and Dense layer architectures, plus I have tried myself and I see it does work... I would really appreciate some explanation or correction in my flow of thinking. Thanks!"
1654,Building a neural network that takes a created feature vector,"['python', 'tensorflow', 'feature-extraction', 'word-embedding']","To be more precise. Lets say I already have a vector that represents something (word, object, image...) and that I can not change the way I get it. What I would like to do is create a NN without the embedding and pooling layer and am wondering if tensorflow supports this kind of aproach. Lets say my vector is 10 features long (10 floats). For each vector I also have a label, lets say there are 3 labels to chose from.What I am (struggling/trying) to do is this. I would like to push this sort of vector input into a keras dense layer with relu activation and 10 neurons (stack maybe 2 or 3) and then as a final layer use sigmoid activation with 3 output neurons. Then fit with labels on 40(?) epochs and so on...My main question is well.. Is this possible? I have yet to finish the code and maybe I am asking this a bit too soon, but nevertheless. 
Is this how one would approach this or would you build the model from embedding layer down and would not use the already made vectors?"
1655,"What is the difference between word2vec, glove, and elmo?","['nlp', 'word2vec', 'word-embedding', 'glove', 'elmo']","What is the difference between word2vec, glove, and elmo? According to my understanding all of them are used for training word embedding, am I correct?"
1656,How to find similar words in Keras Word Embedding layer,"['keras', 'word-embedding']","From Stanford's CS244N course, I know Gensim provides a fantastic method to play around the embedding data: most_similarI was trying to find some equivalent in Keras Embedding layer but I couldn't. It isn't possible out of the box from Keras? Or was it any wrapper on top of it?"
1657,add LSTM/GRU to BERT embeddings in keras tensorflow,"['lstm', 'word-embedding', 'tf.keras']",I am experimenting with BERT embeddings following this code https://github.com/strongio/keras-bert/blob/master/keras-bert.pyThese are the important bits of the code (lines 265-267):I want to add a GRU between BertLayer and the Dense layerbut I get this error TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'.I am not entirely sure how to address this problem. Do I need to reshape bert_output or do I need to create an Embedding layer that the GRU can handle?
1658,Copying embeddings for gensim word2vec,"['gensim', 'word2vec', 'word-embedding']","I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training and set the embeddings of the new Word2Vec equal to the first oneMost of the functions work as expected, e.g.andandHowever, most_similar doesn't work:What am I missing? Disclaimer. I posted this question on datascience.stackexchange but got no response, hoping to have a better luck here. "
1659,How to use BERT pretrain embeddings with my own new dataset?,"['word-embedding', 'transfer-learning', 'bert-language-model']","My dataset and NLP task is very different from the large corpus what authors have pre-trained their model (https://github.com/google-research/bert#pre-training-with-bert), so I can't directly fine-tune. 
Is there any example code/GitHub that can help me to train BERT with my own data? I expect to get embeddings like glove.Thank you very much!"
1660,Understanding number of params in Keras RNN and output shape dimension in Keras Embedding when RNN and Embedding are chained together,"['keras', 'recurrent-neural-network', 'word-embedding']","I have this Keras code from some youtube video:The output of the summary is this:First I don't understand why the number of params is 2080 in simple RNN. Next I don't get why output shape from the embedding layer is (None, None, 32)"
1661,Word Embeddings with Keras using R,"['r', 'keras', 'word2vec', 'word-embedding', 'reticulate']","I m trying to use Keras with R (R version 3.5.0) to generate word embeddings for the Amazon Fine Foods Reviews dataset (using this tutorial: https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras/). But I have the following error when training the model:Here is my script:
(I have not change nothing from the tutorial)"
1662,Identical results with different model parameters,"['python', 'keras', 'neural-network', 'nlp', 'word-embedding']","I'm making a neural network model to assign sentiments to phrases. My problem is no matter what I change in my model, the accuracy score is always identical.I have tried adding or removing layers, changing batch size, changing number of epochs, changing activators etc. Evaluate always shows exactly 0.2057 score. I want to achieve around 50% accuracy"
1663,Sentence iterator to pass to Gensim language model,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.I am trying to implement the following code to create my own wordembedings:with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iteratorI found this solution by the creator of gensim:It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.cheersMore pre"
1664,What is the difference between keras.tokenize.text_to_sequences and word embeddings,"['keras', 'tensorflow2.0', 'tokenize', 'word-embedding', 'tensorflow2.x']","Difference between tokenize.fit_on_text, tokenize.text_to_sequence and word embeddings?Tried to search on various platforms but didn't get a suitable answer."
1665,Quick way to get document vector using GloVe,"['python', 'pandas', 'nlp', 'word-embedding']","I am trying to use GloVe to represent entire document. However, GloVe is initially designed to get word embedding. One way to get the document embedding is to take the average of all word embeddings in the document.I am following the solution posted here to load the GloVe look-up table. However, when I tried to get the document embedding, the runtime is extremely slow (about 1s per document for more than 1 million documents). I am wondering if there is any way I could accelerate this process.The GloVe look-up table could be downloaded here and the following is the code I use to get the document embedding. The data is stored in a pd.DataFrame(), where there is a review column.Note there might be some words in the text_processed_list not present in the look-up table, that is why try...catch... comes into play."
1666,Word2Vec word not found with Gensim but shows up on TensorFlow embedding projector?,"['python-3.x', 'deep-learning', 'nlp', 'word2vec', 'word-embedding']","I've recently started experimenting with pre-trained word embeddings to enhance the performance of my LSTM model on a NLP task. In this case, I looked into Google's Word2Vec. Based on online tutorials, I first downloaded Word2Vec with wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz and used python's gensim package to query the embeddings, using the following code.However, after noticing that many common words weren't found in the model, I started to wonder if something was awry. I tried searching for bosnia in the embedding repo, as shown above, but it wasn't found. So, I went on the TensorFlow embedding projector, loaded the Word2Vec model, and searched for bosnia - it was there.So, my question is: why is this happening? Was the version of Word2Vec I downloaded not complete? Or is gensim unable to load all words into memory and therefore omitting some?"
1667,Getting error while adding embedding layer to lstm autoencoder,"['tensorflow', 'keras', 'lstm', 'autoencoder', 'word-embedding']","I have a seq2seq model which is working fine. I want to add an embedding layer in this network which I faced with an error.this is my architecture using pretrained word embedding which is working fine(Actually the code is almost the same code available here, but I want to include the Embedding layer in the model rather than using the pretrained embedding vectors):This is the summary:when I change the code to add the embedding layer like this:I received this error:So my question, what is wrong with my model?UpdateSo, this error is raised in the training phase. I also checked the dimension of the data being fed to the model, it is (61598, 45) which clearly do not have the number of features or here, Embed_dim.But why this error raises in the decoder part? because in the encoder part I have included the Embedding layer, so it is totally fine. though when it reached the decoder part and it does not have the embedding layer so it can not correctly reshape it to three dimensional.Now the question comes why this is not happening in a similar code?
this is my view, correct me if I'm wrong. because Seq2Seq code usually being used for Translation, summarization. and in those codes, in the decoder part also there is input (in the translation case, there is the other language input to the decoder, so the idea of having embedding in the decoder part makes sense).
Finally, here I do not have seperate input, that's why I do not need any separate embedding in the decoder part. However, I don't know how to fix the problem, I just know why this is happening:|Update2this is my data being fed to the model:and parsed_sentences is 61598 sentences which are padded.Also, this is the layer I have in the model as Lambda layer, I just added here in case it has any effect ever:Any help is appreciated:)"
1668,Specify condition for negative sampling in gensim word2vec,"['gensim', 'word2vec', 'word-embedding']","I'm training word2vec model where each word belongs to a specific class. I want my embeddings to learn differences of words within each class, but don't want them to learn the differences between classes. This can be achieved by negative sampling from only the words of same class as the target word. In gensim word2vec, we can specify the number of words to negative sample using negative parameter, but it doesn't mention any options to modify/filter the sampling function. Is there any method to achieve this?Update:Consider the classes to be like languages. So I have words from different languages. In training data, each sentence/document contains mostly words from same language, but sometimes from other languages. Now I want embeddings where words with similar meanings are together irrespective of the language.But because words from different languages do not occur together as frequently as words from same language, the embeddings basically groups words from same language together. Because of this, I wanted to try negative sampling target words with words from same language so that it learns to distinguish the words within same language. "
1669,word embedding of a lstm sequence,"['tensorflow', 'keras', 'lstm', 'gensim', 'word-embedding']","Suppose, I have a Seq2Seq model. I want to have the Embedding layer in this model.Based on my research I can do it in three ways:train a word embedding separately on my data set or download a pre-trained word embedding, then use the weights of those embedding as the weight of the words in my data set. So here I do not need to have an embedding layer at all, I just load the weights of the already trained words into the words in my data set.I create an embedding layer and set the trainable true, so not only I have an embedding, but also, that embedding will be trained based on my taskI create an Embedding layer, load already trained weights, and set trainable False. in this case, the weights will not get updated.(please correct me if Im wrong).I have used the first approach. I want to know what will be the interpretation of the output of this code:this is the output:consider this [1,2,3],[3,4,1] as two sequence with length=3.I was thinking we use word embedding in lstm to transform each word of the sequence into an embedding. I expected to see two vectors and three items in each vector.The embedding is the word2vec in gensim,Appreciate it if someone shed light on it where I am getting lost?Thanks~"
1670,Discrepancies in gensim doc2vec embedding vectors,"['gensim', 'word-embedding', 'doc2vec']","I use gensim Doc2Vec package to train doc2vec embeddings. I would expect that two models trained with the identical parameters and data would have very close values of the doc2vec vectors. However, in my experience it is only true with doc2vec trained in the PV-DBOW without training word embedding (dbow_words = 0).
For PV-DM and for PV-DBOW with dbow_words = 1, i.e. every case the word embedding are trained along with doc2vec, the doc2vec embedding vectors for identically trained models are fairly different. Here is my codeUPDATEI tried, as suggested by gojomo, to compare the differences between the vectors, and, unfortunately, those are even worse:SECOND UPDATEThis time, after I finally understood gojomo, the things look fine."
1671,Why are word embeddings with linguistic features (e.g. Sense2Vec) not used?,"['nlp', 'artificial-intelligence', 'word2vec', 'word-embedding', 'sense2vec']","Given that embedding systems such as Sense2Vec incorporate linguistic features such as part-of-speech, why are these embeddings not more commonly used?Across popular work in NLP today, Word2Vec and GloVe are the most commonly used word embedding systems. Despite the fact that they only incorporate word information and does not have linguistic features of the words.For example, in sentiment analysis, text classification or machine translation tasks, it makes logical sense that if the input incorporates linguistic features as well, performance could be improved. Particular when disambiguating words such as ""duck"" the verb and ""duck"" the noun.Is this thinking flawed? Or is there some other practical reason why these embeddings are not more widely used."
1672,Uses of Embedding/ Embedding layer in deep learning,"['deep-learning', 'lstm', 'recurrent-neural-network', 'word-embedding']","I am exploring deep learning methods especially LSTM to predict next word. Suppose, My data set is like this: Each data point consists of 7 features (7 different words)(A-G here) of different length.I used one hot encoding as an Input layer. Here is the model Using this model. I got an accuracy of about 60%. 
My question is how can I use embedding layer for my problem. Actually, I do not know much about embedding (why, when and how it works)[I only know one hot vector does not carry much information]. I am wondering if embedding can improve accuracy. If someone can provide me guidance in these regards, it will be greatly beneficial for me. (At least whether uses of embedding is logical or not for my case)"
1673,Word2Vec with POS not producing expected results?,"['keras', 'nlp', 'word2vec', 'word-embedding', 'seq2seq']","I am trying to gauge the impact of part of speech information with Word2Vec embeddings but am not obtaining expected results.I expected POS included word2vec embeddings to perform better in a machine translation task but it is actually performing worse.I am creating two sets of embedding off of the same corpus using Gensim, one is normal Word2Vec, the other, I am changing tokens to ""[WORD]__[POS]"".I am gauging differences in performance by using the embeddings in a Seq2Seq machine translation task. I am evaluating the two approaches with BLEUThis is how I am training the word2vec + POS embeddings with SpaCy:This is my benchmark machine translation model with Keras + Tensorflow:With BLEU, the Word2Vec+POS approach consistently scores the same as Word2Vec or 0.01-0.02 points below the normal Word2Vec embeddings.Does anyone know why this might be happening? Is there a gap in my reasoning or expectations?"
1674,How to handle unseen words for pre-trained Glove word-embedding to avoid keyerror?,"['python', 'nlp', 'word-embedding']","I want to extract features from pre-trained Glove embedding. But I got Keyerror  for certain words. Here is the list of word token. I got Keyerror from 'impulsed', 'darkfield' words because probably these are the unseen words. How can I avoid this error ? . Here is my full code: Error message for 'impulsed' word "
1675,How to use a Keras trained Embedded layer?,"['python', 'tensorflow', 'keras', 'word-embedding']","My model is:And I have it trained. But now during inference time, how can I use that embedding?"
1676,Use Word2Vec to build a sense embedding,"['python', 'gensim', 'word2vec', 'word-embedding']","I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each anchor with the respective lemma1_tag1 (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format lemma1_tag1 because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!"
1677,Concatenate two inputs of different dimension at a specific index in a sequence in Keras,"['python', 'machine-learning', 'keras', 'lstm', 'word-embedding']","I am trying to train an LSTM using two types of embedding layers. Let's say that the following is my tokenized sentence:Now for the words surrounding the ""TARGET_TOKEN"" I have an embedding layer which is (vocab_size x 128) while for the token 'TARGET_TOKEN' at index 4 I have an embedding layer which is (vocab_size x 512). So I need to transform the TARGET_TOKEN embedding from 512 to 128 and then insert this 128 dimension vector at index 4 (this index will change depending on the feature) of the output from the surrounding words embedding layer before feeding this concatenated list (tensor) to the LSTM. In my situation the positioning of the words/tokens is very important and so I do not wish to lose the position of where the token 'TARGET_TOKEN' is in the sentence.Initially I was looking on how to reduce the size of the 512 embeddings and I found that with numpy I could take the average of every 4 adjacent vectors and thus I end up from 512 dimensions to 128 dimensions. However, it is to my understanding that this might not represent the vectors in the right way anymore. Let's call the token 'TARGET_TOKEN' as ""target_token"" and the rest of the words as ""context_tokens"". So instead after further reading I thought could take the output of the target_token embedding layer and pass it through a Dense layer with 128 units (thus reducing its size to 128). Following this I will concatenate the output of the Dense layer with the output of the context_tokens embedding layer. So far I know how to do this. My issue is that positioning is important and it is important that my LSTM learns the target_token embedding with respect to its surrounding context. So long-story-short I need to concatenate at index 4 (maybe I'm looking at this the wrong way but that's how I understand it). However, the concatenate layer in Keras does not have such a parameter and I can only concatenate the two layers without taking into consideration the positioning.My model will take three inputs:and one output (as a sequence).My code looks like this:My expected result should be the following, here the numbers represent the dimensions of the the token vectors.Notice how at index 4 the embedding went from 512 to 128. I am looking into the possibility of transforming the tensor into a list, inserting the output of the target_token_embedding_layer into this list at the desired index and then transforming the list back to a tensor and using that tensor as input for the LSTM. However, I'm still trying to figure this out.Does anyone know how to do this? Any help would be greatly appreciated!"
1678,"Named Entity Recognition with learned word embeddings, LSTM, Keras","['keras', 'deep-learning', 'word-embedding', 'multiclass-classification']","I am trying to build a NER model which will help me classify words. I am stuck with getting the layer dimension mismatch since I am new to this space.My input are sentences - there are 43163 of them of length 70.
So each sentence has 70 words (after padding/truncating) with a total of 43163 of them. 
My X_train hence has the shape of (43163, 70)My y_train has the shape of (43163, 70, 17)
where each word has been converted to a one-hot encoding of length 70 (which corresponds to the NER tag it belongs to)The words are tokenized and there are a total of 35173 tokensI wish to convert these to embeddings which will be learned (I don't want to use word2vec and Glove standard embeddings)
These embeddings now need to be fit into an LSTM which can then be fed to a DenseLayer finally giving me classification.I have tried to look through these blogs (you will see my code looks pretty similar) to no avail - https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/This is the error I get - ValueError: Error when checking target: expected dense_10 to have 2 dimensions, but got array with shape (43163, 70, 17)Please help me understand where I am going wrong or how to fix this ?"
1679,"Read GloVe pre-trained embeddings into R, as a matrix","['r', 'nlp', 'word-embedding', 'text2vec', 'glove']","Working in R. I know the pre-trained GloVe embeddings (e.g., ""glove.6B.50d.txt"") can be found here: https://nlp.stanford.edu/projects/glove/. However, I've had zero luck reading this text file into R so that the product is the word embedding matrix of words by vectors. Has anyone successfully done this, either pulling from a saved .txt file or from the site itself, and if so how was that text converted to a matrix in R?"
1680,What must be the output dim for word embedding in keras?,"['keras', 'deep-learning', 'word-embedding']",What must be the dimension of the dense embedding? How can we set the value of output_dim in keras for word_embedding?
1681,What's the major difference between glove and word2vec?,"['machine-learning', 'nlp', 'word2vec', 'word-embedding', 'glove']","What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?"
1682,Using subword information in OOV token from fasttext in word embedding layer (keras/tensorflow),"['tensorflow', 'keras', 'word-embedding', 'fasttext']","I have my own Fasttext model and trained with it a keras classification model with a word embedding layer.But, I wonder how I can make use of the subword information of my model for OOV words? Since the word embedding layer operated via indices to look up word vectors and OOV words have no index. Even if a OOV token has a index how would I assign it the proper word vector to this OOV on the fly for an already trained model?Thanks in advance!"
1683,LDA2Vec Python implementation example?,"['python', 'word2vec', 'lda', 'word-embedding']","Hi can anyone please help me with the working example of LDA2Vec using python?
Please assume dataframe df having a column ""Notes"" containing text dataI am trying to implement ""cemoody/lda2vec"" github example but getting multiple issues- 
1. how to install spacy package?
2. ImportError: cannot import name 'preprocess' from 'lda2vec'
3. ImportError: cannot import name 'LDA2Vec' from 'lda2vec'Not sure what I am missing here.Any help/links will be really appreciated"
1684,Operation of type Placeholder X is not supported on the TPU. Execution will fail if this op is used in the graph,"['python', 'tensorflow', 'word-embedding', 'tpu', 'bert-language-model']","I am running text classification task using BERT on TPU.
I have used different tutorials to run my experiments as 1, 2, 3, and 4.
My only difference with the second example was that my dataset was not one of the predefined datasets in Bert processors, so I had to load it and preprocess it myself. Also, I wanted to make some changed in the create_model so I had to write it down as follow:When I run the code, with my create_model I see many errors while there is not problem with the training and everything goes well, but I'm not sure whether: 1. the model is using TPU or not because of the following errors, 2. the model is using Bert and fine tune it because of the following errors for all of the create_model function. Any idea?
Here is the error ( millions of times for all functions):Operation of type Placeholder X is not supported on the TPU. Execution will fail if this op is used in the graph."
1685,How can I train the word2vec model on my own corpus in R?,"['r', 'word2vec', 'word-embedding', 'natural-language-processing']","I would like to train the word2vec model on my own corpus using the rword2vec package in R. The word2vec function that is used to train the model requires a train_file. The package's documentation in R simply notes that this is the training text data, but doesn't specify how it can be created. The training data used in the example on GitHub can be downloaded here:
http://mattmahoney.net/dc/text8.zip. I can't figure out what type of file it is. I've looked through the README file on the rword2vec GitHub page and checked out the official word2vec page on Google Code.My corpus is a .csv file with about 68,000 documents. File size is roughly 300MB. I realize that training the model on a corpus of this size might take a long time (or be infeasible), but I'm willing to train it on a subset of the corpus. I just don't know how to create the train_file required by the function."
1686,Training a Neural Network for Word Embedding,"['keras', 'deep-learning', 'nlp', 'word-embedding']","Attached is the link file for Entities. I want to train a Neural Network to represent each entity into a vector. Attach is my code for trainingThe following error encountered when I am trying to execute the code.Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 34826 arrays:"
1687,NameError with tensor flow: Why isn't this working?,"['python', 'tensorflow', 'word-embedding']","I'm trying to get this code running for hours but I could fix the problem. 
Does anyone know how I could fix ist? It throws a NameError: name 'embedding_weights' is not defined, but I have already defined it.Thanks a lot! "
1688,Load a part of Glove vectors with gensim,"['python', 'gensim', 'word-embedding', 'glove']","I have a word list like['like','Python']and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? What I triedI iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.I also tried below, but it loaded the whole file instead of vectors I needWhat I wantMethod like gensim.models.keyedvectors.KeyedVectors.load_word2vec_format but I can set a word list to load."
1689,Why the fixed order of vocabulary in word embedding matter?,"['python', 'pytorch', 'word-embedding']","When I went through this Pytorch word embedding turtorial recently, I noticed that the order of vocabulary will have an influence on the predicting result.Here's an example code to explain the problem, which is modified from Robert Guthrie's previous code.If the sorted() function of the vocab variable is removed, the result will be different. Since I already fixed the Pytorch's random seed, why the result is irreproducible?"
1690,Casing removable from word embeddings?,"['nlp', 'word-embedding', 'fasttext']","Many words have multiple vectors because they are used with multiple casings. It seems to be quite unusual, though, that a word's casing alters its meaning. Can I combine vectors such that there's only one vector per word?I was thinking of using a weighted geometric midpoint. The weighting would be a simple linear transformation that depends on the different word frequencies. How could I show that the new vector is valid, or that it is a good representation? I'm sure there must be other approaches to this."
1691,keras - evaluate_generator yielding different accuracy rates with same training data,"['python', 'machine-learning', 'keras', 'deep-learning', 'word-embedding']","TL;DR My model is trained for 1 epochs - for testing purposes. Yet, when evaluated multiple times, it yields a different accuracy every time I run evaluate_generator method with the same training data. Why does that happen, and is there any way to get the same accuracy rate when evaluating the same trained data on the same model, multiple times?I am working on the linguistic problem of dialogue act classification and my model is based on this paper. Using tools provided by keras and keras_contrib repositories, I am replicating the exact model, but I have a question on why the evaluation gives out a different accuracy rate.For reference, I trained the model for a single epoch, and then saved the trained model in a file, using the utility save_load_utils provided by keras_contrib module. However, whenever I run the model with those weights, which were trained for a single epoch, I am getting a different accuracy rate. I have tried it for 5-10 times and it ranges between 68% to 74%, which is rather large. As I am loading the pre-trained (i.e. for 1 epoch) model weights, I am expecting to get the same accuracy. (i.e. short of any precision differences of floating-point numbers) However, the variance in the results at this rate suggest that I may have done something incorrectly.Does anyone have any idea as to why the model.evaluate_generator method generates results that are so different each time I run it with the same weight, even though I use the same, 1-epoch-trained model's weights to evaluate it? Is there any way to fix my evaluation code so that the accuracy obtained for the same trained model is the same every time I evaluate? (i.e. factoring in any minor differences due to floating-point arithmetic)Below is all the relevant code. The code sample is a little more lengthy compared to a standard StackOverflow question, but I wanted to include all the relevant portions of the code. Apologies to Python programmers for the length of the code. I am a novice Python programmer and I probably could have coded the entire thing in a more concise, Python-idiomatic way.Model preparation coda:Batch preparation functions:Training function:Evaluation function:You may navigate here for a fuller perspective of the graduate thesis project I am working on, but I tried to provide all the relevant bits of functionality required for anyone that can help out."
1692,How to get similar words in pre trained ELMO embedding?,"['keras', 'deep-learning', 'nlp', 'word-embedding', 'elmo']","How to get similar word for a given word in the pre trained ELMO Embedding? For Example: In Glove, we have glove_model.most_similar() to find the most similar word and its embedding for any given word. 
Similarly do we have anything in ELMO?"
1693,keras - embedding layer mask_zero causing exception at subsequent layers,"['python', 'machine-learning', 'keras', 'keras-layer', 'word-embedding']","I am working on a model based on this paper and I am getting an exception due to GlobalMaxPooling1D layer not supporting masking. I have an Embedding layer with mask_zero argument set to True. However, since a subsequent GlobalMaxPooling1D layer does not support masking, I am getting an exception. The exception is expected, as it is actually stated in the documentation of the Embedding layer that any subsequent layers after an Embedding layer with mask_zero = True should support masking.However, as I am processing sentences with variable number of words in them, I do need the masking in the Embedding layer. (i.e. due to the varying length of input) My question is, how should I alter my model that masking remains a part of the model, and does not cause a problem at GlobalMaxPooling1D layer?Below is the code for the model."
1694,How to evaluate performance of Co-occurrence matrix,"['nlp', 'word2vec', 'word-embedding']","I am learning the application of co-occurrence matrix as an alternative to Word2Vec. This paper talks about how to improve the performance of co-occurrence matrix word embedding, and shows us the tables and graphs to visualize the improved performance of his model.But I don't understand how the evaluation is done. His evaluation methods are explained in page 10 and 11. Can anyone explain how the values for Table 8, Table 9 and Figure 2 in this paper are obtained?"
1695,Encoding problem while training my own Glove model,"['python', 'encoding', 'nlp', 'word-embedding', 'glove']","I am training a GloVe model with my own corpus and I have troubles to save it/load it in an utf-8 format.Here what I tried: The saved file glove.model.txt is unreadable and I can't succeed to save it with a utf-8 encoding.When I try to read it, for exemple by converting it in a Word2Vec format:I have the following error:Any idea on how I could use my own GloVe model ?  "
1696,What are some of the data preparation steps or techniques one needs to follow when dealing with multi-lingual data?,"['nlp', 'word-embedding']","I'm working on multilingual word embedding code where I need to train my data on English and test it on Spanish. I'll be using the MUSE library by Facebook for the word-embeddings.
I'm looking for a way to pre-process both my data the same way. I've looked into diacritics restoration to deal with the accents. I'm having trouble coming up with a way in which I can carefully remove stopwords, punctuations and weather or not I should lemmatize. How can I uniformly pre-process both the languages to create a vocabulary list which I can later use with the MUSE library."
1697,How can I count word frequencies in Word2Vec's training model?,"['python', 'word2vec', 'word-embedding', 'word-frequency', 'natural-language-processing']",I need to count the frequency of each word in word2vec's training model. I want to have output that looks like this: Is it possible to do that? How would I get that data out of word2vec? 
1698,How to cluster similar sentences using BERT,"['python', 'nlp', 'artificial-intelligence', 'word-embedding', 'bert-language-model']","For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.A good example of the implementation can be seen in this short article: http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?Any information would be helpful."
1699,Is there a way to increase dimensionality of pre-trained Word Embeddings?,"['machine-learning', 'neural-network', 'nlp', 'word-embedding']","I am almost newly exposed to NLP research, struggling with NLP and Machine Learning techniques that are used in NLP.The question that I'm dealing with now is if there is some method to increase the dimensionality of pre-trained word embeddings (like GloVe embeddings) from a fixed size 100 to let's say 512? The reason I'm asking such question is that I used these embeddings to train RNN network with a pre-defined dimension of 100. Now, I have switched to self-attention mechanism (Transformers) where the model is highly sensitive to training parameters. So, I was wondering if I could somehow (for example, using perceptron, or maybe MLP) to transform 100d embeddings to a new space with 512d. I googled this before asking here, but ended up with finding no reliable source in the end."
1700,Semantic similarity between words A and B : Dependency on frequency of A and B in corpus?,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","Background :Given a corpus I want to train it with an implementation of word2wec (Gensim). Want to understand if the final similarity between 2 tokens is dependent on the frequency of A and B in the corpus (all contexts preserved), or agnostic of it.Example: 
(May not be ideal, but using it to elaborate the problem statement) Suppose word 'A' is being used in 3 different contexts within the corpus : 'B' is being used in 2 different contexts :Question : If I change the frequency of 'A' in my corpus (ensuring no context is lost, i.e. 'A' is still being used at least once in all the contexts as in the original corpus), is the similarity between A snd B going to be the same ?New distribution of 'A' across contextsAny leads appreciated"
1701,Keras input specification for word2vec vectors,"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']","I read all the other answers regarding this topic, but my use case is slightly different.I have a numpy array of shape (800,128,1). Each element in the 800 elements stores a word2vec embedding of shape (128,1). Now I wanted to send this as input with batch size 64 to a Keras model with first layer as Input layer. I'm getting the following error:I understand that the input layer requires 2 dimensions, but which two? Or should I specify input shape on my own as three dimensional?The input layer is currently this:"
1702,How to specify an input with a list of arrays to Embedding layer in Keras?,"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']","I'm trying to do some word-level text generation and stuck with the foloowing problem:My input looks like this:So, I'm going along the sequence (encoded words usnig word2vec) with the rolling window of the fixed size (tokenized _seq is a list of sequence with fixed length).Look the example:  Code block:Output:So, then, I'm trying to input all above to Embedding layer.Embedding parameters are:avg_sent_len is the len of each seqence in x_seqThe model compiles well, but when fitting I get the following error:(31412,) is vocab_size
223396 is x_seq or y_seq length (number of input sequences)
So, could anybody help me?"
1703,How to input the data set in for doc2vec in Keras and train,"['keras', 'out-of-memory', 'gensim', 'word-embedding', 'doc2vec']","I have a data-set that contains search queries terms and the ids of the users that typed/searched for those queries.
I want to get a user-embedding (user2vec) sort of thing to learn to  the type of thing the users search for.Previously, I merged all the queries for each user id and tokenized it and tagged it using gensim.doc2vec Tagged function.when, I tried using gensim.doc2vec to learn the embedding i kept getting memory error each time I run it.I want to do it this time with Keras but don't know how input my data and how to do it.I want to output to show a matrix m by n
where m, is the number of users 
n, is the dimension of the embedding(30)"
1704,How to combine different sets of word embeddings?,"['python', 'scikit-learn', 'nlp', 'word-embedding']","I am building a semantic relations classifier.Example of ""cause-effect"" relation: ""Zinc is essential for growth and cell division.""For this I am using pretrained word embeddings from Spacy as I don´t have anough data to train my own. I took two approaches:I am using a multilayer perceptron with one hidden layer of 150 nodes.Here is the code of the first approach:Here´s the code of the second approach:I tried adding sentences_vec and entities_vec but that threw even worse results on test data. What would be a way to improve the model?"
1705,Text classification: value error couldn't convert str to float,"['scikit-learn', 'word-embedding']","Input for random forest classifier trained model for text classification I am not able to know what should be the input for the trained model after opening the model from the pickle file.Expected output: Non politicalActual output : ValueError: could not convert string to float: 'RT @ScotNational The
  witness admitted that not all damage inflicted on police cars was
  caused"
1706,Limited range for TensorFlow Universal Sentence Encoder Lite embeddings?,"['tensorflow', 'word-embedding', 'tensorflow.js', 'tensorflowjs-converter']","Starting from the universal-sentence-encoder in TensorFlow.js, I noticed that the range of the numbers in the embeddings wasn't what I expected.  I was expecting some distribution between [0-1] or [-1,1] but don't see either of these.For the sentence ""cats are great!"" here's a visualization, where each dimension is projected onto a scale from [-0.5, 0.5]:Here's the same kind of visualization for ""i wonder what this sentence's embedding will be"" (the pattern is similar for the first ~10 sentences I tried):To debug, I looked at whether the same kind of thing comes up in the demo Colab notebook, and it seems like it is.  Here's what I see if I see for the range of the embeddings for those two sentences:And the output shows:So this again isn't what I'm expecting - the range is more narrow than I'd expect.  I thought this might be a TF convention that I missed, but couldn't see it in the TFHub page or the guide to text embeddings or in the paper so am not sure where else to look without digging into the training code.The colab notebook example code has an example sentence that says:Universal Sentence Encoder embeddings also support short paragraphs.
  There is no hard limit on how long the paragraph is. Roughly, the
  longer the more 'diluted' the embedding will be.But the range of the embedding is roughly the same for all the other examples in the colab, even one word examples.I'm assuming this range is not just arbitrary, and it does make sense to me that the range is centered in zero and small, but I'm trying to understand how this scale came to be."
1707,How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?,"['nlp', 'word-embedding']","I have seen that NLP models such as BERT utilize WordPiece for tokenization. In WordPiece, we split the tokens like playing to play and ##ing. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words? "
1708,Output from elmo pretrained model,"['tensorflow', 'sentiment-analysis', 'word-embedding', 'tensorflow-hub', 'elmo']","I am working on sentiment analysis. I am using elmo method to get word embeddings. But i am confused with the output this method is giving. Consider the code given in tensor flow website: The embedding vectors for a particular sentence vary based on the number of strings you give. To explain in detail let  So x1[0] will not be equal to z1[0]. This changes as you change the input list of strings. Why is the output for one sentence depends on the other. I am not training the data. I am only using an existing pretrained model. As this is the case, I am confused how to convert my comments text to embeddings and use for sentiment analysis. Please explain.
Note :To get the embedding vectors I use the following code:  "
1709,Gensim doc2vec most similar gives unsupported operand type(s) error,"['machine-learning', 'nlp', 'gensim', 'word-embedding', 'doc2vec']","I am using a pre-trained  doc2vec model, when I try to find out most similar document to that of my sample document. It gives me  unsupported operand type(s) error.This gives me following error"
1710,Text classification with own word embeddings using Neural Networks in R,"['r', 'neural-network', 'nlp', 'word-embedding']","This is a rather lengthy one, so please bear with me, unfortunately enough the error occurs right at the very end...I cannot predict on the unseen test set!I would like to perform text classification with word embeddings (that I have trained on my data set) that are embedded into neural networks.
I simply have column with textual descriptions = input and four different price classes = target.For a reproducible example, here are the necessary data set and the word embedding:DF: https://www.dropbox.com/s/it0jsbv8e7nkryt/DF.csv?dl=0WordEmb: https://www.dropbox.com/s/ia5fmio2e0plwkr/WordEmb.txt?dl=0And here my code:Now here is where I get stuck:
I cannot use the results of the NN to predict on the unseen test data set:For now the actual accuracy does not really matter since this is only a small example of my data set.I know this is a long one but AAANNY help would be very muuuch appreciated."
1711,Is there an alternative to fully loading pre-trained word embeddings in memory?,"['python', 'machine-learning', 'memory-management', 'nlp', 'word-embedding']",I want to use pre-trained word embeddings in my machine learning model. The word embedings file I have is about 4GB. I currently read the entire file into memory in a dictionary and whenever I want to map a word to its vector representation I perform a lookup in that dictionary.The memory usage is very high and I would like to know if there is another way of using word embeddings without loading the entire data into memory.I have recently come across generators in Python. Could they help me reduce the memory usage?Thank you!
1712,Using pretrained glove word embedding with scikit-learn,"['python', 'keras', 'scikit-learn', 'word-embedding', 'glove']","I have used keras to use pre-trained word embeddings but I am not quite sure how to do it on scikit-learn model.I need to do this in sklearn as well because I am using vecstack to ensemble both keras sequential model and sklearn model.This is what I have done for keras model:I am very new to scikit-learn, from what I have seen to make an model in sklearn you do:So, my question is how do I use pre-trained Glove with this model? where do I pass the pre-trained glove embedding_matrixThank you very much and I really appreciate your help."
1713,how to store a very large variable in tensorflow?,"['tensorflow', 'distributed', 'word-embedding']","i have to training a very large embedding dict(about 1TB). I have many machines, so I can use distribute training of tensorflow.However, I found I cannot construct a very large variable in Parameter Server. Can tensorflow split the variable in different PS automatic?Thx."
1714,"Why no word embeddings (Glove, word2vecetc) used in first attention paper?","['nlp', 'word-embedding', 'machine-translation', 'attention-model']","In the paper Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau et. al. why are there no word embeddings such as Glove or word2vec used? I understand that this was a 2014 paper, but the current implementations of the paper on github don't use any word embeddings as well?For trying to code the paper is using word embeddings reasonable?"
1715,Document similarity with Word Mover Distance and Bert-Embedding,"['python', 'nlp', 'similarity', 'word-embedding']","I am trying to calculate the document similarity (nearest neighbor) for two arbitrary documents using word embeddings based on Google's BERT.
In order to obtain word embeddings from Bert, I use bert-as-a-service.
Document similarity should be based on Word-Mover-Distance with the python wmd-relax package.My previous tries are orientated along this tutorial from the wmd-relax github repo: https://github.com/src-d/wmd-relax/blob/master/spacy_example.pyUnfortunately, the calculations fails with a dimensions mismatch in the distance calculation:
ValueError: shapes (812,1,768) and (768,1,812) not aligned: 768 (dim 2) != 1 (dim 1)"
1716,Use word2vec word embeding as feature vector for text classification (simlar to count vectorizer/tfidf feature vector),"['machine-learning', 'scikit-learn', 'word2vec', 'text-classification', 'word-embedding']",I am trying to perform some text classification using machine learning and for that I have extracted feature vectors from the per-processed textual data using simple bag of words approach(count vectorizer) and tfidf vectorizer.  Now I want to use word2vec i.e. word embedding as my feature vector similar as that of count vectorizer/tfidf vectorizer where I should be able to learn vocabulary from the train data and transform or fit the test data with the learned vocab but I can't find a way to implement that.  
1717,Add words as label for word embedding,"['tensorflow', 'visualization', 'tensorboard', 'word-embedding']","I'm trying to visualize my word embedding with tensorboard, but can't solve problem with label.
The problem is, that I have variable like this:and when I save it for next use in tensorboard:as label I see only index(1...vocabulary_size)When I print word_embeddings variable I see only numbers ie: How do I add the label ie: 
blue:  (0.01359, 0.00075997, 0.24608, ..., -0.2524, 1.0048, 0.06259) (https://www.tensorflow.org/guide/embedding )I'm not very familiar with this area so if I don't add here all the necessary information let me know.I do some search here but answer like this was not very helpful"
1718,Illegal Hardware Instruction Error when using GloVe,"['nlp', 'stanford-nlp', 'word-embedding', 'glove', 'illegal-instruction']","I am trying to train GloVe embeddings. In the GloVe implementation from stanfordnlp there are 4 scripts to run. However, running the second script, coocur, results in an Illegal Hardware Instruction-Error. I don't understand how this error is produced. With the input file 3.txt my commands look like this:I am running these commands on a remote server (Debian GNU/Linux 9 (stretch)). When I run the same commands on the same data locally (18.04.2 LTS (Bionic Beaver)), there is no problem. What could be the cause of this?"
1719,Keras - Issues using pre-trained word embeddings,"['keras', 'nlp', 'word2vec', 'word-embedding']","I'm following Keras tutorials on word embeddings and replicated the code (with a few modifications) from this particular one:Using pre-trained word embeddings in a Keras modelIt's a topic classification problem in which they are loading pre-trained word vectors and use them via a fixed embedding layer.When using the pre-trained embedding vectors I can, in fact, achieve their 95% accuracy. This is the code:The issue happens when I remove the embedding vectors and use completely random vectors, surprisingly achieving higher accuracy: 96.5%. The code is the same, with one modification: weighs=[random_matrix]. That's a matrix with the same shape of embed_matrix, but using random values. So this is the embedding layer now:I experimented many times with random weights and the result is always similar. Notice that even though those weights are random, the trainable parameter is still False, so the NN is not updating them.After that, I fully removed the embedding layer and used words sequences as the input, expecting that those weights were not contributing to the model's accuracy. With that, I got nothing more than 16% accuracy. So, what is going on? How could random embeddings achieve the same or better performance than pre-trained ones?And why using word indexes (normalized, of course) as inputs result in such a poor accuracy?"
1720,Why the FastText word embedding could generate the representation of a word from another language?,"['python', 'gensim', 'word-embedding', 'fasttext', 'natural-language-processing']","Recently, I trained a FastText word embedding from sentiment140 to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:It outputs:Hence, I really want to know why the FastText module trained from sentiment140 could do this. Thank you!"
1721,3 dimensional array as input with Embedding Layer and LSTM in Keras,"['tensorflow', 'keras', 'lstm', 'word-embedding']","Hey guys I have built an LSTM model that works and now I am trying(unsuccessfully) to add an Embedding layer as a first layer.This solution didn't work for me.
I also read these questions before asking:
Keras input explanation: input_shape, units, batch_size, dim, etc,
Understanding Keras LSTMs and keras examples.My input is a one-hot encoding(of ones and zeros) of characters of a language that consists 27 letters. I chose to represent each word as a sequence of 10 characters. Input size for each word is (10,27) and I have 465 of them so it's X_train.shape (465,10,27), I also have a label of size y_train.shape (465,1). My goal is to train a model and while doing that to build a character embeddings.Now this is the model that compiles and fits.After adding Embedding layer:output: ValueError: Input 0 is incompatible with layer bidirectional_31:  expected ndim=3, found ndim=4How do I fix the output shape?
Your ideas would be much appreciated."
1722,Extract relationship concepts from sentences,"['nlp', 'word2vec', 'word-embedding', 'information-extraction', 'relationship-extraction']","Is there a current model or how could I train a model that takes a sentence involving two subjects like: [Meiosis] is a type of [cell division]...and decides if one is the child or parent concept of the other? In this case, cell division is the parent of meiosis."
1723,keras loss and other custom mertic not being accurate,"['python', 'keras', 'deep-learning', 'word-embedding']","I have a standard Bidirectional LSTM model built with keras, nothing fancy, just a stack of layers:================================================================= embedding_1 (Embedding)      (None, 1697, 300)         9776100
  _________________________________________________________________ bidirectional_1 (Bidirection (None, 1697, 150)         225600
  _________________________________________________________________ bidirectional_2 (Bidirection (None, 1697, 60)          43440
  _________________________________________________________________ bidirectional_3 (Bidirection (None, 1697, 40)          12960
  _________________________________________________________________ last_blstm (Flatten)         (None, 67880)             0
  _________________________________________________________________ dense_1 (Dense)              (None, 5)                 339405My loss seems to be decreasing normally, but the same doesn't go for 11800/13500 [=========================>....] - ETA: 2:16 -loss: 107.6926 - acc: 0.2010
  12000/13500 [=========================>....] - ETA: 2:00 - loss: 106.0187 - acc: 0.2019
  12200/13500 [==========================>...] - ETA: 1:44 - loss: 104.4009 - acc: 0.2024
  12400/13500 [==========================>...] - ETA: 1:28 - loss: 102.8412 - acc: 0.2029
  12600/13500 [===========================>..] - ETA: 1:12 - loss: 101.3140 - acc: 0.2021 
  12800/13500 [===========================>..] - ETA: 56s - loss: 99.8337 - acc: 0.2021
  13000/13500 [===========================>..] - ETA: 40s - loss: 98.4028 - acc: 0.2022
  13200/13500 [============================>.] - ETA: 24s - loss: 97.0180 - acc: 0.2030 
  13400/13500 [============================>.] - ETA: 8s - loss: 95.6736 - acc: 0.2031
  13500/13500 [==============================] - 1141s 85ms/step - loss: 94.9990 - acc: 0.2029 - val_loss: 4.7779 - val_acc: 0.2193which is kind of strange, my dataset is balanced for each class and i'm not using any custom loss function, my optimizer is a standard SGD (also tried Adam, same results) and my batch size is 200Also, i tried adding custom metrics, the results are even more strange:I get the following output:Predictions:  [[0.1997815  0.19978242 0.20011796 0.19970864
  0.20060949]
  [0.19989796 0.20036548 0.19994123 0.19979696 0.19999835]  [0.20089898 0.19889246 0.19993879 0.19909698 0.20117281]  ... 
  [0.19948816 0.20006524 0.19998378 0.2000029  0.20045996]  [0.1996356 
  0.19995351 0.19997446 0.20012334 0.20031306]
  [0.20026998 0.19984405 0.20024066 0.19988723 0.19975808]]Val pre: [4 1 4 1 2 4 4 4 2 4]If you look closely at the last line, the softmax should output ""0"" as the predicted class as opposed to 4, it seems the class ""0"" generally is being missed.Any idea what's going on?
Thanks"
1724,Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?,"['python', 'nlp', 'pytorch', 'word-embedding', 'glove']","When using GloVe embedding in NLP tasks, some words from the dataset might not exist in GloVe. Therefore, we instantiate random weights for these unknown words.Would it be possible to freeze weights gotten from GloVe, and train only the newly instantiated weights?I am only aware that we can set:
model.embedding.weight.requires_grad = FalseBut this makes the new words untrainable..Or are there better ways to extract semantics of words.. "
1725,Does the Google News Word2Vec model take up storage every time you run it?,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","This may seem like an odd question but I'm new to this so thought I'd ask anyway.I want to use this Google News model over various different files on my laptop. This means I will be running this line over and over again in different Jupyter notebooks: model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)Does this eat 1) Storage (I've noticed my storage filling up exponentially for no reason) 
2) Less memory than it would otherwise if I close the previous notebook before running the next.My storage has gone down by 50GB in one day and the only thing I have done on this computer is run the Google News model (I didn't do most_similar()). Restarting and closing notebooks hasn't helped and there aren't any big files on the laptop. Any ideas?Thanks. "
1726,Using ELMO with Keras?,"['python', 'keras', 'neural-network', 'deep-learning', 'word-embedding']","so first I was using GloVe in my model it worked fine, but now I want to switch to Elmo but I always get that error: ValueError: A Concatenate layer requires inputs with matching shapes
  except for the concat axis. Got inputs shapes: [(None, 20), (None,
  20), (None, 20, 5)]Can you please help me ?here's a snippet of my code: please let me know if you need more details."
1727,Combining TF-IDF with pre-trained Word embeddings,"['nlp', 'spacy', 'tf-idf', 'word-embedding', 'tfidfvectorizer']","I have a list of website meta-description (128k descriptions; each with avg. 20-30 words), and am trying to build a similarity ranker (as in: show me the 5 most similar sites to this site meta description)It worked AMAZINGLY well with TF-IDF uni- and bigram, and I thought that I could additionally improve it by adding pre-trained word embeddings (spacy ""en_core_web_lg"" to be exact). Plot twist: it does not work at all. Literally did not get one good guess, and its suddendly spits out completely random suggestions. Below is my code. Any thoughts on where I might have gone wrong? Am I overseeing something highly intuitive? I stole parts of the code/logic from this Github Rep
Does anybody see any straightforward errors here? 
Many thanks!! "
1728,keras understanding Word Embedding Layer,"['python', 'tensorflow', 'keras', 'word-embedding']","From the page I got the below code:I also found below code at the page that could help with finding embedding of each word. But i dont know how to create word_to_indexword_to_index is a mapping (i.e. dict) from words to their index, e.g. love: 69
words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}Please ensure that my understanding of para # is correct.The first layer has 400 parameters because total word count is 50 and embedding have 8 dimensions so 50*8=400. The last layer has 33 parameters because each sentence has 4 words max. So 4*8 due to dimensions of embedding and 1 for bias. 33 total+++++++++++++++++++++++++++++++
update - providing the updated code"
1729,Can I use a 3D input on a Keras Dense Layer?,"['keras', 'text-classification', 'keras-layer', 'word-embedding', 'natural-language-processing']","As an exercise I need to use only dense layers to perform text classifications. I want to leverage words embeddings, the issue is that the dataset then is 3D (samples,words of sentence,embedding dimension). Can I input a 3D dataset into a dense layer?Thanks"
1730,Glove Word Embeddings supported languages,"['nlp', 'stanford-nlp', 'word-embedding']","I started experimenting with word embeddings, and I found some results which I don't know how to interpret. I first used an English corpus for both training and testing and afterwards, I used the English corpus for training and a small French corpus for testing (all corpora have been annotated for the same binary classification task). In both cases, I used the pre-trained on tweets Glove embeddings. As the results in the case where I also used the French corpus improved (by almost 5%, reaching ~accuracy = 0.8), I was wondering if Glove was trained on multilingual data.I haven't seen anyone making this statement, in contrast to FastText, for example, where you have embeddings for different languages."
1731,Cannot feed pre-train word embedding before training,"['python', 'tensorflow', 'word-embedding']","I want to load pre-train word embedding before training instead of loading it every train_steps. I follow the step in this post. 
But it will show error: You must feed a value for placeholder tensor 'word_embedding_placeholder' with dtype float and shape [2000002,300]Here is the roughly code:When I change feed_dict in training to:it works, but it is not elegant. Does anyone meet this issue?Goal: I want to load the pre-train embedding only once before training. Instead of recomputing embedding_init every time."
1732,Tensorflow Embedding for training and inference,"['tensorflow', 'nlp', 'word-embedding', 'machine-translation']",I am trying to code a simple Neural machine translation using tensorflow. But I am a little stuck regarding the understanding of the embedding on tensorflow :andIn which case should I use one to another ?
1733,Vectorizing new text data,"['python', 'vectorization', 'text-mining', 'word-embedding', 'natural-language-processing']","I have trained a Word2vec model on the ""brown corpus"". I want to apply the vectorized words to a new text document, whose sentences I then want to cluster by way Affinity Propagation.My text document contains a list of requests such as:My question is:How can I apply the vectorized Brown corpus to my own text data for subsequent clustering purposes?"
1734,How can I train a model that takes in the word vectors on each side and predicts the middle word?,"['python', 'lstm', 'word-embedding']","How can I train a model that takes in the word vectors on each side and predicts the middle word?I have created word embeddings of the 2 words preceding the middle word (word to replace) and the 2 words after the middle word. I would like to see code on how to feed this into an LSTM model, which I have seen only take in one vector to predict the word after."
1735,ELMo Embedding layer with Keras,"['python', 'keras', 'deep-learning', 'lstm', 'word-embedding']","I have been using Keras default embedding layer with word embeddings in my architecture. Architecture looks like this - I want to replace the embedding layer with ELMo embeddings. So I used a custom embedding layer - found in this repo - https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb. Embedding layer looks like this - I changed the architecture for the new embedding layer. But I am getting error - ValueError: Input 0 is incompatible with layer lstm: expected ndim=3, found ndim=2What am I doing wrong here? "
1736,"In tf.keras.layers.Embedding, why it is important to know the size of dictionary?","['tensorflow', 'word-embedding']","Same as the title, in tf.keras.layers.Embedding, why it is important to know the size of dictionary as input dimension?"
1737,Keras extending embedding layer input,"['python', 'tensorflow', 'keras', 'word-embedding']","A keras sequential model with embedding needs to be retrained starting from the currently known weights.A Keras sequential model is trained on the provided (text) training data. The training data is tokenized by a (custom made) tokenizer. The input dimension for the first layer in the model, a embedding layer, is the number of words known by the tokenizer. After a few days additional training data becomes available. The tokenizer needs to be refitted on this new data as it may contain additional words. That means that the input dimension of the embedding layer changes, so the previously trained model is not usable anymore.I want to use the previously trained model as initializer for the new training session. For the new words in the tokenizer, the embedding layer should just use a random initialization. For the words already known by the tokenizer, it should use the previously trained embedding."
1738,Why use cosine similarity in Word2Vec when its trained using dot-product similarity,"['nlp', 'word2vec', 'cosine-similarity', 'word-embedding', 'dot-product']","According to several posts I found on stackoverflow (for instance this Why does word2Vec use cosine similarity?), it's common practice to calculate the cosine similarity between two word vectors after we have trained a word2vec (either CBOW or Skip-gram) model. However, this seems a little odd to me since the model is actually trained with dot-product as a similarity score. One evidence of this is that the norm of the word vectors we get after training are actually meaningful. So why is it that people still use cosine-similarity instead of dot-product when calculating the similarity between two words?"
1739,A classifier trained using the technique of word embedding(doc2vec) and logisitic regression misclassify the data,"['logistic-regression', 'word-embedding', 'doc2vec']","I have a text classification problem in which the data set consist of 16 million records. The data is highly imbalanced and consist of almost 500 classes. I took the approach for word embedding and then used Logistic Regression to build a model in which the input is doc2vec matrix, the accuracy that i achieved was 88% but and recall score and F1-score is closet to 84% but when working on testing data the classifier doesn't perform well for example,
    if a text details are like i love traveling it is classified under Travel Tag, but if it encounters the text again the model classify it into some different category like unknown. This an unusual behavior that i have encountered in the model.Code:Output Expected:
            TravelActual Output:
            UnmappedRegressor code:class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight=""balanced"")logreg.fit(train_vectors_dbow, y_train)Expected result:  Actual"
1740,Getting the document-per-topic loading using TextmineR package by passing term co-occurrence matrix,"['r', 'text-mining', 'word-embedding']","I am using TextmineR package to find the most similar documents to given document list. I used the following code to generate the tcm not dtm Which is used to fit a lda model:Now the model parameter theta here generates word-per-topic loading rather than document-per-topic loading. I want to retrieve  the document number from the document-per-topic loading. Please help in suggesting the method to obtain the document-per-topic distribution from this model while passing term co-occurrence matrix. I have tried to back connect to get document number from document-per-topic loading, but not successful as per the guidelines given at  https://cran.r-project.org/web/packages/textmineR/vignettes/d_text_embeddings.html"
1741,Better way to combine Word embedding to get embedding of a sentence,"['deep-learning', 'nlp', 'text-processing', 'word-embedding']","I have seen in many kaggle kernels and tutorials, average word embeddings is considered to get embedding of a sentence. But, i am wondering if this is a correct approach.Since it discards the positional information of the words in the sentence. is there a better way to combine embedding? maybe hierarchically combining them in a particular way?"
1742,Load word2vec dictionary into gensim,"['nlp', 'gensim', 'word2vec', 'spacy', 'word-embedding']","I've loaded pretrained word2vec embeddings into a python dictionary of the form{word: vector}As an example, an element of this dictionary isw2v_dict[""house""] = [1.1,2.0, ... , 0.2]I would like to load this model into Gensim (or a similar library) so that I can find euclidean distances between embeddings.I understand that pretrained embeddings typically come in a .bin file which can be loaded into Gensim. But if I only have a dictionary of this form, how would I load the vectors into a model?"
1743,tf.nn.static_rnn - input must be a sequence,"['python', 'python-3.x', 'tensorflow', 'lstm', 'word-embedding']","I'm trying to create a 2 layer lstm (incl. dropout) but get an error message that 'inputs must be a sequence'. I use embeddings as the input and not sure how to change these to be a sequence? Any explanations are greatly appreciated. This is my graph definition: EDIT: 
when changing static_rnn to dynamic_rnn the error message changes to the following, failing on the bias (b) variable: After I changed the bias term to this:and get a new error message: "
1744,T-SNE visualisation on list of word vectors,"['python', 'scikit-learn', 'nlp', 'data-visualization', 'word-embedding']","I have a list of ~20k word vectors ('tuple_vectors'), with no labels, each looks like the belowis there a quick, concise way to visualise using t-sne? I've tried with the following"
1745,Large trainable embedding layer slows down training,"['tensorflow', 'optimization', 'word-embedding']","I am training a network to classify text with a LSTM. I use a randomly initialized and trainable embedding layer for the word inputs. The network is trained with the Adam Optimizer and the words are fed into the network with a one-hot-encoding.I noticed that the number of words which are represented in the embedding layer influences heavily the training time, but I don't understand why. Increasing the number of words in the network from 200'000 to 2'000'000 almost doubled the time for a training epoch.Shouldn't the training only update weights which where used during the prediction of the current data point. Thus if my input sequence has always the same length, there should always happen the same number of updates, regardless of the size of the embedding layer."
1746,How do I get word embedding using CoreNlp from Stanford?,"['java', 'vectorization', 'stanford-nlp', 'word-embedding']","I am using CoreNlp to get the information extraction from a large text. However, its using the ""triple"" approach where a single sentence produce many output which is good, but there are some sentences that doesn't make sense. I tried to eliminate this by running another unsupervised NLP and try to utilize function in CoreNlp, yet I stuck at getting word vector form CoreNlp. Can anyone point where do I need to start searching for codes that do the word embedding in CoreNlp? Also I am newbie in java and IT. There are some open libraries like glove, word2vec, text2vec, but I noticed glove already been used in CoreNlp (correct me if wrong). "
1747,How to train millions of doc2vec embeddings using GPU?,"['deep-learning', 'nlp', 'word-embedding', 'chainer']","I am trying to train a doc2vec based on user browsing history (urls tagged to user_id). I use chainer deep learning framework. There are more than 20 millions (user_id and urls) of embeddings to initialize which doesn’t fit in a GPU internal memory (maximum available 12 GB). Training on CPU is very slow.I am giving an attempt using code written in chainer given here
https://github.com/monthly-hack/chainer-doc2vecPlease advise options to try if any."
1748,Normalize vectors in gensim model,"['nlp', 'gensim', 'word-embedding']","I have a pre-trained word embedding with vectors of different norms, and I want to normalize all vectors in the model. I am doing it with a for loop that iterates each word and normalizes its vector, but the model us huge and takes too much time. Does gensim include any way to do this faster? I cannot find it.Thanks!!"
1749,Customizg loss function in Word2vec,"['keras', 'nlp', 'gensim', 'word2vec', 'word-embedding']",I have list of co-occurences and I want to train word2vec model with my own customized loss_function. What is the best way to approach this?Thanks!
1750,How to convert spark mllib word2vec model to glove txt format?,"['apache-spark', 'apache-spark-mllib', 'word-embedding', 'glove']","I use Spark MLlib to train a domain specific word2vec model and I need to use it in glove word2vec format. 
How can I convert it to glove txt format?"
1751,how to reduce the dimension of the document embedding?,"['tensorflow', 'machine-learning', 'deep-learning', 'nlp', 'word-embedding']","Let us assume that I have a set of document embeddings. (D) 
Each of document embedding is consisting of N number of word vectors where each of these pre-trained vector has 300 dimensions.The corpus would be represented as [D,N,300].My question is that, what would be the best way to reduce [D,N,300] to [D,1, 300]. How should I represent the document in a single vector instead of N vectors?Thank you in advance."
1752,"NN in Keras - expected dense_2 to have 3 dimensions, but got array with shape (10980, 3)","['python', 'tensorflow', 'keras', 'neural-network', 'word-embedding']","I want to train a Neutral Network for Multi-Classification Sentiment Analysis using word embedding for tweets.Here is my code:Originally the labels are of type string: 'neutral', 'positive', 'negative'. So I first transform them to integer and then apply one-hot encoding:The reason I chose my last layer to have 3 output units is because it's a multi-classification task and I have 3 classes.Here is the model summary:When the code gets to model.fit(), I get the following error:What am I doing wrong?"
1753,Adding metadata to words in word2vec,"['nlp', 'gensim', 'word2vec', 'word-embedding']","I have a word2vec model and I want to change it by adding some additional data beside the occurrence of the word itself.For example:Category (out of predefined 50), POS etc.I thought of two ways to do it:So my questions are:
1. What will be a better way?
2. How can I create a new loss function and optimize it in Word2Vec? Can I just pass a parameter to Gensim's Word2Vec or do I need to build a new Word2vec model from scratch?"
1754,Some diverging issues of Word2Vec in Gensim using high alpha values,"['python-3.x', 'gensim', 'word2vec', 'word-embedding']","I am implementing word2vec in gensim, on a corpus with nested lists (collection of tokenized words in sentences of sentences form) with 408226 sentences (lists) and a total of 3150546 words or tokens. I am getting a meaningful results (in terms of the similarity between two words using model.wv.similarity) with the  chosen values of 200 as size, window as 15, min_count as 5, iter as 10 and alpha as 0.5. All are lemmatized words and these all are input to models with vocabulary as 32716.The results incurred from default alpha value, size, window and dimensions are meaningless for me based on the used data in computing the similarity values. However higher value of alpha as 0.5 gives me some meaningful results in terms of inducing meaningful similarity scores between two words. However, when I calculate the top n similar words, it's again meaningless. Does I need to change the entire parameters used in the initial training process.I am still unable to reveal the exact reason, why the model behaves good with such a higher alpha value in computing the similarity between two words of the used corpus, whereas it's meaningless while computing the top n similar words with scores for an input word. Why is this the case?Does it is diverging towards optimal solution. How to check this?Any idea why is it the case is deeply appreciated.Note: I'm using Python 3.7 on Windows machine with anaconda prompt and giving input to the model from a file.This is what I have tried.The example of Sentences list is as follows:[['scientist', 'time', 'comet', 'activity', 'sublimation', 'carbon', 'dioxide', 'nears', 'ice', 'system'], ['inconsistent', 'age', 'system', 'year', 'size', 'collision'], ['intelligence', 'system'], ['example', 'application', 'filter', 'image', 'motion', 'channel', 'estimation', 'equalization', 'example', 'application', 'filter', 'system']] The data_d1.txt and data_d2.txt is a nested list (list of lists of lemmatized tokenized words). I have preprocessed the raw data and save it in a file. Now giving the same as input. For computing the lemmatizing tokens, I have used the popular WordNet lemmatizer. I need the word-embedding model to calculate the similarity between two words and computing the most_similar words of a given input word. I am getting some meaningful scores for the model.wv.similarity() method, whereas in calculating the most_similar() words of a word (say, system as shown in above). I am not getting the desired results. I am guessing the model is getting diverged from the global minima, with the use of high alpha values. I am confused what should be the dimension size, window for inducing some meaningful results, as there is no such rules regarding how to compute the the size and window. Any suggestion is appreciated. The size of total sentences and words are specified above in the question. Results what I am getting without setting alpha = 0.5Edit to Recent Comment:Results:Word2Vec(vocab=32716, size=200, alpha=0.025)The similarity between set and _set_ is : 0.000269373188960656
which is meaningless for me as it is very very less in terms of accuracy, But, I am a getting 71% by setting alpha as 0.5, which seems to be meaningful for me as the word set is same for both the domains. Explanation: The word set should be same for both the domains (as I am comparing the data of two domains with same word). Don't get confused with word _set_, this is because the word is same as set, I have injected a character _ at start and end to distinguish the same for two different domains. The top 10 words along with scores of _set_ are:Whereas, the top 10 words for set are:Why the cosine similarity value is 0.00 for the word set for two different data."
1755,Value of alpha in gensim word-embedding (Word2Vec and FastText) models?,"['python-3.x', 'gensim', 'word2vec', 'word-embedding', 'fasttext']","I just want to know the effect of the value of alpha in gensim word2vec and fasttext word-embedding models? I know that alpha is the initial learning rate and its default value is 0.075 form Radim blog.What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.Why this imprecise result for same data with two popular models with different precision? Does the value of alpha plays such a crucial role during building of the model?Any suggestion is appreciated."
1756,Concatenate char embeddings and word embeddings,"['python', 'machine-learning', 'keras', 'nlp', 'word-embedding']","I want to use char sequences and word sequences as inputs. Each of them will be embedded its related vocabulary and then resulted embeddings will be concatenated. I write following code to concatenate two embeddings:When I execute the code, I have the following error:I defined input_shape for each embedding, but I still have same error. How can I concatenate two sequential model?"
1757,"Value Error problem with multicell Dimensions must be equal, but are 20 and 13","['python', 'tensorflow', 'multidimensional-array', 'valueerror', 'word-embedding']","I am working with python 3.6.5 and tensorflow 1.8.0
Nr of neurons are 10 at the moment, input in this example is 3 I have already build a recurrent neuronal network and now wanted to improve it. I need some help!Here is a little excerpt of the code to reproduce my error: You can also replace BasicRNN by LSTM or GRU to get the other messages. I was trying this neurons = tf.contrib.rnn.MultiRNNCell([neurons]*nr_layers, state_ist_tuple = True)and received the errorandis there an error in the MatMul_1? Has anyone ever had similar problems? 
Thank you so much!"
1758,What does the embedding layer for a network looks like?,"['machine-learning', 'keras', 'nlp', 'word-embedding']","I just start with text classification, and I got stuck in the embedding layer. If I have a batch of sequences encoded as integer corresponding to each word, what does the embedding layer looks like? Is there neurons like normal neural layer? I've seen the keras.layers.Embedding, but after looking for the document I'm really confused about how does it works. I can understand input_dim, but why is output_dim a 2D matrix? How many weights do I have in this embedding layer?I'm sorry if my question is not explained clearly, I've no experience in NLP, if this problem about word embedding is common basics in NLP, please tell me and I will check for it."
1759,How to change the tensor shape in middle layers?,"['tensorflow', 'keras', 'nlp', 'embedding', 'word-embedding']","Saying I have a 2000x100 matrix, I put it into 10 dimension embedding layer, which gives me 2000x100x10 tensor. so it's 2000 examples and each example has a 100x10 matrix. and then, I pass it to a conv1d and KMaxpolling to get 2000x24 matrix, which is 2000 examples and each example has a 24 dimension vector. and now, I would like to recombine those examples before I apply another layer. I would like to combine the first 10 examples together, and such and such, so I get a tuple. and then I pass that tuple to the next layer.
My question is, Can I do that with Keras? and any idea on how to do it?   "
1760,Keras google word2vec CNN model InvalidArgumentError,"['python', 'keras', 'deep-learning', 'word2vec', 'word-embedding']","I built a text classification model for imbalanced class classification data. Instead of using keras word vector, i used embedding by using googlenews word2vec vector as the baseline in the embedding layer.Since this is a class imbalance problem, i used f1 metric.I processed text and created word vector as belowI use this function to convert an array of text to number values from word2vec model.I created the matrixAnd replace 0 with index values from word2vec model against corresponding 
tokenized words, upto maximum of 50 words.I computed class weights using sklearn function since this is a class imbalance problem.This is the function to create multiConvnet model.My model is belowwhen i run this, i am getting below error.I did read this post InvalidArgumentError (see above for traceback): indices[1] = 10 is not in [0, 10)As per this post i need to set vocabulary. In my case, this is exactly what i have done by using the parameter vocab_size."
1761,"Understanding word embeddings, convolutional layer and max pooling layer in LSTMs and RNNs for NLP Text Classification","['python', 'tensorflow', 'nlp', 'lstm', 'word-embedding']","Here is my input data:And here is the one hot encoded label (multi-class classification where the number of classes = 3)Here is what I think happens step by step, please correct me if I'm wrong:Converting my input text data['text'] to a bag of indices (sequences)What is happening is my data['text'].shape which is of shape (19579, ) is being converted into an array of indices of shape (19579, 50), where each word is being replaced by the index found in tokenizer.word_index.items()Loading the glove 100d word vectorSo what we have now are the word vectors for every word of 100 dimensions. Creating the embedding matrix using the glove word vector So we now have the a vector of 100 dimensions for EACH of the 20000 words. The And here is the architecture:I get The input to the above architecture will be the training dataof shape (19579, 50)and labels as one hot encodings..My trouble is understanding the following what exactly is happening to my (19579, 50) as it goes through each of the following lines:I understand why we need model_glove.add(Dropout(0.5)), this is to shut down some hidden units with a probability of 0.5 to avoid the model from being overly complex. But I have no idea why we need the Conv1D(64, 5, activation='relu'), the MaxPooling1D(pool_size=4) and how this goes into my model_glove.add(LSTM(100)) unit.."
1762,DeepPavlov elmo is too slow,"['python', 'deep-learning', 'nlp', 'word-embedding']","I tried to use ELMO embeddings (ElmoEmbedder) from DeepPavlov library. It works really slow, 64 second per 100 senteces.I tried to increase mini_batch_size, but it didn't speed up algorithm. Is it possible to speed up ElmoEmbedder?"
1763,How I can get vector from output matrix in FastText ?,"['word2vec', 'word-embedding', 'fasttext']","In this study author have found out that, Word2Vec generates the two kinds of embeddings(IN & OUT).https://arxiv.org/abs/1602.01137Well, you can easily get that using syn1 attribute in gensim word2vec. But in the case of gensim fastText, the syn1 do exists but as the concept of fastText is sub-word based, it's not possible to get a vector for word from output matrix by matching the indexes. Do you know any other way around to calculate vector using output matrix??"
1764,Unable to install genism on google colab,"['python', 'google-colaboratory', 'word-embedding']","I'm trying to insall genism on google colab instance using the following command:       But I'm getting an error:Could not find a version that satisfies the requirement genism (from versions: )
      No matching distribution found for genism"
1765,Should Kernel size be same as word size in 1D Convolution?,"['keras', 'deep-learning', 'conv-neural-network', 'word-embedding']","In CNN literature, it is often illustrated that  kernel size is same as size of the longest word in the vocabulary list that one has, when it sweeps across a sentence.So if we use embedding to represent the text, then shouldn't the kernel size be same as the embedding dimension so that it gives the same effect as sweeping word by word?I see difference sizes of kernel used, despite the word length."
1766,What is the meaning of “size” of word2vec vectors [gensim library]?,"['python', 'gensim', 'word2vec', 'word-embedding']","Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?"
1767,"word2vec: user-level, document-level embeddings with pre-trained model","['python', 'twitter', 'nlp', 'word2vec', 'word-embedding']",I am currently developing a Twitter content-based recommender system and have a word2vec model pre-trained on 400 million tweets.How would I go about using those word embeddings to create a document/tweet-level embedding and then get the user embedding based on the tweets they had posted? I was initially intending on averaging those words in a tweet that had a word vector representation and then averaging the document/tweet vectors to get a user vector but I wasn't sure if this was optimal or even correct. Any help is much appreciated.
1768,Interpreting and using principal components of word embeddings,"['python', 'nlp', 'pca', 'word2vec', 'word-embedding']","Imagine you have a set of semantically related words (e.g. restaurant, food, dish, waiter), along with a few relatively unrelated words (e.g. sad, angry, iphone). How would you go about finding these ""anomalous"" words? I'm using word vectors (e.g. fasttext, glove) to represent these words and one simple way that works to some extent is to sort the vectors based on their distance from their mean. But this isn't perfect...I've considered using PCA, but not sure if that is a good approach or how exactly to find the anomalous words using it. Thanks a lot!"
1769,TypeError: Can't convert 'text': data type not understood,"['python', 'encoding', 'word-embedding', 'sentence', 'python-embedding']","I am trying to do an embedding process for a paragraph, the process is called 'Universal Sentence Encoding' from google. This needs to be done for a deep-learning classification process. which gives me the following error:ERROR:
~\AppData\Local\Continuum\anaconda3\lib\site-
packages\tensorflow_hub\tensor_info.py in_convert_to_compatible_tensor(value,target, error_prefix)
117     tensor = tf.convert_to_tensor_or_indexed_slices(value, target.dtype)
118   except TypeError as e:
--> 119     raise TypeError(""%s: %s"" % (error_prefix, e))
120   if _is_sparse(tensor) != _is_sparse(target):
121     if _is_sparse(tensor):TypeError: Can't convert 'text': data type not understoodI am not sure about what the error is, Any help on this is appreciated!"
1770,keras Bidirectional layer using 4 dimension data,"['machine-learning', 'keras', 'lstm', 'rnn', 'word-embedding']","I'am designing keras model for classification based on article data.I have data with 4 dimension as follows and i want to feed each (word_num, word embedding) data into keras Bidirectional layerin order to get result with 3 dimension as follows.when i tried to feed 4 dimension data for testing like thisand i got the errorhow can i achieve this?"
1771,Confusion about input shape for Keras Embedding layer,"['python', 'machine-learning', 'keras', 'word2vec', 'word-embedding']","I'm trying to use the Keras embedding layer to create my own CBoW implementation to see how it works.I've generated outputs represented by a vector of the context word I'm searching for with size equal to my vocab. I've also generated inputs so that each context word has X many nearby words represented by their one-hot encoded vectors.So for example if my sentence is:""I ran over the fence to find my dog""using window size 2, I could generate the following input/output:[[over, the, to, find], fence] where 'fence' is my context word, 'over', 'the', 'to', 'find' are my nearby words with window 2 (2 in front, 2 in back).Using sample vocab size of 500 and 100 training samples, after one-hot encoding my input and output, it would have the following dimensions:That is, I have 100 outputs each represented by a 500-sized vector. I have 100 inputs each represented by a series of 4 500-sized vectors.I have a simple model defined as:However, when I try to fit my model, I get a dimensional exception:Now, I can only assume I'm using the embedding layer wrongly. I've read both this CrossValidated Question and the Keras documentation.I'm still not sure exactly how the inputs of this embedding layer works. I'm fairly certain my input_dim and output_dim are correct, which leaves input_length. According to the CrossValidated, my input_length is the length of my sequence. According to Keras, my input should be of dimension (batch_size, input_length).If my inputs are 4 words each represented by a word vector of size vocab_size, how do I input this to the model? "
1772,How to sentence embed from gensim Word2Vec embedding vectors?,"['python-3.x', 'gensim', 'word2vec', 'word-embedding', 'doc2vec']","I have a pandas dataframe containing descriptions. I would like to cluster descriptions based on meanings usign CBOW. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using gensim as so:I am however a bit confused now on how to replace the full sentences from my df with document vectors of equal dimensions.For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though gensim, so that I could say something like this:"
1773,vocab size versus vector size in word2vec,"['word2vec', 'word-embedding']","I have a data with 6200 sentences(which are triplets of form ""sign_or_symptoms diagnoses Pathologic_function""), however the unique words(vocabulary) in these sentence is 181, what would be the appropriate vector size to train a model on the sentences with such low vocabulary. Is there any resource or research on appropriate vector size depending on vocabulary size?"
1774,How is cosine similarity performed in this Keras word2vec implementation?,"['python', 'keras', 'word2vec', 'cosine-similarity', 'word-embedding']","I'm following a tutorial
http://adventuresinmachinelearning.com/word2vec-keras-tutorial/
which uses cosine similarity for validation model but dot product (not normalized unlike cosine similarity) for training. That's really confusing but my question is about axis of dot product, for validation model it's:as a result when I log the model summary for the validation model I get:Layer (type) Output Shapedot_1 (Dot) (300, 1, 1)but for training modelnot surprisingly model summary shows a different thingLayer (type) Output Shapedot_2 (Dot) (None, 1, 1)I've read 'Ouput' in the summary (who's value is usually None) shows batch size (if it's fixed for some reason) but it still doesn't make sense to me why it's 300 for the first model because I think batch size in the example is not fixed and 300 is also my embedding dimension. Clearly something's wrong the dot_axes, can anybody confirm? Do I just fix it by putting dot_axes=1 instead of zero? Why is first axis 1 and not 0? Does dot_axes=0 dot over batch size or what (makes no sense to me)?code on github:
https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_word2vec.py"
1775,How to train a model with only an Embedding layer in Keras and no labels,"['python', 'machine-learning', 'keras', 'word-embedding']","I have some text without any labels. Just a bunch of text files. And I want to train an Embedding layer to map the words to embedding vectors. Most of the examples I've seen so far are like this:They all assume that the Embedding layer is part of a bigger model which tries to predict a label. But in my case, I have no label. I'm not trying to classify anything. I just want to train the mapping from words (more precisely integers) to embedding vectors. But the fit method of the model, asks for x_train and y_train (as the example given above).How can I train a model only with an Embedding layer and no labels?[UPDATE]Based on the answer I've got from @Daniel Möller, Embedding layer in Keras is implementing a supervised algorithm and thus cannot be trained without labels. Initially, I was thinking that it is a variation of Word2Vec and thus does not need labels to be trained. Apparently, that's not the case. Personally, I ended up using the FastText which has nothing to do with Keras or Python."
1776,Using GLOVEs pretrained glove.6B.50.txt as a basis for word embeddings R,"['r', 'word-embedding', 'text2vec', 'glove']","I'm trying to convert textual data into vectors using GLOVE in r. My plan was to average the word vectors of a sentence, but I can't seem to get to the word vectorization stage. I've downloaded the glove.6b.50.txt file and it's parent zip file from: https://nlp.stanford.edu/projects/glove/ and I have visited text2vec's website and tried running through their example where they load wikipedia data. But I dont think its what I'm looking for (or perhaps I am not understanding it). I'm trying to load the pretrained embeddings into a model so that if I have a sentence (say 'I love lamp') I can iterate through that sentence and turn each word into a vector that I can then average (turning unknown words into zeros) with a function like vectorize(word). How do I load the pretrained embeddings into a glove model as my corpus (and is that even what I need to do to accomplish my goal?)"
1777,Using pre-trained word embeddings - how to create vector for unknown / OOV Token?,"['neural-network', 'deep-learning', 'nlp', 'pytorch', 'word-embedding']","I wan't to add pre-trained embeddings to a model. But as it seems there is no out-of-vocabulary (OOV) token resp. no vector for unseen words existent. So what can I do to handle OOV-tokens I come across? I have some ideas, but none of them seem to be very good:I could just create a random vector for this token, but ideally I'd like the vector to within the logic of the existing model. If I just create it randomly I'm afraid the vector accidentally could be very similar to a very frequent word like 'the', 'for', 'that' etc. which is not my intention.Or should I just initialize the vector with plain zeros instead?Another idea would be averaging the token over other existing vectors. But averaging on what vectors then? On all? This doesn't seem to be very conclusive either.I also thought about trying to train this vector. However this doesn't come very handy if I want to freeze the rest of the embedding during training. (A general solution is appreciated, but I wanted to add that I'm using PyTorch - just in case PyTorch already comes with a handy solution to this problem.)So what would be a good and easy strategy to create such a vector?"
1778,What is “unk” in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?,"['neural-network', 'deep-learning', 'nlp', 'word-embedding', 'glove']","I found ""unk"" token in the glove vector file glove.6B.50d.txt downloaded from https://nlp.stanford.edu/projects/glove/. Its value is as follows:Is it a token to be used for unknown words or is it some kind of abbreviation?"
1779,Python/Gensim - What is the meaning of syn0 and syn0norm?,"['python', 'deep-learning', 'nlp', 'gensim', 'word-embedding']","I know that in gensims KeyedVectors-model, one can access the embedding matrix by the attribute model.syn0. There is also a syn0norm, which doesn't seem to work for the glove model I recently loaded. I think I also have seen syn1 somewhere previously. I haven't found a doc-string for this and I'm just wondering what's the logic behind this?So if syn0 is the embedding matrix, what is syn0norm? What would then syn1 be and generally, what does syn stand for?"
1780,How to load pre-trained word embeddings in keras and output different classes,"['python', 'keras', 'lstm', 'word-embedding']","I want to compare two LSTMs, one trained on Wikipedia data and one on my own. I have a problem when designing the first model. I defined two word embeddings vocabularies, one that is only built on wikipedia data and one that is also trained on my corpus. I would like to define a first LSTM that has an embedding layer (which loads the pre-trained weights from Wikipedia), but the output should be different than its vocabulary size(10000), in fact I would like to output as many classes as the ones contained into the other vocabulary (50000). This is what I have now:Here my variables and shapes:Thank you for your help!"
1781,Why can't spacy differentiate between two homograph tokens in the following code?,"['python', 'nlp', 'word2vec', 'spacy', 'word-embedding']","A homograph is a word that has the same spelling as another word but has a different sound and a different meaning, for example,lead (to go in front of) / lead (a metal) .I was trying to use spacy word vectors to compare documents with each other by summing each word vector for each document and then finally finding cosine similarity. If for example spacy vectors have the same vector for the two 'lead' listed above , the results will be probably bad.In the code below , why does the similarity between the two 'bank'
tokens come out as 1.00 ?The output for given program istoken1 =  bank  token2 = bank1.0"
1782,How to get token ids using spaCy (I want to map a text sentence to sequence of integers),"['nlp', 'spacy', 'word-embedding']",I want to use spacy to tokenize sentences to get a sequence of integer token-ids that I can use for downstream tasks. I expect to use it something like below. Please fill in ???Preferably the integers refers to some special embedding id in en_core_web_lg..spacy.io/usage/vectors-similarity does not give a hint what attribute in doc to look for.I asked this on crossvalidated but it was determined as OT. Proper terms for googling/describing this problem is also helpful.
1783,TensorFlow: How to constrain/update only some rows of a variable?,"['python', 'tensorflow', 'keras', 'deep-learning', 'word-embedding']","I have an embedding matrix variable I want to constrain the norm to be unitBut the embs matrix is too large, and only some rows changed by the previous gradient applying step. I want to reduce computation by only constraining/updating these 'active' embeddings. I have a list of row numbers, e.g., e = [1, 3, 5], same as one-hot ID used in embedding lookup. How could I constrain/update only these ""active"" embeddings?"
1784,Keras autoencoder with pretrained embeddings returning incorrect number of dimensions,"['python', 'machine-learning', 'keras', 'deep-learning', 'word-embedding']","I have been attempting to replicate a sentence autoencoder loosely based off of an example from the Deep Learning with Keras book.I recoded the example to use an embedding layer instead of the sentence generator and to use fit vs. fit_generator. My code is as follows: I verified that my layer shapes are the same as those in the example, however when I try to fit my autoencoder, I get the following error:A few other things I tried included switching texts_to_matrix to texts_to_sequence and wrapping/not wrapping my padded strings I also came across this post which seems to indicate that I am going about this the wrong way. Is it possible to fit an autoencoder with the embedding layer as I have coded it? If not, can someone help explain the fundamental difference between what is going on with the provided example and my version? EDIT: I removed the return_sequences=True argument in the last layer and got the following error: ValueError: Error when checking target: expected bidirectional_1 to have shape (300,) but got array with shape (80,)After updating my layer shapes are: Am I missing a step between the RepeatVector layer and the last layer of the model so that I can return a shape of (None, 80, 300) rather than the (None, 300) shape it is currently generating? "
1785,Test data giving prediction error in Keras in the model with Embedding layer,"['python', 'tensorflow', 'keras', 'nlp', 'word-embedding']","I have trained a Bi-LSTM model to find NER on a set of sentences. For this I took the different words present and I did a mapping between a word and a number and then created the Bi-LSTM model using those numbers. I then create and pickle that model object.Now I get a set of new sentences containing certain words that the training model has not seen. Thus these words do not have a numeric value till now. Thus when I test it on my previously existing model, it would give an error. It is not able to find the words or features as the numeric values for those do not exist.To circumvent this error I gave a new integer value to all the new words that I see.However, when I load the model and test it, it gives the error that: The training data contains 5445 words including the padding word. Thus = [0, 5444]5444 is the index value I have given to the paddings in the test sentences. Not clear why it is assuming the index values to range between [0, 5442).I have used the base code available on the following link: https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstmThe code:max_len is the total number of words in a sentence and n_words is the vocab size. In the model the padding has been done using the following code where n_words=5441:The padding in the new dataset:Not sure which of these paddings is correct?However, the vocab only includes the words in the training data. When I say:How to use predict for text sentences which contain words that are not present in the initial vocab?"
1786,Word embeddings perform poorly for text classification,"['nlp', 'word2vec', 'text-classification', 'word-embedding']","I am working on a text classification use case. The text is basically contents of legal documents, for example, companies annual reports, W9 etc. So there are 10 different categories and 500 documents in total. Therefore 50 documents per category. So the dataset consists of 500 rows and 2 columns, 1st column consisting of text and 2nd column is the Target. I have built a basic model using TF-IDF for my textual features. I have used Multinomial Naive Bayes, SVC, Linear SGD, Multilayer Perceptron, Random Forest. These models are giving me an F1-score of approx 70-75%.I wanted to see if creating word-embedding will help me improve the accuracy. I trained the word vectors using gensim Word2vec, and fit the word vectors through the same ML models as above, but I am getting a score of about 30-35%. I have a very small dataset and lot of categories, is that the problem? Is it the only reason, or there is something I am missing out? "
1787,Add exception in Spacy tokenizer to not break the tokens with whitespaces?,"['python-3.x', 'nlp', 'spacy', 'cosine-similarity', 'word-embedding']","I am trying to find word similarity between a list of 5 words and a list of 3500 words.                                                                            The problem that I am facing:    The List of 5 words I have are as below   In the list of 3500 words, there are words like     The Spacy models through their 'nlp' object seem to break the tokens in the second list into cloud, computing, docket, installation.This in turn causes similar words to appear inaccurately, For example when I run the following code                                                                                I get results like (cloud, cloud) while I expected (cloud, cloud computing). It looks like the word 'cloud computing' is broken into two separate tokens.Are there any workarounds? Any help is appreciated.I would want an exception where contextually linked words like 'cloud computing' is not broken into two like 'cloud' , 'computing' but retained as 'cloud computing'"
1788,Merging sequence embedding with Time Series Features,"['keras', 'lstm', 'word-embedding', 'sequence-to-sequence']","I am having trouble around certain aspects of the Keras implementation of LSTM. This is a description of my problem:I am trying to train a model for word correctness prediction. My model has two types of inputs: e.g. As each sentence in my training set has different length, I should zero-pad all of my sentences such that they all have the same length. My question is how about the second input, should I do padding! and how? as they are vectors.Model Architecture :"
1789,How to represent ELMo embeddings as a 1D array?,"['machine-learning', 'nlp', 'classification', 'text-classification', 'word-embedding']","I am using the language model ELMo - https://allennlp.org/elmo to represent my text data as a numerical vector. This vector will be used as training data for a simple sentiment analysis task.In this case the data is not in english, so I downloaded a custom ELMo model from - https://github.com/HIT-SCIR/ELMoForManyLangs (i assume this behavs similar as the offical allennlp repo)To convert a text document to an ELMo embedding the function sents2elmo is used. The argument is a list of tokenized sentences if I understood the documentation correct. So one sample in my training data could be embedded as following:This will return a list of two numpy arrays, one for each sentence, and each token in the sentence will be represented as one vector of size 1024. And since the default parameter of sents2elmo(output_layer) is -1, this vector represents the average of the 3 internal layers in the language model.How can the embeddings be represented as a 1D array? Shall I just average all the word vectors for one sentence. And then average all the sentence vectors? Does this approach destroy any information? If so, are there other ways of doing this?Thanks!"
1790,How to calculate similarity for pre-trained word embeddings,"['r', 'keras', 'word2vec', 'word-embedding', 'glove']","I want to know the most similar words to another from a pretrained embedding vectors in R. E.g: words similar to ""beer"". For this, I download the pretrained embedding vectors on http://nlp.stanford.edu/data/glove.twitter.27B.zip and applied the code below:Source code:But I do not how to get the words most similar. 
I found the examples but doesn't work because the structure of embeddings vectors are differentHow to calculate similarity for pre-trained word embeddings in R?"
1791,Doc2vec predictions - do we average the words or what is the paragraph ID for a new paragraph?,"['nlp', 'word2vec', 'word-embedding', 'doc2vec']","I understand that you treat the paragraph ID as a new word in doc2vec (DM approach, left on the figure) during training. The training output is the context word. After a model is trained, suppose I want to get 1 embedding given a new document. Do I feed each word to the network and then average it to get the embedding? Or is there another way? I can feed this to gensim, but I am trying to understand how it works. "
1792,What is the difference between “context vector” and “word vector” in spaCy?,"['python', 'similarity', 'spacy', 'word-embedding']","In the section about vector similarity it's noted that small models contain only context vectors and other models word vectors. I've been trying to find out how context vectors are trained/computed (also in general), but no such luck.What are context vectors and how does their similarities compare to similarities between word vectors?"
1793,Discrepancy documentation and implementation of spaCy vectors for German words?,"['documentation', 'spacy', 'word-embedding']","According to documentation:spaCy's small models (all packages that end in sm) don't ship with
  word vectors, and only include context-sensitive tensors. [...]
  individual tokens won't have any vectors assigned.But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True. It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?"
1794,How can we use artificial neural networks to find similar documents?,"['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'word-embedding']","How can we use ANN to find some similar documents? I know its a silly question, but I am new to this NLP field.
I have made a model using kNN and bag-of-words approach to solve my problem. Using that I can get n number of documents (along with their closeness) that are somewhat similar to the input, but now I want to implement the same using ANN and I am not getting any idea.Thanks in advance for any help or suggestions."
1795,Keras - Translation from Sequential to Functional API,"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']","I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it a lot and trying multiple unsuccessful solutions.https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfaThe step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.What I tried to do was translate his piece of code - which is implemented in the Sequential API of Keras - to the Functional API to solve the deprecation of the Merge layer, by replacing it with the keras.layers.Dot layer. However, I'm still stuck in this step of merging the two models (word and context) into the final model, whose architecture must be like this:Here's the code that the author used:And here is my attempt to translate the Sequential code implementation into the Functional one:However, when executed, the following error is returned:ValueError: Layer dot_5 was called with an input that isn't a symbolic
  tensor. Received type: . Full
  input: [,
  ]. All inputs to
  the layer should be tensors.I'm a total beginner to the Functional API of Keras, I will be grateful if you could give me some guidance in this situation on how could I input the context and word models into the dot layer to achieve the architecture in the image.              "
1796,How to convert word2vec to glove format,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']",I did some research and found that gensim has a script to convert glove to word2vec GLove2Wrod2Vec. I am looking to do the opposite.Is there any simple way to convert using gensim or any other library
1797,How to use my own sentence embeddings in Keras?,"['keras', 'nlp', 'lstm', 'word-embedding', 'sentence-similarity']","I am new to Keras and I created my own tf_idf sentence embeddings with shape (no_sentences, embedding_dim). I am trying to add this matrix as input to an LSTM layer. My network looks something like this:I'm struggling with how the matrix should be shaped. I am getting this error: I already checked this post: Sentence Embedding Keras but still can't figure it out. It seems like I'm missing something obvious.Any idea how to do this?"
1798,Use Glove vectors without Embedding layers in LSTM,"['python', 'keras', 'nlp', 'word-embedding', 'glove']","I want to use the Glove vectors in Language Modeling. But the problem is if I use Embedding layer in the model, I can't predict the output vector and match the word. What I mean here is, I want to give the glove vector representation for my sentences as the Input. and get them out from the Lstm layer and get the vectors and match it with Glove vectors I want to use the glove vectors without embedding layer.  can someone propose a method to do it? I am using the keras and python3What I want is to use the embedding layer as one model1 and return the output vector and give it to another LSTM model2 as the input. where it gives the index of the word vector."
1799,How to evaluate Word2Vec model,"['python', 'nlp', 'word2vec', 'embedding', 'word-embedding']","Hi have my own corpus and I train several Word2Vec models on it.
What is the best way to evaluate them one against each-other and choose the best one? (Not manually obviously - I am looking for various measures).It worth noting that the embedding is for items and not word, therefore I can't use any existing benchmarks.Thanks!"
1800,What is the most efficient way to store a set of points (embeddings) such that queries for closest points are computed quickly,"['information-retrieval', 'embedding', 'word-embedding', 'data-retrieval']","Given a set of embeddings, i.e. set of [name, vector representation]
how should I store it such that queries on the closest points are computed quickly. For example given 100 embeddings in 2-d space, if I query the data struct on the 5 closest points to (10,12), it returns   { [a,(9,11.5)] , [b,(12,14)],...}The trivial approach is calculate all distances, sort and return top-k points. Alternatively, one might think of storing in a 2-d array in blocks/units of mXn space to cover the range of the embedding space. I don't think this is extensible to higher dimensions, but I'm willing to be corrected."
1801,Normalize Fasttext word embedding vector generated by model,"['python', 'tensorflow', 'normalization', 'word-embedding', 'fasttext']","I'm currently using FastText to get the word embeddings for some input text data in order to catch similarity among them and give those embedding as input to a neural network for a NER task.
I've first tried to use the fastText .vec file and I was able to get good similarity result from them, then I've tried to use the .bin model file so that I can get embeddings also from out-of-vocabulary terms but I've noticed that the vectors generated from the .vec and the .bin file are different.
The .vec file contains embeddings clipped to a range of [-1,1] while this is not true for the embedding contained in the .bin file.
Using the .bin file I get very bad results so I'm trying to undestand if this is due to the fact the these vectors (.bin) are not normalized.Is there a good way to normalize these vectors in a range [-1, 1] using Python?Thanks in advance."
1802,How to classify text documents in legal domain,"['python', 'svm', 'text-classification', 'word-embedding', 'doc2vec']","I've been working on a project which is about classifying text documents in the legal domain (Legal Judgment Prediction class of problems).
The given data set consists of 700 legal documents (well balanced in two classes). After the preprocessing, which consists in applying all the best practices (such as deleting stopwords,etc.), there are 3 paragraphs for each document, which I could consider all together or separately. On average, the text documents size is 2285 words.I aim to use something different from the classical n-grams model (which doesn't take into account any words order or semantic) :I was wondering if there's someone who has some experience in this particular domain, who can suggest me other ways or how to improve the model since I'm not getting particularly good results: 74% accuracy. Is it correct using Doc2Vec for transforming text into vectors and using them for feeding a classifier?My model represantation:"
1803,Loading pre-trained word vectors with fastrtext in R,"['r', 'word-embedding', 'fasttext']","I'd like to use the pretrained word vectors by FastText in R. This should be possible with the 'fastrtext' library, but I'm having trouble loading the files.

How do I read the .vec file with R? 
How do I load the .bin file into R? 

For .bin files I've tried:Error in model$load(path) : Failure in fastrtext. Exit code: 1Clearly, that's not the way to go. Any ideas?"
1804,Merging layers on Keras (dot product),"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']","I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it for hours and trying a lot of unsuccessful solutions.https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfaThe step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.I've seem many discussions about it, and the majority of answers was the you need to use the Functional API of Keras to merge layers now. But the problem is, I'm a total beginner in Keras and have no idea how to translate my code from Sequential to Functional, here's the code that the author used (and I copied):And when I run the block, the following error is shown:What I'm asking here is some guidance on how to transform this Sequential into a Functional API structure."
1805,LSTM network on pre trained word embedding gensim,"['python', 'machine-learning', 'deep-learning', 'lstm', 'word-embedding']","I am new to deep learning. I am trying to make very basic LSTM network on word embedding feature. I have written the following code for the model but I am unable to run it.    The error I am getting is ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (3019, 300). On searching, I found that people have used Flatten() which will compress all the 2-D features (3019,300) for the dense layer. But I am unable to fix the issue. While explaining, kindly let me know how do the dimension work out.Upon request:My X_training had dimension issues, so I am providing the code below to clear out the confusion,I think the following code is giving 2-D numpy array as I am initializing it that wayMy objective is just to use gensim pretrained model for LSTM on some comments that I have."
1806,Which Pad token in Google-News Word2vec Pre-trained Model,"['python', 'gensim', 'word2vec', 'word-embedding']",I'm using the pre-trained model from Google to do word embedding. What kind of string should I give to the model to receive and array full of zeros? I need to pad some sentences and I need an array of zeros to embed the padding. Maybe should I create the array of zeros instead of the model?
1807,How to form sentence embeddings from word embeddings using glove on dataframe trained tensors?,"['python', 'machine-learning', 'word-embedding', 'glove']","I am working with a dataset containing snippets of event information. My dataframe looks similar to:My job is to cluster these events based on their meanings. I do not know how many events there should be, that's the job of the unsupervised learning. In order to proceed with the DBSCAN clustering I have embedded all words in my dataframe into vectors using GloVe (rather doc2Vec, etc).How do you convert word vectors into sentence vectors, to proceed to clustering?I have read this article as well as some other posts and papers, which use other sentence embedding algorithms, not GloVe word embedding. Also, some repos like InferSent and Google universal sentence encoder are pretty good, however they are using pre trained tensors. Given these constraints, that I must use GloVe and dataframe trained tensors rather than pretrained ones, how can I form sentence vectors from word vectors?"
1808,word embeddings in tensorflow (no pre_trained) [closed],"['tensorflow', 'deep-learning', 'embedding', 'word-embedding']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I am new to tensorflow and trying to look at different examples of tensorflow to understand it better.Now I have seen this line being used in many tensorflow examples without mentioning of any specific embedding algorithm being used for getting the words embeddings. Here are some examples:I understand that the first line will initialize the embedding of the words by random distribution but will the embedding vectors further be trained in the model to give more accurate representation of the words (and change the initial random values to more accurate numbers) and if yes what is the actual method being used when there is no mention of any obvious embedding methods such as using word2vec and glove inside the code (or feeding the pre_tained vectors of these methods instead of random numbers in the beginning)?"
1809,Difference of Pre-Padding and Post-Padding text when preprossing different text sizes for tf.nn.embedding_lookup,"['python-3.x', 'tensorflow', 'machine-learning', 'text-classification', 'word-embedding']","I have seen two types of padding when feeding to embedding layers. eg:considering two sentences:word1 = ""I am a dog person.""word2 = ""Krishni and Pradeepa both love cats.""word1_int = [1,2,3,4,5,6] word2_int = [7,8,9,10,11,12,13]padding both words to length = 8padding method 1(putting 0s at the beginning)word1_int = [0,0,1,2,3,4,5,6] word2_int = [0,7,8,9,10,11,12,13]padding method 2(putting 0s at the end)word1_int = [1,2,3,4,5,6,0,0] word2_int = [7,8,9,10,11,12,13,0]I am trying to do an online classification using the 20 news groups dataset. and I am currently using the 1st method to pad my text. Question: Is there any advantage of using the 1st method over the other one in my implementation?Thank you in advance!My code is shown below:This is the code sample that I am using to pad all the sentences."
1810,How to store word vectors embeddings? [closed],"['deep-learning', 'lstm', 'word2vec', 'opennlp', 'word-embedding']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I'm very new to NLP and Deep Learning field and want to understand that after vectorization of a whole corpus using Word2Vec, Do I need to store the word vector values locally?
If yes I want to make a chatbot for android. Can anyone please guide me for this?"
1811,Non English Word Embedding from English Word Embedding,"['tensorflow', 'nlp', 'gensim', 'word-embedding', 'chainer']","How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?What are the best ways to generate high quality word embedding for non - english words .Words may include (samsung-galaxy-s9)"
1812,Tensorflow serving - How to use large word embeddings?,"['tensorflow', 'nlp', 'assets', 'tensorflow-serving', 'word-embedding']","I am kind of new to Tensorflow. I have the simple LSTM task to classify text. Im converting words to vectors with word2vec with 300 dims with tf.nn.embedding_lookup
my word embeddings are in np.array and I am loading it with np.load().When I save the model either with tf.train.Saver() or builder.save() my exported model is ~1.5 GB big. I am unable to load this model to Tensorflow serving due to size restrictions. I have read something about assets and storing external files to asset directory. Does it apply also to wordvectors? How to implement it?Thanks"
1813,What is the operation behind the word analogy in Word2vec?,"['python', 'gensim', 'word2vec', 'word-embedding']","According to https://code.google.com/archive/p/word2vec/: It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.So we can try from the supplied demo script:Please note that paris france berlin is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in Gensim and try to compute the vectors myself. For example:So, what is the word analogy actually doing? How should I reproduce it?"
1814,"How does Keras 1d convolution layer work with word embeddings - text classification problem? (Filters, kernel size, and all hyperparameter)","['python', 'tensorflow', 'keras', 'conv-neural-network', 'word-embedding']","I am currently developing a text classification tool using Keras. It works (it works fine and I got up to 98.7 validation accuracy) but I can't wrap my head around about how exactly 1D-convolution layer works with text data.What hyper-parameters should I use?I have the following sentences (input data):It's a very simple model (I have made more complicated structures but, strangely it works better - even without using LSTM):My main question is: What hyper-parameters should I use for Conv1D layer?If I have following input data:Does it mean that filters=32 will only scan first 32 words completely discarding the rest (with kernel_size=2)? And I should set filters to 951 (max amount of words in the sentence)?Examples on images:So for instance this is an input data: http://joxi.ru/krDGDBBiEByPJAIt's the first step of a convoulution layer (stride 2): http://joxi.ru/Y2LB099C9dWkOrIt's the second step (stride 2): http://joxi.ru/brRG699iJ3Ra1mAnd if filters = 32, layer repeats it 32 times? Am I correct?
So I won't get to say 156-th word in the sentence, and thus this information will be lost?"
1815,Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models,"['python-3.x', 'gensim', 'word2vec', 'word-embedding']","I build two word embedding (word2vec models) using gensim and save it as (word2vec1 and word2vec2) by using the model.save(model_name) command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as a). How to compute the degree of similarity (cosine-similarity or similarity) of the extracted top word (say 'a'), for the two word2vec models? Does most_similar() will work in this case efficiently? I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?Any idea is deeply appreciated."
1816,sentence embeddings for alternatives of a word in a sentence,"['nlp', 'word-embedding']",Looking for Embeddings which can throw the alternatives of a given word in a sentence looking at the context info.Ex:I read a great book at lunch.Generated sentences:I read a good book at lunch.I read a good novel at lunch.I read an excellent book at lunch.I read an excellent novel at lunch.Any help would be greatly appreciated.
1817,Use pretrained embedding in Spanish with Torchtext,"['nlp', 'deep-learning', 'pytorch', 'word-embedding', 'torchtext']","I am using Torchtext in an NLP project. I have a pretrained embedding in my system, which I'd like to use. Therefore, I tried:But, apparently, this only accepts the names of a short list of pre-accepted embeddings, for some reason. In particular, I get this error:I found some people with similar problems, but the solutions I can find so far are ""change Torchtext source code"", which I would rather avoid if at all possible.Is there any other way in which I can work with my pretrained embedding? A solution that allows to use another Spanish pretrained embedding is acceptable.Some people seem to think it is not clear what I am saking. So, if the title and final question are not enough: ""I need help using a pre-trained Spanish word embedding in Torchtext""."
1818,How to build an embedding layer in Tensorflow RNN?,"['python', 'tensorflow', 'rnn', 'word-embedding']","I'm building an RNN LSTM network to classify texts based on the writers' age (binary classification - young / adult).Seems like the network does not learn and suddenly starts overfitting:
Red: train
Blue: validationOne possibility could be that the data representation is not good enough. I just sorted the unique words by their frequency and gave them indices. E.g.:So I'm trying to replace that with word embedding.
I saw a couple of examples but I'm not able to implement it in my code. Most of the examples look like this:Does this mean we're building a layer that learns the embedding? I thought that one should download some Word2Vec or Glove and just use that.Anyway let's say I want to build this embedding layer...
If I use these 2 lines in my code I get an error:TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64So I guess I have to change the input_data type to int32. So I do that (it's all indices after all), and I get this:TypeError: inputs must be a sequenceI tried wrapping inputs (argument to tf.contrib.rnn.static_rnn) with a list: [inputs] as suggested in this answer, but that produced another error:ValueError: Input size (dimension 0 of inputs) must be accessible via
  shape inference, but saw value None.Update:I was unstacking the tensor x before passing it to embedding_lookup. I moved the unstacking after the embedding.Updated code:*seqlen: I zero-padded the sequences so all of them have the same list size, but since the actual size differ, I prepared a list describing the length without the padding.New error:ValueError: Input 0 of layer basic_lstm_cell_1 is incompatible with
  the layer: expected ndim=2, found ndim=3. Full shape received: [None,
  1, 64]64 is the size of each hidden layer.It's obvious that I have a problem with the dimensions... How can I make the inputs fit the network after embedding?"
1819,How to use own word embedding with pre-trained embedding like word2vec in Keras,"['python', 'keras', 'nlp', 'lstm', 'word-embedding']","I have a co-occurrence matrix stored in a CSV file which contains the relationship between words and emojis like this:This co-occurrence matrix is huge which has 1584755 rows and 621 columns. I have a Sequential() LSTM model in Keras where I use pre-trained (word2vec) word-embedding. Now I would like to use the co-occurrence matrix as another embedding layer. How can I do that? My current code is something like this:Also, if the co-occurrence matrix is sparse then what would be the best way to use it in the embedding layer? "
1820,Using pretrained gensim Word2vec embedding in keras,"['python', 'keras', 'gensim', 'word2vec', 'word-embedding']","I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?PS: It is different from other questions because I am using gensim for word2vec training instead of keras."
1821,FastText word embedding models based on skip-gram or CBOW,"['facebook', 'word2vec', 'word-embedding']",Can anybody tell whether these three models are based on skip-gram or CBOW methology?Thanks in advance
1822,Word2Vec Skipgrams - Should couples span sentences?,"['python', 'keras', 'word2vec', 'word-embedding']","I am trying to train a Skip-gram word2vec model using negative sampling. from what I understand I need to generate couples (target, context) and a label where 0 = not in context and 1 = in context.Should we make skipgram couples sentence by sentence? or should we flatten the sentences in to one large sentence and generate skipgrams from that? In other words, should the generated couples span sentences?The only difference between the two code snippets below is one of them generates couples that span the two sentences like so:results:We can see that there are couples that stretch across sentencesOr we can have couples that don't span sentences:Get dataInitialize some variablesFlatten the sentences into one large sequenceSplit dataset into sentences and generate couples based off each sentence.Print out our words"
1823,InvalidArgumentError Sentiment analyser with keras,"['python-3.x', 'keras', 'deep-learning', 'sentiment-analysis', 'word-embedding']","I have built a sentiment analyzer using Keras as a binary classification problem. I am using the Imdb dataset using GRU.
My code is:I keep receiving an Error message which I don't completely understand:Can anyone help me understand what causes this error and how to solve it?"
1824,Error when checking model input keras when predicting new results,"['python', 'tensorflow', 'machine-learning', 'keras', 'word-embedding']","I am trying to use a keras model I built on new data, except I have an input error when trying to predict the predictions.Here's my code for the model:And my code to predict the output predictions of my new data:The error I get when running this code:My error comes from maxlen because for my training data, maxlen=57 and with my new data, maxlen=36. So I tried to set in my prediction code maxlen=57 but then I get this error:What should I do in order to resolve these issues? Change my embedding layer?"
1825,how to assign weights to articles in the corpus for generating word embedding (e.g. word2vec)?,"['word2vec', 'corpus', 'word-embedding']","there are certain articles in the corpus that I found much more important than other articles (for instance I like their wording more). As a result, I would like to increase their ""weights"" in the entire corpus during the process of generating word vectors. Is there a way to implement this? The current solution that I can think of is to copy the more important articles multiple times, and add them to the corpus. However, will this work for the word embedding process? And is there a better way to achieve this? Many thanks!"
1826,pytorch - modify embedding backpropagation,"['pytorch', 'word-embedding']","I would like to modify the back-propagation for the embedding layer but I don't understand where the definition is.In the definition available in https://pytorch.org/docs/stable/_modules/torch/nn/functional.html in the embedding function, they call torch.embedding and here there should be defined how the weights are updated. So my question is:
Where can I find the documentation of torch.embedding?"
1827,"Keras example word-level model with integer sequences gives `expected ndim=3, found ndim=4`","['python', 'machine-learning', 'keras', 'lstm', 'word-embedding']","I'm trying to implement the Keras word-level example on their blog listed under the Bonus Section -> What if I want to use a word-level model with integer sequences?I've marked up the layers with names to help me reconnect the layers from a loaded model to a inference model later. I think I've followed their example model:but I get on the line What am I missing? Or is the example on the blog assuming a step that I've skipped? Update:And I'm training with:Update 2:So, I'm still not quite there. I have my decoder_lstm and decoder_outputs defined like above and have fixed the inputs. When I load my model from an h5 file and build my inference model, I try and connect to the training model with but I get an errorTrying to pass decoder_embedding rather than decoder_inputs fails too.I'm trying to adapt the example of lstm_seq2seq_restore.py but it doesn't include the complexity of the embedding layer.Update 3:When I use decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, ...) to build the inference model I've confirmed that decoder_embedding is an object of type Embedding but I get:The full code for this model is on Bitbucket."
1828,Doc2vec - About getting document vector,"['word-embedding', 'doc2vec']","I'm a very new student of doc2vec and have some questions about document vector.
What I'm trying to get is a vector of phrase like 'cat-like mammal'.
So, what I've tried so far is by using doc2vec pre-trained model, I tried the code belowWhen I tried this code, I could get a vector for one word 'cat', but not 'cat-like mammal'.
Because word2vec only provide the vector for one word like 'cat' right? (If I'm wrong, plz correct me)
So I've searched and found infer_vector() and tried the code belowWhen I tried this code, I could get a vector, but every time I get different value when I tried
    phrase_vec = m.infer_vector(phrase) 
Because infer_vector has 'steps'.When I set steps=0, I get always the same vector.
    phrase_vec = m.infer_vector(phrase, steps=0)However, I also found that document vector is obtained from averaging words in document.
like if the document is composed of three words, 'cat-like mammal', add three vectors of 'cat', 'like', 'mammal', and then average it, that would be the document vector. (If I'm wrong, plz correct me)So here are some questions."
1829,"What is the parameter “size” means in gensim.model.Word2Vec(sentence, size)?","['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","I have just been started learning about word embeddings and gensim and I tried this code 
. In this article during the visualisation it says we need PCA to convert high-dimensional vectors into low-dimensions. Now we have a parameter ""size"" in Word2Vec method, so why can't we set that size equals to 2 rather using PCA. 
So, I tried to do this and compare both graphs (one with 100 size and other with 2 as size) and got very different result. Now I am confused that what this ""size"" depicts? How the size of vectors affect this?This is what I got when I used 100 as size.This is what I got when I used 2 as size."
1830,Keras - Look up an embedding,"['python', 'keras', 'word-embedding']","What I am trying to do:I am trying to look up the word embeddings for each word from a sequence. This is a sequence of numbers generated from text.Background:My sequence (of shape (200,)) looks something like this:These number represent a word from a vocabulary (of 10000 words). I have an some embedding weights that I created using the negative sampling method found here.The extracted embedding weights are of shape (10000 , 106) of which I can load into a new embedding layer. I want to look up each number in the sequence from this new embedding layer with the loaded weights and have it return 200 vectors of size 106 corresponding to the sequence.Here is what I have done so far:Is this the correct way to look up the embeddings?"
1831,NLP: is there any model that generates sentence embedding with self-defined length?,"['python', 'nlp', 'sentiment-analysis', 'word-embedding']",I found universal sentence encoder https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1but seems like it can only output sentence vector with 512 dim. I'm new to NLP and like to know whether there exists a model that generates sentence representation with user-defined length.thanks!
1832,Randomly select vector in gensim word2vec,"['python-3.x', 'nlp', 'gensim', 'word2vec', 'word-embedding']","I trained a word2vec model using gensim and I want to randomly select vectors from it, and find the corresponding word.
What is the best what to do so?"
1833,Keras Tensorflow Graph Split between GPU and CPU,"['python', 'tensorflow', 'keras', 'word-embedding']","I was curious about how Keras deals with CPU and GPU allocation under the hood. Unfortunately, the documentation on the Keras site is pretty limited in this regard. I was having trouble with my Embedding layer fitting in memory so I followed the instructions in this answer and forced only my Embedding layer to be on the CPU, with the rest (Dense layer, etc.) on the GPU. I don't have any issues now, but was curious how this works. Is there a version of the graph on both the CPU and GPU? Or is data passed to the rest of the graph on the GPU after embedding is done? If so, how are weights trained?"
1834,Can I interpret doc2vec components?,"['nlp', 'word2vec', 'word-embedding', 'doc2vec', 'lightgbm']","I am solving a binary text classification problem with corporate filings. Using Doc2Vec embeddings of length 100 with LightGBM is producing great results. However, for this project it would be very valuable to approximate a thematic meaning for at least one of the components. Ideally, this would be a feature ranked with high importance by LightGBM explained anecdotally with a few examples.Has anyone attempted this, or should interpretation be off the table for a high-dimensional model with this level of complexity?"
1835,How to use padding value with get_keras_embedding,"['keras', 'gensim', 'word2vec', 'word-embedding']","I try to integrate gensim's get_keras_embedding into a Keras model.First I do text preprocessingThen I build a modelApparently the problem is with the padding value. The default value is 0 which is 'the' in gensim word2vec, if I use non-existing value it fails with the following error message. What is the right way to use padding with gensim word2vec?I would appreciate any help."
1836,Pytorch embedding RuntimeError: Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'index',"['python', 'typeerror', 'pytorch', 'word-embedding']","I'm getting this error saying But what does it mean by argument #3 ""index""? I can't find ""index"" argument in torch.embedding (here source : https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding) 
It seems like I'm passing the embedding the wrong parameters. I even changed the data type of my input like below but the error persists.Any comment (even though it's short!) or just listing keywords to look at will be highly appreciated!Here is a full traceback.Update: I even sent the whole model.network to cpu but still getting the same error. At this point, I'm suspecting this might be a bug...model.py code: https://github.com/byorxyz/san_mrc/blob/master/src/model.pyNetwork defined: https://github.com/byorxyz/san_mrc/blob/master/src/dreader.py "
1837,OOV (Out Of Vocabulary) word embeddings for Fasttex in low RAM environments,"['machine-learning', 'nlp', 'word-embedding', 'fasttext']",Is there a way to obtain the vectors for OOV (Out Of Vocabulary) words using fasttext but without loading all the embeddings into memory?I normally work in low RAM environments (<10GB of RAM) so loading a 7GB model into memory is just impossible. To use word embeddings without using that much RAM one can read a .vec (which is normally a plain-text) file line by line and store it into a database (which you later access to request a word vector). However to obtain OOV vectors with fasttext you need to use the .bin files and load then into memory. Is there a way you can avoid loading the whole .bin file?
1838,Seralizing a keras model with an embedding layer,"['tensorflow', 'keras', 'word-embedding']","I've trained a model with pre-trained word embeddings like this: With the architecture looking like this: When I go to convert the model to json, I get this error:Similarly, if I use the model.save() function, it seems to save correctly, but when I go to load it, I get Type Error: Expected Float32. My question is: is there something I am missing when trying to serialize this model? Do I need some sort of Lambda layer or something of the sorts? Any help would be greatly appreciated!"
1839,Getting Error while classifying text data using word2vec,"['keras', 'word2vec', 'text-classification', 'word-embedding']",I want to use my own word dataset for creating the embeddings. And use my own label data for training and testing my model. For that I have already created my own word embeddings using word2vec. And facing problem in training my model with label data.I am getting error while trying to train model. My model creation code:And this is how I have created embedding matrix-By doing so I am getting the following error-I am getting error in fitting training data into the model. I think I have mistaken in calculting the training data shape and injecting it into the model.
1840,Embedding in Keras,"['keras', 'nlp', 'data-science', 'word-embedding']","Which algorithm is used for embedding in Keras built-in function?
Word2vec? Glove? Other?https://keras.io/layers/embeddings/"
1841,gensim word2vec - update model data,"['gensim', 'word2vec', 'word-embedding']","I have an issue similar to the one discussed here - gensim word2vec - updating word embeddings with newcoming dataI have the following code that saves a model as text8_gensim.binHere is the code that adds more data to the saved model (after loading it)The model loads fine; however when I try to call model.wv.similarity function I get the following errorKeyError: ""word 'eft' not in vocabulary""Am I missing something here?"
1842,How to combine both word embeddings and pos embedding together to build the classifier,"['nlp', 'word2vec', 'word-embedding', 'pos']","You known POS is like 'NP', 'VERB'. How can I combine these features to word2vec?Just like the follow vectors?"
1843,How to properly use get_keras_embedding() in Gensim’s Word2Vec?,"['python', 'keras', 'gensim', 'word2vec', 'word-embedding']","I am trying to build a translation network using embedding and RNN. I have trained a Gensim Word2Vec model and it is learning word associations pretty well. However, I couldn’t get my head around how to properly add the layer to a Keras model. (And how to do an ‘inverse embedding’ for the output. But that’s another question that had been answered: by default you can’t.)In Word2Vec, when you input a string, e.g. model.wv[‘hello’], you get a vector representation of the word. However, I believe that the keras.layers.Embedding layer returned by Word2Vec's get_keras_embedding() takes a one-hot/tokenized input, instead of a string input. But the documentation provides no explanation on what the appropriate input is. I cannot figure out how to obtain the one-hot/tokenized vector of the vocabulary that has 1-to-1 correspondence with the Embedding layer’s input.More elaboration below:Currently my workaround is to apply the embedding outside Keras before feeding it to the network. Is there any detriment in doing this? I will set the embedding to non-trainable anyway. So far I have noticed that memory use is extremely inefficient (like 50GB even before declaring the Keras model for a collection of 64-word-long sentences) having to load the padded inputs and the weights outside the model. Maybe generator can help.The following is my code. Inputs are padded to 64-words long. The Word2Vec embedding has 300 dimensions. There are probably a lot of mistakes here due to repeated experimentation trying to make embedding work. Suggestions are welcome.Which throws an error when I try to fit the model with text:The following is an excerpt from Rajmak demonstrating how to use a tokenizer to convert words into the input of a Keras Embedding.Keras embedding layer can be obtained by Gensim Word2Vec’s word2vec.get_keras_embedding(train_embeddings=False) method or constructed like shown below. The null word embeddings indicate the number of words not found in our pre-trained vectors (In this case Google News). This could possibly be unique words for brands in this context.Here the embedding_layer is explicitly created using:However, if we use get_keras_embedding(), the embedding matrix is already constructed and fixed. I do not know how each word_index in the Tokenizer can be coerced match the corresponding word in get_keras_embedding()'s Keras embedding input.So, what is the proper way to use Word2Vec's get_keras_embedding() in Keras?"
1844,How to implement word embedding for persian language,"['keras', 'nlp', 'persian', 'word-embedding']","I have this code that works for English language but does not work for Persian languageoutputplesae take example with code
thanks"
1845,Pytorch nn.embedding error,"['pytorch', 'word-embedding']",I was reading pytorch documentation on Word Embedding. Output:This looks good but if I replace  line lookup_tensor by I am getting the error as:RuntimeError: index out of range at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:343I don't understand why it gives RunTime error on line hello_embed = embeds(lookup_tensor). 
1846,How to check via callbacks if alpha is decreasing? + How to load all cores during training?,"['callback', 'gensim', 'multicore', 'word-embedding', 'doc2vec']","I'm training doc2vec, and using callbacks trying to see if alpha is decreasing over training time using this code:And while training I see that alpha is not changing over time. On each callback I see alpha = 0.03.
Is it possible to check if alpha is decreasing? Or it really not decreasing at all during training? One more question: 
How can I benefit from all my cores while training doc2vec?As we can see, each core is not loaded more than +-30%. "
1847,How to correctly use mask_zero=True for Keras Embedding with pre-trained weights?,"['python', 'tensorflow', 'keras', 'word-embedding']","I am confused about how to format my own pre-trained weights for Keras Embedding layer if I'm also setting mask_zero=True. Here's a concrete toy example.Suppose I have a vocabulary of 4 words [1,2,3,4] and am using vector weights defined by:I want to embed sentences of length up to 5 words, so I have to zero pad them before feeding them into the Embedding layer. I want to mask out the zeros so further layers don't use them. Reading the Keras docs for Embedding, it says the 0 value can't be in my vocabulary. mask_zero: Whether or not the input value 0 is a special ""padding""
  value that should be masked out. This is useful when using recurrent
  layers which may take variable length input. If this is True then all
  subsequent layers in the model need to support masking or an exception
  will be raised. If mask_zero is set to True, as a consequence, index 0
  cannot be used in the vocabulary (input_dim should equal size of
  vocabulary + 1).So what I'm confused about is how to construct the weight array for the Embedding layer, since ""index 0 cannot be used in the vocabulary."" If I build the weight array asthen normally, word 1 would point to index 1, which in this case holds the weights for word 2. Or is it that when you specify mask_zero=True, Keras internally makes it so that word 1 points to index 0? Alternatively, do you just prepend a vector of zeros in index zero, as follows?This second option seems to me to put the zero into the vocabulary. In other words, I'm very confused. Can anyone shed light on this?"
1848,"Gensim Doc2vec trained, but not saved","['model', 'save', 'gensim', 'word-embedding', 'doc2vec']","While I trained d2v on a large text corpus I received these 3 files: Bun final model has not saved, because there was not enough free space available on the disk. Is there a way to resave my model using these files in a shorter time, than all training time? Thank you in advance! "
1849,"Can home-made embeddings work for RNNs, or do they HAVE to be trained?","['tensorflow', 'keras', 'rnn', 'word-embedding']","Let's say I'm training an RNN for classification, using a vocabulary of 100 words. I can skip the embedding and pass in the sentences as one-hot vectors, but using one-hot vectors for a space of 100 features seems very wasteful in terms of memory. And it just gets worse as the vocab grows. Is there any reason why I couldn't create my own embedding where each value from 0-100 was converted into binary and stored as an array of length 7, i.e. 0=[0,0,0,0,0,0,0], 1=[1,0,0,0,0,0,0], ..., 100=[1,1,0,0,1,0,0]? I realize the dimensionality is low, but aside from that, I wasn't sure if this random embedding is a bad idea since there are no relationships between the word vectors like there are with GLoVe. BTW I can't use pre-made embeddings here, and my sample size isn't huge which is why I'm exploring making my own.  "
1850,Could use help formatting data correctly for a Keras SimpleRNN,"['python', 'keras', 'classification', 'rnn', 'word-embedding']","I'm struggling a bit getting data into the right format for a simpleRNN, or I'm struggling to define the model correctly. I'm hoping someone can spot the problem?I'm trying to do classification of a list X of vectors of length 278 that contain integer values chosen from a dictionary vocab of length 9026 features as either belonging to class 0 or 1. Here's an example of my input data:So for example np.array(X).shape=(1000,278) and len(y)=1000
My model is:I prepare them as follows:When I run the model:I get the following error:If I change the input data toand run the model, I get this error:OK so obviously I've got something wrong since my final dense layer is looking for shape 278 and not 2. So I tried this model without explicitly defining the input_length:and when I run the model, I getI'm very confused. Can anyone help me diagnose this?"
1851,How to rotate a word2vec onto another word2vec?,"['gensim', 'word2vec', 'word-embedding']","I am training multiple word2vec models with Gensim. Each of the word2vec will have the same parameter and dimension, but trained with slightly different data. Then I want to compare how the change in data affected the vector representation of some words.But every time I train a model, the vector representation of the same word is wildly different. Their similarity among other words remain similar, but the whole vector space seems to be rotated.Is there any way I can rotate both of the word2vec representation in such way that same words occupy same position in vector space, or at least they are as close as possible.Thanks in advance."
1852,What embedding-layer output_dim is really needed for a dictionary of just 10000 words?,"['tensorflow', 'keras', 'deep-learning', 'word-embedding']","I'm training up an RNN with a very reduced set of word features, around 10,000. I was planning on starting with an embedding layer before adding RNNs, but it is very unclear to me what dimensionality is really needed. I know that I can try out different values (32, 64, etc.), but I'd rather have some intuition going into it first. For example, if I use a 32-dimensional embedding vector, then only 3 different values are needed per dimension to fully describe the space (32**3>>10000). Alternatively, for a space with this small number of words, does one even really need to use an embedding layer or does it make more sense to just go from an input layer right to the RNN?"
1853,What does convolution do on embedding axis in NLP?,"['python', 'keras', 'nlp', 'conv-neural-network', 'word-embedding']","I'm try to understanding what convolution neural network does in NLP.For example, my input sentence matrix has dimension (100,200). Here 100 is the length of my sentence, 200 is the dimension of word embedding. Then I used convolution layer to extract feature. In Keras, something like Conv1D(filters=128, kernel_size=3, padding='same', activation='tanh', strides=1).But why the output dimension is (100,128)? I can understand the first number, because I use padding same, and stride 1, so the dimension should be the same. But why the second dimension is 128, shouldn't it be 200*128? What does the kernel actually look like? I'm assuming it only scan along the sentence, but why the embedding dimension get lost, the kernel just summed it up?I add a picture to illustrate it better. If it is a 1D kernel, and do convolution over the word sequence, why after convolution the word embedding dimension becomes 1(shown in picture)? That doesn't make sense to me."
1854,How to interpret CBOW word embeddings?,"['nlp', 'word2vec', 'word-embedding']","In context of word2vec, it is said that ""words occurring in similar contexts have similar word embeddings""; for example, ""love"" and ""hate"" may have similar embeddings because they appear in contextual words such as ""I"" and ""movie"", just for an example. I get the intuition with skip-gram: both embeddings of ""love"" and ""hate"" should predict the context words ""I"" and ""movie"", thus the embeddings should be similar. However, I can't get it with CBOW: it says that the average embeddings of ""I"" and ""movie"" should predict ""love"" and ""hate""; does that necessarily lead to that the embeddings of ""love"" and ""hate"" should be similar? Or do we interpret word embeddings of SG and CBOW in different ways?"
1855,Use proxy sentences from cleaned data,"['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']","Gensim's Word2Vec model takes as an  input a list of lists with the inner list containing individual tokens/words of a sentence. As I understand Word2Vec is used to ""quantify"" the context of words within a text using vectors. 
I am currently dealing with a corpus of text that has already been split into individual tokens and no longer contains an obvious sentence format (punctuation has been removed). I was wondering how should I input this into the Word2Vec model? Say if I simply split the corpus into ""sentences"" of uniform length (10 tokens per sentence for example), would this be a good way of inputting the data into the model? Essentially, I am wondering how the format of the input sentences (list of lists) affects the output of Word2Vec? "
1856,How to get word vectors from Keras Embedding Layer,"['python', 'dictionary', 'keras', 'keras-layer', 'word-embedding']","I'm currently working with a Keras model which has a embedding layer as first layer. In order to visualize the relationships and similarity of words between each other I need a function that returns the mapping of words and vectors of every element in the vocabulary (e.g. 'love' - [0.21, 0.56, ..., 0.65, 0.10]).Is there any way to do it?"
1857,Word2vec Model size is very small and it is not recognizing words,"['python', 'python-3.x', 'word2vec', 'gensim', 'word-embedding']","I trained my word2vec model using the gensim package on 6.4 GB text data which is preprocessed using the following code snippet :But every time I  train the model, its size is 147 Kb which doesn't seems right and when I tried generating vectors from the trained model it says :The following is the code I used for training my word2vec model :Please help me resolving this issue."
1858,Keras: Embadding on Sentences Array as Input,"['python', 'machine-learning', 'neural-network', 'keras', 'word-embedding']","I am new to keras. I try to create a neural network with an embedding layer as input layer. As far as I understand it right now the structure is like the following:This works with a dataset of labelled sentences. Now I want to change the structure of my dataset. I will have labelled sets of sentences (arrays of sentences).As far as I know, I can't use the Embedding layer anymore as input layer. This is because it expects a sentence as input and not an array of sentences. Can I change something so that I still can use the embedding layer in my model but have arrays of sentences as input? The array length of my arrays of sentences is all the time the same as my sentence length stays the same overall sentences."
1859,How is embedding matrix being trained in this code snippet?,"['python', 'tensorflow', 'lstm', 'word-embedding']","I'm following the code of a coursera assignment which implements a NER tagger using  a bidirectional LSTM.But I'm not able to understand how the embedding matrix is being updated. In the following code, build_layers has a variable embedding_matrix_variable which acts an input the the LSTM. However it's not getting updated anywhere.Can you help me understand how embeddings are being trained?"
1860,Paragraph Vector or Doc2vec model size,"['nlp', 'gensim', 'word-embedding', 'doc2vec', 'deeplearning4j']","I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. Thank you!"
1861,Gensim Word2Vec select minor set of word vectors from pretrained model,"['python', 'keras', 'word2vec', 'gensim', 'word-embedding']","I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?"
1862,can I tokenize using spacy and then extract vectors for these token using pre trained word embeddings of fastext,"['nlp', 'spacy', 'word-embedding', 'fasttext']","I am tokenizing my text corpus which is in german language using the spacy's german model. 
Since currently, spacy only has small german model, I am unable to extract the word vectors using spacy itself. 
So, I am using fasttext's pre-trained word embeddings from here:https://github.com/facebookresearch/fastText/blob/master/README.md#word-representation-learningNow facebook has used ICU tokenizer for tokenization process before extracting word embeddings for it. and i am using spacy
Can someone tell me if this is okay?
I feel spacy and ICU tokenizer might behave differently and if so then many tokens in my text corpus would not have a corresponding word vectorThank for your help!"
1863,loading of fasttext pre trained german word embedding's .vec file throwing out of memory error,"['nlp', 'gensim', 'word-embedding', 'fasttext']",I am using gensim to load the fasttext's pre-trained word embeddingde_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')But this gives me a memory error.Is there any way I can load it?
1864,Fine-tuning Glove Embeddings,"['machine-learning', 'nlp', 'word2vec', 'word-embedding']","Has anyone tried to fine-tune Glove embeddings on a domain-specific corpus?
Fine-tuning word2vec embeddings has proven very efficient for me in a various NLP tasks, but I am wondering whether generating a cooccurrence matrix on my domain-specific corpus, and training glove embeddings (initialized with pre-trained embeddings) on that corpus would generate similar improvements."
1865,Python module Gensim error “cannot import name utils”,"['python', 'pip', 'gensim', 'word-embedding']",Hi I am using Gensim Word2Vec for word embedding in python.But i am getting error like:ImportError: cannot import name utils. Thank you
1866,Can we use char rnn to create embeddings for out of vocabulary words?,"['nlp', 'lstm', 'rnn', 'word-embedding', 'fasttext']","I have word embeddings for 10 million words which were trained on a huge corpus. Now I want to produce word embeddings for out of vocabulary words.  Can I design some char RNN to use these word embeddings and generate embeddings for out of vocab words? Or is there any other I can get embeddings for OOV words?FastText is capable of producing word embeddings for OOV but it does not have distributed way of training or GPU implementation, so in my case, it could take almost 3 months to finish training. Any suggestions on this?"
1867,How to make the tensorflow hub embeddings servable using tensorflow serving?,"['tensorflow', 'tensorflow-serving', 'word-embedding']","I am trying use an embeddings module from tensorflow hub as servable. I am new to tensorflow. Currently, I am using Universal Sentence Encoder embeddings as a lookup to convert sentences to embeddings and then using those embeddings to find a similarity to another sentence. My current code to convert sentences into embeddings is: Prepared_text is a list of sentences. How do I take this model and make it a servable? "
1868,Embedding in pytorch,"['pytorch', 'word-embedding']",I have checked the PyTorch tutorial and questions similar to this one on Stackoverflow.I get confused; does the embedding in pytorch (Embedding) make the similar words closer to each other? And do I just need to give to it all the sentences? Or it is just a lookup table and I need to code the model?
1869,Word Embedding to word,"['neural-network', 'nlp', 'keras', 'word-embedding']","I am using a GloVe based pre-trained embedded vectors for words in my I/P sentences to a NMT-like model. The model then generates a series of word embeddings as its output for each sentence.How can I convert these output word embeddings to respective words? One way I tried is using cosine similarity between each output embedding vector and all the i/p embedding vectors. Is there a better way than this?Also, is there a better way to approach this than using embedding vectors?"
1870,Word Embeddings with neural networks in keras [closed],"['r', 'machine-learning', 'nlp', 'word-embedding']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.Can someone give me an intuitive explanation for why we use word embeddings and how neural networks make this process easier and better? "
1871,Visualize Gensim Word2vec Embeddings in Tensorboard Projector,"['python', 'tensorflow', 'gensim', 'tensorboard', 'word-embedding']","I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. It didn't work. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.
Equivalent to my code:  That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.
One idea that has been presented to me is implementing the appropriate tensorflow code, but I don’t know how to code that, just import files in the live demo.  Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do.
  Ok. Done with that too! Thanks for your help!"
1872,How can I get the word2vec.bin file,"['nlp', 'chatbot', 'word2vec', 'word-embedding', 'seq2seq']","I want to build a chatbot using python and deeplearning methodology.Iam referring the below link
chatbot code
But I troubled in the word2vec.bin file as describing in the code.Where should I get the bin file?"
1873,memory error when using gensim for loading word2vec,"['python', 'word2vec', 'gensim', 'word-embedding', 'google-news']","I am using gensim library for loading pre-trained word vectors from GoogleNews dataset. this dataset contains 3000000 word vectors each of 300 dimensions. when I want to load GoogleNews dataset, I receive a memory error. I have tried this code before without memory error and I don't know why I receive this error now.
I have checked a lot of sites for solving this issue but I cant understand.
this is my code for loading GoogleNews:and this is the error I received:can anybody help me? thanks."
1874,What is the work of “tf.nn.embedding_lookup”,"['tensorflow', 'text-classification', 'word-embedding', 'embedding-lookup']","I am trying to implement Embedding layer for text classification using CNN.
Embedding layer
with tf.device('/cpu:0'), tf.name_scope(""embedding""):
      self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=""W"")
      self.embedded_chars = tf.nn.embedding_lookup(self.W, self.inputTensor)
      self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)I couldn't understand tf.nn.embedding_lookup working."
1875,Gensim Doc2Vec: I'm gettting different vectors from documents that are identical,"['python', 'gensim', 'word-embedding', 'doc2vec']","I have the following code and I think I am getting the vectors in a wrong way, because for example the vectors of two documents that are 100% identical are not the same.And the output has to be like this:Where the first word of each line is the name of each file, and what follows is the corresponding vector for that file. I need to save the vectors in this way to use an external software."
1876,Got a smaller vocabulary after using tf.contrib.learn.preprocessing.VocabularyProcessor,"['python', 'tensorflow', 'nlp', 'word2vec', 'word-embedding']","Firstly,I trained word embedding with the code above, I don't think there is anything wrong with it. And I created a list vocab to store the words in the vector file. Then Vocab is a list of  415657 words. And I got a vocabulary of 412722. I know that vocab_processor.fit won't take upper and lower case as two words. This is really strange. How is this happening?
I checked the vector file again. There are no overlapping words at all."
1877,Are the features of Word2Vec independent each other?,"['nlp', 'word2vec', 'text-classification', 'word-embedding']","I am new to NLP and studying Word2Vec. So I am not fully understanding the concept of Word2Vec.Are the features of Word2Vec independent each other?For example, suppose there is a 100-dimensional word2vec. Then the 100 features are independent each other? In other words, if the ""sequence"" of the features are shuffled, then the meaning of word2vec is changed?"
1878,Training Word Vectors on whole corpus?,"['nlp', 'deep-learning', 'word2vec', 'word-embedding']","I am training word2vec model on my corpus and a friend of mine asked me if it is right to train the word2vec model on the whole corpus? Because when creating word embeddings I am using the whole corpus, so basically I am leaking the testing information to my network in form of these vectors, which is not ideal when training a neural network.On the contrary, suppose that I am using a pre-trained word embeddings from google or any other source for that matter, if they have used the same document while creating these embedding, which I will be using to test my network, I will leaking the information anyway.So my question is what is the right way to train the word2vec?Separating test and train data before creating word vectors?Creating word vectors on the whole corpus?"
1879,Keras image captioning model not compiling because of concatenate layer when mask_zero=True in a previous layer,"['tensorflow', 'keras', 'deep-learning', 'lstm', 'word-embedding']","I am new to Keras and I am trying to implement a model for an image captioning project. I am trying to reproduce the model from Image captioning pre-inject architecture (The picture is taken from this paper: Where to put the image in an image captioning generator) (but with a minor difference: generating a word at each time step instead of only generating a single word at the end), in which the inputs for the LSTM at the first time step are the embedded CNN features. The LSTM should support variable input length and in order to do this I padded all the sequences with zeros so that all of them have maxlen time steps.The code for the model I have right now is the following:The model (as it is above) compiles without any errors (see: model summary) and I managed to train it using my data. However, it doesn't take into account the fact that my sequences are zero-padded and the results won't be accurate because of this. When I try to change the Embedding layer in order to support masking (also making sure that I use voc_size + 1 instead of voc_size, as it's mentioned in the documentation) like this:I get the following error:I don't know why it says the shape of the second array is [?, 25, 1], as I am printing its shape before the concatenation and it's [?, 25, 200] (as it should be). 
I don't understand why there'd be an issue with a model that compiles and works fine without that parameter, but I assume there's something I am missing.I have also been thinking about using a Masking layer instead of mask_zero=True, but it should be before the Embedding and the documentation says that the Embedding layer should be the first layer in a model (after the input). Is there anything I could change in order to fix this or is there a workaround to this ?"
1880,Keras imdb sentiment model - how to predict sentiment of new sentences?,"['keras', 'word-embedding']",I'm working my way through the Deep Learning with Python book where there is an example for learning word embeddings for sentiment:I would like to pass in a sentence and make a prediction on the sentiment.  My first thought was to pass in an array of indices (because that's how the works are represented in the model if I have understood correctly) such as:How can I pass in a list of words instead of indices?  I.e. how can I retrieve a list of word indices for my new sentence?Update - I tried to tokenize some words:The sentiment is very similar which suggests that the tokenizing approach does not work.
1881,Unable to run Poincare Embeddings example to get hierarchical representation,"['python', 'hierarchical-data', 'word-embedding']","I'm trying to replicate Poincaré Embeddings for Learning Hierarchical Representations present on Github by Facebook Research. But I'm unable to run example.sh to embed the mammals subtree in the reconstruction setting.OS: Ubuntu 16.04, Using Anaconda 4.3.30, Python 3.6.5, Pytorch 0.4.0Command: NHTHREDS=2 ./example.shError Log: Any help will be appreciated. "
1882,"How to get a random embedding, from an embedding matrix in TensorFlow?","['tensorflow', 'word-embedding']","Assume that I have created an embedding matrix for some words in my training set, like below:I can use tf.nn.embedding_lookup(word_embedding, inputs) to get the embeddings for the words that are in a specific batch, during training. But how can I get a random embedding and use it in, let us say, a matmul operation?"
1883,torch.nn.embedding has run time error,"['python', 'pytorch', 'word-embedding']","I want to use torch.nn.Embedding. I have followed the codes in the documentation of embedding command.
here is the code:The documentation says that you will receive this output:but I don't receive this output. instead I receive this error:Can anybody guide me about this error? and about the work of torch.nn.Embedding?"
1884,embedding word positions in keras,"['python', 'nlp', 'keras', 'word2vec', 'word-embedding']","I am trying to build a relation extraction system for drug-drug interactions using a CNN and need to make embeddings for the words in my sentences. The plan is to represent each word in the sentences as a combination of 3 embeddings: (w2v,dist1,dist2) where w2v is a pretrained word2vec embedding and dist1 and dist2 are the relative distances between each word in the sentence and the two drugs that are possibly related.I am confused about how I should approach the issue of padding so that every sentence is of equal length. Should I pad the tokenised sentences with some series of strings(what string?) to equalise their lengths before any embedding?"
1885,Why cant I build this maven as an executable,"['maven', 'rdf', 'semantic-web', 'word-embedding']","The maven project for rdf2vec builds an executable without any errors however the jar file throws ""no main manifest attribute"" error. My attempt to resolve it:Replace the following snippet :<manifest>
    <mainClass>fully.qualified.MainClass</mainClass>
</manifest>with :<manifest>
    <addClasspath>true</addClasspath>
    <mainClass>walks.WalkGenerator</mainClass>
</manifest>Thereby making WalkGenerator as the default main class for the entire package. 
This sadly, does not do the trick.Another issue is that I cant find the MANIFEST file. "
1886,semantic and syntactic performance of Doc2vec model,"['python-3.x', 'word-embedding', 'doc2vec']","I am trying to check the semantic and syntactic performance of a doc2vec model- doc2vec_model.accuracy(questions-words), but it doesnt seem to function since models.deprecated.doc2vec – Deep learning with paragraph2vec, says it has been deprecated since version 3.3.0 in the gensim package.It gives this error message Though it works with word2vec model well, is there any way I can get it done apart from doc2vec_model.accuracy(questions-words)? or it's impossible?"
1887,How to use GloVe word-embeddings file on Google colaboratory,"['python', 'google-colaboratory', 'word-embedding']",I have downloaded the data with wgetIt is saved as zip and I would like to use glove.6B.300d.txt file from the zip file. What I want to achieve is : Of course I am having this error:How can I unzip and use that file in my code above on Google colab?
1888,"Why does Tensorflow's sampled_softmax_loss force you to use a bias, when experts recommend no bias be used for Word2Vec?","['tensorflow', 'deep-learning', 'word2vec', 'word-embedding']","All the tensorflow implementations of Word2Vec that I have seen has a bias in the negative sampling softmax function, including on the official tensorflow websitehttps://www.tensorflow.org/tutorials/word2vec#vector-representations-of-wordsThis is from Google's free Deep Learning course https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynbHowever, from both Andrew Ng and Richard Socher's lectures, they do not include a bias in their negative sampling softmaxes. Even where this idea originated, Mikolov states that:biases are not used in the neural network, as no significant
  improvement of performance was observed - following the Occam's razor,
  the solution is as simple as it needs to be.Mikolov, T.: Statistical Language Models Based on Neural Networks, p. 29
http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdfSo why do the official tensorflow implementations have a bias, and why does there not seem to be an option to not include a bias in the sampled_softmax_loss function ?"
1889,import gensim vs import gensim.test.utils,"['python', 'gensim', 'word-embedding']","The following line works fine:while the following line generates error:Error:I was working on converting GloVe to word2vec format:
glove2word2vec"
1890,R Keras predict_prob for multi input model is not working. I can train the model sucessfuly but while scoring I get error,"['r', 'tensorflow', 'keras', 'rnn', 'word-embedding']","The multi-input model that takes as input 2 list text embedding and structured features and generate class probabilities. I can train the model successfully but during predict_prob I get an errorWhen I try to generate a prediction based on input structured features(one_hot_encode_test) and text (train tokens) I get error
""Error in py_get_attr_impl(x, name, silent) : 
  AttributeError: 'Model' object has no attribute 'predict_proba"""
1891,Method of vectors in various vector length to fixed length (NLP),"['nlp', 'word2vec', 'word-embedding', 'dictvectorizer']","Recently I have been looking around about Natural Language Processing and its vectorization method and advantages of each vectorizer.I am into character to vectorize, but it seems like the most concerns about the character vectorizer for each word is the embedding to have fixed length.I do not want to just embed them with 0, which is well known as 0 padding, for instance, the target fixed length is 100 and 72 characters only exists then all 28 of 0 will be padded at the end.""The example of paragraphs and phrases.... ... in vectorizer form"" < with length 72becomes[0, 25, 60, 12, 24, 0, 19, 99, 7, 32, 47, 11, 19, 43, 18, 19, 6, 25,
  43, 99, 0, 32, 40, 14, 20, 5, 37, 47, 99, 11, 29, 7, 19, 47, 18, 20,
  60, 18, 19, 2, 19, 11, 31, 130, 130, 76, 0, 32, 40, 14, 20, 7, 19, 47,
  18, 20, 60, 11, 37, 43, 99, 11, 29, 99, 17, 39, 47, 11, 31, 18, 19,
  43, 0, 19, 77, 0, 0, 0, 0, 0, 0, 0, 0, ...., 0, 0, 0, 0, 0, 0]..I want to make the vectors be in a fair distribution form in N fixed dimensions, not like the one aboveIf you know any papers or algorithms preferring consider this matter, or common way to produce a fixed length vectors from various length of vectors please share ...Further information added as gojomo requested;I am trying to get the character level vectors for words in corpus.Let say, in above example, ""The example of paragraphs...."" starts withT [40] h [17]e [3]e [3] x [53]a [1]m [21]p [25]l [14]e [3]Notice that each character has its own number (etc, could be ascii) and word represents the vectors of character vectors combination, for example,The [40, 17, 3]example [3, 53, 1, 21, 25, 14, 3]which the vectors are not in same dimension. With the case mention above, many people are padding 0 at the end to make it in uniform sizeFor example, if someone wants to make the dimension of each word to be 300, then 297 of 0s will be padded to letter ""The"" and 293 of 0s will be padded to ""example""., like The [40, 17, 3, 0, 0, 0, 0, 0, ...., 0]example [3, 53, 1, 21, 25, 14, 3, 0, 0, 0, 0, 0, ...., 0]Now I do not think this padding method is appropriate to my experiments so I want to know if there are any methods to convert its vectors to in uniform form with not sparsed form(if this term is allowed).Even with the phrase with two words, ""The example"" only takes 11 characters long , still not long enough either.Whatever the case is that, I would like to know if there are some well known techniques to convert the informal length of vectors to some fixed length.Thank you !"
1892,Value error when adding a word embedding layer to the CNN model,"['python', 'tensorflow', 'conv-neural-network', 'word-embedding', 'fasttext']","I am trying to add a FastText embedding layer to the famous text classification architecture with CNN: https://github.com/dennybritz/cnn-text-classification-tfI load my FastText embedding like this: And below is how I add my embedding layer in Tensorflow: Initialization of the CNN model:Single training step: For the reference, below is how my x_train and y_train looks like: When I start training, I get this error:This error seems like a numpy array creation error, where it is tried to create an array element as a sequence. 
BUT:
I don't receive any error when I remove the embedding while my train data is completely the same thus batches are as well. What could cause this error?"
1893,Fasttext algorithm use only word and subword? or sentences too?,"['nlp', 'vectorization', 'word2vec', 'word-embedding', 'fasttext']","I read the paper and googled as well if there is any good example of the learning method(or more likely learning procedure)For word2vec, suppose there is corpus sentenceI go to school with lunch box that my mother wrapped every morningThen with window size 2, it will try to obtain the vector for 'school' by using surrounding words ['go', 'to', 'with', 'lunch']Now, FastText says that it uses the subword to obtain the vector, so it is definitely use n gram subword, for example with n=3,['sc', 'sch', 'cho', 'hoo', 'ool', 'school']Up to here, I understood.
But it is not clear that if the other words are being used for learning for 'school'. I can only guess that other surrounding words are used as well like the word2vec, since the paper mentions=> the terms Wc and Wt are both used in functionswhere Wc is context word and Wt is word at sequence t.However, it is not clear that how FastText learns the vectors for word...Please clearly explain how FastText learning process goes in procedure?..More precisely I want to know that if FastText also follows the same procedure as Word2Vec while it learns the n-gram characterized subword in addition. Or only n-gram characterized subword with word being used?How does it vectorize the subword at initial? etc"
1894,Multiple embedding layers in keras,"['tensorflow', 'keras', 'embedding', 'word-embedding']","With pretrained embeddings, we can specify them as weights in keras' embedding layer. To use multiple embeddings, would specifying multiple embedding layer be suitable? i.e.This suggests to sum them up and represent them into a single layer, which is not what I am after."
1895,How to generate GloVe embeddings for POS tags? Python,"['python-3.x', 'machine-learning', 'nlp', 'spacy', 'word-embedding']","For a sentence analysis task, I would like to take the sequence of POS tags associated with the sentence and feed it to my model as if the POS tags are words. I am using GloVe to make representations of each word in the sentence and SpaCy to generate POS tags. However, GloVe embeddings do not make much sense for POS tags. So I will have to somehow create embeddings for each POS tag. What is the best way to do create embeddings for POS tags, so that I can feed POS sequences into my model in the same way I would feed sentences? Could anyone point to code examples of how to do this with GloVe in Python?Added contextMy task is a binary classification of sentence pairs, based on their resemblance (similar meaning vs different meaning). I would like to use POS tags as words, so that the POS tags serve as an additional bit of information to compare the sentences. My current model does not use an LSTM as a way to predict sequences. "
1896,tf.nn.embedding_lookup taking too much time,"['python', 'tensorflow', 'deep-learning', 'word2vec', 'word-embedding']","I am trying GloVe word embedding with tensorflow embedding_lookup but tensorflow is taking too much time , for example :after clearning this text , and converting to vector form using GloVe it looks like this:Now if i am trying tensorflow embedding_looup like this:But its taking more time , while manually i can do faster using numpy and index method. Where i am doing wrong ?"
1897,Correct gradients for word2vec negative sampling skip gram model,"['python', 'machine-learning', 'nlp', 'word2vec', 'word-embedding']","I am trying to implement skip-gram word2vec in python using negative sampling.
From my understanding, I should be maximizing the equation (4) from the paper by Mikolov Et al. I have taken the gradients of this equation with respect to Vc, U, and U_rand. Where Vc is the center vector corresponding to the center word, U is the context vector corresponding to a word in the context of the center word and U_rand is the context vector of a randomly sampled word.I am then calculating the cost function for each combination of word and context word adding them up and printing out the total sum of the whole corpus. I am running this a few times and I do not see an improvement on the whole corpus sum of costs. The cost goes up and then down repeatedly.  I got the following gradientsgrad J with respect to Vc = (1-sigma(V•U))*U - Summation over random
  vectors (1-sigma(-V•U_rand))*U_randgrad J with respect to U = (1-sigma(V•U))*Vgrad J with respect to U_rand = (1-sigma(-V•U_rand))*-VSo with that being said, I have a few questions: "
1898,what actually word embedding dimensions values represent?,"['python', 'tensorflow', 'nlp', 'word2vec', 'word-embedding']","I have a doubt in word2vec and word embedding , I have downloaded GloVe pre-trained word embedding (shape 40,000 x 50)  and using this function to extract information from that:Now if I call this function for word 'python'  something like :it gives me 1x50 shape vector like this:I need help in understanding the output matrix. What does these value represent and there significance in generating new word"
1899,How to measure the word weight using doc2vec vector,"['python', 'algorithm', 'word-embedding', 'doc2vec']","I'm using the word2vec algorithm to detect the most important words in a document, my question is about how to compute the weight of an important word using the vector obtained from doc2vec, my code is like that:thank you for your consideration."
1900,How to build word embedding model using Tflearn?,"['tensorflow', 'machine-learning', 'nlp', 'tflearn', 'word-embedding']","UpdatedI am working on the word embedding model for answer Matching score prediction using Tflearn. I have to build a model using sentence vector using tflearn dnn classifier,  Now I have to add a word embedding layer to the dnn model. How to do that? Thanks in advance.""JVMdefines"": enables a computer to run a Java programis coverted as :""JVMdefines"": [[list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])enables a computer to run a Java program :
list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]My question:  Is there any method that the machine can able to analyze.enables a ""machine"" to run a Java programThat is It can detect computer and machine as in same meaning."
1901,GloVe - uneven encoding vector,"['python', 'nlp', 'word-embedding']",I just downloaded glove.840B.300d.zipand noticed that not all elements are encoded to the same vector size. In particularproduced:Is this a bug or I am doing something wrong?
1902,Python - Efficiently find n nearest vectors,"['python', 'vector', 'similarity', 'cosine-similarity', 'word-embedding']","I'm trying to write a Python method to efficiently return the n closest words to a given word, based on their respective embedding vectors. Each vector is 200 dimensions, and there are a couple million of them.Here's what I have at the moment, which simply does a cosine similarity comparison against the target word and every other word. This is very, very slow:Does anybody have any better suggestions?"
1903,How to use Keras Embedding layer when there are more than 1 text features,"['keras', 'word2vec', 'word-embedding']","I understand how to use the Keras Embedding layer in case there is a single text feature like in IMDB review classification. However, I am confused how to use the Embedding Layers when I have a Classification problem, where there are more than a single text feature. For example, I have a dataset with 2 text features Diagnosis Text, and Requested Procedure and the label is binary class (1 for approved, 0 for not approved). In the example below, x_train has 2 columns Diagnosis and Procedure, unlike the IMDB dataset. Do I need to create 2 Embedding layers, one for Diagnosis, and Procedure? If so, what code changes would be required? "
1904,how to convert Word to vector using embedding layer in Keras,"['keras', 'nlp', 'word2vec', 'word-embedding']",I am having a word embedding file as shown below click here to see the complete file in github.I would like to know the procedure for generating word embeddings So that i can generate word embedding for my personal dataset   
1905,Word-Embeddings compression for deep learning models,"['compression', 'word-embedding']","Can anyone suggest any techniques/concepts to compress word-embedding like Glove 300d?
As it increases the size of model."
1906,How to embed out of vocab words at the time of testing in word2vec model?,"['nlp', 'word2vec', 'word-embedding']","I was training my word2vec model (skip-gram) on a vocab size of 100 000. But at the time of testing I got few words which weren't in the vocab. To find their embeddings I tried 2 approaches: Calculate minimum edit distance word from vocab and acquire its embedding.Constructed different n-grams from the word and searched them in the vocab.Despite of applying these methods, I am not able to get rid of out of vocab words problem completely.Does word2vec take all n-grams of a word into account while training like fastText does?Note - In fastText if our input word is quora then it considers all of its possible n-grams in the corpus."
1907,How can I load Word2vec with Gensim without getting an AttributeError?,"['python', 'word2vec', 'gensim', 'word-embedding']","I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: xxxx.model.wv and a bigger one xxxx.model.wv.syn0.npy.When I call the following line:I get the following error:How can I solve this error?"
1908,Shape of Input in Functional Keras,"['tensorflow', 'neural-network', 'keras', 'word-embedding']","I want to do a word embedding + BLSTM in keras using a functional model. I have to declare the input as:Which should be the value of X here? Since the inputs will be sentences, I am not very sure what I should put in there."
1909,Gensim Word2Vec most similar different result python,"['python', 'string', 'word2vec', 'gensim', 'word-embedding']","I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of Hermione have been replaced with Hermione_1; in the second, all the occurrencies of Hermione have been replaced with Hermione_2. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:How is possible that model.wv.most_similar(""Hermione_1"") and model.wv.most_similar(""Hermione_2"") give me different output? 
Their neighbour are completely different. This is the output of the four print:"
1910,Word2Vec Python similarity,"['python', 'similarity', 'word2vec', 'gensim', 'word-embedding']","I made a word embedding with this code:I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between model[""word""],model.wv.most_similar(), model.similar_by_vector() and model.similarity()?
Which one should I use?"
1911,Using LDA for word embedding,"['machine-learning', 'gensim', 'lda', 'topic-modeling', 'word-embedding']","I want to do word_embedding using LDA to represent each of documents in my corpus with a vector that each dimension shows one of the detected topics by LDA model, but I don't know how to do it. Any suggestion will be appreciated.
I use python 3.6 and gensim library for LDA."
1912,How do I combine features like word embeddings and sentiment polarity for text classification using LSTM neural networks?,"['keras', 'recurrent-neural-network', 'feature-extraction', 'text-classification', 'word-embedding']","Embeddings layer of LSTM is fed with the weights=embedding_matrix from the vocab, and model.fit has X_train which is the tokenized text data.
My X_train has shape (12,000 , 100) and embeddings_matrix has shape (34613, 300) where 34613 are the number of tokens(vocab from complete data ~15000 sentences).I created a sentiment_matrix associating a polarity -1/0/1 with every word of shape (34613, 1).Given the sentiment polarity is a per word information(just like embeddings are), how do I prepare the sentiment feature, and how to give this as input to the neural network?I looked up the keras functional API https://keras.io/getting-started/functional-api-guide/ 
But I couldn't get it to work, so please help."
1913,Add metadata in your RNN model after embedding layer with shared weights,"['tensorflow', 'neural-network', 'keras', 'rnn', 'word-embedding']","I have a embedding matrix with shared weights for text threads after which I wanted to add metadata into my model. However, adding a new layer using same function as before for initializing model layers is giving me dimension errors. Can someone tell me how I can proceed?"
1914,Python Tf idf algorithm,"['python', 'tf-idf', 'words', 'word-embedding']","I would like to find the most relevant words over a set of documents.I would like to call a Tf Idf algorithm over 3 documents and return a csv file containing each word and its frequency. After that, I will take only the ones with a high number and I will use them.I found this implementation that does what I need https://github.com/mccurdyc/tf-idf/. I call that jar using the subprocess library. But there is a huge problem in that code: it commits a lot of mistake in analyzing words. It mixs some words, it has problems with ' and - (I think). I am using it over the text of 3 books (Harry Potter) and , for example, I am obtaining words such hermiones, hermionell, riddlehermione, thinghermione instead of just hermione in the csv file.Am I doing wrong something? Can you give me a working implementation of the Tf idf algorithm? Is there a python library that does that?"
1915,Save and load word embedding word2vec python,"['python', 'word2vec', 'word-embedding']","I create and load a word embedding using this code:Why, for example, this code produces different output?Outputs:In the second one there is the u in front of all the results"
1916,Project a word on an axis of similarity between 2 words,"['vector', 'data-visualization', 'word2vec', 'gensim', 'word-embedding']","I am using Word2Vec for word embeddings. I want to project a word W on an axis which represent the similarity of the word W to two given word W1 and W2 in a way that we can see to which word W is more similiar to, like this:
imageWhat is the best way to do it?"
1917,How to deal with out-of-vocaboular words in NLP applications when word embedding is used?,"['tensorflow', 'nlp', 'named-entity-recognition', 'word-embedding']",The following NLP application uses word embedding. But I am not sure what if a word in an input text is not available in the embedding. Does anybody know what is the standardard practice to deal with words that are not in the embedding for NLP (or NER in particular)? Thanks.https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
1918,What is “unk” in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?,"['neural-network', 'deep-learning', 'nlp', 'word-embedding', 'glove']","I found ""unk"" token in the glove vector file glove.6B.50d.txt downloaded from https://nlp.stanford.edu/projects/glove/. Its value is as follows:Is it a token to be used for unknown words or is it some kind of abbreviation?"
1919,How to vectorize text in order to use it as feature for time series prediction? (Keras),"['r', 'machine-learning', 'nlp', 'time-series', 'word-embedding']","What is the best way to vectorize text in order to use it as one of many features for time series prediction? The time series is daily and I have 8 to 10 different news headlines per date (~16,000 headlines in total). Each headline consists of max 25 words. The headlines are cleaned (lower case, punctuation and number removal, stop word removal and lemmatized) and tokenized on a word level. How can I vectorize the headlines and aggregate them on a daily level so that I can use them as an input feature? Because all other features (e.g. Federal Funds Rate, Gold Price, etc.) are just a single integer per date. I thought of using word embeddings. But training a word embedding model (word2vec or GloVe) on only 16,000 headlines probably won’t achieve good results. However, even if I use pre-trained word vectors I am worried about the column dimensionality of my time series data frame in R. Since the word vector is 100 dimensional and I have 25 words per headline and 8-10 headlines per date, the dimension of my time series would be 100x25x10 = 25,000 columns and 1,700 rows (1,700 days). So do you have any idees of how I can include the news headlines as a feature for time series prediction? If it helps, I plan to implement a LSTM Neural Network in Keras using R for predicting the trend (up or down) of a traded asset.Ideas and advice are much appreciated.Thanks a lot."
1920,How can I handle a date such as '1990-03-27' in Glove model for creating embeddings?,"['nlp', 'stanford-nlp', 'word2vec', 'word-embedding']","I want to create embeddings for words in my document. What do I transform the above mentioned date into so that I can create embeddings in glove?
Thanks in advance. "
1921,make part of word embedding trainable,"['tensorflow', 'word-embedding']","Is there anyway to update only a small subset of pretrained glove word embeddings in TensorFlow. I am looking for pointers on how to implement such case in TensorFlow. The reason I want only a few words' embedding to be updated is that I am doing a question answering task and is using pretrained glove embeddings. But I would like to make words in the query that indicate high level query type to be trainable and get updated in the training process, words such as 'what', 'when', 'where', 'who' etc. "
1922,A package to replace H2O's `h2o.word2vec` function? [closed],"['r', 'text-mining', 'word2vec', 'h2o', 'word-embedding']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 2 years ago.I am successfully using h2o.word2vec function to load pre-trained GloVe embeddings in R.embed_model <- h2o.word2vec(pre_trained = glove, vec_size = 300)
df_doc_vecs <- h2o.transform(embed_model, tokenized_words, aggregate_method = ""AVERAGE"")However, because my production environment will not support H2O, I need to find another package with similar functionality. Specifically I just need to be able to replace h2o.word2vec and h2o.transform with something else to:[1] Use GloVe or any other pre-trained embeddings[2] Create sentence or document level vectors out of themAny suggestions? Has to be in R..."
1923,Word embedding as features for classification,"['nlp', 'text-classification', 'word-embedding']","W.r.t text classification, now a common approach is to combine (often sum or average) word embeddings and use the result vector as features.Are there any reference document(s) that establish the comparison of this approach for text classification over traditional feature engineering approaches? [Comparison based on accuracy] [could be on popular datasets like IMDB, sentiment-140 etc]"
1924,How do i build a model using Glove word embeddings and predict on Test data using text2vec in R,"['r', 'word2vec', 'text-classification', 'word-embedding', 'text2vec']","I am building a classification model on text data into two categories(i.e. classifying each comment into 2 categories) using GloVe word embeddings. I have two columns, one with textual data(comments) and the other one is a binary Target variable(whether a comment is actionable or not). I was able to generate Glove word embeddings for textual data using the following code from text2vec documentation.How do i build a model and generate predictions on test data?"
1925,How to load embeddings (in tsv file) generated from StarSpace,"['gensim', 'word-embedding']","Does anyone know how to load a tsv file with embeddings generated from StarSpace into Gensim? Gensim documentation seems to use Word2Vec a lot and I couldn't find a pertinent answer.Thanks,Amulya"
1926,Semantically weighted mean of word embeddings,"['python', 'vector', 'semantics', 'word2vec', 'word-embedding']","Given a list of word embedding vectors I'm trying to calculate an average word embedding where some words are more meaningful than others. In other words, I want to calculate a semantically weighted word embedding.All the stuff I found is on just finding the mean vector (which is quite trivial of course) which represents the average meaning of the list OR some kind of weighted average of words for document representation, however that is not what I want.For example, given word vectors for ['sunglasses', 'jeans', 'hats'] I would like to calculate such a vector which represents the semantics of those words BUT with 'sunglasses' having a bigger semantic impact. So, when comparing similarity, the word 'glasses' should be more similar to the list than 'pants'.I hope the question is clear and thank you very much in advance! "
1927,Does the size of embedding have a huge influence on the training speed?,"['tensorflow', 'word-embedding']","In my text classification task, I use vectors to represent the words.
After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s.
Here is the word embedding tensor creatation:And the sentences are made of word ids and vectors will be looked up from the embedding tensor above.So what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?"
1928,Train only some word embeddings (Keras),"['python', 'nlp', 'keras', 'word-embedding']","In my model, I use GloVe pre-trained embeddings. I wish to keep them non-trainable in order to decrease the number of model parameters and avoid overfit. However, I have a special symbol whose embedding I do want to train.Using the provided Embedding Layer, I can only use the parameter 'trainable' to set the trainability of all embeddings in the following way:Is there a Keras-level solution to training only a subset of embeddings?  Please note:"
1929,Export gensim doc2vec embeddings into separate file to use with keras Embedding layer later,"['keras', 'gensim', 'word-embedding', 'doc2vec']","I am a bit new to gensim and right now I am trying to solve the problem which involves using the doc2vec embeddings in keras. I wasn't able to find existing implementation of doc2vec in keras - as far as I see in all examples I found so far everyone just uses the gensim to get the document embeddings.Once I trained my doc2vec model in gensim I need to export embeddings weights from genim into keras somehow and it is not really clear on how to do that. I see thatSupposedly gives the word2vec embedding weights (according to this). But it is unclear how to do the same export for document embeddings. Any advise?I know that in general I can just get the embeddings for each document directly from gensim model but I want to fine-tune the embedding layer in keras later on, since doc embeddings will be used as a part of a larger task hence they might be fine-tuned a bit."
1930,R - Document-context matrix from dtm-tf and word embeddings,"['r', 'machine-learning', 'deep-learning', 'text-classification', 'word-embedding']","I have a term-frequency, document-term matrix (dtm-tf) in which each row is a document, each column is a term, and each number in the matrix represents the number of occurences of the term in the document. I also have a term-context matrix (a matrix of word vectors/embeddings) where each row is a term from the dtm and each column is essentially number that can be used to relate it to other words used in a similar context.Here is a very minimal reproducible example of what I have:From these, I would like to generate a document-context matrix where each row is a document and each column is a number indicative of context.I think that the best way of going about this would be to average the corresponding values in the word vectors of every word in each document.Currently I'm doing this:What would be the most efficient way for me to do this? If there are better methods than averaging, feel free to mention that as well."
1931,Document representation with pre-trained Word Vectors for Author Classification/Regression (GP),"['machine-learning', 'nlp', 'word2vec', 'text-classification', 'word-embedding']","I am trying to replicate (https://arxiv.org/abs/1704.05513) to do a Big 5 author classification on Facebook data (posts and Big 5 profiles are given).After removing the stop words, I embed each word in the file with their pre-trained GloVe word vectors. However, computing the average or coordinate-wise min/max word vector for each user and using those as input for a Gaussian Process/SVM gives me terrible results. Now I wonder what the paper means by:Our method combines Word Embedding with Gaussian
  Processes. We extract the words from the users’ tweets and
  average their Word Embedding representation into a single
  vector. The Gaussian Processes model then takes these
  vectors as an input for training and testing.What else can I ""average"" the vectors to get decent results and do they use some specific Gaussian Process?"
1932,Google Colab upload word embeddings,"['nlp', 'word-embedding', 'google-colaboratory']","I'm using Google Colab for my DL model (NLP), I uploaded and imported my training data (screenshot) and now I'd like to pre-train on GloVe word embeddings. If I upload the same way, it will take hours I guess, and even then I'm not sure if it works.Did anyone come across the same problem?Thanksuploading training data
"
1933,How to efficiently transform a sparse word embedding matrix to gensim KeyedVectors object,"['python', 'sparse-matrix', 'word2vec', 'gensim', 'word-embedding']","I have a large sparse word embedding matrix that is trained from sklearn tfidf which has nothing to do with the Gensim word2vec. It is very similar to: gensim Word2vec transfer learning (from a non-gensim model) and How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?However, given that the matrix is very sparse, I would like to store them more memore efficiently and reload them by gensim KeyedVecotrs. Or create the KeyedVectors instance without saving the sparse matrix and then directly save the gensim word2vec object. Thanks. follow-up:I end up successfully doing in this way:However, I have to make the Xsparse word embedding matrix dense. Not sure if there is any more efficient way. Thanks for any answer."
1934,How to search Word2Vec or GloVe Embedding to find words by semantic relationship,"['machine-learning', 'nlp', 'keras', 'word2vec', 'word-embedding']",Common examples of showing Word Embedding's strength is to show semantic relationship between some words such king:queen = male:female. How can this type of relationship be discovered? Is that through some kind of visualization based on geometric clustering? Any pointer will be appreciated.
1935,Getting embedding matrix of all zeros after performing word embedding on any input data,"['keras', 'tokenize', 'word2vec', 'word-embedding']","I am trying to do word embeddings in Keras. I am using 'glove.6B.50d.txt' for the purpose. I am able to get correct output till the preparation of embedding index from the ""glove.6B.50d.txt"" file.But I'm always getting embedding matrix full of zeros whenever I map the word from the input provided by me to that in the embedding index.Here is the code:Here when I print the embedding matrix ,I get all zeros in it (i.e not a single word in input is recognized).Also if I print the embeddings_index.get(word) for each iteration, it is unable to fetch the word and returns NONE.Where am I going wrong in the code?"
1936,How to save and load Glove models?,"['python-3.x', 'word-embedding']","I am using Python 3.5 to do my research. I want to make use of Glove word embeddings. How can I save and load my Glove model after glove.fit?
I have coded it like this"
1937,Word alignment task vs dictionary induction,"['nlp', 'word-embedding']","I've been reading up on multi-lingual word embedding methods, and couldn't quite grasp the difference between two of the evaluation methods used - word alignment and dictionary induction.
My curiosity was heightened by looking at Table 1 from this paper, where the Bilingual Autoencoders methods overperforms Inverted index for the Word Alignment task, but it's the other way around for Dictionary Induction.
Thanks for the help!  "
1938,What is the best way to handle missing words when using word embeddings?,"['machine-learning', 'nlp', 'deep-learning', 'word2vec', 'word-embedding']",I have a set of pre-trained word2vec word vectors and a corpus. I want to use the word vectors to represent words in the corpus. The corpus has some words in it that I don't have trained word vectors for. What's the best way to handle those words for which there is no pre-trained vector?I've heard several suggestions. use a vector of zeros for every missing worduse a vector of random numbers for every missing word (with a bunch of suggestions on how to bound those randoms)an idea I had: take a vector whose values are the mean of all values in that position from all pre-trained vectorsAnyone with experience with the problem have thoughts on how to handle this?
1939,Load vectors into gensim Word2Vec model - not KeyedVectors,"['machine-learning', 'nlp', 'word2vec', 'gensim', 'word-embedding']","I'm attempting to load some pre-trained vectors into a gensim Word2Vec model, so they can be retrained with new data. My understanding is I can do the retraining with gensim.Word2Vec.train(). However, the only way I can find to load the vectors is with gensim.models.KeyedVectors.load_word2vec_format('path/to/file.bin', binary=True) which creates an object of what is usually the wv attribute of a gensim.Word2Vec model. But this object, on it's own, does not have a train() method, which is what I need to retrain the vectors. So how do I get these vectors into an actual gensim.Word2Vec model?"
1940,How to group multiple words that together convey one particular meaning using word embedding NLP,"['nlp', 'keras', 'lstm', 'word-embedding']","I am working on a NLP project and I am new to this field. I am doing WORD EMBEDDINGS in KERAS. I wanted to embed multiple words, (which together convey a particular meaning) together as one word.For eg:  Copper pipe, both of them together convey one meaning but as separate words looses the context completely.Similarly, Mechanical Engineer, hot water, N-Dimension Vector space  etcHow do I do it so that together they get ONE embedding vector ??"
1941,How do I create a Keras Embedding layer from a pre-trained word embedding dataset?,"['python', 'tensorflow', 'keras', 'word2vec', 'word-embedding']",How do I load a pre-trained word-embedding into a Keras Embedding layer? I downloaded the glove.6B.50d.txt (glove.6B.zip file from https://nlp.stanford.edu/projects/glove/) and I'm not sure how to add it to a Keras Embedding layer. See: https://keras.io/layers/embeddings/
1942,TensorFlow gradients: Getting unnecessary 0.0 gradients by tf.gradients,"['tensorflow', 'embedding', 'word-embedding', 'gradient']","Let's suppose I have the following variableembeddings = tf.Variable(tf.random_uniform(dtype=tf.float32,shape =
  [self.vocab_size, self.embedding_dim], minval=-0.001, maxval=0.001))sent_1 = construct_sentence(word_ids_1)sent_2 = construct_sentence(word_ids_2)Where construct_sentence is a method of obtaining a sentence representation based on the placeholders word_ids_1 and word_ids_2Let's suppose I have some loss:loss = construct_loss(sent_1, sent_2, label)Now, when I try to get the gradients using:gradients_wrt_w = tf.gradients(loss, embeddings)Instead of getting only the gradients with respect to the specific variables involved in construct_sentence and construct_loss, I get the gradients of every embedding in the variable embeddings (the gradients being 0 for those embeddings that are not involved in the loss & sentence representations). How can I get the gradients wrt variables I am only interested in?Moreover, I get repetitions of some variables (with the same value) because of the partial derivatives involved.  Since embeddings is a 2D Variable I can't do a simple lookup like this:tf.gradients(loss, tf.nn.embedding_lookup(embeddings, word_ids))This introduces a huge performance slow-down, since I am working with large number of word embeddings and I want to take the derivative only wrt some word embeddings per time.Moreover, I am getting a lot of duplicate gradients (because of the partial derivatives) and I tried using the  tf.AggregationMethod but that didn't work out."
1943,Matrix and vector dimensions in w2v,"['word2vec', 'word-embedding']","I need an explanation of the dimensions of matrices and vectors in word2vec (CBOW algorithm). What they exactly are, why do we need them and what I should know about that? "
1944,Converting sparse IndexedSlices to a dense Tensor,"['tensorflow', 'keras', 'word-embedding', 'gated-recurrent-unit']",I got the following warning:For the following code:That is I make use of WordSeq (from WordBatch) output from the Embedding layer of a GRU network. How should I modify the code to make it work without converting to dense tensor?
1945,What is the preferred ratio between the vocabulary size and embedding dimension?,"['machine-learning', 'keras', 'nltk', 'word-embedding', 'nltk-trainer']","When using for example gensim, word2vec or a similar method for training your embedding vectors I was wonder what is a good ratio or is there a preferred ratio between the embedding dimension to vocabulary size ?
Also how does that change with more data coming along ? As I am still on the topic how would one chose a good window size when training your embedding vectors ?I am asking this because I am not training my network with a real-life language dictionary, but rather the sentences would describe relationships between processes and files and other processes and so on.
For example a sentence in my text corpus would look like:smss.exe irp_mj_create systemdrive windows system32 ntdll dll DesiredAccess: Execute/Traverse, Synchronize, Disposition: Open, Options: ,
  Attributes: n/a, ShareMode: Read, AllocationSize: n/a, OpenResult:
  Opened""As you may imagine the variations are numberous but the question still remains how can I fine tune these hyperparameters the best way so that the embedding space will not over-fit but also have enough meaningful features for each word. Thanks,Gabriel"
1946,TensorFlow: How to write the word-embeddings to a file,"['python', 'tensorflow', 'word2vec', 'word-embedding']","I'm new to TF. I was following a a tutorial which I found online. The tutorial builds word-embeddings and then computes the similarity between words (for a given set) using the cosine sim. What I was trying to do is to store the generated word-embeddings to a file.Below is the code (snippets) :Error:I have also tried the basic python print functionality. This results in 
printing Any pointers as to how to resolve this issue?"
1947,Reducing Memory requirements for distance/adjacency matrix,"['python-3.x', 'scipy', 'word-embedding']","Based on a subsample of around 5,000 to 100,000 word-embeddings (GloVe, 300-dimensional) I need to construct an adjancency matrix, i.e. a matrix of 1's and 0's indicating if the euclidean (or cosine) distance between two words is smaller than x.Currently, I'm using scipy.spatial.distance.pdist:With increasing vocabulary size, my memory fills up rather quickly and pdist fails with a MemoryError (when common_model has the shape (91938, 300) and contains float64).Iterating the model manually and creating the adjacency directly without the distance matrix in between would be a way, but that was extremely slow.Is there another way to construct the adjacency matrix in a time- and memory-optimal way?"
1948,How to initialize word-embeddings for Out of Vocabulary Word?,"['machine-learning', 'nlp', 'deep-learning', 'word-embedding']","I am trying to use CoNLL-2003 NER (English) Dataset and I am trying to utilize pretrained embeddings for it. I am using SENNA pretrained embeddings. Now I have around 20k words in my vocabulary and out of this I have embedding available for only 9.5k words.
My current approach is to initialize an array of 20k X embedding_size with zeros and initialize the 9.5k words whose embeddings is known to me and make all the embeddings learn-able.My question is what is the best way to do this? Any reference to such research will be very helpful?"
1949,What is the difference between syntactic analogy and semantic analogy?,"['nlp', 'word-embedding', 'fasttext']",At 15:10 of this video about fastText it mentions syntactic analogy and semantic analogy. But I am not sure what the difference is between them.Could anybody help explain the difference with examples?
1950,Why does skipgram model take more time than CBOW,"['word2vec', 'word-embedding']",Why does skipgram model take more time than CBOW model. I train the model with same parameters (Vector size and window size).
1951,How can I get a vector after each training iter in word2vec?,"['python-3.x', 'nlp', 'word2vec', 'gensim', 'word-embedding']","I want to get a vector of words every few iter in word2vec, e.g., I would like to use the model below.In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.How can I do this?"
1952,Character embeddings with Keras,"['python', 'nlp', 'keras', 'lstm', 'word-embedding']","I am trying to implement the type of character level embeddings described in this paper in Keras. The character embeddings are calculated using a bidirectional LSTM.To recreate this, I've first created a matrix of containing, for each word, the indexes of the characters making up the word:I then define a BiLSTM model with an embedding layer for the word-character matrix. I assume the input_dimension will have to be equal to the number of characters. I want a size of 64 for my character embeddings, so I set the hidden size of the BiLSTM to 32:And this is where I get confused. How can I retrieve the embeddings from the model? I'm guessing I would have to compile the model and fit it then retrieve the weights to get the embeddings, but what parameters should I use to fit it ?Additional details:This is for an NER task, so the dataset technically could be be anything in the word - label format, although I am specifically working with the WikiGold ConLL corpus available here: https://github.com/pritishuplavikar/Resume-NER/blob/master/wikigold.conll.txt
The expected output from the network are the labels (I-MISC, O, I-PER...)I expect the dataset to be large enough to be training character embeddings directly from it. All words are coded with the index of their constituting characters, alphabet size is roughly 200 characters. The words are padded / cut to 20 characters. There are around 30 000 different words in the dataset.I hope to be able learn embeddings for each characters based on the info from the different words. Then, as in the paper, I would concatenate the character embeddings with the word's glove embedding before feeding into a Bi-LSTM network with a final CRF layer.I would also like to be able to save the embeddings so I can reuse them for other similar NLP tasks."
1953,"Tensorflow, compare an indexed value in a tensor with an integer for if condition","['python', 'tensorflow', 'word-embedding']","I am using TensorFlow to do a customized embedding training similar to continuous bag of words (CBOW) model. However, unlike 'CBOW', which has a fixed length sliding window, my sliding window can be considered as flexible. Here is the problem:Let's say, the embedding is word embedding. For word t, I have a tensor showing indexes of its context words: [-1, 1, 2, -1]. The maximum window size is 4, so the length of the vector is 4. But sometimes I do not have 4 context words for a word, so I use '-1' to mean 'no word in this position', and other integers are the index of a word. I also have an 'embedding' tensor, which is the embeddings for all the words.   What I am trying to do is to get the average embedding for the context words in order to represent the context. For example, if the context words are [-1, 1, 2, -1], I would get (1 * (embedding for word 1) + 2 * (embedding for word 2) ) / 2. I just need to neglect all the -1.So in my code, I try to loop through the context word tensor to compare each value with -1 and use an if condition to control if I would add the embedding of this context word. I tried different ways for this, but always get 'TypeError: Using a tf.Tensor as a Python bool is not allowed.'Is there a way to solve this problem? Or even better, is there a better representation of positions with no words so I can compute more efficiently (Tried to use NaN but also get a lot of troubles...)?Thanks a lot for the help and hopefully I have a clear description of the problem."
1954,Is tensorflow embedding_lookup differentiable?,"['tensorflow', 'nlp', 'deep-learning', 'word-embedding', 'sequence-to-sequence']","Some of the tutorials I came across, described using a randomly initialized embedding matrix and then using the tf.nn.embedding_lookup function to obtain the embeddings for the integer sequences. I am under the impression that since the embedding_matrix is obtained through tf.get_variable, the optimizer would add appropriate ops for updating it. What I don't understand is how backpropagation happens through the lookup function which seems to be hard rather than being soft. What is the gradient of the this operation wrt. one of it's input ids?"
1955,Gensim word embedding training with initial values,"['machine-learning', 'nlp', 'word2vec', 'gensim', 'word-embedding']","I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 isand in year 2 it's something around:Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus)."
1956,How to read Chinese word embedding in plain text with numpy,"['python', 'numpy', 'word-embedding']","I'm a new beginner of numpy. I want to use pretrained Chinese word embedding for RNN training. I want to read the embedding in plain text and convert them into .npy file. But I always got errors. I used python3.4, and numpy1.13.The embedding are stored in plain text as the examples show, separated by space:Chinese word1 0.001334 0.001473 -0.001277 -0.001093 ……Chinese word2 0.000417 -0.000250 -0.000319 -0.001105 ……When I use np.loadtxt:I got the error:If I try np.genfromtxtI gotI googled a lot, but still fail to solve the problems, I have wasted a lot time. Are there anyone can help me? Thanks so much!!"
1957,Compute gradients w.r.t. the values of embedding vectors in PyTorch,"['nlp', 'lstm', 'chatbot', 'pytorch', 'word-embedding']","I am trying to train a dual encoder LSTM model for a chatbot using PyTorch.I defined two classes: the Encoder class defines the LSTM itself and the Dual_Encoder class applies the Encoder to both context and response utterances that I am trying to train on: The following error occurs:I do understand why the problem occurs (surely it makes no sense to compute the gradient w.r.t. the indices).
But I do not understand how to adjust the code so that it computes the gradients w.r.t. the content values of the embedding vectors.All help highly appreciated!(Also see the thread in the PyTorch forum)"
1958,HTML Embeddings into Neural Networks?,"['neural-network', 'artificial-intelligence', 'word-embedding']","I'm beginning my journey into Neural Networks and trying to understand both character and word embeddings as ways to input text data into a NN.  Specifically, I am trying to embed HTML tag information.  I tried googling some different combinations of my problem and came up empty.  My current understanding is that embeddings ""embed"" words or characters into an N-Dimensional space, which allows NNs to be able to understand them as inputs.  So in this case, something like word2vec would not necessarily help me because it is not meant to understand the ""meaning"" of HTML elements?  So thus a character embedding would be better?  If anyone could point me in a direction that would be awesome, as I am having trouble finding this on my own.Thanks in advance."
1959,Get word-embedding dictionary with glove-python model,"['python', 'python-3.x', 'nlp', 'word-embedding']","I trained a Glove model in python using Maciejkula's implementation (github repo).
For the next step I need a word-to-embedding dictionary.
However I can't seem to find an easy way to extract such a dictionary from the glove model I trained.I can extract the embeddings by accessing model.word_vectors but this only returns an array containing the vectors without a mapping to the corresponding words.
There is also the model.dictionary attribute containing word-to-index pairs.
I thought that these indexes might correspond to the embedding-indexes in the model.word_vectors array, but I'm not sure that this is correct.Do the indexes correspond or is there another easy way to get a word-to-embedding dictionary from a glove-python model? I realize that Sanj asked I similar although wider question, but since there is no response yet I thought I'd ask this more specific question."
1960,Creating Bigrams Phrases of a Column in Pandas using Gensim and attaching it to the to same dataframe,"['pandas', 'nlp', 'word2vec', 'gensim', 'word-embedding']","I am trying to built a predictive model using address data. I have transformed the address data into a bigram model using Gensim Phrases  but I am facing  issues while transforming the address data into the corresponding bigrams and attaching as a separate column which can be further used for countvectorization.My codeGensim Bigram Phrases modelSample input from column of dataframeExpected output(New data after passing the gensim phrase model)I am not able to replace the corresponding addresses with the respective bigrams from gensim 
 phraseses model iteratively  . My expected output is to replace all the old addresses with newly generated bigram phrases .  So that i can pass it to a countvectorizer Any help is appreciated."
1961,Explicit CPU placement in TensorFlow,"['tensorflow', 'nlp', 'gpu', 'cpu', 'word-embedding']","I found there are a piece of code in official model sample which confused me. Why using tf.device(""/cpu:0"") here? Except the case GPU memory leak, is there any other situation which we need to designate CPU operations explicitly?"
1962,why embedding_lookup only used as encoder but no decoder in ptb_word_ln.py,"['python', 'tensorflow', 'tensorboard', 'sample', 'word-embedding']","I have a question about embedding_lookup while I looking the tensorflow's official sample code ptb_word_ln.py.
the embedding_lookup nodeI found it is only used as an input. the output doesn't use this. so the loss evaluation cannot be benefit from this embedding. so what is the benefit using embedding_lookup here? If I want to use this word-embedding in the optimizer, shouldn't I connect it with loss function explicitly?the source code as following:"
1963,Can I export the embedding matrix of words in tensorflow?,"['tensorflow', 'neural-network', 'nlp', 'deep-learning', 'word-embedding']",Here is my code and the embedding is the variable for word to look up their own embedding vector. I have trained the embedding matrix and I want to extract it from the model saved. The model also contain other parameters for example the neural networks above embeddings. Can I implement it?
1964,How to convert gensim Word2Vec model to FastText model?,"['nlp', 'word2vec', 'gensim', 'word-embedding', 'fasttext']","I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?"
1965,word2Vec vector representation for text classification algorithm,"['python', 'word2vec', 'word-embedding']","I am trying to use word2vec in text classification algorithm.
I want t create vectorizer using word2vec, I have used below script. But I am not able to get one row for each document instead I am getting matrix of different dimension for every document. 
For example for 1st document matrix of 31X100,  2nd 163X100 and 3rd 73X100 and so on.
Actually I need dimension of every document as 1X100 , so that i can use these as input feature for training modelCan anyone help me here."
1966,Skip-gram with Word2Vec not working properly,"['scikit-learn', 'neural-network', 'word2vec', 'gensim', 'word-embedding']","I am trying to build a word2vec  similarity dictionary. I was able to build one dictionary but the similarities are not being populated correctly. Am I missing anything in my code?Input sample data  TextMy code: My results:Here are my expected outputs for the similarity: sheungwan, wanchai, chaiwan. I am guessing my skipgrams are not working properly. How can I fix this?"
1967,"Tensorflow with softmax, to implement word2vec: Value Error: No gradients provided for andy variable","['python', 'tensorflow', 'neural-network', 'word2vec', 'word-embedding']","I try to implement the Word2Vec Algorithm for learning in Tensorflow, but got stuck and have no idea how to solve that. The error I get is:ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ...I have created a simpler example of my code to show the error:My research makes me think that the placeholders in the softmax are the problem here, but I don't really understand why and I'm not really sure about this. What is it really I'm doing wrong? And how can I solve that? :-) Solutions, Ideas?"
1968,OpenNLP FeatureGenerator with real number values,"['java', 'opennlp', 'named-entity-recognition', 'word-embedding']","I'm trying to create a custom Word Embedding feature generator to use on the OpenNLP TokenNameFinder model. However, if I understand it correctly, the custom features need to implement AdaptiveFeatureGenerator class, and the signature to generate the features is:      which basically restricts the features to be categorical (since the features are list of Strings)... It would be great if someone could provide me some insights on whether numerical features are possible to create in OpenNLP, and if there is any work-around for this. Thank you so much for your time!"
1969,Changing dimnesions of pretrained word2vec vectors,"['deep-learning', 'word2vec', 'word-embedding', 'pre-trained-model']",I have two different word embedding pretrained models that I want to combine together so that a missing word from one model can be complimented by the other model (in case the other model has the word that is missing in the first model). But the vectors are of different dimensions in the models. The first model vectors are of 300 dimensions and the second model vectors are of 1000 dimensions.Can I simply retain the first 300 dimensions and discard the rest (700) in the second model and build one combined model of 300 dimensions?
1970,What does a weighted word embedding mean?,"['machine-learning', 'nlp', 'word2vec', 'tf-idf', 'word-embedding']","In the paper that I am trying to implement, it says,In this work, tweets were modeled using three types of text
  representation. The first one is a bag-of-words model weighted by
  tf-idf (term frequency
  - inverse document frequency) (Section
  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a
  sentence by averaging the weighted word embeddings of all words, the
  weight of a word is given by tf-idf (Section
  2.1.2).I am not sure about the third representation which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. "
1971,Text Classification with an input using SciKit Learn,"['python', 'scikit-learn', 'artificial-intelligence', 'text-classification']","Using a text classification algorithm from Sci-Kit Learn; numpy I have created a model for text classification; predicts what type of coffee people would like to have based on the features of the coffee people really like. It is a text classification,prediction algorithm. I have created a model which seems to work however, I am not sure how to make inputs to the model. Would I need to use tokenize.texts_to_matrix on the inputs that I make?.Could anyone suggest how to make inputs to my algorithm and whether I would need a change the format of the input I enter(whether it is just a single word or an array? CSV coffee file dataset sample:category,text
Cat1,Feature1 Feature2 Feature3 Feature4 Feature5 Feature6 Feature7
...."
1972,How to get max_features in TfidfTransformer in sklearn?,"['python', 'scikit-learn', 'feature-extraction', 'text-classification', 'tf-idf']","I'm trying to use the TfidfTransformer pipeline, but I couldn't find the max_features parameters?When I runHere is the output:"
1973,Classification of text into a large number of categories [closed],"['machine-learning', 'classification', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 8 days ago.I'm currently working on a task involving the classification of unstructured text into a large number of categories. The difficulties I'm facing are in part due to the fact that there are so many categories that a given chunk of text can be placed into (in the ideal scenario, there would be several hundred possible categories, but even managing to classify the text into the most commonly appearing 50 or so categories would be a huge improvement). I'm at a bit of a loss as to how to approach this task and was hoping that someone might be able to offer some suggestions on how to proceed (particularly in terms of what kinds of models/libraries might be useful) or point me in the direction of resources to learn more."
1974,How to get last layer before classification layer on fasttext?,"['text', 'similarity', 'text-classification', 'fasttext']",I would like to solve a text similarity problem using fasttext. I am able to create model to get text classification labels using fasttext. But I would like to get document vector which is created for classification layer input generating fasttext model. Then using some similarity methods get the scores .How can I do that ? Any help would be greatThanks
1975,Why do I get AttributeError: 'NoneType' object has no attribute 'something'?,"['python', 'attributeerror', 'nonetype']","I keep getting an error that saysThe code I have is too long to post here. What general scenarios would cause this AttributeError, what is NoneType supposed to mean and how can I narrow down what's going on?"
1976,Would training a BERT Multi-Label Classifier for 100 labels decrease accuracy a lot?,"['python-3.x', 'tensorflow', 'machine-learning', 'text-classification']","I am trying to train a text classifier which would be able to classify a sentence as being of a certain query type. I have used the BERT Model and trained a Multi-Label classifier which does the job with 90% accuracy for about 20 labels.
My question is that if I have to train the model for 100/200 labels would the accuracy be impacted severely?"
1977,Issues with category predict based on text description - Cast string to float is not supported,"['python', 'pandas', 'tensorflow', 'keras', 'text-classification']","I am trying to create a model to predict category(text) based on description(text)I am following the approach as listed hereUsing tensorflow version 2.2.0After reading the csv file the  output ofIn total there are 100+ unique categories and multiple descriptions(max len = 20 words) per category.I add a target columnCreate the training, test &  validation dataConvert to a tensorflow datasetcreate the tensorflow data sets for training, validation, testing phasesCreate the feature_columns.Create the feature layerCreate the modelCompile the modelFit the modelOn running the model, get the errorI am not sure where is the error in the code. Also, is this the right approach to solve the problem described earlier?"
1978,export google cloud auotml model,"['text-classification', 'automl', 'google-cloud-automl-nl']","I have trained, deployed and evaluated a model successfully on google cloud platform's natural language product AutoML Text & Document Classification. When i see all deployed models, there is no option to export the model, only delete option is available. Along with exporting data, is there a way to export the automl model as well, so that I can run the model on my local or any other machine?There are videos explaining to export model in auto ml vision product, but I couldnt draw much similarities with natural language model. Thanks"
1979,Is my quanteda dictionary detecting terms in texts that are not contained in the dictionary?,"['dictionary', 'text-classification', 'quanteda']","(First time asking a question; new-ish to R and new to quanteda).I have about 1.5m tweets and I have created a dictionary of about 1,700 abusive terms in an Excel file. I would like to use the quanteda dictionary to classify as abusive all those tweets which contain any of the abusive terms in the dictionary. The dictionary contains multi-word expressions. When I run the code below (on a random sample of 100 tweets), tweets that do not contain terms from the dictionary are classified as abusive, but I'm not sure why. I wondered if it has to do with the multi-word expressions in the dictionary. For example, if one of the multi-word expressions in the dictionary contains the word ""the"", is the dictionary classifying all tweets that contain the word ""the"" as abusive?"
1980,Why we add 3 to original_word_index in tensorflow text classification?,"['python', 'text-classification']","I am new in deep learning and trying to learn tensorflow. There is one thing that i can't understand in tensorflow text classification tutorial,this is original word index which texts encodedbut after we get original index we shift index to 3 right and use modified index to decode text dataso i want to mean:  original word index was used to encode words and then we use modified word index to decode them, i think it is not logical but looks like it works. How is this possible?"
1981,Text classification - is it overfitting? How can I prove?,"['python', 'machine-learning', 'artificial-intelligence', 'text-classification', 'multilabel-classification']","I have a multi classification problem and my data involves sequence of letters. It is a labelled data (used label encoder to encode string labels to numeric). There could be partial strings for the same class. May strings match but some could be just slightly different.I am preparing my data with k-mer and countvectoriser (fitted on train data and transformed train and test data). With the combination of kmer size and ngram sizes, the dimension (feature size) varies between 8000+ to 35000+. I do not think that there is test information leak at the training of the model.I fit different algorithms on the train data and test to review the generalisation. The test scores (accuracy, f1-score, precision and recall) are coming pretty high (more than 99%). Even though this is testing, do you think the model could be overfitting due to high dimensionality (curse of dimensionality)? I understand that if training score is high and generalises poorly then its overfitting but here the test scores are very high. This is not models as different algorithms giving similar results, its certainly about the data.If I apply PCA to get 10 components which covers 99% variance, the test score on testing is high too. If I use selectkfeatures to select just about 10 best features, then the scores come down.Really looking for your thoughts on how I can prove that this is not overfitting? Should I always go for reduced features size (through selection or pca) with such high dimension size? Thanks.Regards,
Vijay"
1982,i got 80 % accuracy and good f1 -score for trained and tested data but on imported data i got 40%,"['python', 'sentiment-analysis', 'text-classification']",I have a trained and tested model for sentiment analysis and it return 80% accuracy and 80% f1-score.i saved the model and try it on different data i got 40% and it predict many wrong records.What i am doing wrong and how to check if the model is good and can be applied on real data?
1983,"Fast BERT Camembert, arguments located on different GPUs error","['python-3.x', 'pytorch', 'gpu', 'text-classification']","I'm trying to implement Fast-Bert with 'camembert-base' model type.
I can easily create my databunch with BertLMDatabunch.From raw_corpus
then I create the learner.
I'm on a cloud env, with 3 GPUs, on ubuntu env, with 32 cores and RAM 130Mo.
When I'm trying to fit the model I always have this error message after this information"
1984,RStudio --> Error: Python module tensorflow.keras was not found,"['r', 'tensorflow', 'keras', 'deep-learning', 'text-classification']","But I am working in RStudio. and facing the below error related with Keras & Tensorflow.Error: Python module tensorflow.keras was not found.Detected Python configuration:python:         C:/Users/AppData/Local/r-miniconda/envs/r-reticulate/python.exe
libpython:      C:/Users/AppData/Local/r-miniconda/envs/r-reticulate/python36.dll
pythonhome:     C:/Users/AppData/Local/r-miniconda/envs/r-reticulate
version:        3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]
Architecture:   64bit
numpy:          C:/Users/AppData/Local/r-miniconda/envs/r-reticulate/Lib/site-packages/numpy
numpy_version:  1.18.5Any help on this is very appreciable."
1985,Copy specific data from sheet to sheet,"['google-apps-script', 'copy', 'classification', 'text-classification', 'data-transfer']","https://docs.google.com/spreadsheets/d/1mpALs4rdNj-TFFgCQ0CEBP453v-FfQJR_bBxOzQakWU/edit#gid=1256640730I have a ""masterlist"" sheet collect data from Google Forms and I need the script to copy the data from master list to the related sheet according to their ""Location"".Example 1Sheet 1: Contain all the data (Master List)
Name......Address...............................Location
Rose.......21,Radje Road,88888.........Kedah
Rose.......21,Radje Road,88888.........Kedah
Rose.......21,Radje Road,88888.........Penang
Stone......5,Jae Road,22222...............PenangSheet 2: Copy data from Sheet 1 (Location-Kedah Only)
Name......Address...............................Location
Rose.......21,Radje Road,88888.........Kedah
Rose.......21,Radje Road,88888.........KedahSheet 3: Copy data from Sheet 1 (Location-Penang Only)
Name......Address...............................Location
Rose.......21,Radje Road,88888.........Penang
Stone......5,Jae Road,22222.........PenangExample 2Sheet 1: Contain all the data (Master List)
Name.....Gender
Bryan.....Male
Mei.....Female
Lily.....Female
xx.....Female
xx.....MaleSheet 2: Copy data from Sheet 1 (Gender-Female Only)
Name.....Gender
xx.....Female
Lily.....Female
Mei.....FemaleSheet 3: Copy data from Sheet 1 (Gender-Male Only)
Name.....Gender
Bryan.....Male"
1986,"TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams), which accuracy should be considered?","['machine-learning', 'text-classification', 'tf-idf']","Here you can see i am calculating the frequency at Count Vectors,WordLevel, N-Gram Vectorshow i will interpret the result because i am receiving different results at each level?
can anyone tell how i can get the collectives results?"
1987,Low accuracy for text classification using LSTM model,"['python', 'keras', 'neural-network', 'lstm', 'text-classification']","I am trying to train an LSTM model for fake news detection using title and text features of the dataset. Below is the code of my model:The accuracy for both train and validation data is around 47%confusion matrix:classification report:I have tried different combinations of epochs, batch_size, 2 LSTM layers with different units, but no luck. Please help me on this."
1988,Saving a trained multi-input classification algorithm in Python,"['python', 'machine-learning', 'text-classification', 'multiclass-classification']","I developed a script that predicts probable tags for some text, based on previously manually tagged feedback. I used several online articles to help me (namely: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5).Because I want the probability for each tag, here's the code I used:It works very well for my purposes: it returns a prediction for each possible tag. But my issue is that it retrains the algorithm every time I try to make a prediction. What I'd like to do is to train the algorithm in a script, save the trained algorithm, load it in another script where the prediction is made.I'd like to be able to do this in script 1:And this in the other script:But I can't seem to make it work. It just gives me the same prediction when I try to separate it."
1989,Text Classification using word2vec along with another independent variable,"['word2vec', 'text-classification', 'multilabel-classification', 'multiclass-classification']","I am working on Text classification with Multi class labels. Currently I am taking Description column as an independent variable.I am calculating the Word2vec for Description Column and taking it as Xand passing 'labels' to y. One the sentence vector got created I am splitting the data into 'train' and 'test' as below :and then I am using OneVsRest Classifier   :Now I want to add one more features df['Status'] to my 'Training' data. How can I do that?
How can I add this feature to my sentence vector or to my 'training data'?"
1990,Understanding a sigmoid prediction in Tensorflow?,"['python', 'tensorflow', 'text-classification', 'multiclass-classification', 'sigmoid']","I'm creating a tensorflow model that will predict if a body of text belongs to one or more categories. I'm using a sigmoid function to do this with a binary crossentropy as my loss function. I have 27 different categories I want to make a prediction on. So when I use the model to predict on a body of text it returns an array of shape (len of text,27). So it is predicting on each word it seems. Is there any way to have it just predict on the sentence as a whole? Or could I avg. together these predictions to get a prediction of the overall sentence?"
1991,Classification: Tweet Sentiment Analysis - Order of steps,"['python', 'machine-learning', 'classification', 'sentiment-analysis', 'text-classification']","I am currently working on a tweet sentiment analysis and have a few questions regarding the right order of the steps. Please assume that the data was already preprocessed and prepared accordingly. So this is how I would proceed:In the next steps, I would like to identify the best classifier. Please assume those were already imported. So I would go on by:Is this the right approach or would you recommend changing something (e. g. doing the cross-validation alone and not within the hyperparametrization)? Does it make sense to test the test data as the final step or should I do it earlier to assess the accuracy for an unknown data set?"
1992,"What is the difference between args wordNgrams, minn and maxn in fassttext supervised learning?","['text-classification', 'supervised-learning', 'fasttext']","I'm a little confused after reading Bag of tricks for efficient text classification.
What is the difference between args wordNgrams, minn and maxnFor example, a text classification task and Glove embedding as pretrainedVectorsan input sentence is 'I love you'.
Given minn=2,maxn=3, the whole sentence is transformed into [<I, I>], [<l, <lo, lo, lov,.....] etc
For the word love, its fasttext embedding = (emb(love) (as a complete word) + emb(<l)+emb(<lo)+....) / n.
For the sentence, it is splitted into [I love, love you] (because wordNgrams=2) and these 2-gram embeddings are [(fasttext emb(I)+fasttext emb(love))/2, (fasttext emb(love)+fasttext emb(you))/2].
The sentence embedding is average of 2-gram embeddings and has dimensionality as 300. Then it is fed through a layer which has #labels neurons (i.e. multiplied with a matrix whose size is [300, #labels]).Is this right? Please correct me if I'm wrong"
1993,ValueError in while predict where Test data is having different shape of word vector [duplicate],"['python', 'machine-learning', 'scikit-learn', 'text-classification']","Below is my code I am trying for text classification model;Till now only training set has been vectorized into a full vocabulary. In order to perform analysis on test set I need to submit it to the same procedures.
So I didAnd finally when trying to predict its showing error;But when I use pipeline from sklearn.pipeline import Pipeline then it worked fine;Can’t I code the way I was trying?"
1994,My accuracy of Model using Logistic regression is increasing as i am increasing the value of C which is the parameter for regularization,"['logistic-regression', 'text-classification', 'regularized']","I am using Logistic regression as a learning algorithm for text classification. While selecting best value of C through Gridsearch, I am always getting the highest value from set of passed values of C.As C is inversely related to Regularization strength, what could be the reason for this happening?
Higher value of C leads to overfitting of model but contradictory it is increasing the accuracy in my case."
1995,"how to get automatically segregate the products, item, qty, price fields explicitly, and fetch it from “Invoice image” Cloud Vision OCR data?","['python', 'machine-learning', 'deep-learning', 'ocr', 'text-classification']","We are facing a few challenges in getting the exact data from the ""Invoice Bill"" Image.
Using Machine Learning, we are trying to split the Invoice bill image converted (OCR) to text  data such as purchased items, date, title, address, etc.,
We have multiple invoice formats of companies or business entities.How can we classify the text which we get from the bill (image) converted Text?,  also how to train the  Machine Learning system to automatically segregate the products, item, qty, price fields explicitly, and fetch it?Here is the sample Receipt FYI"
1996,Getting Different results on Each Iteration using Long Short Term Memory[LSTM] for text classification,"['machine-learning', 'deep-learning', 'recurrent-neural-network', 'text-classification']","I am using LTSM Deep-learning  technique to classify my text, First i am  dividing them into text and lables using panda library and  making their tokens and then dividing them into into training and text data sets,whenever i runs the code, i get different results which varies from (80 to 100)percent.
Here is my code,Train on 794 samples, validate on 89 samples
Epoch 1/5
794/794 [==============================] - 19s 24ms/step - loss: 1.6401 - accuracy: 0.6297 - val_loss: 0.9098 - val_accuracy: 0.5843
Epoch 2/5
794/794 [==============================] - 16s 20ms/step - loss: 0.8365 - accuracy: 0.7166 - val_loss: 0.7487 - val_accuracy: 0.7753
Epoch 3/5
794/794 [==============================] - 16s 20ms/step - loss: 0.7093 - accuracy: 0.8401 - val_loss: 0.6519 - val_accuracy: 0.8652
Epoch 4/5
794/794 [==============================] - 16s 20ms/step - loss: 0.5857 - accuracy: 0.8829 - val_loss: 0.4935 - val_accuracy: 1.0000
Epoch 5/5
794/794 [==============================] - 16s 20ms/step - loss: 0.4248 - accuracy: 0.9345 - val_loss: 0.3512 - val_accuracy: 0.8652
99/99 [==============================] - 0s 2ms/step
Test set
Loss: 0.348
Accuracy: 0.869
in the last run accuracy was 100 percent."
1997,How to train a model to classify input to one or more classes,"['python', 'tensorflow', 'keras', 'text-classification']","I use this sample code to train a model to classify a random number into a one of 10 classesIn this sample, a sample value of x_train[0] isand a sample output of y_train[0] iswhich means that the x_train[0] values map to the 7th feature.In this online sample sample code, there's only one matched class for each input.How can I change my code to train 5 features into one or more than class in the same time?For example, a possible y_train[z] value may be [0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]?"
1998,"Keras model accuracy, loss, val_accuracy and val_loss don't change","['python', 'tensorflow', 'keras', 'deep-learning', 'text-classification']","I'm trying to make model for text classification, and my accuracy, loss, val_accuracy and val_loss do not change. What's the problem?and model predictions are same:p.s This is my model:"
1999,Accuracy not growing across epochs on keras,"['python', 'keras', 'deep-learning', 'recurrent-neural-network', 'text-classification']","I'm new to machine learning and deep learning and I'm trying to classify texts from 5 categories using neural networks. For that, I made a dictionary in order to translate the words to indexes, finally getting an array with lists of indexes. Moreover I change the labels to integers. I also did the padding and that stuff. The problem is that when I fit the model the accuracy keeps quite low (~0.20) and does not change across the epochs. I have tried to change a lot of params, like the size of the vocabulary, number of neurones, dropout probability, optimizer parameter, etc. The key parts of the code are below."
2000,How do I categorise non-english email using procmail and command line tools?,"['email', 'command-line-interface', 'text-classification', 'non-english', 'procmail']","I am subscribed to a mail list where some of the messages are non-english which I cannot understand.How do I filter the non-english messages to /dev/null using procmail and/or command line tools?I use procmail to filter my email, so ideally any alternative tool would also require a procmail recipe.I'd prefer not to have to train my own language models."
2001,Amazon SageMaker: Customer Error: Training did not complete successfully for binary text classification,"['amazon-web-services', 'machine-learning', 'amazon-s3', 'text-classification', 'amazon-sagemaker']","This is my first try of Amazon SageMaker. Essentially I am trying to create a spam detection filter using a binary classifier with Blazing Text in SageMaker. Attempted to train the model w/these commands:andBut then when I try to run this, I get the following log:I believe my training and test datasets are preprocessed correctly, so any and all help would be much appreciated. Thank you so much!"
2002,"ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 896, received input shape [None,128]","['tensorflow', 'keras', 'text-classification', 'cnn', 'glove']","I am using the CNN architecture (see code below) for text classification task (with 5 classes).
The data I am using is reviews_Home_and_Kitchen_5.json downloaded from hereI created a sentence embedding matrix for 1000 sentences taking the embedding from Glove model ('glove.840B.300d.txt')The model compiles and you can see the summary below. However, whenever I am trying to fit the model I keep getting the following error:
ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 896 but received input with shape [None, 128]The solutions I found online involved using different version of TF or using different loss function. I tried both and couldn't solve the issue.Can anyone help?"
2003,BERT HuggingFace gives NaN Loss,"['machine-learning', 'keras', 'text-classification', 'transformer', 'huggingface-transformers']","I'm trying to fine-tune BERT for a text classification task, but I'm getting NaN losses and can't figure out why.First I define a BERT-tokenizer and then tokenize my text:Next, I load my integer labels which are: 0, 1, 2, 3. Then I load the pretrained Transformer model and put layers on top of it. I use SparseCategoricalCrossEntropy Loss when compiling the model:Finally, I run the model using previously tokenized input_ids and input_masks as inputs to the model and get a NAN Loss after the first epoch:EDIT: The model computes losses on the first epoch but it starts returning NaNs 
at the second epoch. What could be causing that problem???Does anyone has any ideas about what I am doing wrong? 
All suggestions are welcomed!"
2004,Training spacy-transformers text classifier - attempting a minimal training example,"['spacy', 'text-classification', 'spacy-pytorch-transformers']","After a number of experiments along the lines indicated in these examples:https://colab.research.google.com/github/explosion/spacy-pytorch-transformers/blob/master/examples/Spacy_Transformers_Demo.ipynbhttps://github.com/explosion/spacy-transformers/blob/master/examples/train_textcat.pyI find I am unable to observe the effect of any learning when calling nlp.update on the spacy-transformer models. I have tried with en_trf_bertbaseuncased_lg as below, and with the en_trf_distilbertbaseuncased_lg model with no luck. I am able to get text classification via the spacy TextCategorizer and LSTM examples working howeverr. Hence I would like to ask what I could do to modify the below code to achieve a score of anything less than 1.0 for ""THE_POSITIVE_LABEL"" when calling doc.cats for this test sentence. It currently runs without error but always returns 1.0 for the score. I attempted this example after running a proper training set and watching identical P,R,F scores a loss value that just jumped around every evaluation. The corrected version could then serve as a simple test of the functionality."
2005,Select texts by topic (LDA),"['python', 'gensim', 'text-classification', 'lda']","Would it be possible to look for texts that are within a certain topic (determined by LDA)? I have a list of 5 topics with 10 words each, found by using lda.I have analysed the texts in a dataframe’s column. 
I would like to select/filter rows/texts that are in one specific topic. If you need more information, I will provide you. What I am referring to is the step that returns this output:created by The original column with texts that have been analysed is called Texts and it looks like: My expected output would be Thanks "
2006,How to extract only a specific value from a python sublist that I got as an API response from Monkeylearn,"['python', 'nlp', 'text-classification', 'sublist', 'monkeylearn']","I have been training a text classification model in Monkeylearn and as a response to my API query, I get a python list as a result. I want to extract only the specific text classification value from it. Attaching the code below.Output I get is :I want to only print the classifications - tag_name ie ""Batting"" from this list.the output I get is: List"
2007,Spacy TextCat Score in MultiLabel Classfication,"['spacy', 'text-classification', 'multilabel-classification']","In the spacy's text classification train_textcat example, there are two labels specified Positive and Negative. Hence the cats score is represented as cats = [{""POSITIVE"": bool(y), ""NEGATIVE"": not bool(y)} for y in labels]I am working with Multilabel classfication which means i have more than two labels to tag in one text. I have added my labels as textcat.add_label(""CONSTRUCTION"") and to specify cats score I have used cats = [{""POSITIVE"": bool(y), ""NEGATIVE"": not bool(y)} for y in labels] I am pretty sure this is not correct. Any suggestions how to specify the scores for cats in multilabel classification and how to train multilabel classification? Does the example from spacy works for multilabel classification too?"
2008,Improve Accuracy of Email Classification?,"['performance', 'nlp', 'logistic-regression', 'text-classification', 'tf-idf']","I am building an email classification model. Currently, I am using NLTK's stopwords and lemmatization during the pre-processing of data. Following are the parameters for TF-IDF vectorizer that I am using:I am using LogisticRegression for classification.I am getting the following results from the above code:How Can I improve this accuracy??Note - I am working on the ""Consumer Complaints Dataset"". I am only using 3300 rows from that database and I have balanced my database i.e 300 emails from each category11 categories * 300 emails = 3300 rows."
2009,Is it possible to create a multi-class Text Classifier Tensorflow Lite model by TFLite Model Maker?,"['python', 'tensorflow', 'word2vec', 'text-classification', 'tensorflow-lite']","I try to build an android apps to predict text classification using AverageWordVecModelSpec that have been provided by Tensorflow Lite Model Maker.I'm using books content to test if my apps works. There are 3 books I've provided for this experiment. Here's the code:It works when i only use 2 classes/books (same as examples provided by tensorflow team):
it works normal even though it has small acurracy-- because i only takes 20 sample page per book as dataset actually You can see that i have rational loss value here,
But i have a problem when i've try to add the 3rd class:Here's the training result involving 3rd class:
enter image description hereYou can see that it's not rational for having loss value more than 1. 
I've tried to find which line of code (from Tensorflow Model Maker) that i should change to solve it and ended up to this question in this forum.So is it possible to have multiclass model for textclassifier using
  AverageWordVecModelSpec TFlite model maker?"
2010,Text Classification of News Articles Using Spacy,"['machine-learning', 'classification', 'spacy', 'text-classification', 'multilabel-classification']","Dataset : Csv files containing around 1500 data with columns (Text,Labels) where Text is the news article of Nepali Language and Label is its genre(Health, World,Tourism, Weather) and so on.I am using Spacy to train my Text Classification Model. So far, I have converted the dataset to a dataframe which looks like this  
and then into a spacy acceptable format through the code which gives me the list of tuples in my training dataset like [('text...','label...'),('text...','label...')]Now, how can I do text classification here?In the spacy's documentation, I foundDo we have to add the labels according to the labels or should we use positive/negative as well? Does spacy generate the labels according to our dataset after training or not? Any suggestions please?"
2011,Text preprocessing for text classification using fastText,"['python', 'nlp', 'text-classification', 'fasttext']","What text preprocessing produces the best results for supervised text classification using fastText?The official documentation shows a only a simple prepocessing consisting of lower-casing and separating punctuations. Would classic preprocessing like lemmatization, stopwords removal, masking numbers would help?"
2012,What approach/modules should I use to classify/categorize text?,"['r', 'text-classification']","If I have an input list of some business names and categories. What approach/libraries would I use to suggest the best category for a new business name?Pre-categorized input data:Uncategorized data:Desired output:Note that ""Pizza Inn"" should be categorized as a ""Restaurant"" since businesses that contain the word ""pizza"" are categorized as ""Restaurant"" 100% of the time in the input data, but the word ""Inn"" is associated with the category ""Hotel"" half the time.It seems there is a dizzying array of tools for this, but it's hard to tell which are appropriate for this type of task."
2013,Removing stop-words and selecting only names in pandas,"['python', 'regex', 'pandas', 'text-classification']","I'm trying to extract top words by date as follows: in the following dataframe:How you can see, there are many stop-words (""the"", ""an"", ""a"", ""be"", ...), that I would like to remove in order to have a better selection. My aim would be to find some key words, i.e. patterns, in common by date so I would be more interested and focused on names rather than verbs. Any idea on how I could remove stop-words AND keep only names?EditExpected output (based on the results from Vaibhav Khandelwal's answer below): I would need to extract only nouns (reasons should be more frequent so it would be ordered based on frequency).I think it should be useful nltk.pos_tag where tag is in ('NN'). "
2014,What is the best method to display the text preprocessing process in python?,"['python', 'text-mining', 'preprocessor', 'text-classification']","I'm confused to change this code so that each preprocessing process can display data or the results of its processing. Such as the lower process itself, the process tokenize itself so it's not like in this code that displays the overall results. Can you help me to change this code ?"
2015,How to use Tf-idf features for training your model?,"['machine-learning', 'scikit-learn', 'text-classification', 'naivebayes', 'tfidfvectorizer']","I used the above code to get features for my text document.Then I used this code to train my model. 
Can someone explain how exactly are the above features being used while training the model as that feature1 variable is not being used anywhere while training ??"
2016,'numpy.ndarray' object has no attribute 'lower',"['numpy', 'scikit-learn', 'text-classification', 'naivebayes', 'tfidfvectorizer']","I am fairly new to ML, I am trying to fit some data on my NB-classifier.Code for fitting the data: The shape of my training & test data is But keep getting :'numpy.ndarray' object has no attribute 'lower'Here is full trace to the error:"
2017,How to calculate the information gain and entropy of a dataset with ten features?,"['algorithm', 'machine-learning', 'decision-tree', 'text-classification', 'multilabel-classification']","I have a dataset of 10K, and I created the following ten features:Each of the features has an output from the dataset. Now I want to make the tree. But first, how should I calculate the entropy and information gain? "
2018,Alternative to TfidfVectorizer,"['python', 'machine-learning', 'text-classification']","Is there any alternative to TfidfVectorizer function of sklearn.feature_extraction.text module? I've heard of fastText and GloVe, but couldn't find a good expalnation of how to use it to vectorize text.Edit: Basically I've a feature called narration, which consists of English sentences. In order to feed this into any ML algorithm I've to convert it into a numeric matrix representation. TfIdf was one way. Is there any other way that I can try out? (May or may not be under sklearn)"
2019,Labelling texts using sklearn: Index Error,"['python', 'scikit-learn', 'text-classification']","I am trying to assign labels to texts using clusters. To do this, I am following the steps at this link:https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52However, after selecting all the terms in the cluster (I have three clusters) as follows: where len(terms)=2009,I get the following error:IndexError: list index out of rangewhen I run this code: where true_k=3. 
This link: List index out of range (python jupyter) uunfortunately does not have any relevant information to fix this issue. 
It seems that cluster 1 may cause the problem and also the number of words [:10]. Do you know why it happens and how to fix it?
If you need more code and info, I will be happy to update the question. SampleDoc...
I have 130 rows. Code"
2020,sklearn how to use saved model to predict new data,"['machine-learning', 'scikit-learn', 'svm', 'text-classification']","I use sklearn trained a SVM text classifier, used tf-idf(TfidfVectorizer) to extract the feature.
now I need to save the model and load it to predict the text unseen. I will load the model in another file,  what confuses me is how to extract the new text tf-idf feature"
2021,Keyword based text classification,"['nlp', 'text-mining', 'feature-extraction', 'text-classification', 'document-classification']","I want to classify some texts based on available keywords in each class. In other words, I have a list of keywords for each category. I need some heuristic methods using these keywords and determine top similar categories for each text. I should say that in the current phase of the project, I didn't want to use a machine learning-based method for text classification."
2022,Mutli-Class Text Classifcation (using TFIDF and SVM). How to implement a scenario where one feedback may belong to more than one class?,"['machine-learning', 'scikit-learn', 'svm', 'text-classification', 'tfidfvectorizer']","I have a file of raw feedbacks that needs to be labeled(categorized) and then work as the training input for SVM Classifier(or any classifier for that matter). But the catch is, I'm not assigning whole feedback to a certain category. One feedback may belong to more than one category based on the topics it talks about (noun n-grams are extracted). So, I'm labeling the topics(terms) not the feedbacks(documents). And so, I've extracted the n-grams using TFIDF while saving their features so i could train my model on. The problem with that is, using tfidf, it returns a document-term matrix that's train_x, but on the other side, I've got train_y; The labels that are assigned to each n-gram (not the whole document). So, I've ended up with a document to frequency matrix that contains x number of rows(no of documents) against a label of y number of n-grams(no of unique topics extracted).Below is a sample of what the data look like. Blue is the n-grams(extracted by TFIDF) while the red is the labels/categories (calculated for each n-gram with a function I've manually made).Instead of putting code, this is my strategy in implementing my concept: The problem lies in that part where TFIDF producesx_train = tf.Transform(feedbacks), which is a document-term matrix and it doesn't make sense for it to be an input for the classifier against y_train, which is the labels for the terms and not the documents. I've tried to transpose the matrix, it gave me an error. I've tried to input 1-D array that holds only feature values for the terms directly, which also gave me an error because the classifier expects from X to be in a (sample, feature) format. I'm using Sklearn's version of SVM and TfidfVectorizer.Simply, I want to be able to use SVM classifier on a list of terms (n-grams) against a list of labels to train the model and then test new data (after cleaning and extracting its n-grams) for SVM to predict its labels. The solution might be a very technical thing like using another classifier that expects a different format or not using TFIDF since it's document focused (referenced) or even broader, a whole change of approach and concept (if it's wrong). I'd very much appreciate it if someone could help. "
2023,how to set time_step in LSTM in pytorch,"['nlp', 'pytorch', 'lstm', 'recurrent-neural-network', 'text-classification']","I'm dealing with string classification, and I want to use LSTM, but I have some questions about setting time_step. My data set looks like:Then I convert it into binary code:Now when I try to train LSTM, following is how I build LSTM:Then I try to train it with following codes:"
2024,Get incorrect predictions from Keras with multiple data input,"['python', 'keras', 'neural-network', 'text-classification']",I'm using input of both text and numeric data in a classification task and is interested to see the text for the instances of incorrect predictions in order to see if there is a pattern in what is incorrectly classified - is there a way of doing so? X1_train/X1_test constitutes the text data whereas X2_train/X2_test consists of the numeric data
2025,How can I identify important features (leaks) in text classification using sklearn.naive_bayes.MultinomialNB,"['python', 'scikit-learn', 'text-classification', 'naivebayes']","I am interested in identifying potential leaks in some text for a binary classification task. 
The reason being my suspicion of the high accuracy(96%), so I would like to see which words weigh the most when categorizing the text. The data has a mean length of 2500 characters. And the code is as such:I can do the following for words/segments, but that is rather unhelpful considering I am using TFIDF.I have also tried the following command, which returns meta parameters, not a matrix or function:A solution would be something identifying the most important words"
2026,Should I split documents into single sentences or use them as is to train Brain.js text classification model?,"['javascript', 'machine-learning', 'nlp', 'text-classification', 'brain.js']","I'm new to NLP and trying Brain.js to create a multi-class text classifier.
I have a few hundred labeled documents for this experiment. Each document has up to 30 sentences.I'm basing the test on this repo :
simple_phrase_classifierI was wondering what's the best way to feed the model with training data.Same question about using the model, should I just apply it on the complete document or should I split it to separate sentences then apply the model on each sentence.What's the best practice?"
2027,How to improve a German text classification model in spaCy,"['python', 'nlp', 'spacy', 'text-classification']","I am working on a text classification project and using spacy for this. Right now I have an accuracy equal to almost 70% but that is not enough. I've been trying to improve the model for past two weeks, but no successful results so far. And here I am looking for an advice about what I should do or try. Any help would be highly appreciated! So, here is what I do so far:1) Preparing the data:I have an unbalanced dataset of German news with 21 categories (like POLITICS, ECONOMY, SPORT, CELEBRITIES etc). In order to make categories equal I duplicate small classes. As a result I have 21 files with almost 700 000 lines of text. I then normalize this data using the following code:Some explanations to the above code:POS - a list of allowed parts of speech. If the word I'm working with at the moment is a part of speech that is not in this list -> I delete it.stop_words - just a list of words I delete.splitter.split_compound(word)[0] - returns a tuple with the most likely division of the compound word (I use it to divide long German words into shorter and more widely used). Here is the link to the repository with this functionality.To sum up: I find the lemma of the word, make it lower case, delete stop words and some parts of speech, divide compound words, delete punctuation. I then join all the words and return an array of normalized lines.2) Training the modelI train my model using de_core_news_sm (to make it possible in the future to use this model not only for classification but also for normalization). Here is the code for training:Some explanations to the above code:data - list of lists, where each list includes a line of text and a dictionary with categories (just like in the docs)'categories' - list of categories'n_iter' - number of iterations for training3) At the end I just save the model with to_disk method.With the above code I managed to train a model with 70% accuracy. Here is a list of what I've tried so far to improve this score:1) Using another architecture (ensemble) - didn't give any improvements2) Training on non normalized data - the result was much worse 3) Using pretrained BERT model - could'n do it (here is my unanswered question about it)4) Training de_core_news_md instead of de_core_news_sm - didn't give any improvements (tried it because according to the docs there could be an improvement thanks to the vectors (if I understood it correctly). Correct me if I'm wrong)5) Training on data, normalized in a slightly different way (without lower casing and punctuation deletion) - didn't give any improvements6) Changing dropout - didn't helpSo right now I am a little stuck about what to do next. I would be very grateful for any hint or advice.Thanks in advance for your help!"
2028,name 'doc2vec' is not defined,"['python', 'word2vec', 'text-classification']",I am trying to use doc2vec for text classification but after importing when i am trying to use it inside a function its says doc2vec in not defined. Please help me to identify which all libraries should I be installing to use doc2vec.here I have 2 columns called Action(Text or sentences) and Category(actual tags)
2029,How to optimize fine-tuned BERT's model size in TensorFlow 2.0?,"['tensorflow2.0', 'text-classification', 'bert-language-model']","After fine tuning BERT for classification the model size is ~1.3GB while the pre-trained model size was ~400MB. This happens due to additional variables related to Adam saved with the model and can be removed when saving, as explained in https://github.com/google-research/bert/issues/99 for TensorFlow 1.x.How to do this in TensorFlow 2.x?"
2030,Probability output scores of text classification model seems to have max values,"['python', 'text-classification', 'softmax', 'bert-language-model']","I am training a text classification model using a pretrained BERT model from the huggingface library. It's performing really well and I'm getting a validation accuracy of 90% or higher. But when I look at the probability output scores for each category it seems to have a ceiling value. For my four categories the usual example looks like this: [0.43, 0.19, 0.19, 0.19]. Where the probability of the category with the highest probability does not get higher than 0.43 in any instance. This remains when I train for more epochs. My questions is pure out of curiosity, since the model I have now can be deployed. Does anyone know if this behavior is normal or if this is an issue that needs to be resolved?"
2031,converting the prediction results back to text Python,"['python', 'text-classification']","I have built a text classification model, the model takes users tweets and predicts each user's gender. I have used TF-IDF to transform the tweets, and used encoding to encode my target variable 0,1 for male and female. Now I have the prediction results in an numpy.ndarray form. My question is how to map the prediction back to their original form (male/female) and assign them to the corresponding users?labels encoding predictionThe output is "
2032,Are the vectorization settings considered hyperparameters in ML?,"['machine-learning', 'text-classification', 'hyperparameters']","Short definition of HP:""In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.""Examples of HP: ""alpha"" in naive bayes, C in SVM, nr. of layers in NN.I know that one tunes these hyperparameters on a validation set (distinct from the training and test set)For text classification, however, one vectorizes the text before training; with vectorization there are also many settings (e.g. the max_features() function in sklearn's Vectorizer). I found that these vectorization settings greatly affect training,validation & test set performance.My question is: are the vectorization settings considered hyperparameters, if so, is the fact that we have to define them before training, is that considered a limitation for modelling generalization?"
2033,Character-based Text Classification with Triplet Loss,"['python', 'machine-learning', 'keras', 'recurrent-neural-network', 'text-classification']","Im trying to implement a text-classifier using triplet loss to classify different job descriptions into categories based on this paper. But whatever i do, the classifier yields very bad results. For the embedding i followed this tutorial and the NN architecture is based on this article.I create my encodings using:My Neural Network is set up the following way:The triplet loss is defined as follows:The model is then trained with:My goal is to: First, train the network with no label data in order to minimize the space of the different phrases and second, add a classification layer and create the final classifier.My general problem is that even the first phase shows sinking cost-values it overfits and the validation results jump around and the second phase fails badly as I'm not able to train the model to actually classify.My questions are the following:All ideas and suggestions are really appreciated!"
2034,Output top 2 classes from a multiclass classification algorithm,"['python-3.x', 'scikit-learn', 'text-classification', 'multiclass-classification']","I am working on a multiclass classificiation problem for text , where I have a lot of different classes (15+).
I have trained a Linearsvc svm method(method is just and example).
But it outputs just single class with highest probability, Is there a way that algorithm outputs two classes at the same timesample code i am using: current output :desired output:Its is okay if i get output in just one column as i can split and make two columns from it."
2035,I am getting an vocabulary not fitted or provided,"['text-classification', 'multilabel-classification', 'countvectorizer']",I am loading my models and giving them a input from my dataset I am vocabulary not fitted or provided as an output.NotFittedError: Vocabulary not fitted or provided
2036,expected byte or unicode string,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","I have been trying to do text classification. There 2 columns action and category. I have divided the dataset into train and test split.There is some kind of np.nan is an invalid document, expected byte or unicode string.I'm getting error in the last line above:This seems to be some kind of object error"
2037,sklearn: Found arrays with inconsistent numbers of samples when calling naive_bayes.MultinomialNB(,"['python', 'scikit-learn', 'scipy', 'text-classification', 'naivebayes']","I have looked at similar questions as such as this one. But none of the mentioned solutions worked in my case.I am trying to build a text classification prediction model.However, Naive_bayes returns the below error:my training datareturnsmy training labels returns my test datasetreturnsI tried every possible type of transformation. Can any one recommend a solution? thanks "
2038,Predicted result saving to csv from dataframe,"['python-3.x', 'pandas', 'csv', 'text-classification', 'sklearn-pandas']","I'm trying to find combined accuracy of more than 2 predicted dataframe results, 
After Prediction of Result saved result into csv and to get the Final Combined accuracy all the saved result is imported and concatinated. But while computation it was not working as expected and came to following outcome the results are changed on saving to csv. Do we have another way to save result into csv so, original form of result is preserved. the predicted classification report is as below "
2039,sklearn svm when each sample vector has a lot of zeros,"['python', 'scikit-learn', 'svm', 'text-classification']","I want to perform a text classification task with 4 categories and using svm.SVC from sklearn package.
Suppose the corpus has M different words in it.
I'm currently representing each doc as an M element vector filled with tf-idf values and so a lot of elements of a single doc are zeros as that doc doesn't have all of the corpus' words.So my X_train for feeding to svm.SVC looks like that.I was wondering if there is a way to represent each doc in another way (maybe dictionary that maps the non-zero indexs to their values?) so that it may speed up the svm training process.Or does the classifier automatically handle these criteria in which sample vectors contain a lot of zeros?"
2040,Text classifier to predict gender from Tweets,"['python', 'twitter', 'text-classification']",I am trying to build a text classifier to predict the gender of twitter users based on their tweets.   It is my first time building a text classification model and I am not sure of my steps.Two DataframesI have a data frame that acts as the training dataset. It contains each user ID and their gender. I also have a dataset that acts as the training and testing dataset. It contains the tweets of the users.tuples?I want to perform features extraction to train my model. So far I have created tuples using each row from each data frame as below.The output is As well as a tuple of user id and tweets the output is I am not sure if I should merge the tweets and gender into one tuple and use that to train my model.Could someone please guide me through the correct steps to extract the features for the model?
2041,Multilabel text classification with Sklearn,"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'multilabel-classification']","I have already tried everything that I can think of in order to solve my multilabel text classification in Python and I would really appreciate any help. I have based my result in here using multilabelbinarizer and in this web page .I am trying to predict certain categories in a dataset written in Spanish where I have 7 different labels, where my dataset is shown here. I have a message written and different labels for each of the rows. Each of the text messages has either one or two labels, depending on the message.So far, so good, but when I try to validate the problem it seems as almost every category is classified as ""None""OutputI also tried with MultiLabelBinarizer and I had the same problem, what am I doing wrong? Trying with MultiLabelBinarizer raised the following results: With the following outputThank you so much for your help"
2042,Dataset Language identification,"['nlp', 'multiprocessing', 'multilingual', 'text-classification', 'supervised-learning']",I am working on a text classification problem with a multilingual dataset. I would like to know how the languages are distributed in my dataset and what languages are these. The number of languages might be approximately 8-12. I am considering this language detection as a part of the preprocessing. I would like to figure out the languages in order to be able to use the appropriate stop words and see how less data in some of the given languages could affect the occuracy of the classificatin. Is langid.py or simple langdetect suitable? or any other suggestions?Thanks
2043,FastText recall is 'nan' but precision is a number,"['python-3.x', 'nlp', 'text-classification', 'precision-recall', 'fasttext']","I trained a supervised model in FastText using the Python interface and I'm getting weird results for precision and recall. First, I trained a model:Then I get results for the test data:But the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:Then when I look for the precision and recall for each label, I always get recall as 'nan':And the output is:Does anyone know why this might be happening?P.S.: To try a reproducible example of this behavior, please refer to https://github.com/facebookresearch/fastText/issues/1072 and run it with FastText 0.9.2 "
2044,How to write feature extractors class for pipeline in task of text classification?,"['python', 'pipeline', 'cross-validation', 'feature-extraction', 'text-classification']","I'm doing a text authorship attribution model.
The classifier is SVM (linear kernel), and I want to use cross_val_score from sklearn.model_selection for evaluation.The question is how to feed to the classifier via pipeline different features, mainly custom, not from libraries' transformers (e.g. average sentence length, frequency of punctuation marks, vocabulary richness, etc.) to train classifier considering all of them.This code for standard library transformer tf-idf works great:The problems come when I try to create a custom transformer class (using examples from here). I get warnings and Accuracy = nan.Warning message:I have spent hours, but still have no idea of what's wrong and how to feed a custom feature to pipeline, not to mention multiple custom features combined with typical vectorizers. Does anyone have any idea of why it happens or how to fix it?"
2045,transform multiple textual columns to numerical columns without losing column name,"['machine-learning', 'text-classification', 'tf-idf']",I have a dataframe that have multiple text columns and I want to transform them to numerical value so that I can use a classifier on them. I did a concatenation on them and used TFidf.But here I lose the column name information. I want to transform each column into a column of numerical value without having to concatenate all columns. What are the methods of doing that?
2046,Using fastText Sentence Vector as an Input Feature,"['text-classification', 'fasttext', 'mlp']",I want to use the fastText Sentence Vector as an input Feature. I am attempting to perform Binary Classification of sentences using MLPs and will train the algorithm using the fixed sized feature generated by the above code. Is this a plausible thing to do?
2047,"Excellent performances on training test, bad on test set","['data-science', 'text-classification', 'tfidfvectorizer']","I'm doing text classification and I'm dealing with weird results. I have two datasets, one labeled and the other one unlabeled. When I use some classifiers (SVM, Naive Bayes, knn, Random Forest, Gradient Boosting) on the labeled one I have excellent performances, even without tuning, with all the classifiers (more than 98% of BAC), but when I try to predict results on the unlabeled dataset I have very different predictions for every classifier. I used TF-IDF as vectorizer and I tried to use also bigrams and trigrams but nothing has changed. I tried also to create different new observations using SMOTE (even if I don't have problems with imbalanced dataset) just in order to see if, with new observations, algorithms would generalize better with new data but, even in this case, nothing has changed. What can I do in order to resolve this problem? Why is this happening? Do you have any idea?"
2048,Predicting new content for text-clustering using sklearn,"['python', 'scikit-learn', 'text-classification', 'tf-idf', 'tfidfvectorizer']","I am trying to understand how to create clustering of texts using sklearn. I have 800 hundred texts (600 training data and 200 test data) like the following: and I would like create clusters from those. 
To transform the corpus into vector space I have used tf-idf and to cluster the documents using the k-means algorithm. 
However, I cannot understand if the results are those expected or not as unfortunately the output is not 'graphical' (I have tried to use CountVectorizer to have a matrix of frequency, but probably I am using it in the wrong way). 
What I would expect by doing tf-idf is that when I test the test dataset
When I TEST:test_dataset = [""'Please don't inject bleach': Trump's wild coronavirus claims prompt disbelief."", ""Donald Trump has won the shock and ire of the scientific and medical communities after suggesting bogus treatments for Covid-19"", ""Bleach manufacturers have warned people not to inject themselves with disinfectant after Trump falsely suggested it might cure the coronavirus.""](the test dataset comes from the column df[""0""]['Names'])
I would like to see which cluster(made by k-means) the texts belongs to.
Please see below the code that I am currently using: where df[""0""]['Names'] is the column 'Names' of the 0th dataframe.
A visual example, even with a different dataset but pretty same structure of dataframe (just for a better understanding) would be also good, if you prefer. All the help you will provide will be greatly appreciated. Thanks "
2049,Cannot Find Module 'preproc' in Python/PySpark,"['python', 'pyspark', 'text-classification']","I am trying to follow this tutorial: https://runawayhorse001.github.io/LearningApacheSpark/textmining.htmlI have loaded my data into a PySpark DataFrame, however when I get to the preprocessing step, I receive the error, ""ModuleNotFoundError: No module named 'preproc'"" I can't find any information online about what to pip install in order to be able to use the preproc module. !pip install preproc within a Jupyter notebook returns, ""Defaulting to user installation because normal site-packages is not writeable
WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.
Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.
To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.
ERROR: Could not find a version that satisfies the requirement preproc (from versions: none)
ERROR: No matching distribution found for preproc""python -m pip install preproc within cmd returns, ""ERROR: Could not find a version that satisfies the requirement preproc (from versions: none)
ERROR: No matching distribution found for preproc""How do I proceed finding the correct package to install?"
2050,Sklearn text classification: Why is accuracy so low?,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","Alright, Im following https://medium.com/@phylypo/text-classification-with-scikit-learn-on-khmer-documents-1a395317d195 and https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html trying to classify text based on category. My dataframe is laid out like this and named result:The goal would be to categorize a post by its type, and target just assigns number 1-16 to each of the 16 types. To classify the text I do this:And depending on how much I shorten result in the beginning, accuracy peaks at around 0.4 for all algorithms. It is supposed to be 0.8-0.9.I read scikit very low accuracy on classifiers(Naive Bayes, DecissionTreeClassifier) but dont see how to apply it to my dataframe. My data is simple - has category (type) and text (post). What is wrong here?EDIT - naive bayes take 2:"
2051,what is the difference between text classification and feature selection,"['machine-learning', 'text-classification', 'feature-selection']",can we select features without classification and if I have a text how can i know which are the features to choose? I need example regarding text not real word object example. if anyone can explain please? 
2052,Multilingual free-text-items Text Classification for improving a recommender system,"['nlp', 'multilingual', 'text-classification', 'unsupervised-learning', 'supervised-learning']","To improve the recomender system for Buyer Material Groups, our company is willing to train a model using customer historial spend data. The model should be trained on historical ""Short text descriptions"" to predict the appropriate BMG. The dataset has more that 500.000 rows and the text descriptions are multilingual (up to 40 characters).1.Question: can i use supervised learning if i consider the fact that the descriptions are in multiple languages? If Yes, are classic approaches like multinomial naive bayes or SVM suitable?2.Question: if i want to improve the first model in case it is not performing well, and use unsupervised multilingual emdedding to build a classifier. how can i train this classifier on the numerical labels later?if you have other ideas or approaches please feel free :). (It is a matter of a simple text classification problem)"
2053,Use Naive Bayes having carried out text preprocessing with TF-IDF,"['python', 'text-classification', 'tf-idf', 'naivebayes']","Is it possible to use Naive Bayes for text classification when the text has been preprocessed using TF-IDF? I have read some conflicting information on this, where some people appear to say that you can't as TF-IDF returns continuous variables, where others say Naive Bayes works just as well with continuous variables. Also, I understand that there are three types of Naive Bayes; multinomial, bernoulli and gaussian; is there on particular one that is suitable to use with text that has been preprocessed using TF-IDF? Thanks"
2054,Order/context-aware document / sentence to vectors in Spacy,"['nlp', 'spacy', 'text-classification', 'document-classification', 'spacy-pytorch-transformers']","I would like to do some supervised binary classification tasks with sentences, and have been using spaCy because of its ease of use. I used spaCy to convert the text into vectors, and then fed the vectors to a machine learning model (e.g. XGBoost) to perform the classfication. However, the results have not been very satisfactory.In spaCy, it is easy to load a model (e.g. BERT / Roberta / XLNet) to convert words / sentences to nlp objects. Directly calling the vector of the object will however will default to an average of the token vectors. Here are two questions:1) Can we do better than simply getting the average of token vectors, like having context/order-aware sentence vectors using spaCy? For example, can we extract the sentence embedding from the previous layer of the BERT transformer instead of the final token vectors in spaCy?2) Would it be better to directly use spaCy to train the downstream binary classification task? For example, here discusses how to add a text classifier to a spaCy model. Or is it generally better to apply more powerful machine learning models like XGBoost?Thanks in advance!"
2055,How can I apply Semi Supervised Learning on Text Classification using Keras?,"['python-3.x', 'machine-learning', 'keras', 'deep-learning', 'text-classification']",I am solving a text classification problem through semi supervised learning. My total dataset contains 1500 samples out of I have labelled 300 samples and left the rest for my model to label it. I have trained my model on 300 annotated samples using Sequential Model with embedding layer and bidirectional LSTM layer. I could not find a semi supervised learning approach to solve this problem that would label the rest of my unlabeled samples. Mentioning the code which I have tried:Link of annotated data
2056,Subscript out of bounds error from naive bayes function,"['r', 'machine-learning', 'data-modeling', 'text-classification', 'supervised-learning']","I am creating a user defined Naive Bayes Function where I show the accuracy, miss-classification and recall. I have yet to encode the recall. Help would much appreciated Below is my code for a Naive Bayes function:I am not sure why I am receiving the following error:Below is output of my train dataset: Testing data follows same layout!"
2057,Text classification: Fasttext?,"['python', 'text-classification', 'fasttext']","I would like to classify some text. I have never done before but I read about using Fasttext (https://github.com/facebookresearch/fastText/tree/master/python#installation). 
I have tried to follow the instructions: but I have got the following error:I would be interested in Italian language (in case of packages). 
Do you know the reason why I have been getting that error? I have never used it before, so I hope you can give me any advice, suggestions and tips about its use. "
2058,Classification - get exact label value to check how close to another class (Python),"['python', 'python-3.x', 'classification', 'text-classification', 'naivebayes']","I am doing text classification in python with 3 alghoritms: kNN, Naive Bayes and SVM. I have 3 classes - easy, medium and hard. The accuracy is quite fine. Is there a way to check for new text its exact value? After label encoding 0 is easy, 1 is medium and 2 is hard. So base on new text for example it is classified as medium but I want to know how close it was to easy/hard. Some of my code snippets:Now when I use Naive.predict() I get 0, 1 or 2. Is there a way to get the EXACT value for example 0,5897237489 which is 1 but I see that it is closer to 0 than 2 "
2059,SkLearn model for text classification,"['python', 'machine-learning', 'scikit-learn', 'artificial-intelligence', 'text-classification']","I have a classifier multiclass, trained using the LinearSVC model provided by Sklearn library.
This model provides a decision_function method, which I use with numpy library functions to interpret correctly the result set. But, I don't understand why this method always tries to distribute the total of probabilities (which in my case is 1) into between each one of the possibles classes.I expected a different behavior of my classifier. I mean, for example, suppose that I have a short piece of text like this:But my classifier was trained with three types of texts, let say ""maths"", ""history"" and ""technology"".So, I think it has very sense that each of the three subjects has a probability very closed to zero (and therefore far to sum 1) when I try to classify that.Is there a more appropriate method or model to obtain the results that I just described?Do I use the wrong way the decision_function?Sometimes, you may have text that has nothing to do with any of the subjects used to train a classifier or vice versa, it could be a probability about 1 for more than one subject.I think I need to find some light on these issues (text classification, none binary classification, etc.)Many thanks in advance for any help! "
2060,How to create a BERT Layer with Keras?,"['tensorflow', 'deep-learning', 'text-classification']","I am trying to user a BERT layer to classify text comments into positive or negative:The error reads: 
Unknown module spec type: <class 'tuple'>The bert_inputs are tensors of shape=(None, 563)"
2061,How to make prediction on Keras Text classification?,"['python', 'tensorflow', 'keras', 'text-classification']","I've trained a model with this reference: https://www.tensorflow.org/tutorials/keras/text_classification_with_hubHere is my code:I've saved the model as imdb_model.h5. I want to make a prediction on a custom text. For example ""The best movie, I have ever seen"". How can I do it?"
2062,How to implement KMeans Clustering with Word2Vec for a text-classification model?,"['python', 'nlp', 'word2vec', 'text-classification', 'one-hot-encoding']","I wish to achieve a text vectorization of variable length input text based upon it's KMeans Clustering labels and encode it in someway to use it later for a text-classification purpose.QuestionsFor example:
I wish to achieve this in first query...Approach so far:I have used Gensim's Word2Vec to vectorize train data with a output dimension of 300. Then passed word2vec scores of each unique word to a sklearn's KMeans Clustering model choosing a cluster of 5 which gives me an output label for each word.For new words in test data, I've put a check if that word is not available in word2vec's vocabulary, instead of assigning a random score and passing it to fitted KMeans to get a random label, I've directly assigned it a cluster label (that I imagine contains those kind of words).Now for Query 2, I have two possible ways I could think of... A. I am deciding to first label-encode all cluster labels with a mapping (0,1,...5) and then pad all the sequences with -1 uptil a chosen max_length. Resulting data is then again mapped with 0: [1,0,0,0,0] in a One-Hot-Encoder style mapping. B. Use sklearn's MultiLabelBinarizer to hot-encode it in more clean way.Suppose this is my dataframe ..Here's 3.A. what looks likeI mapped cluster labels I got from word2vec -> Kmeans with 0,1,..4 and then padded..Then I re-mapped it using a list in OneHotEncoder style ....Here's what 3.B. looks like:I know it's a long post, but please bear with me on this and let me know about your valuable suggestions/examples or more options. My end goal is represent these text lines for classification. These lines would often have names, places, locations etc which makes it difficult for me to consider easier options like BoW model, etc. Let me know about the 3 questions I've asked and which approach you think would fit better ??Thanks in advance!"
2063,How to get unknown class in multi-class' prediction?,"['tensorflow', 'keras', 'recurrent-neural-network', 'text-classification', 'multilabel-classification']","I have a multi-class sentence classification problem and i want to achieve some sort of unknown class in prediction. There are n different labels and sometimes the sentence must not be classified or classified as unknown\None. If you have some options to achive this behavior, i'd be very pleasured"
2064,ValueError: The number of classes has to be greater than one; got 1 class,"['python', 'text-classification', 'valueerror']","And Error : ValueError                                Traceback (most
  recent call last)  in ()
        1 SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
  ----> 2 SVM.fit(Train_X_Tfidf,Train_Y)
        3 # predict the labels on validation dataset
        4 predictions_SVM = SVM.predict(Test_X_Tfidf)
        5 # Use accuracy_score function to get the accuracy1 frames /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py
  in _validate_targets(self, y)
      529             raise ValueError(
      530                 ""The number of classes has to be greater than one; got %d""
  --> 531                 "" class"" % len(cls))
      532 
      533         self.classes_ = clsValueError: The number of classes has to be greater than one; got 1
  classHow to solve that?"
2065,How to get better text classification accuracy using deep learning and word2vec features>,"['python', 'tensorflow', 'deep-learning', 'word2vec', 'text-classification']","I am trying to perform text classification on Roman Urdu Dataset using word2vec word embedding and deep learning model. My approach is based on first of all loading the corpus and cleaning the data, tokenize the sentences and
later sentences to words.The tokenized data are being trained on word2vec model using gensim
library having vector size 300 and window size 3. Later these word    embedding are  used to get the feature vector for
each document by    getting mean of word vector.After that the acquired doc vectors are being split into training and
testing data and finally sent to deep learning model to text
classification (Positive,Negative, Neutral). I am getting an accuracy of not more than 44% can anyone suggest what is wrong with my approach.Below is my code for reference.Creating doc vectors (by taking mean of word vectors):My deep learning model:"
2066,How to use type/token ratio as a feature for a LinearSVC in Python?,"['python', 'machine-learning', 'scikit-learn', 'svm', 'text-classification']","I have a .csv file with each row containing two columns: the text itself and the calculated type/token ratio. Unfortuantely, the classifier only assigns the two majority class labels to the test-texts out of the eight possible, which results in a f1 score similar to a chance-based baseline. This happens to every ratio feature I tried, including average word length and average sentence length. All such features are floats. What do I need to do in order to make use of such features?"
2067,Accuracy and prediction Classifiers,"['machine-learning', 'keras', 'lstm', 'decision-tree', 'text-classification']","I have trained LSTM AND decision tree on my data set (type of text classification). I have used K-cross fold validation with k=10. Decision tree accuracy 61%
LSTM accuracy 90%Now when I predict on totally unseen data then decision tree predicts more well and good as compared to LSTM.Why it happens? If LSTM accuracy is more then why decision tree performs more well on unseen data as compare to LSTM?"
2068,An algorithm for computing the edit-distance between two words,"['python-3.x', 'nlp', 'text-classification', 'fasttext', 'edit-distance']","I am trying to write Python code that takes a word as an input (e.g. book), and outputs the most similar word with similarity score. I have tried different off-the-shelf edit-distance algorithms like cosine, Levenshtein and others, but these cannot tell the degree of differences. For example, (book, bouk) and (book,bo0k). I am looking for an algorithm that can gives different scores for these two examples. I am thinking about using fastText or BPE, however they use cosine distance. Is there any algorithm that can solve this?"
2069,Text classification by Keras. Test accuracy does not change,"['tensorflow', 'keras', 'text-classification']","I use stack-overflow-data.csv.........................After that I tried to plot the history object and got the following picture:
enter image description hereI don't understand why my validation accuracy curve does not increase. It has max value in epoch 1 and after that only decreased. I suspect that this curve should increase to the same value (for example about epoch number 10) and after that stay with the same values."
2070,Separate the words in the sentence for text classification problem,"['python-3.x', 'machine-learning', 'deep-learning', 'neural-network', 'text-classification']","I am solving a text classification problem and while annotating my data I found very long words which are sentence itself but are not separated by space. One of the example which I found while annotating my data point is:Throughnumerousacquisitionsandtransitions,AnacompstillexiststodaywithagreaterfocusondocumentmanagementDesired output:Through numerous acquisitions and transitions, Anacomp still exists today with a greater focus on document management.I have looked upon various frameworks such as Keras, PyTorch to see if they provide any functionality to solve this issue but I couldn't find anything. "
2071,Out of core learning for multi-label text classification problem,"['machine-learning', 'scikit-learn', 'nlp', 'text-classification', 'multilabel-classification']","I am working on a multi-label text classification problem (Total target labels 90). The data distribution has a long tail and class imbalance. I am working with a sample of 100k records using the OVR strategy (One Versus Rest). Since the dataset is huge, I am trying out the partial_fit method. I came to know that there were some issues previously and similar question was asked back in 2017. I tried the partial_fit and found the same issue still exist or maybe I am not doing it correctly.Scikit-learn version : 0.22.2.post1CodeError "
2072,Machine learning with multiple labeled data,"['python-3.x', 'machine-learning', 'nlp', 'text-classification', 'multilabel-classification']","I have 'Invoice report' where i want to train my 'Invoice Description' and 'Part number' both of them are Text/alphanumeric.Invoice Description like ....
MT-8200-50A;IntelliTone Pro 100 Toner and Prob
CB-84901-1005-15;Cognex Ethernet Cable, 15 meterPart Numbers like
1152
ASR900and i want to predict the 'Manufacturer' which is also an text like (Cisco, nokia,Dell....) where i have 100+ labels/Manufacturer.so which approach will to advisable.I did used the LinearSVC and got 90% accuracy, but i just used one Target Variable 'Invoice Description'.My Question:
I want to use both 'Invoice Description' and 'Part number' , do i need to perform countvectorizer/tfidf for both. if yes... how? need your assistance.and Since we have multiple ''Manufacturer' do we need to create Dummy variables for it?"
2073,Use Text as feature column in Tensorflows existing Estimator,"['tensorflow', 'text-classification']","I try to build an Classification with the existing estimator to predict if an article will be sold or not. I tried to use a linearClassifier, because I'm a beginner in Tensorflow and Pyhton.I have a dataset with price, category and size, which is perfect for numeric or category feature columns. But I also have a description of the article, only 3-6 words per article and around 6500 different words as per my analysis.
I tried to use shared embed, with one category column per word, but this not work. And when I add all 6500 columns directly to the model it is very slow. What is the best way and easiest way to handle the description? At best with code example. The word order doesn't matter, but for example if it's from a brand it will sell better than noname.Many thanks for your answersEdit: I tried with this post Tensorflow pad sequence feature columnBut I now have the problem that tf.data.Dataset.from_tensor_slices((dict(dataframe), labels)) don't workdfall.head() isThe result isI already tried to use
dfall['description']=dfall['description'].apply(np.asarray)
but then I gotFor all have same problem the solution is "
2074,Text Classification with Custom Vocabulary in Python,"['python', 'python-3.x', 'text', 'nlp', 'text-classification']","I have some list of words/lexicon and I want to use them for a BOW classification. In sklearnit is possible to use countvectorizer and tfidfvectorizer in sklearn, the two approaches builds the vocabulary the use from the training data. But in my case I have built a kind of list of words(dictionary) that can be used to discriminate between the classes for text classification. Is there any library or package I can use in python?"
2075,Is there any better way to classify narration based on keywords in R?,"['r', 'text-mining', 'text-classification', 'fuzzy-search']","I'm trying to classify narrations based on certain keyword dictionary. My approach is to identify the keyword with the least string distance with the narration. This works fairly well, but I encountered one example where this approach didn't seem appropriate. Following is a snippet of the codeFrom what I understand, stringdist function first recycles the shorter length string to match the longer length and then calculates the disctance based on the number of iterations needed to match both the strings. What I don't understand is, b3 being a sub string of the narration a, yet does not have the closet distance compared to other key words. Wanted to know if there's any reason behind it and what other alternative approach I can try for better matching? "
2076,Computing classification metrics for sequence labelling task,"['nlp', 'classification', 'precision', 'text-classification', 'precision-recall']","I intend to calculate accuracy/precision/recall/F1 measures for sentence classification task. I previously have computed it for whole text classification which is quite easy, but got confused at doing it for sentence classification as we perform at sentence-level and not text-/sentence(s)-level. Note that a text might contain several sentences... Here is an example:Suppose we have the following text, with predicted labels in []:Seq2seq networks are a good way of learning sequences. [0] They perform reasonably fine at generating long sequences. [1] These networks are utilized in downstream tasks such as NMT and text summarization [0]. blah blah blah [2]So the prediction is [0, 1, 0, 2] and suppose the gold labels for the sentences above are: [1, 1, 0, 0]. So is the accuracy of this equal to correct / total = (1 + 1) / 4 = 0.5? What about other metrics such as Precision, Recall, and F1? Any ideas? "
2077,How to deploy my own Tensor flow text classification Model to an android app?,"['python', 'android', 'tensorflow', 'deployment', 'text-classification']","I have created my own Emotion classification model for Urdu Language Text. Now I want to deploy that model to android app. As we know tensor flow offers to deploy ML model to an android app, but I do not understand how to deploy my own model.However, I have looked tensor flow documentation but I do not understand the workflow of it properly. Does anyone know how to deploy? Anyone help?"
2078,matching and text classification with spacy and NLP,"['nlp', 'spacy', 'text-classification']","I've been using the Spacy rules based matcher to extract small sections from longer texts to create a dataframe that I store as csv I then use to classify with scikit or spacy or other. Matcher is working beautifully but now confused and beginning to question my approach.   Since I have files that were manually coded with relevant text extractions which I've been using to create the matching rules, can I actually just train a model on the manual text in spacy and then use that model to classify on entire new docs? Or do you train a model to do the matching?The documents the text examples come from are much longer, whereas the examples are ~ a sentence or two max.I ask because it will be necessary later to extract and classify different variables and want to be sure I'm not missing something really obvious or wasting time"
2079,Text Classification : LSTM vs Feedforward,"['machine-learning', 'keras', 'nlp', 'lstm', 'text-classification']","I am training a text classification model. Task : Given a description, identify the quantifier For ex 
1) This field contains the total revenue amount in USD -> amount 2) This has city code -> code3) total deposit amount is 34 -> amount 4) contains first name info -> name 5) contains last nme -> name For the given task, it makes sense to model this as a text classification problem. I took two approachesApproach 1 : a) Use glove embedding to get vector represenation b) Use feedforward NN to classify data into 1 of 11 possible output classes This approach gives me 80% test accuracy Approach 2 : I plan to use LSTM because they can also learn the context and from previous words The problem is irrespective of what i do LSTM never gets above 40% accuracy mark. It gets stuck on it from start to end. Morevoer, the feedforward net ( Approach 1 ) can detect simple cases like ""total amount is 6 usd"" but LSTM is unable to get even this correct and predicts it as OtherMy question is why does LSTM ( with added power of context ) fails to improve upon feedforward. What should i do to improve it . "
2080,Improve Keras Model for text classification,"['python', 'lstm', 'text-classification']","I am making a model for classification of webpages titles into one of 101 classes regarding food (most of the titles regard recipes). The medium length of my sequences is 42. I cleaned the text (bad words, changed to lowercase etc) and tokenized it using a Tokenizer. I put a LSTM layer in my model, and I get 83% accuracy on the test set. I'm pretty sure this can be improved making some changes to the network, do you have any suggestions? Thank you in advance! That's my model:"
2081,Predict Name Origin with RNN,"['python', 'tensorflow', 'keras', 'neural-network', 'text-classification']","I'm trying to train a RNN to predict a names origin. The dataset is from a Pytorch tutorial, and I basically need to redo the tutorial using tensorflow/keras. Dataset:Data-Preprocessing: RNN Model: My model trains fine, and I get a better accuracy than I hoped for originally. Basically, what I am trying to do now, is create a function that takes a string input, and outputs an origin based on the network's training (i.e input = 'Sergey', output= 'Russian'). The function to do this in the Pytorch tutorial requires several other functions. I basically want to recreate this function:in the context of the Neural Network I created. "
2082,"How to multi-label classify movies to film festivals based on its metadata, where the metadata is predominantly individual words?","['python', 'machine-learning', 'vectorization', 'text-classification', 'multilabel-classification']","I have created a data-set of various movies produced in the past few years, technicians worked for the film, genre, country it represented, runtime, language, the respective film festival that film has won, etc.the data-set is similar to this,  it is an excel file. I'm interested in multi-label classification of the movies to film festivals based on the inherent features of the movie(irrespective of the plot)I thought we need to work in numbers/vectors to multi-label classify the data. But, I'm unaware of how vectorization of names(proper nouns) and few individual words can be carried out.Is there any other way I can carry out the process to achieve my goal of multi-label classification with the above data? Please help me identify it. Thank you."
2083,PyTorch Implementation of Disconnected Recurrent Neural Networks,"['deep-learning', 'nlp', 'pytorch', 'recurrent-neural-network', 'text-classification']","I'm finding a PyTorch implementation of this network Disconnected Recurrent Neural Networks. 0 implementation in https://paperswithcode.com/, where can I find it."
2084,Use JSON-dataset to do text classification with tensorflow,"['python', 'tensorflow', 'keras', 'text-classification']",I have a json file looking like this:I want to build a text classifier with Tensorflow and keras which learns using the text and user_rating entries and is able to add a rating to unrated entries (like the first on in upper example).How do I bring this dataset into a Tensorflow-friendly format? I use the introductions on https://www.tensorflow.org/tutorials/keras/text_classification_with_hub but I don't know which format I need and how to preprocess my dataset.
2085,Remove training data from spacy model,"['python', 'spacy', 'text-classification']",I have trained a spacy textcat model but then I realized that there were some incorrect training data: data from one category happened to be labeled with another category. My question is: is it possible to remove these training examples from the model without retraining it? Something like nlp.update() but in reverse? Would appreciate any help!
2086,get index of predicted value (Text classification),"['scikit-learn', 'prediction', 'text-classification', 'confusion-matrix', 'tfidfvectorizer']","I have a dataframe(df_cleaned) with the column ""name"" and ""text"" . The text column is vectorized(TFIDFvectorizor) in order to do the text classification. Once I got the confusion matrix(sklearn), I tried to map it with input data which is vector by using the code. And these are the result i've got.I want to know how to obtain the index of the original data in order to map it with ""name"" column. "
2087,training fasttext models with social generated content,"['machine-learning', 'nlp', 'text-classification', 'fasttext']","I am currently learning about text classification using Facebook FastText. I have found some data from Kaggle that contains characters such as �� or twitter username and hashtags. I tried searching the web however there is no clarification of how you really need to clean/pre-process your text before training a model. In some blogs I've seen authors writing about tokenisation however its not mentioned in fasttext. Another point it that fasttext git has examples of clean data, such as stackoverflow but nothing for twitter or such platform. Question is, what is the best practice to pre-process user(social) generated content before training a model? What needs to be redacted? Thanks"
2088,Difference between blank and pretrained models in spacy,"['python', 'spacy', 'text-classification']","I am currently trying to train a text classifier using spacy and I got stuck with following question: what is the difference between creating a blank model using spacy.blank('en') and using a pretrained model spacy.load('en_core_web_sm'). Just to see the difference I wrote this code:and it gave me the following result:hello hello False INTJeveryone everyone True PRON, , False PUNCTit -PRON- True PRON's be True AUXa a True DETwonderful wonderful False ADJday day False NOUNtoday today False NOUNThen I tried this (for the same text)and the result washello hello Falseeveryone everyone True, , Falseit -PRON- True PRON's 's Truea a Truewonderful wonderful Falseday day Falsetoday today FalseNot only are the results different (for example, lemma for 's is different) but there are also no POS tagging for most of words in blank model.So obviously I need a pretrained model for normalizing my data. But I still don't understand how it should be with my data classifier. Should I 1) create a blank model for training text classifier (using nlp.update()) and load a pretrained model for removing stop words, lemmatization and POS tagging or 2) only load a pretrained model for both: normalizing and training my text classifier?Thanks in advance for any advice!"
2089,Document classification: Preprocessing and multiple labels,"['word2vec', 'text-classification', 'tf-idf', 'doc2vec']","I have a question about the word representation algorithms:
Which one of the algorithms word2Vec, doc2Vec and Tf-IDF is more suitable for handling text classification tasks ?
The corpus used in my supervised learning classification is composed of a list of multiple sentences, with both short length sentences and long length ones. As discussed in this thread, doc2vec vs word2vec choice is a matter of document length. As for Tf-Idf vs. word embedding, it's more a matter of text representation.My other question is, what if for the same corpus I had more than one label to link to the sentences in it ? If I create multiple entries/labels for the same sentence, it affects the decision of the final classification algorithm. How can I tell the model that every label counts equal for every sentence of the document ? Thank you in advance,"
2090,How to choose the Chi Squared threshold in feature selection,"['python', 'scikit-learn', 'text-classification', 'tf-idf', 'feature-selection']","About this: NLP in Python: Obtain word names from SelectKBest after vectorizingI found this code:This code computes the chi squared test and should keep the best features within a chosen threshold. 
My question is how to choose a theshold for the chi squared test scores?"
2091,Number of outputs in dense softmax layer,"['machine-learning', 'text-classification', 'multiclass-classification']","I've been working through a Coursera course for extra practice and ran into an issue I don't understand.
Link to Collab So as far as I've worked on ML neural network problems, I've always been taught that the output layer of a multiclass classification problem will be Dense, with number of nodes equal to the number of classes. E.g. Dog, cat, horse - 3 classes = 3 nodes.  However, in the notebook, there are 5 classes in the labels, checked using len(label_tokenizer.word_index) but using 5 nodes I had terrible results and with 6 nodes the model worked properly.  Can anyone please explain why this is the case? I can't find any online example explaining this. Cheers!"
2092,How to use dependency parsing features for text classification?,"['python-3.x', 'spacy', 'text-classification']","I did dependency parsing for a sentence using spacy and obtained syntactic dependency tags.OutputWall/NNP <--compound-- Street/NNPStreet/NNP <--compound-- Journal/NNPJournal/NNP <--nsubj-- published/VBDjust/RB <--advmod-- published/VBDpublished/VBD <--ROOT-- published/VBDan/DT <--det-- piece/NNinteresting/JJ <--amod-- piece/NNpiece/NN <--dobj-- published/VBDon/IN <--prep-- piece/NNcrypto/JJ <--compound-- currencies/NNScurrencies/NNS <--pobj-- on/INI'm not unable to understand, how can I use this information to generate dependency-based features for text classification. What are the possible ways to generate features from this for text classification?Thanks in advance............"
2093,Fine tuning CNN hyperparameters for complex text classification,"['python', 'keras', 'deep-learning', 'text-classification', 'cnn']","I'm working on a CNN model for complex text classification (mainly emails and messages). The dataset contains around 100k entries distributed on 10 different classes. My actual Keras sequential model has the following structure:In compiling the model I'm using the Nadam optimizer, categorical_crossentropy loss with LabelSmoothing set to 0.2 . In a model fit, I'm using 30 Epochs and Batch Size set to 512. I also use EarlyStopping to monitor val_loss and patience set to 8 epochs. The test size is set to 25% of the dataset.Actually the training stops after 16/18 epochs with values that start to fluctuate a little after 6/7 epoch and then go on till being stopped by EarlyStopping. The values are like these on average:loss: 1.1673 - accuracy: 0.9674 - val_loss: 1.2464 - val_accuracy: 0.8964with a testing accuracy reaching:loss: 1.2461 - accuracy: 0.8951Now I'd like to improve the accuracy of my CNN, I've tried different hyperparameters but as for now, I wasn't able to get a higher value. Therefore I'm trying to figure out:Thank you very much to anybody who will help! :)"
2094,Keras deep learning sentiment analysis - supervised or unsupervised,"['machine-learning', 'keras', 'deep-learning', 'text-classification']","I am a bit confused because about the topic deep learning.My question: Let's assume that we've got a task to solve. Reviews should be classified where they are positive or negative by usage of Keras deep learning model.Now: Does this task belong to supervised or unsupervised learning? Why? And how does deep learning and neural network work here? How do they learn? Isn't it better, if a machine learning algorithm is being used for this task?"
2095,How to classify texts that are related to the bible based on their content,"['machine-learning', 'text', 'nlp', 'classification', 'text-classification']","I have a database of texts from comments of social networks (FB,Twitter).
My goal is to classify texts that have strong relation to the bible based on their content (for example if there are cites or ""biblical"" words that are used.
This is a binary classification problem and i need help to figure out how to approach it (maybe use the bible as a dictionary somehow). Thanks!"
2096,Searching a straight forward approach for classifying text into given topics,"['machine-learning', 'nlp', 'text-classification']","I am searching for a straight forward approach to classify text automatically into several categories, for example ""Sports"" or ""Religion"". 
What I am trying to do exactly, is extracting the non-html Text of multiple Websites for example with beautiful-soup and take that as an input for this approach to calculate the possibility that it matches the topics. The topics itself as well as the trained model which is used for this should already be in place.
Do you know of any solution that could manage this without the need to train such a model myself?"
2097,Text Classification: Separate the labels mentioned in the sentences,"['python-3.x', 'nlp', 'text-classification']",""" The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.,__label__2 ""__label__2 should be on separate column.
I have 100000 rows of sentences.Can anyone please help me out to resolve using python."
2098,Text Classification - How to transform and fit multiple string features into a machine learning model?,"['python', 'machine-learning', 'text-classification']","In my dataset there are 3 input columns(Manufacturer,short text,supplier) from which I wish to create a  vectorize feature list. And then I am expecting to fit the same into a machine learning model. Last column (category) is the label. I have 2 million rows in the dataset.
I am unable to vectorize all three columns at once.Data sampleErrorPlease let me know how to vectorize all 3 columns together.Thanks in advance."
2099,How to make BERT model converge?,"['tensorflow', 'keras', 'nlp', 'text-classification', 'bert-language-model']","I am trying to use BERT for sentiment analysis but I suspect I am doing something wrong. In my code I am fine tuning bert using bert-for-tf2 but after 1 epoch I am getting an accuracy of 42% when a simple GRU model was getting around 73% accuracy. What should I be doing different to effectively use BERT. I suspect I am traning the bert layers from the first batch which may be an issue as the dense layer is randomly initialized. Any advice would be appreciated, Thanks!"
2100,Machine Learning - Classification Problem,"['python', 'machine-learning', 'classification', 'text-classification']",Is there any command to call mis-classified values while predicting any values in a model in-short the ones which have been classified inaccurately. 
2101,Adding categories to the trained textcat model in spacy,"['python', 'spacy', 'text-classification']","I am new to spacy and wrote my first text categorization program for articles with classes like politics, economics, sport etc. I trained a model and everything works fine. Now I found some new data with classes, that don't exist in the trained model, like environment and hobbies. I am just interested whether it is possible to add these categories without retraining the whole model? Just update it. Or do I have to create a blank model and train it again?"
2102,Text classification with imbalanced data,"['machine-learning', 'nlp', 'data-science', 'text-classification', 'imbalanced-data']","Am trying to classify 10000 samples of text into 20 classes. 4 of the classes have just 1 sample each, I tried SMOTE to address this imbalance, but I am unable to generate new samples for classes that have only one record, though I could generate samples for classes with more than 1 sample. Any suggestions?"
2103,What is the appropriate LSTM model for multiclass text classification? Tensorflow 2.x,"['python', 'tensorflow', 'lstm', 'text-classification']","I'm working in proyect to predict personality type from Myers-Briggs test. I would like to get an appropiate LSTM model for this multiclass text classification problem.Size:Padded: training and validation data are padded with max lenght of 240.E.g.:SHAPE:Expected output:
There can be two types of output:What I tried?: I already tried some of the LSTM models from the TF website, however, I would like to know your own answer on this topic.Before trying to close the question, please make any suggestion to improve it, will be considered."
2104,Predict one sample after trining LSTM,"['python-3.x', 'keras', 'nlp', 'classification', 'text-classification']","This is my network details : I trained the model and evaluated, but I want to predict just one sample. I tried to use but it gives me the following error :
ValueError: Error when checking input: expected embedding_1_input to have shape (60,) but got array with shape (1,)I tried to use :and gives me this error : AttributeError: 'str' object has no attribute 'ndim'I tried this as well :but it also give me the same error : AttributeError: 'str' object has no attribute 'ndim'Any suggestion please ?"
2105,How to solve loss: nan & accuracy: 0.0000e+00 in a LSTM problem? Tensorflow 2.x,"['python', 'tensorflow', 'text-classification']","I'm working in a LSTM problem. I'm trying to predict MBTI (Myers-Briggs test) personality type based on text classification (there's 16 personality types).I have a csv file, which was preprocessed: the stopwords were removed, it was lemmatized, tokenized, sequenced and padded. The file doesn't have any NaN values and the text sequence have only int numbers.However, the problem is generated when trying to train the model I get: loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00As requested: how's the x, y data and label looks like with the resultsWhat I tried?Expected output:
Maybe I'm building wrong the model, so I will explain which is the main idea. I would like to get one output or sixteen outputs, which determines the accuracy of your personality type.If you'll like to check, here is the code:mbti personalityDataframe:mbti_dfAny suggestions to improve the question will be considered"
2106,Visualize text classes in a scatter-plot,"['python', 'nlp', 'text-classification', 'torch']",I am looking for ways to investigate in my train data 'modellability' and check if the classes are well distinguished in terms of vocabulary... etc.I am a bit embarrassed but I was wondering if it is possible to do a scatter plot for text classification model in torch? or any other approach to investigate in the data quality.
2107,Building and Evaluating a FilteredClassifier in Weka Java API,"['java', 'machine-learning', 'weka', 'text-classification']","I am having an issue with Weka Java API when i am trying to build and evaluate a classification model(NaiveBayesUpdateable). Also CSV files were used as training and testing dataset with columns ID, Class and Text (Supervised dataset). Code used for the time being:Upon running this piece of code, the following error was obtained:What i am doing wrong?Any response is highly appreciated. thx"
2108,How to test siamese model in pytorch?,"['model', 'classification', 'pytorch', 'text-classification', 'siamese-network']","I'm new to neural networks and recently use Pytorch.
I'm trying to use siamese neural network, so i found and tried this project. I have already completed the stages described at the model train stage, then I got this model
In the train process I use my own dataset which contains characters like the Omniglot dataset.
After I train and get this model, how can I classify the model, like giving each character a name ??or how can I test this model by inputting a picture and then seeing the results?can anyone help me ?? please"
2109,Intepreting results of an NLP using a Confusion Matrix,"['python', 'neural-network', 'text-classification', 'confusion-matrix']","I am trying to use a confusion matrix to determine what problems my neural network ran into when being trained on the 20newsgroup dataset. I was able to run the network and got decent accuracies, but the Confusion Matrix I produced to analyze the results doesn't make a lot of sense without added context. In theory, a method to label the confusion matrix/modify its output to the context of the words processed by the network, that would probably help me make sense of it. Preparing the data:Neural Net Model:Confusion Matrix:Output:"
2110,How to build a deep learning text classifier using convolutional neural networks (python),"['python-3.x', 'deep-learning', 'neural-network', 'conv-neural-network', 'text-classification']","What are the steps I would need to take to build a deep learning text classifier, more specifically a text classifier that identifies an author (authorship attribution) in a set of unlabeled texts? The model I am looking at using is word-word CNN (convolutional neural network) which has proven to be very successful in things such as text classification. I am looking to build this model in python.I am new to deep learning so any resources and information is appreciated."
2111,How can fix learning rate with text classification on tensorflow?,"['tensorflow', 'keras', 'deep-learning', 'sentiment-analysis', 'text-classification']","I have been coding sentiment analysis model with tensorflow keras.
I am using csv dataset which has labels(pos:1, neg:0) in row 1 and English texts in row 2.
The results I expect is to show number between 0 and 1 when I input some texts through txt file.
However, though I set model, the loss rate is keeping negative score and accuracy rate is not increasing, including validation rates.
I do not know what the matter is. Thus, I attached my codes. 
Thank you very much.enter image description here"
2112,How to make OneClassSVM model more accurate? (Scikit-learn),"['python-3.x', 'machine-learning', 'scikit-learn', 'text-classification', 'one-class-classification']","I have been attempting to classify an author using multiple texts written by this author, which I would then use to find similarities in other texts to identify that author in the test group.I have been successful with some of the predictions, however I am still getting results where it failed to predict the author.I have done pre-processing the texts beforehand with stemming, tokenizing, stop words, removing punctuation etc. in an attempt to make it more accurate. I am unfamiliar with how exactly the OneClassSVM parameters work. What parameters could I use to best suit my problem and how could I make my model more accurate in it's predictions?Here is what I have so far:"
2113,Categorization - Data Aggregation using iPython,"['numpy', 'machine-learning', 'text-classification', 'sklearn-pandas']",I have 196 crime descriptions assigned to roughly 1700 records on a csv file that I need to group into 20 categories so I can make meaningful symbols of them on a map. I assigned my own categories to 80 records hoping that this would be enough to train a simple classification model. I tried running the data through some numpy and sklearn methods from this post: https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f but the results were not meaningful (I'm in over my head!). Here is a link to the git repo that has the iPython and associated csv files: https://github.com/jshaunobryan/Data_CategorizationI ran the iPython steps with the version of the csv files with all of the nulls removed for the Aggregated Descriptions. The csv file CrimeCodeAgg has all rows of data before removing the AggregatedDesc Null values. The code in the iPython notebook yielded a couple of index errors but more importantly I think I'm just going about this all wrong. For those of you without access to github here is the code from the iPython notebook:I'm not sure how to get the table data on to SO but here's a link to the raw data on the Alameda County Open Data Hub: https://data.acgov.org/datasets/76da968f7ef049a086c020191b58a83b_0/dataCan anyone tell me if there is an easier ML method I can use that will simply read my csv file and assign groups based on the number of classifications I desire? Maybe a way where I can do away with the training attempt in the Aggregated Description column?
2114,One class SVM model for text classification (scikit-learn),"['python-3.x', 'machine-learning', 'scikit-learn', 'text-classification', 'one-class-classification']","I am attempting to classify a train set of texts to be used for predicting similar texts in the test set of texts. I am using the one_class_svm model. 'author_corpus' contains a list of texts written by a single author and 'test_corpus' contains a list of texts written by both other authors and the original author. I am attempting to use one_class_svm to identify the author in the test texts.I am getting the value error:How might I implement this model to accurately predict authorship of texts in the test set given the single author in the train set? Any help is appreciated. For reference, here is a link to the one_class_svm model guide: https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM"
2115,Which classification model should I use for author attribution in machine learning?,"['python-3.x', 'machine-learning', 'scikit-learn', 'classification', 'text-classification']",I aim to have a train set of texts written by a specific author and a larger test set of unknown texts. I want to be able to predict whether or not each text (or class) in the test set was written by the specific author of the train set of texts. What classification model should I use to achieve this and how might I implement it?
2116,How to Separate Dataframe Based on the Label value? [duplicate],"['python', 'machine-learning', 'jupyter-notebook', 'sentiment-analysis', 'text-classification']",Let say I have this dataframe. I want to split it based on the SENTIMENT column. Like this below.Anyone knows the solution in Python (Jupyter) for that case? Your help will help my thesis project. Thank you :D
2117,BERT for NER with hugginface Tensorflow 2,"['python', 'tensorflow', 'nlp', 'text-classification', 'ner']","I need some help in using BERT for NER in Tensorflow. Basically all tutorials are in PyTorch. I'd really appreciate some advice in either of the two approaches.1) I am interested in using the Huggingface Transformers package, since they have a TFBertForTokenClassificationclass, but there is no tutorial or guide surrounding how to use it. Nor how to train on your own dataset.2) I have implemented BERT in Tensorflow for text classification with no additional APIs/libraries other than the raw bert library. One approach would be to adapt this for NER, however, I am unsure how I would alter the model. Currently, I've just thrown a dropout layer and a dense layer over the top of the pretrained BERT model, with softmax accross the dense layer."
2118,How to find most important features for text classification in Keras model?,"['keras', 'deep-learning', 'text-classification']","I am working on a problem where I need to classify phrases in one of the two categories (let's A & B). I used the Keras SepCNN model (similar to this) for that and it is giving me some results.Now, I want to analyse the predictions and more specifically I want to know why the model classified a certain phrase in category A or B, which set of features played an important role in labeling that phrase as category A or B?I am not sure if this is even possible to do but need inputs on how to approach this.Model Structure for your reference
Vectorize text (tfidf)
SelectKBest features (using scikit-learn feature_extraction)
Fit the model
Predict on test setI looked on the internet and found nothing helpful. It would be very helpful if someone can point me the right direction.
I also explored eli5 library but that does not seem to work for Keras SepCNN model (at-least I could not get it to work, let me know if anyone has used successfully it?)"
2119,How to prepare custom dataset for text classification in Tensorflow 2.x?,"['python', 'machine-learning', 'text-classification', 'tensorflow-datasets']","I'm trying to predict personality types from the Myers-Briggs test. I created my own csv_file with 16 rows and 2 columns, which looks like this:What I already try?
Preprocessing: I tried to preprocess the description column getting rid of stopwords and applying tokenization:I'm still unsure how to correctly prepare custom datasets in text classification, specialy in this kind of problem.If there's anything what I can do to improve this question, let me know, I will be glad to fix it."
2120,Using Naive Bayes to do multi classification,"['python', 'text-classification', 'naivebayes']","I have a dataset as below:I wish to do classification by using naive bayes or any other BEST algorithms available. However, I received error when using naive bayes as below:Errros:Please can anyone please help me with this? Or can someone show me some other best examples of algorithms to use for machine learning like decision tree, svm or anything."
2121,Why we say that one-vs-all classifiers doesn't take consider the label dependencies?,"['classification', 'text-classification']","In the multi-label classification settings or in multi-class classification settings(even though there are different settings), there can be two ways to build these frameworks. And people usually said that setting number 2 doesn't take consideration of label dependencies. Why is that?"
2122,Binary classifier of words in list,"['python', 'machine-learning', 'text-classification']","I have extracted text using OCR from a number of stylized documents in Swedish. Now I want formalize the data and extract city names. Due to the OCR working imperfectly, the names are sometimes spelled wrong and shows up at semi-random positions in the text. Therefore regex does not work. Is it a good procedure/possible to transform the text into bags of words and train an algorithm to binary categorize words as city vs not city? Consider example data set:I want to use Col1 as X variable and Col2 as Y variable. Is this a good procedure or should I use another method?"
2123,How to Implement Random Forest From Scratch for Text Classification using FastText Pre-Trained Model?,"['python', 'jupyter-notebook', 'random-forest', 'text-classification', 'fasttext']","I need help for my thesis research. I need some references, actually source code of Random Forest Classifier From Scratch (without sklearn.ensemble library).I need it for text classification (actually sentiment analysis) using
  FastText pre-trained model.Anyone knows or have the source? I will really appriciate your help. It will help my thesis project. Thank you for your attention."
2124,How to use machine learning to classify text using available keywords?,"['machine-learning', 'text-classification', 'lda', 'topic-modeling']","I have around 200 keywords for 15 different categories, I want to apply machine learning to classify/detect the text for different category.I have used LDA topic modelling, but that is not helpful as it is giving topic explicitly. What can else to used for this?"
2125,why take the first hidden state for sequence classification (DistilBertForSequenceClassification) by HuggingFace,"['time-series', 'sequence', 'tensorflow2.0', 'text-classification', 'huggingface-transformers']","In the last few layers of sequence classification by HuggingFace, they took the first hidden state of the sequence length of the transformer output to be used for classification. Is there any benefit to taking the first hidden state over the last, average, or even the use of a Flatten layer instead?"
2126,How get the label predicted using sklearn and numpy?,"['python', 'numpy', 'machine-learning', 'scikit-learn', 'text-classification']","I am trying to using sklearn to predict some texts using a folder where each subfolder is a collection of txt files:This the result from print(dict(zip(labels_str, counts))):But the result from cls.predict is only an int on an array:Or even [1], [3] etc... when I change the text_to_predict value.So, how can I get one of the sub-folders' name as result of prediction?"
2127,Text classification with word2vec stack overflow tag predictor,"['python', 'machine-learning', 'data-science', 'text-classification']","I am working stack overflow tag predictor.I have a dataframe df which contains a feature 'post' and label 'Tags' which can be multi lable.My df is :So I want to use word2vec for classification and predict the tags.I want to use all machine learning classifier like SVM,  random forest etc.I also want classification report of tags.So please help me."
2128,How do I make my algo work with KNN text classification?,"['python-3.x', 'nlp', 'knn', 'text-classification']","Trying to make my classification accepting a text (string) and not just a number (numeric). Working with data, carrying a load of pulled articles, I want the classification algo to show which ones to proceed with and which ones to drop. Applying a number, things are working just fine, yet this is not very intuitive, although I know that the number represents a relationship to one of the two classes I am working with.How do I change the logic in the algo to make it accept a text as search criteria and not just an anonymous number, picked from the 'Unique_id' column? Columns are, btw...'Title', 'Abstract', 'Relevant', 'Label', 'Unique_id'. The reason for concatenating df's at algo end is that I want to compare results. Finally. it should be noted that the col 'Label' consists of a list of keywords, so basically I want the algo to read from that col.I did try, reading from data sources, changing the 'index_col='Unique_id' to 'index_col='Label', but that did not work out either.An example of what I want:This is the full code (view end of algo to see above example as it runs today, using a number to identify nearest neighbor):"
2129,Methods other Tf-Idf for feature selection from text,"['machine-learning', 'nlp', 'feature-extraction', 'text-classification', 'feature-selection']","What are the other methods than Tf-idf for selecting features from a text document. To my knowledge the text document is converted into a tf-idf matrix and chi-square feature selection is applied to select the best features. I came across minimum redundancy maximum relevance for classification. Is the algorithm to be applied on tf-idf scores?  If not, what are the ways in which mrmr can be applied for text classification?"
2130,Text classification algorithm not working with text str as search criteria,"['python-3.x', 'string', 'nlp', 'knn', 'text-classification']","I am trying to make my classification accept a text and not just a number. Working with data, carrying a load of pulled articles, I want the classification algo to show which ones to proceed with and which ones to drop. Applying a number, things are working just fine, yet this is not very intuitive, although I know that the number represents a relationship to one of the two classes I am working with.How do I change the logic in the algo to make it accept a text as search criteria and not just an anonymous number, picked from the 'Unique_id' column? Columns are, btw...'Title', 'Abstract', 'Relevant', 'Label', 'Unique_id'. The reason for concatenating df's at algo end is that I want to compare results. Finally. it should be noted that the col 'Label' consists of a list of keywords, so basically I want the algo to read from that col.I did try, reading from data sources, changing the 'index_col='Unique_id' to 'index_col='Label', but that did not work out either.An example of what I want:This is the full code (view end of algo to see above example as it runs today, using a number to identify nearest neighbor):"
2131,IMDB data set - PyTorch RNN - Does not learn,"['python', 'machine-learning', 'pytorch', 'recurrent-neural-network', 'text-classification']","I am new to pytorch and trying to learn from online examples. I tried to create RNN for classifying imdb reviews. My code is in gist here: 
https://gist.github.com/Chandrak1907/24810970561db246458056d265d504beI see that RNN accuracy is around 50% and parameters are not changing much during training.
Can you pls help me in identifying bug in my code?Thank you,"
2132,How to use Hugging Face Transformers library in Tensorflow for text classification on custom data?,"['python', 'tensorflow', 'text-classification', 'huggingface-transformers']","I am trying to do binary text classification on custom data (which is in csv format) using different transformer architectures that Hugging Face 'Transformers' library offers. I am using this Tensorflow blog post as reference. I am loading the custom dataset into 'tf.data.Dataset' format using the following code:After this when I tried using the 'glue_convert_examples_to_features' method to tokenize as below:which throws an error ""UnboundLocalError: local variable 'processor' referenced before assignment"" at: In all the examples, I see that they are using the tasks like 'mrpc' etc which are pre-defined and have a glue_processor to handle. Error raises at the 'line 85' in source code.Can anyone help with solving this issue using with 'custom data' ?"
2133,iterate over categorical index pandas sklearn,"['python', 'scikit-learn', 'text-classification', 'multiclass-classification']","I want to check which one  is wrong in classification using confusion matrix
I want to iterate over pandas categorical index but I'm not sure is it array or not?this is the categoricalindex when i print 
print(df[""topic""].factorize()[1])and got the error when try to iterate"
2134,Pytorch dataloader for sentences,"['python', 'deep-learning', 'nlp', 'pytorch', 'text-classification']","I have collected a small dataset for binary text classification and my goal is to train a model with the method proposed by Convolutional Neural Networks for Sentence ClassificationI started my implementation by using the torch.util.data.Dataset. Essentially every sample in my dataset my_data looks like this (as example):Next I took a look at Writing custom dataloaders with pytorch:
using:I would suspect that enumerating over a batch would yield something the following:However it is more like this:I guess it has something to do that they are not equal size.
Do they need to be the same size and if so how can i achieve it? For people knwoing about this paper, what does your training data look like?edit:"
2135,see wrong classification in confusion matrix,"['python', 'scikit-learn', 'text-classification']","I want to see the wrong classification from confusion matri by iterate all data in dataframe, but it produce same data over and over again, is there something wrong with the code?this is the df data and indices test "
2136,Show k nearest neighbors for text classification,"['python', 'scikit-learn', 'classification', 'knn', 'text-classification']","I have a CSV file (corpus.csv) with graded abstracts (text) in the following format in corpus:I am trying to create a KNN classification program in python, which is able to get an user input abstract such as, ""This is a new unique abstract"" and then classify this user input abstract closest to the corpus (CSV) and also returns the score/grade of the predicted abstract. I have the following code:At the moment if I use the aforementioned code then the ""predicted"" gives an output, which is for example [3.2]. However, I also want the output to be [3.2, UoM, ""Hello, this is abstract two and yet counting.""]I want to show the k nearest neighbors (not just the score but also the corresponding institution name and abstract). How can I achieve that?"
2137,How to make a prediction as binary output? - Python (Tensorflow),"['python', 'tensorflow', 'prediction', 'text-classification']","I'm learning text classification using movie reviews as data with tensorflow, but I got stuck when I get an output prediction different (not rounded, not binary) to the label.CODEThe expected ouput should be:Prediction: [0.]
  Actual: 0What the output is giving:Prediction: [1.8203685e-19] 
  Actual: 0The output prediction should be 0 or 1, representing if the review was good or not.FULL CODE"
2138,Generalised method to clean data for text classification,"['python', 'regex', 'text-classification']","Whilst searching for a text classification method, I came across this Python code which was used in the pre-processing stepOPI then tested this section of code to understand the syntax and its purposeQuestion: why didn't the code replace 0, a, and m although 0-9a-z was specified inside the [ ]? Why did it replace ; although that character wasn't specified?Edit to avoid being marked as duplication:My perceptions of the code are:Additional questions:Are my perceptions reasonable?Should numbers/digits be removed from text?Could you please recommend an overall/general strategy/code for text pre-processing for (English) text classification?"
2139,Multilabel text classification/clustering in python with tf-idf,"['python', 'cluster-analysis', 'document', 'text-classification', 'tf-idf']","I have five plain text documents in a directory that are already clustered based on their content and named as such cluster1.txt, cluster2.txt and so on,  so they are functioning as my corpus. Otherwise they don't have any labels, they are just named as such. 
My task is to cluster a new text document with new sentences, but not the whole document as a whole, instead I should cluster each sentence into one of these 5 cluster or classes and also do a confusion matrix with the recall and precision score to show how similar the sentences are to the clusters.I first tried to do it with a kNN and then a kmeans, but I think my logic is flawed since this is not a clustering problem, it's a classification problem, right?
Well at least I tried to preprocess the text (removing stop words, lemmatize, lowercasing, tokenizing) and then I calculated the termfrequency with a countvectorizer and then the tf-idf
I kinda have problems with the logic with this problem.Anyway, this is what I tried so far, but now I'm kinda stuck, any help is appreciated"
2140,Text classification into predefined categories,"['python', 'svm', 'text-classification']","I am trying to classify text data into a few categories. But in the data set, there can be data that does not belong to any of the defined categories. And after deploying the final product, the product should be deal with text data that does not belong to the predefined category.To implement that solution I am currently using the SVM text classifier. And I am planning to define another category as ""non""to deal with the data that does not belong to predefined categories.Is this a correct approach?"
2141,How to implement a sentence relationship classifier based on semantic roles?,"['neural-network', 'architecture', 'nlp', 'artificial-intelligence', 'text-classification']","I am trying to build an model that automatically classifies the relationship between two product requirement sentences as a) positive correlation, b) negative correlation, or c) no correlation. According to my definition, two requirements interact whenever the satisfaction of one requirement influences the satisfaction of the other one.For example, the two requirements ""Staff should be able to view members entering the gym and log all visitors."" and ""The system should allow the user to monitor and evaluate who is entering the building."" show a positive correlation, as the fulfilment of the first requirement (at least partially) satisfies the second requirement.So far, my idea was to... What I am struggling with right now is how to structure and model the classification network.One idea I had was to use a separate network for each semantic role and use the output of each network as input for a final ANN / Decision Tree / SVM. Another idea was to use CNNs in order to learn relationships among different roles and distance patterns.What do you think? Would such an architecture make sense or should I pass all values directly into a single deep ANN?  Thanks in advance!
Chris"
2142,How to fine grain neutral sentiment as positive or negative,"['sentiment-analysis', 'text-classification']","I'm working on multimodal sentiment analysis with visual and textual cues.My input dataset is containing neutral sentiment in ground truth but I require to do a binary classification to categorize  my input samples as either positive/negativeIs there any possibility to use this neutral class in aiding to remove non-opinion key terms thereby increasing the accuracy of binary categorization?Is it advised only to adopt a multi-class classification algorithm to categorize as positive, negative or neutral?P.S: My requirement is to do a binary classificationThanks in advance"
2143,InvalidArgumentError: 2 root error(s) found. Incompatible shapes in Tensorflow text-classification model,"['python', 'tensorflow', 'deep-learning', 'nlp', 'text-classification']","I am trying to get code working from the following repo, which is based off this paper. It had a lot of errors, but I mostly got it working. However, I keep getting the same problem and I really do not understand how to troubleshoot this/what is even going wrong. The error occurs the second time the validation if statement critera is met. The first time is always works, then breaks on the second. I'm including the output it prints before breaking if its helpful. See error below:Here is the code (which is slightly different from the repo in order to get it to run:Versions:
Python 3tensorflow == 1.15.0pandas == 0.25.3numpy == 1.17.5The data comes from here and looks like this:The dataset to train the model can be found here:
https://github.com/cmeaton/Hierarchical_BiLSTM-CRF_Encoder/tree/master/swda_parsedI'm having a hard time understanding what this error even means and how to approach understanding it. Any advice would be much appreciated. Thanks."
2144,Snorkel: Can i have different features in data set to for generating labelling function VS training a classifier?,"['python', 'machine-learning', 'text-classification', 'snorkel']","I have a set of features to build labelling functions (set A)
and another set of features to train a sklearn classifier (set B)The generative model will output a set of probabilisitic labels which i can use to train my classifier.Do i need to add in the features (set A) that i used for the labelling functions into my classifier features? (set B)
Or just use the labels generated to train my classifier?I was referencing the snorkel spam tutorial and i did not see them use the features in the labelling function set to train a new classifier.As seem in cell 47, featurization is done entirely using a CountVectorizer:And then straight to fitting a keras model:"
2145,How to extract vertical label and value from scanned documents?,"['python', 'machine-learning', 'deep-learning', 'data-mining', 'text-classification']",I am making an document parser which extracts data fields from the documents and store them in a structured way. Each field in my dataset is horizontal which is easy to extract.But the model fails on vertical fields for example I want to extract invoice number and date from such images which isn't possible from any type of pre-trained OCR -
2146,How to classify unseen text data?,"['python', 'machine-learning', 'keras', 'deep-learning', 'text-classification']",I am training an text classifier for addresses such that if given sentence is an address or not.As addresses can be of n types it's very difficult to make such classifier. Is there any pre-trained model or database for the same or any other non ML way.
2147,Tensorflow: What's the best practice to get a section of a manual from a question?,"['python', 'tensorflow', 'deep-learning', 'recurrent-neural-network', 'text-classification']","I would like to use Tensorflow to create a smart faq. I've seen how to manage a chatbot, but my need is to let the user searching for help and the result  must be the most probable chapter or section of a manual.For example the user can ask: ""What are the O.S. supported?""The reply must be a list of all the possible sections of the manual in which could be the correct answer.
My text record set for the training procedure is only the manual itself. I've followed the text classification example, but i don't think is what i need because in that case it would  only understand if a given text belongs to a category or another one.What's the best practice to accomplish this task (i use Python)?Thank you in advance"
2148,how to convert saved model from sklearn into tensorflow/lite,"['tensorflow', 'machine-learning', 'scikit-learn', 'text-classification', 'tensorflow-lite']",If I want to to implement a classifier using sklearn library. Is there a way to save the model as  or conver the file into saved tensorflow file in order to convert it to tensorflow lite later?
2149,Document classification includes of non-dictionarical words,"['machine-learning', 'deep-learning', 'nlp', 'text-classification', 'document-classification']","I have to classify text or excel/data columns belongs to which group, let us say name of person, name of organization, address or other. With the existing concepts in like like bag-of-words, tf-idf and applying in classification algorithms(naive Bayes, svm, other models) I couldn't achieve the accuracy. Because name of person, organization  may reflect on address, so that names may be classified as address. And in my case other fields like numbers, labels, sentence should classify as other category. Since numbers are present in address, many times even categorical names, numbers are considered as address.Even if I give huge number of training data set, if new names of person, organization, other fields may occur, in those cases the model may be failure.Is there any way to handle all these problems. please suggest me.  "
2150,KNN for predict class from new data,"['r', 'knn', 'text-classification', 'tf-idf']","How do you provide a class for new data that does not have a class?I use the KNN algorithm and here is the code for modeling. 
(Text Classification)I try this codeError in knn(model_data[train_set, ], newdata, classifier[train_set],  : 
  dims of 'test' and 'train' differI know the dims is different, test is 37 288 and newdata is 1 1. "
2151,scikit-learn: FeatureUnion to include hand crafted features,"['python', 'scikit-learn', 'nlp', 'text-classification', 'multilabel-classification']","I am performing multi-label classification on text data. 
I wish to use combined features of tfidf and custom linguistic features similar to the example here using FeatureUnion. I already have generated the custom linguistic features, which are in the form of a dictionary where keys represent the labels and (list of) values represent the features. Training data structure is as follows:How can the above dict be incorporated into FeatureUnion? My understanding is that a user-defined function should be called that returns boolean values corresponding to the presence or absence of string values (from custom_features_dict) in the training data. This gives the following list of dict for the given training data:How can the above list be used to implement fit and transform?The code is given below:"
2152,"RoBERTa classification RuntimeError: shape '[-1, 9]' is invalid for input of size 8","['python', 'pytorch', 'text-classification']","My data-structure for the label column is: [[0,1,0,0,0,1,0,0], [0,1,1,0,0,1,0,0], [0,0,0,0,0,0,0,1]....] for every row there is one label-list like [0,1,0,0,0,1,0,0] in the label-column And for the texts there is one text (newspaper article) per row.(got it from that source: https://github.com/ThilinaRajapakse/simpletransformers#minimal-start-for-multilabel-classification)the model can be trained if i train it with only 4 entries. But when i want to train it with the whole dataset it gives me that: RuntimeError: shape '[-1, 9]' is invalid for input of size 8:I have no idea where that size 8 comes from and what to do now since it works with very few entries.
Can anyone help?"
2153,how to covert dataset to X_train if it contain string on it,"['python', 'python-3.x', 'algorithm', 'machine-learning', 'text-classification']","i want to try to implement LDA or PDA in sklearn but the data has string on iti got :Cannot center sparse matrices: pass with_mean=False instead. See docstring for motivation and alternatives.orValueError: could not convert string to float: 'お気に入り の Ubuntu : 無償 OS & amp ; 無償 ソフト で 何 でも 揃う ! : 10 . 04 LTS 日本語 Remix 版'usually to implement other algorithm, i use pipelinei also tried to  fit and transform manually like 
    vect = CountVectorizer()X_train data for reference
"
2154,"In Text Classification with XGBoost, how do I get back the original label?","['python-3.x', 'tensorflow', 'keras', 'nlp', 'text-classification']","I am aware of the question How to get original value for binary encoding using category_encoder package
 but there is no answer so here we are.I have two processes that do text classification on user entered comments. Process one creates the model. Process two does predictions on new data. The source code is below.I think my problem is understanding where the original labels are stored. I would have assumed in the joblib file but I might be wrong. When I do NewComments.head(), I get back numbers instead of text. I'm assuming these numbers are the encoding. I tried to run inverse_encoding in the second process, but I got a message about labels it had not seen before.How do I get back my original labels?"
2155,"Receiving, “An error was thrown and was not caught: The validation data provided must contain …” when creating a Text Classifier Model with CreateML","['validation', 'text-classification', 'createml']","I am using Playground to create a Text Classifier Model using CreateML and keep getting the error:My code is relatively simple, using two columns from a data table. The textColumn is labeled ""text"" and the labelColumn is labeled ""class"":The only difference I can find between this and the code provided in the Apple Developer Documentation is that instead of their documentation is:and version 11.2.1 of Xcode gives me a failure if I try using the line from the Apple Developer Documentation.Thanks in advance for any help you can offer."
2156,Can sentiment classification problem be resolved using regression?,"['python', 'regression', 'classification', 'text-classification']","I have a data set of tweets, where each tweet has an average confidence score. 
For example
Tweet Average Confidence Standard Deviationtoo much thoughts inside his headdd we can t even imagine 0.3 0.163951 His ass need to stay up 0.8 0.161962First time I heard his name in camp, he seems amazing 0.19 0.181962Average Confidence is the average of the confidences predicted by several supervised models for a specific instance to belong to the positive class.Standard Deviation is the confidences' standard deviation from Average Confidence for a particular instance.If i consider it as a regression task, how to handle multi label dataEDIT
"
2157,Keras: Multiclass CNN Text Classifier predicts the same class for all input data,"['keras', 'conv-neural-network', 'word2vec', 'text-classification']","I am trying to predict one out of 8 classes for some short text data with word embeddings and CNN. The occuring problem is that the CNN Classifier predicts everything in one class (the biggest one of the training data, distribution ~40%). The text data should be preprocesed quite well, because the classification works fine with other classifiers like SVM or NB. '''
    #first, build index mapping words in the embeddings set to their embedding vector
    import os
    embeddings_index = {}
    f = open(os.path.join('', 'embedding_word2vec.txt'), encoding='utf-8')
    for line in f:
        values = line.split()
        words = values[0]
        coefs = np.asarray(values[1:])
        embeddings_index[word] = coefs
    f.close()'''I am grateful for every hint.
Thank you."
2158,Is there any model/classifier that works best for NLP based projects like this?,"['machine-learning', 'deep-learning', 'nlp', 'text-classification']","I've written a program to analyze a given piece of text from a website and make conclusory classifications as to its validity. The code basically vectorizes the description (taken from the HTML of a given webpage in real-time) and takes in a few inputs from that as features to make its decisions. There are some more features like the domain of the website and some keywords I've explicitly counted.The highest accuracy I've been able to achieve is with a RandomForestClassifier, (>90%). I'm not sure what I can do to make this accuracy better except incorporating a more sophisticated model. I tried using an MLP but for no set of hyperparameters does it seem to exceed the previous accuracy. I have around 2000 data points available for training.Is there any classifier that works best for such projects? Does anyone have any suggestions as to how I can bring about improvements? (If anything needs to be elaborated, I'll do so.)Any suggestions on how I can improve on this project in general? Should I include the text on a webpage as well? How should I do so? I tried going through a few sites, but the next doesn't seem to be contained in any specific element whereas the description is easy to obtain from the HTML. Any help?What else can I take as features? If anyone could suggest any creative ideas, I'd really appreciate it."
2159,How do emoticons and hashags affect the accuracy of Google Natural Language Classify Text?,"['machine-learning', 'nlp', 'text-classification', 'google-natural-language']","I have a project where I have to find categories of text using google natural language classify text. https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/classifyTextMy text documents will be having emoticons and hashtags. 
Can anyone tell me how these will affect the score? Should I get rid of them before calling the API or let them be?I have tried multiple documents by myself and am getting conflicting results. Can anyone guide me with this?"
2160,Is there any ML classifier that generally works best for NLP projects?,"['machine-learning', 'deep-learning', 'neural-network', 'nlp', 'text-classification']","I've written a program that reads word vectors from a particular website and makes conclusary classifications.I'm getting the highest accuracy and F Score for a RandomForestClassifier. I'm not sure what I can do to make this accuracy higher except changing the model. I tried to use MLPs but landed with a lower accuracy. Should I use some other neural network?Does anyone know what models generally work for such NLP based programs? In a nutshell, what the program does is look through the HTML of a given webpage for certain features, vectorize the words it can find (using predefined vector spaces) and make classifications based on that. I'm getting an accuracy over 90% for the RandomForestClassifier. Any help?"
2161,How to define log-count ratio for multiclass text dataset (fastai)?,"['nlp', 'text-classification', 'naivebayes', 'fast-ai']","I am trying to follow Rachel Thomas path of sentiment classification with Naive Bayes. In the video she uses a binary dataset (pos. and neg. movie reviews). When it comes to apply Naive Bayes, this is what she does:Defintion: log-count ratio r for each word f:where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents.--> it is very simple to apply the log-count-ratio to a dataset with 2 labels!Problem: 
My dataset is not binary! Lets assume I have 5 labels: label_1,...,label_5How do I get the log-count ratio r for multilabel dataset?My approach:Is this correct? Does it mean I get multiple ratios?"
2162,Machine Learning: Classifier Accuracy is not Good when predicting it on another dataset [closed],"['python', 'machine-learning', 'reshape', 'shapes', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 7 months ago.same question here but with a different context, that's why unable to implement because he already has two datasets of same shape and size. But in my case, I am shaping the size of Train Features so that it becomes equal in size to TEST Features. All code is working good, but the same issue performance of Linear SVM, Kernel SVM, Multinomial NaiveBayes is less than ""60%"". for example, classification report in case of MultinomialNB isI think my new sparse matrix after shaping of processed_TRAIN_features is not good... is it true or not? in the below code I am enhancing columns from 9434 to 10782if this is true or not? if anything wrong with it please tell me. rest of the code is herethanks in advance"
2163,How do I get the sequence of vocabulary from a sparse matrix,"['python', 'scikit-learn', 'text-classification']","I have a list of vocabularies ['Human', 'interface', 'machine', 'binary', 'minors', 'ESP', 'system', 'Graph'] and a list of sentences [""Human machine interface for lab abc computer applications"", ""A survey of user opinion of computer system response time"", ""The EPS user interface management system"", ""Relation of user perceived response time to error measurement"", ""The generation of random binary unordered trees"", ""The intersection graph of paths in trees"", ""Graph minors IV Widths of trees and well quasi ordering"", ""Graph minors A survey""].
I use 'CountVectorizer' from 'sklearn' to fit the sentences into a sparse matrix based on the eight words. And I get a output below.Now I'm trying to find out the sequence of that eight words in the matrix. Any help will be appreciated."
2164,Simple Two class (binary) classification using deep learning in matlab for text detection,"['deep-learning', 'text-classification']","im completely new to this area and specially in matlab. so I need to either train or use a pre trained model for text detection. I dont need it to recognize the text class I want it to just tell me if a region that i give it to the cllasifier is text ot not. and the text is in english and it could be in any form or fonts.so we can say the images would be cropped or completely without background and i need to determin either its text or not.my question is that is there such a pretrained model available somewhere?
or i have to train it myself?
if i should train it, how much data i need and how much time it takes on a laptop with these specs:cpu : core i 7,gpu: gtx-960m,ram: 16gbthanks in advance. im really short in time to do this so if there is any pre trained option available i will take that. thanks."
2165,Documents likelihood with Machine Learning,"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'textmatching']","What I'm trying to achieve?Sample data I'veIncoming Data and my expectationsExpectation: tier1: 0 %, tier2: 0 %Expectation: tier1: X %, tier2: Y%Some more questions:
1. What is the best approach to solving this?
2. The data which I've shown here is just 2 features, but real input data has so many more fields with a different type. What is the best way to extract the features!"
2166,How to find outliers in document classification with million documents?,"['python', 'machine-learning', 'text-classification', 'outliers', 'cosine-similarity']","I have million documents which belongs to different classes (100 classes). I want to find outlier documents in each class (which doesn't belong to that class but wrongly classified) and filter them. I can do document similarity using cosine similarity by comparing the tokens of each document. 
I am not able to apply this to filter the wrongly classified documents for a given class. 
Example: Consider the 3 classes for simplicity with the documents under them.How can I figure out effectively and efficiently that doc4(and other similar docs) is wrongly classified in ClassA, so that my training data does not contain outliers?"
2167,Is word embedding + other features possible for classification problem?,"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'sklearn-pandas']","My task was to create a classifier model for a review dataset. I have 15000 train observations, 5000 dev and 5000 test. The task specified that 3 features needed to be used: I used TFIDF (5000 features there), BOW (2000 more features) and the review length (1 more feature). So, for example, my X_train is an array shaped (15000,7001).I was investigating and I found that word embedding (word2vec especially) could be a nice alternative to BOW. 
My question is, can it be used (can it be put in the same ""array"" format as my other features?) in addition to my other features? I did some research on it but didn't quite answered my question. "
2168,Multi-label text classification with non-uniform distribution of class labels for every train data,"['python', 'classification', 'sentiment-analysis', 'text-classification', 'multilabel-classification']","I have a multi-label classification problem, I want to classify texts with six labels, each text can have one to six labels but this label distribution is not equal. For example, 10 people annotated sentence1 as below:These labels are the number of votes for that class. I can  normalize them like sad 0.7, anger 0.2, fear 0.1, happy 0.0,...
What is the best classifier for this problem?  What is the best type for labels I mean I should normalize them or not?
What keywords should I search for this kind of multi-label classification problem where the probability of labels is not equal?"
2169,Is it possible to use Generative Adversarial Networks (GANs) for text classification?,"['text-classification', 'gan']","I am working on classification of fake and real news. I did use a CNN Model for this problem and got satisfactory results. But, I was just wondering if it's at all possible to use any type of GAN for this sort of a problem.  "
2170,COCOA Multilabel imbalance classficiation - binary-class and multi-class imbalance learners,"['classification', 'text-classification', 'multilabel-classification', 'multiclass-classification', 'imbalanced-data']","I want to ask about the term in this paper: Zhang, M., Li, Y., & Liu, X. (2015). Towards Class-Imbalance Aware Multi-Label Learning. IJCAI.In that paper there is terms : binary-class and multi-class imbalance learners. I have search many paper but there have not found the characteristic of the term. is there any reference or paper that explain the terms ? "
2171,How to calculate similarities between test and train documents,"['vector', 'text-classification', 'cosine-similarity']",I'm trying to calculate similarities between test and train documents and label them. Here is the code but it doesn't work. I'd also appreciate if somebody could explain the idea.Here's the error:Edit: I've defined two more functions and have been working on a different solution. Here are they:I believe something like this would work but there are still error messages... What should I change?
2172,Getting keywords from messages,"['python', 'nlp', 'nltk', 'gensim', 'text-classification']","My aim here is text summarization, not sure if I'm doing it correctly but here's the plan. I've got a dataframe called train_data. Each cell in every row contains messages. Now, I am looking to iterate over each cell or each message in the dataframe column to get the keywords from each message, using the gensim.summarization.keyword package. I understand that the keyword function takes text as an input and I can't pass the whole df column inside so tried to iterate each cell over the keyword function as text but it doesn't seem to work. What am I missing here? Here's my code.I then plan to count the length of original vs new message(ie keyword column) to get the compression rate/ratio. "
2173,I need to do file classification with ML from different files format (sharepath),"['text', 'classification', 'text-classification', 'document-classification']","I have already read the text from different files.and also get the TF-IDF keyword for each  text dataFinal_keyword column have TF-IDF keywords. Find below the sample screenshots of dataset.Now, how to predict the file name, when we are proving the some keywords or sentencese.ginput:  azure, cloud computing,..output: file name:- Azure.txtEven i tried with these two algorithm  SVM, Naive Bayes. but model are failed to predict.If anyone know how to do it. please give some solution."
2174,How to create actual dataframes out of k-fold-stratified in Python,"['python-3.x', 'pandas', 'machine-learning', 'scikit-learn', 'text-classification']","Out of the indices which i get back from sklearn Stratifiednfold, how to create from every fold a corresponding dataframe?prints out the lists with indices. How to map these back to my original Dataframe?I need them because i want to add my augmented data to the trainingdata before i run my texclassification model on it. "
2175,NLP problem : How to identify a question which needs a specific answer,"['neural-network', 'text-classification']","I am building a chatbot system. I have an existing system which always answers abstract answers.Ex.What is the theme of the article?Answer. line 1... line 2.. line 3.But there are some questions which needs specific answerslikewhat is the name of the writer?Answer: Mr XI would like to make a concreteness identification module. Based on this initial classification, it will go to any of the module ( abstract or concrete).I would like to get suggestion from you how to achieve this goal. Any suggestion of probable code with a dataset will be appreciated."
2176,Understanding why tf-idf gives higher classification accuracy by KNN than BoW and GloVe,"['python', 'feature-extraction', 'text-classification']","I am trying to classify 107 test questions into predefined concepts (c1, c2, c3, c4) by KNN classifier in Python. My data is in csv file so it's all in one document. I have split the data to 80% training and 20% testing.  My code works as follows:The accuracy results as follows:Why the accuracy is very high with TF-IDF comparing to BoW and GloVe?"
2177,problems classifying EndNote data using kernlab::ksvm,"['r', 'classification', 'text-classification', 'kernlab']","I am trying to classify text exported from Endnote using kernlab::ksvm. The training data is already marked as positive or negative. And the testing data has the same response column ""corpus"" which now is a dummy marked negative. the original data is; test n=13924 and trainn n=8057, but I used dput() with 10 rows in the example, I hope that is sufficient to figure out the problem. Error in .local(object, ...) : test vector does not match model !I have tried working with factor levels and different structures but been unsuccessful, google has also betrayed me, so you are my last hope.. "
2178,MultiClass Text Classification using TensorFlow. output issue,"['python', 'tensorflow', 'keras', 'text-classification', 'tf.keras']","I'm checking the BBC news code from towardsDataScience multi-class problem, there are 5 labels and 6 prediction outputs, I tried giving it entertainment news and it returns I just added testlabel after entertainment label and output was!model is hereif i change the last dense to 5, it still give following error in training. Can somebody explain this to me? why is that? , and how to get same-outputs as number of labels? "
2179,Find how similar a text is - One Class Classifier (NLP),"['python', 'twitter', 'nlp', 'classification', 'text-classification']","I have a big dataset containing almost 0.5 billions of tweets. I'm doing some research about how firms are engaged in activism and so far, I have labelled tweets which can be clustered in an activism category according to the presence of certain hashtags within the tweets.Now, let's suppose firms are tweeting about an activism topic without inserting any hashtag in the tweet. My code won't categorized it and my idea was to run a SVM classifier with only one class.This lead to the following question:Thanks in advance for your help!"
2180,Text Classification Problem : Name and approach of this type of classification,"['machine-learning', 'deep-learning', 'nlp', 'text-classification', 'multiclass-classification']","I have a labelled data-set comprising of text segments and corresponding labels. Each label consists of three parts, and there can be multiple or zero labels assigned to a given text segment. The task is to predict the label for any given text segment, where each label consists of three parts (action, performed, person), and there may be zero or more labels for a text segment. There are fifteen classifiers for action, two for performed, and two for person. Annotated data size is 6000 text segments, in which 4000 text segments are assigned at least one label. What is this type of text classification called (other than multi-class labelling)? Also, which classification approach is recommended for this type of classification problem? "
2181,K-fold cross validation on paragraphs from documents without leakage of document id's in train- and test set (Python),"['python-3.x', 'machine-learning', 'cross-validation', 'text-classification']","I want to create a classifier for multi-label classification with ""paragraph text"" as input and 6 output-labels. Each paragraph can have 1 or more labels assigned to it. Also the document-id and paragraph-id from that specific document are available in a column in the csv.I don't have a lot of data available (yet) so I figured out that using train_test_split() isn't a good way to go  & I already did some research on how to do Kfold cross validation etc but I'm having some issues in understanding how I can use a column from my csv-file to determine whether or not to add a paragraph to the train- or to the test set:(See picture below)
For example: If ""paragraph id"" number 1 from ""document id"" number 4 is used in the train-set, then the other 3 paragraphs from document number 4 shouldn't be used in the test-set. In other words, I need to create a train- and test-set on document-level. But it will be used to predict if a certain paragraph has certain label(s). Does this make sense? I hope my question is clear.
I would really appreciate it to get some tips & ideas or recommendations on this or on which specific topics I should do more research.
This link helped me to understand what I'm actualy up to but what is explained there isn't written in Python so, I'm still confused.EDIT:
I think I get it, I think I need to manualy split my data in 2 CSV's. 1 training-csv and another test-csv. And then, use the training-csv for cross validation (resulting in a training and validation set)..."
2182,"Error in `[.simple_triplet_matrix`(opinions.tdm, 1:10, ) : subscript out of bounds R","['r', 'text-mining', 'data-cleaning', 'text-classification', 'inspect']","I am following the exercise on this link: 
https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/In the section of Inspect:inspect(opinions.tdm[1:10,])I got this message, and I cannot follow. Error in [.simple_triplet_matrix(articles.tdm, 1:10, ) : 
  subscript out of boundsSomebody can help me please!
Thanks "
2183,Text classification with torchnlp,"['python', 'neural-network', 'nlp', 'pytorch', 'text-classification']","i'm trying to build a neural network using pytorch-nlp (https://pytorchnlp.readthedocs.io/en/latest/).
My intent is to build a network like this:I'm encountering a major problem with the dimensions of the input sentences (each word is a vector) but most importantly with the attention layer : I don't know how to declare it because i need the exact dimensions of the output from the encoder, but the sequences have varying dimensions (corresponding to the fact that sentences have different number of words).I've tried to look at torch.nn.utils.rnn.pad_packed_sequence and torch.nn.utils.rnn.pack_padded_sequence since they're supported by LSTM, but i cannot find the solution.Can anyone help me?I thought about padding all sequences to a specific dimension, but I don't want to truncate longer sequences because I want to keep all the information."
2184,Unable to train my keras model : (Data cardinality is ambiguous:),"['machine-learning', 'nlp', 'text-classification', 'tensorflow2.0', 'tf.keras']","I am using the bert-for-tf2 library to do a Multi-Class Classification problem. I created the model but training throws the following error:I am referring the medium article called Simple BERT using TensorFlow 2.0 
The git repo for the library bert-for-tf2 can be found here.Please find the entire code here.Here is a link to my colab notebookReally appreciate your help!"
2185,How to create a text classifier using Naive Bayes with character level Bigram,"['python', 'python-3.x', 'text-classification', 'naivebayes', 'n-gram']","So I have a txt file which contains the tweets and the category that is split by semicolon (;) something just like this:This is how I read the txt file:and this is how the tweets are classified by words:The output will be the category which in this case is ""international"". So now I need to classify the tweets using character level Bigram that looks something like this:output:How do I implement that Bigram code into my Naive Bayes classifier code? Thanks in advance."
2186,How can I get back data from integers. My model.predict() is not working,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'text-classification']","I have a csv. contains 'gender','diagnosis','test','physical_exam','medicine' these columns. I want to predict 'medicine' column on the base of 'gender','diagnosis','test','physical_exam' these columns.
I have done this:but this code giving me error How can I predict 'medicine' column?
this code's output should be like that ""Olmesartan Clopidogrel Rosuvastatin5 Ivabradine "". These are the name of medicines"
2187,no applicable method for 'predict' applied to an object of class “factor”,"['machine-learning', 'shiny-server', 'text-classification']",Hi I am running this in shiny web and calling my train model. my new data is a class of data frame its not type factor. but it still hitting the error of object class as a factor when fitting the model for prediction. This is text classification problem with a multiclass classification which works perfectly fine for train and test data.
2188,TypeError: Cannot cast array data from dtype('float64') to dtype('<U32') for KNN Text Classification in Python,"['python', 'scikit-learn', 'text-classification', 'knn']","I have the following code:I am trying to use KNN for text classification. However, I am getting the error in the last line of code as follows:TypeError: Cannot cast array data from dtype('float64') to dtype('U32') according to the rule 'safe'The complete traceback of the error is as follows:Anyone knows how to resolve this error or issue?Thank you in advance.P.S. Although there is a solution for similar error, however, the solution is not fit for this test case. Existing answer: Numpy.dot TypeError: Cannot cast array data from dtype('float64') to dtype('S32') according to the rule 'safe' "
2189,How can I get the probability estimate of a text classification prediction with LibShortText using the logistic regression classifier?,"['python', 'nlp', 'probability', 'text-classification', 'libshorttext']","LibShortText is an open source library for short-text classification  that relies on liblinear.I read on https://github.com/cjlin1/liblinear: -b probability_estimates: whether to output probability estimates, 0 or 1 (default 0); currently for logistic regression onlyHow can I get the probability estimate of a prediction with LibShortText using the logistic regression classifier?"
2190,Doc2Vec infer_vector not working as expected,"['python', 'text-classification', 'doc2vec']","The program should be returning the second text in the list for most similar, as it is same word to word. But its not the case here.Output:Its showing the first text in the list as most similar instead of the second text. Can you please help with this ?"
2191,How can I know to which class each score corresponds to in LibShortText prediction output file?,"['python', 'nlp', 'text-classification', 'libshorttext']","I use LibShortText for short-text classification.I trained a model and use it to get class predictions on my test set by running:The output file contains the score of each class for each test sample. She is the beginning of the output file:How can I know to which class each score corresponds to? I know I could infer it by looking at the predicted class and the maximum score for several test samples, but I'm hoping there exist some mmore direct way."
2192,New techniques of text classification nlp,"['nlp', 'text-classification']","I have to make a text classification program with new techniques (Not using ""bag of words"" and ""TF-IDF"").
I read about EDA but I was confused.
Any ideas?"
2193,Identifying Grammatically Correct Nonsense Sentences,"['python', 'python-3.x', 'nlp', 'text-classification']","I have two files file1.csv and file2.csv. file1.csv contains a stupid sentence in each row. file2.csv identify which column it is (type0 corresponding to 0, type1 corresponding to 1). I want to do a NLP classification task and I know usually how to do it. But in this situation I am bit confused and do not know how to arrange and organize my dataset, so that I can train my sentences and labels. Appreciate if someone give me a hint to progress. file1.csv in the following format,file2.csv in the following format.My purpose is to classify the stupid sentences. "
2194,I want to implement a machine learning or deep learning model for text classification (100 classes),"['python', 'machine-learning', 'text-classification', 'multilabel-classification']","I have a dataset that is similar to the one where we have movie plots and their genres. The number of classes is around 100. What algorithm should I choose for this 100 class classification? The classification is multi-label because 1 movie can have multiple genres
Please recommend anyone from the following. You are free to suggest any other model if you want to.It would be useful if you also give the necessary library in python"
2195,How to replace misspelled words with words from dictionary while ignoring text reference codes?,"['python', 'nltk', 'lstm', 'text-classification', 'lda']","Topic modelling case here. So I've loaded my first round of preprocessed text data into a document term matrix, however looking at the dtm, I realized that there were words like 'aacc', 'aacct', 'aaccount' and a few different variations like that which basically just means accounts. Is there a way to replace those few words variations that meant account to the word account? I've tried the following code :but it doesn't actually give an output of the word to be 'account'. It's a little tricky as I'm looking to replace actual misspelled words from dictionary and ignore the other words that appears or seems to be misspelled but are just reference codes? Also trying to remove duplicate characters in a string of sentence e.g 'tthe aabove aand areply tthankss'.Hope I'm clear enough, thank you in advance. "
2196,How do you include all words from the corpus in a Gensim TF-IDF?,"['python', 'nlp', 'gensim', 'text-classification', 'tf-idf']","If I have some documents like this:And I compute a TF-IDF matrix for this in Gensim like this:Then for each document, I get a TF-IDF like this:But I want the TF-IDF vector for each document to include words with 0 TF-IDF values (i.e. include every word mentioned in the corpus):How can I do this in Gensim? Or maybe there is some other library that can compute a TF-IDF matrix in this fashion (although like Gensim, it needs to be able to handle very large data sets, e.g. I achieved this result in Sci-kit on a small data set, but Sci-kit has memory problems on a large data set)."
2197,Is there any method to find a threshold to decide whether classification model result is reliable or not,"['text-classification', 'confidence-interval']","I am working on document classification task, and I have constructed an neural network to classify. But I encountered such question: I already had each confidence corresponding to the class, in normal, I will use the highest confidence as the final predicted class. But I am not sure the class is right result, I am going to set a threshold to decide whether the prediction is reliable or not.So, is my thought on the right way? if not how what else should I improve, if yes is there any method to implement it."
2198,Does ktrain package combine input embedding with bert embedding when used for test classification?,"['keras', 'text-classification', 'embedding', 'bert-language-model']",I am running the code given in the link below. What embeddings does the ktrain package of python use for bert text classification. I believe the code is using a pre-trained model of Bert. In that is it combining bert embeddings with input embeddings. Does the embeddings in the model take into account the input text given as well Relevant part of Codehttps://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroups-BERT.ipynb
2199,"Keras Prediction result (getting score,use of argmax)","['keras', 'deep-learning', 'nlp', 'text-classification', 'elmo']","I am trying to use the elmo model for text classification for my own dataset. The training is completed and the number of classes is 4(used keras model and elmo embedding).In the prediction, I got a numpy array. I am attaching the sample code and the result below...when I print the predicts variable I got this.Anyone have any idea about what is the use of taking the 0th index value only. Considering this as a list of lists 0th index means first list and the argmax returns index the maximum value from the list. Then what is the use of other values in the lists?. Why isn't it considered?. Also is it possible to get the score from this? I hope the question is clear. Is it the correct way or is it wrong?I have found the issue. just posting it others who met the same problem. Answer: When predicting with Elmo model, it expects a list of strings. In code, the prediction data were split and the model predicted for each word. That's why I got this huge array. I have used a temporary fix. The data is appended to a list then an empty string is also appended with the list. The model will predict the both list values but I took only the first predicted data. This is not the correct way but I have done this as a quick fix and hoping to find a fix in the future"
2200,BrokenProcessPool on language model pre-training,"['nlp', 'classification', 'text-classification', 'language-model', 'fast-ai']","I have been training a Language Model from Wikipedia in order to create a text classifier in FastAi.
I have been using Google colab for it.
But after a few minutes of training, the process stops with the following error:I was trying to solve this by varying the value of bs to 64,32,16. Also changing the value of num_workers but still failing.The process is as follows, the ram memory begins to fill and approximately halfway through the process is full and stops the execution of the script.Details of the Google Colab Machine:GPU Machine, RAM: 25.51 GB, Disk: 358.27 GB.Best Regards!"
2201,My text classifier model doens't improve with multiple classes,"['python', 'pandas', 'tensorflow', 'machine-learning', 'text-classification']","I'm trying to train a model for a text classification and the model take a list of maximum 300 integer embedded from articles. The model trains without problem and all but the accuracy won't go up. The target consists of 41 categories encoded into int from 0 to 41 and were then normalized.The table would look like thisAlso, I don't know how my model should look like since I refered on two different example as per belowI have tried modifying my model based on both model but the model accuracy won't change and even getting lower per epochShould I add more layers to my model or I have done something stupid that I haven't realized?Note: If the 'df.pickle' download link broken, use this link"
2202,"I am working on multiclass text classification, How to pass one hot encoded in to keras model for training in ytrainset?","['tensorflow', 'machine-learning', 'keras', 'text-classification']","I am working on text classification problem. i have 9 labels in my ytrain.but when i pass xtrain and ytrain to model , it give me error : that expected to have shape(1,) but got (9,). and my size of ytrain is (32,9). Picture of Ytrain is attachted :: Below is my model : Traceback:
Error when checking target: expected dense_9 to have shape (1,) but got array with shape (9,)'''"
2203,How to build keras classification model using two text features as input,"['keras', 'text-classification']","I am trying to build a text classification model which uses two input text features to finally predict 10 classes but I need to give significant impact for each input branch on the final output i.e. each branch should participate with 50% of the final decisioncurrently the setup am trying to achieve is as belowmy question is, would this be the best practice or I should look into something else ?"
2204,"How to verify that validation labels are in the same range as training labels, Python Numpy","['python', 'numpy', 'error-handling', 'preprocessor', 'text-classification']","As a part of a project I need to train a multilabel text-classifier in Python. I'm following some kind of a guideline for this but because of my low experience in Python I'm having some issues to understand a part of the code that verifies that validation labels are in the same range as training labels. + The reason why this is throwing up an error. The code that I'm trying to understand is thisone: 
(More specific the 2 first lines from this code are the ones that are confusing to me) Also the error that it gives me: In case if this matters for you to know this, the code written before this codeblock is: Which gives me this output: 
[67 68 69 70]Any help here to give me a better understanding would be greatly appreciated. "
2205,How can I test my natural language processing model with “real” cases?,"['python', 'machine-learning', 'keras', 'nlp', 'text-classification']","I am introducing myself to Natural Languaje Processing and artificial neural networks and I have followed this wonderful tutorial
Once finished it, I would like to know if there is any way to test the model with phrases that I can invent, (That film entertained me a lot) for example.
Because it is very good to know the percentage of success on the test set, but I want to know how to test it."
2206,Is it possible to train the sentiment classification model with the labeled data and then use it to predict sentiment on data that is not labeled?,"['nltk', 'python-3.7', 'sentiment-analysis', 'text-classification', 'training-data']","I want to do sentiment analysis using machine learning (text classification) approach. For example nltk Naive Bayes Classifier.
But the issue is that a small amount of my data is labeled. (For example, 100 articles are labeled positive or negative) and 500 articles are not labeled.
I was thinking that I train the classifier with labeled data and then try to predict sentiments of unlabeled data. 
Is it possible? 
I am a beginner in machine learning and don't know much about it. I am using python 3.7. Thank you in advance. "
2207,How to do Text classification using tensorflow?,"['python', 'tensorflow', 'machine-learning', 'scikit-learn', 'text-classification']","I am new to tensorflow and machine learning. I am facing issues with writing a tensorflow code which does the text classification similar to one I tried using sklearn libraries. I am facing major issues with vectorising the dataset and providing the input to tensorflow layers. I do remember being successful in one hot encoding the labels but the tensorflow layer ahead did not accept the created array.
Please note, I have read majority of text clasification answered questions on stackoverflow but they are too specific or have complex needs to resolve.
My problem case is too narrow and requires very basic solution.It would be great help if anyone could tell me the steps or tensorflow code similar to my sklearn machine learning algorithm. Dataset used is avaialable at : https://www.kaggle.com/virajgala/classifying-text"
2208,How to put an array of strings with two variables into a text file?,"['python', 'arrays', 'python-3.x', 'text-classification', 'naivebayes']",I'm trying to make a text classifier app. I have an array of strings which contains two parameters separated by comma like below:And with that array of strings I can execute this code:with output:What I'm trying to do is to put that array of strings into a text file and still able to execute the code with the same output as above.
2209,Pandas Get rows if value is in column dataframe,"['python', 'pandas', 'numpy', 'data-science', 'text-classification']","I have Information Gain dataframe and tf dataframe. the data looks like this :Information GainTerm Frequencylet's say table Information Gain = IG
and table tf = TFI wanted to check if IG.Term is in TF.index then get the row values so it should be like this :NB : I don't need the IG value anymore"
2210,Countif() word in python,"['python', 'pandas', 'numpy', 'classification', 'text-classification']","I have Information Gain dataframe and tf dataframe. the data looks like this :Information GainTerm Frequencylet's say table Information Gain = IG
and table tf = TFI wanted to count the 'term' from IG if the 'term' is contain in TF 'term' and it's class (A / B) value is 1. It's similar to COUNTIF(range_term, term) in excel but I don't know how to do that with pandas or numpy or anything else."
2211,NLP using replacement tokens,"['python', 'nlp', 'text-classification']","I read a lot of articles that deal with different NLP classification tasks and I saw that most of them specify in the pre-processing section that they use replacement tokens:  e.g. We removed and replaced the URLs, emojis and punctuation with replacement tokens: <URL>, <EMOJI>, <PUNCT>.I am quite new to this domain and I was wondering if there is some special way to deal with this kind of tokens/tags? Is it necessary to use < > or is this just a way to signal this replacement and for helping the classifier in finding a pattern? Any help would be greatly appreciated."
2212,Approach for classifying job description sentences,"['python', 'r', 'nlp', 'text-classification', 'topic-modeling']","I need to classify/categorize the various sentences in the job_experience section of n=630 job descriptions. I'm particularly interested in extracting the work experience and ability-related sentences, but I need to be able keep them attached to the job_title that they are associated with. Current state of these job descriptions: many different ways of saying similar things (e.g., ""Needs Microsoft Office skills."" ""Experience using Microsoft Word, PowerPoint."" ""Minimum 3 years experience in related field."" ""Minimum three years experience in similar role."").In the future, we will need to condense these job description statements so that, for example, the same statement can be applied to multiple jobs, and where managers select from a drop-down list of job experience statements.So I would like to categorize these individual sentences so that we can begin condensing them and deciding on which statements will be used going forward.I've been researching what I should do and I would appreciate any suggestions on which approach will be the most efficient. I am familiar with R but use it mostly for data wrangling and visualization. LDA, kmeans text clustering, feature identification... these are the things I'm finding in my research (scikit-learn.org) and mostly with application in Python. My data looks like: My goal is to have a dataset like this (new column = job_exp_category):Thank you for any insight SO community."
2213,How to prevent validation loss increasing and validation accuracy stuck in keras?,"['python-3.x', 'keras', 'deep-learning', 'text-classification']","I'm doing a task of news multi label classification. I use CNN in keras and I have tried l2, shuffle,BatchNormalization, earlystopping ,Dropoutin my code, but it still looks like overfitting that the validation accuracy is fluctating around 0.5 and validation loss is stuck. 
Here is my codeAnd there is the result:What should I do to solve this problem and increase validation accuracy?"
2214,scikit-learn add extra data to SGDClassifier,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","I'm trying to do text classification with scikit-learn. I have text that does not classify well. I think I can improve the predictions by adding data I can deduce in the form of an array of integers. For example, sample 1 would come with [3, 1, 5, 2] and sample 2 would come with [2, 1, 4, 2]. This would also be true of the test data.The idea is that the classifier could use both the text and the numbers to classify the data.I've read the documentation for scikit learn and I can't find how to do it. It must be possible because all that is classified, internally, is vectors of numbers. So adding another vector of numbers should not be that much of a problem, but I can't figure out how. partial_fit adds more samples, it does not add more information about the existing samples. Is there a way to do what I'm trying to do. I tried to combine GaussianNB with SGDClassifier, but it turns out I don't know how to do that. (Was it a bad idea?)What should I do?"
2215,"Python NLP - Sklearn - text classifier, unigrams and bigrams the same for both negative and positive labels","['python', 'scikit-learn', 'nlp', 'text-classification', 'sklearn-pandas']","I'm trying to create a text classifier to determine whether an abstract indicates an access to care research project.  I am importing from a dataset that has two fields: Abstract and Accessclass.  Abstract is a 500 word description about the project and Accessclass is 0 for not access-related and 1 for access-related. I'm still in the developing stages, however when I looked at the unigrams and bigrams for 0 and 1 labels, they were the same, despite very distinctly different tones of text.  Is there something I'm missing in my code? For example, am I accidentally double adding negative or positive? Any help is appreciate. "
2216,How can I extract sentences from data-frame in python and keep the paragraph key?,"['nlp', 'text-classification']",I have a data frame which included 1604 paragraph as follows: I want to extract all the sentences (even in a NAIVE way using dots) and provide a new data frame which has in each row one sentence and the previous column values especially the paragraph key (mainly index in the first column in the left)I have worked on that and could provide the chapter column for each sentence as follows:which gives me this data frame(dfAstroNova is the name of the original data frame)as you see I have the chapter key. My question is how to add paragraph key (which is the number of column text in main data frame to a new data frame) Then I have one other column which shows that this sentence belong to which paragraph in the original data frame or better one additional column which includes for each sentence the corresponded paragraph? 
2217,Bag of words after tokenization,"['python', 'text-mining', 'text-classification']","I have examined many text mining approaches and got stuck while creating bag of words. I understand the point that this is to convert words to numbers so machine could understand it, but the issue is that I have just finished tokenizing, removing stop words, lemmatizing. And ended with a list of words like: Basically I decided to tokenize by words. Some words are repeated, not distinct. Each bag of words example I read has at the beginning has array of sentences rather that array of words:Is my approach correct? Does it make sense to prepare bag of words if I have an array of single words? Or is my tokenization wrong? Or maybe the order is wrong? But in some examples (like here: https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34) we usually tokenize texts at the beginning.This is supposed to be part of text classification and preparation for using algorithms like Naive Bayes, SVM etc. but I'm stuck at this point... Maybe I got it wrong and bag of words should be created from all my occurrences not just one document?"
2218,Python NLTK and Pandas - text classifier - (newbie ) - importing my data in a format similar to provided example,"['python', 'pandas', 'nlp', 'nltk', 'text-classification']","I'm new to text classification, however I get most of the concepts.  In short, I have a list of restaurant reviews in an Excel dataset and I want to use them as my training data.  Where I'm struggling is with the example syntax for importing both the actual review and the classification (1 = pos, 0 = neg) as part of my training dataset.  I understand how to do this if I create my dataset manually in a tuple (i.e., what I have current have #'ed out under train).  Any help is appreciated.  "
2219,"Sklearn (NLP text classifier newbie) - issue with shape and vectorizer, X and Y not matching up","['python', 'scikit-learn', 'nlp', 'text-classification', 'sklearn-pandas']","I want to create a text classifer that looks at research abstracts and determines whether they are focused on access to care, based on a labeled dataset I have.  The data source is an Excel spreadsheet, with three fields (project_number, abstract, and accessclass) and 326 rows of abstracts.  The accessclass is 1 for access related and 0 for not access related (not sure if this is relevant). Anyway, I tried following along a tutorial by wanted to make it relevant by adding my own data and I'm having some issues with my X and Y arrays.  Any help is appreciated. "
2220,How to use Bert for long text classification?,"['nlp', 'text-classification', 'bert-language-model']","We know that bert has a max length limit of tokens = 512, So if an acticle has a length of much bigger than 512, such as 10000 tokens in text
How can bert be used?"
2221,Is there any way to classify text based on some given keywords using python?,"['python-3.x', 'machine-learning', 'text-classification']","i been trying to learn a bit of machine learning for a project that I'm working in. At the moment I managed to classify text using SVM with sklearn and spacy having some good results, but i want to not only classify the text with svm, I also want it to be classified based on a list of keywords that I have. For example: If the sentence has the word fast or seconds I would like it to be classified as performance.I'm really new to machine learning and I would really appreciate any advice."
2222,Cross validation on the whole dataset and vectorisation of data,"['scikit-learn', 'pipeline', 'cross-validation', 'text-classification']","I have a methodological doubt concerning cross validation (CV). I have found some academic papers that report results applying CV over a whole dataset. So, let's say I want to do that. My doubt comes when we have to work with text and apply first some transformations on the data, like vectorisation. In that case, should we apply vectorisation on the whole dataset, without splitting into train/test? The best solution I have found for that is to use a pipeline like the following one, which I use to vectorise a Part of Speech representation of texts:Then I use this pipeline to run CV over the whole dataset:I would like to make sure that this method is correct and sound.
Thanks!"
2223,Best practices in terms of,"['python', 'cross-validation', 'text-classification', 'one-hot-encoding', 'dimensionality-reduction']","These questions are more about theory and less about a specific issue with my syntax. I have a dataset that I cannot disclose at this time, nor any of my code. However, my dataset has 199 features and one target variable that I must convert from a multi-class (30 possible values) to binary (true or false). I am attempting to use some information about these records to predict whether a record should be designated as true or false. In terms of my feature set some columns exist as integer, float, and string values. However, regardless of data type, many of these are nominal-categorical features and several of them exhibit high cardinality. For example, one column contains a list of numeric error codes with more than 4k unique values. For reference, if I one-hot encode the entire dataset (minus my response variable, of course) I end up with more than 13k columns. I'm currently exploring hashing as a way to keep dimensionality low but am worried about information loss due to 'collisions'.Given this backstory, my questions are as follows:1. Does transforming my text-based response variable labels to numeric values create ordinality?In this example 1 = 'True' and 0 = any reason that would indicate 'False'. It seems that label-encoding is not recommended for features because it creates ordinality. Does the same problem exist when transforming a response variable usingshould I just convert them to 'true' or 'false' and leave them as strings?2. Should  numeric, nominal, categorical variables be converted to 'category' data type? In the example of my 'error code' column, I don't know if it's better to leave them as **int datatype or convert them to category. Do all numeric values have inherent ordinality, or merely data that I have converted myself through some means, such as label-encoding?3. How to balance information loss and high dimensionalityFrom what I'm reading, hashing can cause information loss to to 'collisions' and one-hot encoding can create the problem of 'high dimensionality'. Is there a best practice for balancing these two or is it use-case specific? Is it possible to test both methods within any cross-validation testing I might conduct?If you are able to share any insight in regards to these questions, I'd be most appreciative and look forward to our discussions.Thanks"
2224,How to solve fix 'list index out of range' while accessing large amount of data from file?,"['python', 'file', 'text-classification', 'filewriter']",I am working on a classifier that will access 200000 data items from a dataset but it only accesses about 1400 data correctly and shows  list index out of range.How can I access all of the items from the dataset? Here the structure of the dataset. Here is the code: it shows the following output. 
2225,"What should be done first? Intent classification or Slot filling, for example, seq2seq model","['nlp', 'dialog', 'chatbot', 'text-classification']","In goal oriented dialogue system what should be done first? Intent classification or Slot filling, for example, seq2seq model."
2226,spacy textcategorizer surprisingly works the same well when switching to different language than texts are,"['python', 'nlp', 'spacy', 'text-classification']","I have a general question regarding the way of textcategorizer in spacy works.
From different pieces of information on the spacy documentation and my tests it seems that it doesn't use the pretrained models and their features at least in  languages that have small models like German or of course are so called apha supported languages like Polish without models. 
I am using spacy 2.1.8 and  my categorization task is to categorize German text to 18 exclusive categories.
It works well and I gained surprisingly well cross validated accuracy, however while trying to understand the mechanics I found some surprising things when testing 
I am not loading the model as using spacy.blank
The core of my code looks like follows:Then I am using nlp.update in minibatches and optimizing with averages First strange things was that switch to completly different language like Polish and doing the same categorization on German texts doesn't change the results, final accuracy is the same after 5folds CV.
I also tried Finnish and Spanish even also the same.
So it proves for me it doesn't use the properties of the pretrained models even if there are available context-sensitive tensors that are shared across the pipeline like for German(From issue https://github.com/explosion/spaCy/issues/2523)
As per description from spacy.io for the architecture that I am using:
""A neural network model where token vectors are calculated using a CNN. The vectors are mean pooled and used as features in a feed-forward network.""
so from that I understand that it uses tokenizer( specific for the language only and then calculate ! (so not using the ones from model pretrained)the so context-specific token vectors for texts and this enough for NN to adjust weights to have right categorization even if the token are not so true while choosing other lang than text is. 
Anybody has other explanation or sth to add?                        "
2227,Should I chunk by document into blocks of text for document classification?,"['nlp', 'text-classification', 'document-classification']","I am dealing with huge financial contract documents . My need is to classify a document based on a particular ""clause"" present or not . My contract document is very huge say 40 page pdf file . My clause that needs to be trained is in some page -x "" . Please note ""x"" is different in different documents based on vendor . Also x can be split in two pages in the same document.My question is , when I train a doc classifier should I need to train with only text that is in page-x or the text will be split across pages (x1,x2,x3) or . with entire text (in all pages)Should I need too train my document classifier with entire Text or sub section of the texts that represent the clause .During prediction I am forced to provide full text of the document . Will the classifier perform if I have it rained as distinct chunks ?"
2228,Applying one_hot over rows in a dataframe,"['python', 'numpy', 'text', 'python-3.6', 'text-classification']","I have a data frame, the fifth column include text, which I want to apply one_hot on it for further text manipulation. so I'm trying the below code and it works on individual rows, which gives me an integer representation of each of the words in each cell, my question is how can I apply this function over all rows? "
2229,Sklearn Target Data - Only Integer Scalars Can be Converted to A Scalar Index,"['python', 'scikit-learn', 'text-mining', 'text-classification']","I am following this tutorial to generate the plot confusion matrix, based on this data:In plot_confusion_matrix function, I have to pass the true target of test data (twenty_test.target), predicted target of test data (mnb_prd, I am using Naive Bayes Classifier), and the true target names of test data (twenty_test.target).However, in redefining variable classes, it returned error only integer scalars. I think I already set the passing variables right based on the tutorial, so maybe there is something that needs to be add. 
Any help appreciated. Thank you.THE STACK TRACE"
2230,Import plain text into a MATLAB matrix as numbers for semantic neural net analysis,"['matlab', 'matrix', 'text', 'text-classification', 'converters']","It seems one must employ a script to import plain text into MATLAB, convert it to numerical format, and store it in a matrix. This step is required for any sort of analysis of the text data in MATLAB, such as extraction of semantic features using neural networks or classifying texts for known features, for instance. I did not find any way to do it without scripting. Here's what I wrote:Can this be streamlined further, if not for anything else then for elegance?"
2231,"FScore, Precision, Recall and Accuracy of textual data in Python","['python', 'nltk', 'precision', 'text-classification', 'precision-recall']","I am blank how to calculate Fscore, Precision , Recall and accuracy in Python. I have an excel file, having ""1172 rows x 3 columns"". Where column 1 contains the Original Words, Column 2 contains Gold Lemma and Column 3 contains the tested Lemma.Image is attached of excel fileHow can i calculate the Fscore, Precision , Recall and accuracy in Python. I am using Jupytor
My data is in UTF-8 format, Language is Urdu"
2232,Finding the datasets in paper “Hierarchical Attention Networks for Document Classification”,"['deep-learning', 'dataset', 'sentiment-analysis', 'text-classification', 'imdb']","I'm an AI research trainee and trying to re-implement the model mentioned in the paper ""Hierarchical Attention Networks for Document Classification"" on the same datasets they used for their research (as shown in the picture). However, I've been finding these materials to no avails. Can someone tell me about a website/database that may contain these academic resources. Can I contact the paper authors and request the datasets?
Thank you."
2233,Which ML method for multiclass (non-binary) text classification should I choose (from SparkML)?,"['machine-learning', 'pyspark', 'apache-spark-mllib', 'text-classification']","I am working on a quite big dataset that will be processed on the cluster, so this is why I am using PySpark for that puropose.The presentable records of this dataset have a such structure:After some preprocessing/data cleansing operations I would like to create and then obviously train a model that will classify issues (Issue) into some categories,  that are still unknown. I am a newbie in the ML area. I have readen some articles about TF-IDF, but not sure if this could be suitable for this case. Could anyone help? Thank you in advance. If you need more information do not hestiate to comment."
2234,What is the difference between IBM NL Classifier and NLU custom model classification?,"['nlp', 'ibm-cloud', 'text-classification']","What is the difference between IBM NL Classifier and NLU custom model classification?NL Classifier is trained on text (probably short text)
and when checking NLU custom model, it can also be trained on custom data for classification.Anyone know what the differences are? 
Thanks"
2235,"text-classification ValueError: Iterable over raw text documents expected, string object received","['python', 'nlp', 'text-classification']","I've build a word classification model to classify a single word as 'term' or 'non-term'. And now I have input which is a list of sentences. I need use my model to find out how many words are 'term' for each sentence.The input is a list of sentences, after pre-processing it looks like this:
So I wrote the code:And I expected the output like predicted = [[non-term, non-term], [non-term], [term], [non-term],.....] something like this. So it will be easier to count the number of 'term' for each sentence later. 
However, I had a 'ValueError: Iterable over raw text documents expected, string object received.'
If I just use a single word as input, such as:Then it works fine.
I'd appreciate if someone can help me fix this. Thank you!"
2236,Evaluate Machine Learning Text Classifier,"['python', 'nlp', 'logistic-regression', 'text-classification']","I have built a binary text classifier. Trained it to recognize sentences for clients based on 'New' or 'Return'. My issue is that real data may not always have a clear distinction between new or return, even to an actual person reading the sentence. 
My model was trained to 0.99% accuracy with supervised learning using Logistic Regression. And this would give me an accuracy of 0.998.
I now can pass a whole list of sentences to test this model and it would catch if the sentences has a new or return word yet I need an evaluation metric because some sentences will have no chance of being new or return as real data is messy as always. My question is: What evaluation metrics can I use so that each new sentence that gets passed through the model shows a score?
Right now I only use the following codeFor example I would expectTherefore I'd know to not trust those will metrics below a certain percentage because the tool isnt reliable. "
2237,How to distinguish the direction of important features from xgboost or random forest?,"['nlp', 'random-forest', 'xgboost', 'sentiment-analysis', 'text-classification']","I'm now working on binary text classification problem (like sentiment analysis), and it's trivial to pull out top important features of xgboost or random forest just by feature_importances_Suppose we have two labelling 1 and 0 for this classification problem. Then there's any way to print out the direction of the features (positive or negative)? Say, word feature A has an enrichment or high tfidf with labelling 1.Certainly I could pull out the tfidf column of this specific word feature, and correlate with the labelling with pearson coefficient, and the +/- of coefficient would indicate the direction, right? Any other more elegant way for this or xgboost and random forest has built-in such functions. (I didn't find)Thanks"
2238,How does Ulmfit's language model work when applied on a text classification problem?,"['nlp', 'lstm', 'text-classification', 'language-model', 'fast-ai']","I have been playing around with Ulmfit a lot lately and still cannot wrap my head around how the language model’s ability to make sound predictions about the next word affects the classification of texts. I guess my real problem is that I do not understand what is happening at the low level of the network. So correct me if I am wrong but the procedure is like this right (?):The language model gets pre-trained and then fine-tuned. This part seems clear to me: Based on the current and preceding words you form probabilities about the next words.
Then the model gets stripped from the softmax layer designed to create the probability distribution.
You add the decoder consisting of a reLU-Layer (what is this layer actually doing?) and another softmax layer that outputs the probability of class membership of a given text document. So here are a lot of things I do not understand: How is the text document taken in and processed? Word for word I assume? So how do you end up with the prediction at the end? Is it averaged over all words?
Hmm you can see I am very confused. I hope you can help me understand Ulmfit better! Thanks in advance!"
2239,Multiple digits classifier using MobileNetV1,"['tensorflow', 'neural-network', 'conv-neural-network', 'text-classification', 'mobilenet']","I want to train a model to be able to classify 16 number of digits in image, the numbers of digits are constants. So I create my dataset like these images (Print random 16 number with different font and size on SUN dataset background) :My training dataset is about 3000 images and 600 images for validation.So I wanted to use MobileNetV1 as feature extractor and connect to 16 softmax output layers for each digits.Here is my code snippet to create modelso my dataset picture size is (650,65) and train tensor shape is (3000,650,65,3) with normalized values between -1 and 1and my train labels is 16 arrays of one hot tensors with shape (16,3000,10)my model compiled successfully and training started but my loss and val_loss is not improved during epochs.Basically I wanted to create model like this pictures because this model is working great on this digits classification problem.
The top of this model architecture is something like MobileNetV1 and the input is resizer and normalizer blocks."
2240,build a binary classification model with LSTM,"['python', 'machine-learning', 'classification', 'lstm', 'text-classification']","I have a dataset in csv format with 49 columns, some of them are strings and some of them ar integers.I have added a new column to use as label called ""input"" with appropriate label as 0 & 1.Here's a sample of the dataset:
The requirement is to consider all of these feature columns for model training.What options I have to train this model?
What steps should I follow?
Any resource(article, video etc) will be very appreciated.Thank You,"
2241,Using spam classification in a different application?,"['python', 'nlp', 'classification', 'text-classification']","I want to use the concept of spam classification and apply it to a business problem where we identify if a vision statement for a company is good or not. Here's a rough outline of what I've come up with for the project. Does this seem feasible?Prepare dataset by collecting vision statements from top leading companies (i.e. Fortune 5000)Let features = most frequent words (excluding non-alphanumerics, to, the, etc)"
2242,Android TextClassification + ActionMenu TYPE_FLOATING Icon,"['android', 'icons', 'floating-action-button', 'text-classification', 'action-menu']",I adapted my code from the documentation here: TextClassificationI changed the ActionMode type to ActionMode.TYPE_FLOATING along with corresponding ActionMode.Callback2 and onGetContentRect()The ActionMode.TYPE_PRIMARY ActionMode does show the icon as expected. The ActionMode.TYPE_FLOATING ActionMode appears but without the icons. I'm getting this:But I'm looking for this:Here's my code:Is there built-in code for the icon version or does a custom view have to be made?Update:Further investigation suggests that app:actionLayout is ignored when trying to use a custom menu resource file with a custom action layout file.
2243,Is this classification model overfitting?,"['machine-learning', 'scikit-learn', 'classification', 'text-classification', 'sklearn-pandas']","I am performing a url classification (phishing - nonphishing) and I plotted the learning curves (training vs cross validation score) for my model (Gradient Boost). My ViewIt seems that these two curves converge and the difference is not significant. Tt's normal for the training set to have a slightly higher accuracy). (Figure 1)The QuestionI have limited experience on machine learning, thus I am asking your opinion. Is the way I am approaching the problem right? Is this model fine or is it overfitting?Note: The classes are balanced and the features are well chosen Relevant code"
2244,Confidence on the predictions of DecisionTreeClassifier,"['python', 'scikit-learn', 'classification', 'decision-tree', 'text-classification']","I'm trying to understand how classification algorithms work to create a kind of generic pipeline, so I just started with a LinearSVC model. Generally speaking, I'm doing:I really need, as the output, the predicted labels and their confidence. But when I try to do it with other models, I can´t find a way to get the confidences. E.g.In this case, the predict_proba returns, for each predicted label, a tuple containing [0,1] as the values (instead of the confidence between 0 and 1 as a single number). I mean, absolutely all the predictions have a 0.0 or 1.0 value, no values in the middle. E.g. if I run Is this normal? How can I get such a number? Or which other models allow me to get the predictions' confidences?
Best,"
2245,How to shrink a bag-of-words model?,"['scikit-learn', 'nlp', 'classification', 'random-forest', 'text-classification']",The question title says it all: How can I make a bag-of-words model smaller? I use a Random Forest and a bag-of-words feature set. My model reaches 30 GB in size and I am sure that most words in the feature set do not contribute to the overall performance.How to shrink a big bag-of-words model without losing (too much) performance?
2246,Detect Multiple Intents from Call Center Conversation transcripts,"['machine-learning', 'nlp', 'text-classification', 'multiclass-classification']","I am trying to build a Model that can take the User conversation, that involves dialogues, as Input and Find all the Intents involved in it. This is Basically an Intent Detection Problem. However, Normally labeling Sentences and extracting the features out of it and building an Intent classifier wouldn't work here because Multiple Intents might be Available in a Single Conversation. Is there any Tool / Way / any pipeline that I should follow to achieve this Use case."
2247,Text classification based on optical character recognition [closed],"['machine-learning', 'computer-vision', 'ocr', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 10 months ago.The problem statement:I need to extract the items table from receipts, like those you get in supermarkets.
It's not clean A4 invoice, where tables usually have lines.I get the characters, along with there bounding boxes, from the OCR engine.
I then align it to the X axis.
Now, I need to find the tables.Why I do not take a Deep Learning Approach:I don't want to use deep learning for that, as it will be very huge project, very risky, very hard to debug, and I don't have enough data (few hundreds). Ho, and i don't have enough experience in training RNNs...I am searching for a conventional machine learning approach:I am considering both machine learning algorithm or my own algorithm.
I assume ML is better, but I'm not sure what algorithm will give an array of results ( = lines of items, or at least the y asix of each line).In addition, what is the feature vector for such supervised ML? i have receipts with 2 lines and receipts with 10 lines. By the way, when I'm saying ""line"" i mean line item. each item can be written in two line.Any advice on how to solve this probelm?"
2248,How to update existing model with new training data in TFlearn,"['python', 'tensorflow', 'neural-network', 'text-classification', 'tflearn']","I'm new for Tensorflow , I'm using This tutorial- https://sourcedexter.com/tensorflow-text-classification-python/ for text classification build model.i'm saving the model using below code.This gives below files.But when i save Existing model(with new Training dataset). i'm getting prediction value only for new trained datas not for existing datas. How to append new train dataset in existing save model.Thanks."
2249,How to do sequence classification with pytorch nn.Transformer?,"['machine-learning', 'deep-learning', 'pytorch', 'text-classification', 'transformer']","I am doing a sequence classification task using nn.TransformerEncoder(). Whose pipeline is similar to nn.LSTM().I have tried several temporal features fusion methods:Selecting the final outputs as the representation of the whole sequence.Using an affine transformation to fuse these features.Classifying the sequence frame by frame, and then select the max values to be the category of the whole sequence.But, all these 3 methods got a terrible accuracy, only 25% for 4 categories classification. While using nn.LSTM with the last hidden state, I can achieve 83% accuracy easily. I tried plenty of hyperparameters of nn.TransformerEncoder(), but without any improvement for the accuracy.I have no idea about how to adjust this model now. Could you give me some practical advice? Thanks.For LSTM: the forward() is:For transformer:"
2250,One huge or multiple small models for text classification,"['python', 'tensorflow', 'machine-learning', 'neural-network', 'text-classification']","I am working on classification of large text database (millions of textes) labled to some thousands of categories / subcategories (very similar to the amazon product dataset). My question is, if i could get a higher (in general) accuracy by the creation of multiple models e.g. Separate models for each main category to get the final subcategory. (many category-models)compared to One huge model over all textes and all subcategories. I am wondering if the vocabulary that is used in the textes of a main category and their subcategories is easier (and of smaller size) and more precise to learn for a model? But there will be an additional problem if the main-model predicts a wrong main-category - then the search for the right subcategory will have no chance to predict the right result.Is there any research on this? 
Any rules of thumb?
Many thanks!"
2251,Is text classification fast enough for type ahead search?,"['machine-learning', 'text-classification']",I'm working on designing a typeahead service that can be used to search for many different things. I was thinking about creating a text classification model to categorize these searches before actually making the search. Here's an example of the result I'd want from the classification model.InputOutputThen I'd take the categories that have a likeliness above some value and actually do the typeahead search. Also I'd return the results ordered by the likeliness rank.We have been collecting data about people's searches and tracking what they were actually looking for so we should have the data needed to train a text classification model.My question is can text classification models be fast enough to be used with a type ahead service and not be prohibitively expensive? Are there certain types of text classification algorithms that I should be looking at?
2252,keras embedding layer cannot be trained with vectors from googlenews word2vec,"['python-3.x', 'keras', 'text-classification', 'embedding']","I aim to do text classification, using keras in python, on plenty of positive and negative comments about some movies. I've done word2vec process on each comment, using Googlenews word2vec, and now I have some vectors with values between -1 to 1.I need to pass these vectors into a network which has an embedding layer as its first layer. What I realized is the embedding layer is unable to process the vectors as it givs a specific output in model.predict, for each input vector, which means it's not been trained at all as it does not matter what we have in the input.As I was completely confused with the results, I made some random input vectors of the size 300 (the same size of googlenews vectors), and built a simple network including an embedding layer, a flatten layer and a dense layer to examine the output of network after predicting. What I realized was if I change the domain of input vectors to, for example 0 to 2, the embedding layer will be traind well and the output of model.predict will be accurate enough. Here is a part of the code (p1 is the number of vectors and p2=300):I expect the output to has at least different values, while I get the same output for the all input vectors with valuee between 0 and 1(which are the result of word2vec).Does anyone have any idea about what's happening in embedding layer?"
2253,How to change this RNN text classification code to text generation?,"['tensorflow', 'machine-learning', 'nlp', 'recurrent-neural-network', 'text-classification']","I have this code to do text classification with TensorFlow RNN, but how to change it to do text generation instead?The following text classification has 3D input, but 2D output. Should it be changed to 3D input and 3D output for text generation? and how?The example data are:For classification feeding ""british gray is"" results in ""cat"". What I wish to get is feeding ""british"" should result in the next word ""gray""."
2254,X has 7 features per sample; expecting 18282,"['python-3.x', 'scikit-learn', 'text-classification']","I'm trying to make a textclassification model with sklearn. I'm quite new to python and also sklearn. I already made the model with some training data and saved the model. But there's an error when I try to reuse the model in an another python program/file.I already looked in some similar problems here on stackoverflow, but I couldn't find a solution for me.I made some comments, so you can read the code more easily.And since I was training with different methods to evaluate which was better I made a train_model method.This is the ""correct_model"":This model gives me something around 80% accuracy on the validation data.So this is my test file where I wanted to test, if I can load and reuse the model:Then I get this error:What I expected is, that it will give me ""3"" as a result which is the encoding for a specific label. These predictions works in the same file where I also train the model, but somehow I can not use new validation data.I think I made some mistake when fitting and or transforming the data."
2255,How can I calculate correlation between subjects?,"['python', 'correlation', 'text-classification', 'multilabel-classification', 'multiclass-classification']","How can I calculate correlation between classes of the texts?
E.g., I have 3 texts:So, each text has a label (class). So, it is close to the text classification problem. But I need to calculate the measure of ""difference"".I can count Tfidf and get the matrix:I need to get a score which will tell me:
 - how much the subject ""final"" is close to ""Crowned"".What metric should I use?////////////////////////////////////////////////////////////////
Suppose you have 5 texts:After school, Kamal took the girls to the old house. It was very old and very dirty too. There was rubbish everywhere. The windows were broken and the walls were damp. It was scary. (1)
Amy didn’t like it. There were paintings of zombies and skeletons on the walls. “We’re going to take photos for the school art competition,” said Kamal. Amy didn’t like it but she didn’t say anything. (2)
“Where’s Grant?” asked Tara. “Er, he’s buying more paint.” Kamal looked away quickly. Tara thought he looked suspicious. “It’s getting dark, can we go now?” said Amy. She didn’t like zombies. (3)
Then, they heard a loud noise coming from a cupboard in the corner of the room. “What’s that?” Amy was frightened. “I didn’t hear anything,” said Kamal. Something was making strange noises. (4)
“What do you mean? There’s nothing there!” Kamal was trying not to smile. Suddenly the door opened with a bang and a zombie appeared, shouting and moving its arms. Amy screamed and covered her eyes.  (5)Each text has labels:1st text - school, house, scary
2nd text - zombies, paint
3rd text - zombies, dark, paint
4th text - noise, frightened
5th text - zombie, screamedthe 1st task is to find the correlation between text. Seems @MarkH has already given me the right direction (cosine similarity)
the 2nd task is to find the correlation between labels. You see that almost all labels are ""zombie"". Also, the 3rd sentence and the 2th sentence have 2 equal labeles: ""zombies, paint"". 
Suppose we have 10000 texts. So what chance these lables describes the same thing and we can delete one of label (paint) and use onle 1 (zombie)? So, it's like a contribution to the variation. 
Does it affect too much if we remove some lables? Can we remove/unit some labels?"
2256,How can I apply classification algorithm for text data which is in the form of numerical tokens?,"['machine-learning', 'data-science', 'text-classification', 'tf-idf', 'naivebayes']","I am trying to work on a classification problem: The data is of reviews of a particular product category from an e-commerce platform. Please find below the description of each attribute:The sample dataset is attached in the picture.I am thinking to try TF-IDF however, given the text format don't know how to use the same.I expect to predict the category based on the text column provided."
2257,Group features of TF-IDF vector in scikit-learn,"['python', 'scikit-learn', 'text-classification', 'tfidfvectorizer']",I'm using scikit-learn to train a text classification model based on TF-IDF feature vector by following piece of code:I need to rank the extracted features in decreasing order of their TF-IDF weight and group them into two non-overlapped sets of features and finally train two different classification model. How can I group the main feature vector into an odd-ranked set and an even-ranked set?
2258,How to classify a resume segments in Keras? [closed],"['python', 'keras', 'deep-learning', 'text-classification']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 11 months ago.I need to apply deep learning in resume analysis, by taking the text and split it into paragraphs, then pass each paragraph into a deep learning model that predicts if this paragraph is skills, education, experience, etc. So we can call it is a multi-label classification problem.By the way, I have a dataset that contains paragraphs and its labels!Can I perform that with text classification in Keras? Which model I can use and what will be the necessary steps in text pre-processing?I am pretty new to this field. Thanks in advance!"
2259,How add new samples to the same label using Naive Bayes on php-ml?,"['php', 'machine-learning', 'text-classification', 'naivebayes', 'php-ml']","I am newbie on Text Classification and I am trying to create some proof-of-concepts to understand better the concepts of ML using PHP.
So I got this example, and I've tried to add a new small text to ""reinforce"" one of my labels (categories), in this case, Japan:The problem is, after added Japan again on array of labels, the result was:But I was expecting:So, How add new samples to the same label?"
2260,How to train for multi-label text classification in Sagemaker?,"['text-classification', 'amazon-sagemaker', 'multilabel-classification']","I have chosen BlazingText algorithm provided by Sagemaker.Text in my training set can have one or more labels, and I want to predict the most likely labels for an article.I didn't find how to exactly setup the training file for this. I have made the lines in the training file in the following format __label__1 __label__2 token1 token2 ...
__label__2 token token token ...Am i doing it right it right?"
2261,The names of the columns in CountVectorier sparse matrix in python,"['python', 'sparse-matrix', 'text-classification', 'countvectorizer']","When I use the code below:It returns the term frequency document as a sparse matrix.I found out how to get the data, indices, and indptr of the sparse matrix.My problem is how can I get the names of the columns (which should be the features or words)?"
2262,What's the best way to classify time text data?,"['r', 'machine-learning', 'text-classification']","This is a straight-forward question:I have time text data that looks like the following: 110
120+
50 minutes
50 Minutes
35-40
30
1 hour and a half 
1 hour 20 MinutesWhat's the best way to clean this data so that I can analyze it? Is this a job for machine learning? If so, what are the librarys/tools that would help me most in this situation?One thought is using gsub:""as.numeric(gsub(""([0-9]+).*$"", ""\1"", Timedata))"" but that over-simplifies the data.I would like the data to look like this after cleaning:110
120
50
50
37.5
30
90
80"
2263,Text Classification - what can you do vs. what are your capabilities?,"['nlp', 'classification', 'stanford-nlp', 'text-classification', 'luis']",Text Classification basically works on the input training sentences. Little or less number of variations of in the sentences do work. But when there is a scenario likeWhat can you do <<==>> What are your capabilitiesThis scenario does not work well with the regular classification or bot building platforms.Are there any approaches for classification that would help me achieve this ?
2264,Seeking Advises on how to perform One Class classification on a text data,"['matlab', 'machine-learning', 'svm', 'text-classification', 'unsupervised-learning']","I have some doubts about the implementation of the one-class classifier. This question is trying to explain how am I implementing the one class classifier and I hope you will advise me whether I am approaching the right way or not. I am experimenting on applying one class classification upon a text dataset with 2 classes (class 1 and class 0). It is a balanced dataset. Initially, my train matrix is of size 3000x100. After eliminating a class for the purpose of training my model for one class, it's reduced to 1500x100 (contains only class 1). And the test/validation matrix is of size 2000x100 where data of 2 classes exist (contains data for class 1 and for class 0).To train the model, I am applying the following codeblock upon train data and have tried to calculate the precision, recall and F1:where ReducedDataset and ReducedClassSet represent the train matrix and classes. The scores I have achieved are 1, 1 and 1. After applying the model on the test dataI get the scores of 1,0.5004 and NaN(which seems illogical to me)
My question is am I following the proper way of applying one class classifier? If am not, what should be the right approach then?I am looking for your advice, in this regard.Thanks,"
2265,"Machine learning, how do I combine multiple features in one predicting model when working with text data","['python-2.7', 'machine-learning', 'scikit-learn', 'text-classification']","I'm trying to classify medical terms and non-medical terms. I can extract single different features (such as count vectors, and tfidf vectors in word level, ngram level, and character levle) for one classification report. But I wonder how can I combine these features together to generate one report. Any help will be appreciated."
2266,How to Use TF-IDF and combine it with Information Gain for feature selection in text classification?,"['text-classification', 'information-retrieval', 'tf-idf', 'feature-selection', 'information-gain']","i don't know the concept of how to combine TF-IDF result and use it in information gain mathematically .
can someone explain it for me please?"
2267,I get 'single' characters as learned vocabulary on word2vec genism as an output,"['nlp', 'gensim', 'word2vec', 'feature-extraction', 'text-classification']","I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone helpI try to feed tokens instead of the raw text into the models"
2268,modeling the context with LSTM,"['python', 'keras', 'text-classification']","I have around 500 documents, these documents contain ""malicious"" sentences (positive class).  What I want to do is to identify these sentences from the rest of ""legitimate"" sentences.What I did is: I built a LSTM network (many-to-one structure) which takes each sentence with the label (malicious or not). I noticed that the results are weak. The identification of these sentences needs context information to decide if they are malicious.
Thus, I build another bi-LSTM (many-to-many structure) to model the context. 
I feed to the network sentence i, i-1, and i+1. I will use the embeddings from the last_layer-1 to feed them to a classifier.Each sentence is represented with 512 embeddings, thus, I have 3 steps and each has 512 dimension.What confuses me is that I'm not sure if this approach will utilize the context, does it?"
2269,Why to reshape MSER contours before detecting texts?,"['python', 'opencv', 'reshape', 'text-classification', 'mser']","I am using MSER from opencv-python to detect text using the code from this stackoverflow question. Can anyone help me understand why the contour p is being reshaped to (-1, 1, 2) before computing the convex hull of the objects?The code is as below:"
2270,How to access data from text file for training a model on TPU?,"['python', 'tensorflow', 'deep-learning', 'text-classification', 'tensorflow-datasets']","I am trying to implement a LSTM text classification model in TF1.x for TPU without enabling eager execution. However, I just can not find a correct way to prepare the data for feeding it to the model. I am following these two tutorials, tutorial 1 for getting dataset and tutorial 2 for implementing it for running on Google Colab using TPU.
Since it is text classification, I have to encode the input text tokens using tf.Tokenizer() below is how I am doing it-all_labeled_data consists (example, label) pairs. I am sure I am making a mistake the way I am accessing the data. If you have any comments/suggestions, please feel free to do so. If you want to see the whole code then it is here. Also, feel free to run it on Google Colab with Python 2 and TPU enabled.
Thank you!"
2271,"Keras Functional Api, InvalidArgumentError: input 1 should contain 3 elements, but got 2","['python', 'tensorflow', 'machine-learning', 'keras', 'text-classification']","I am using the functional API of Keras for a text classification task. I am combining word embeddings with metadata, which I both convert into one hot encodings.
I am getting this error when I fit the model:
tensorflow.python.framework.errors_impl.InvalidArgumentError: input 1 should contain 3 elements, but got 2[[{{node training/Adam/gradients/concatenate_1/concat_grad/ConcatOffset}}]]
I am running a docker container in a K8s cluster, tensorflow is 1.8 and keras 2.2.4.**EDIT:**I have already checked that the shape of the nlp data and the metadata is (55744, 500). The layers are like: Metadata layer shape: (?, 500, 1) and NLP layer shape: (?, 500).This is the code:**EDIT:**This is the error stack:I have already increased the GPU memory, because I was getting Allocation of 5574400000 exceeds 10% of system memory and I use steps_per_epoch, because the functional API was throwing errors when I was using batch_size instead. 
Can someone tell what is the problem with the input/output sizes that causes the InvalidArgumentError? Is it in the way the model is build or in the input data itself?
There is also [[{{node training/Adam/gradients/concatenate_1/concat_grad/ConcatOffset}}]] in the error message.This is how the model looks like:"
2272,How to insert my messages as input in my feature columns in SVM estimator in the Tensorflow package,"['tensorflow', 'machine-learning', 'svm', 'text-classification']","I am trying to implement my text classification model which classifies messages as spam or not in Tensorflow using SVM Model. 
I have only 2 features rn that is a contact name and message, do I need to convert them into numbers or can it be directly used as input in feature column"
2273,Text Classification with word2vec,"['python-3.x', 'word2vec', 'text-classification']","I am doing text classification and plan to use word2vec word embeddings.
I have used gensim module for word2vec Training.I have tried several Options. But I am getting error that word 'xyz' not in vocabulary. I am not able to find my mistake.Please help me to solve my issue.Now I am using my created word embeddings on the downstream classification task.where I have two priorities. I want to classify it.I am using folllowing network for classificationI am getting error here:Please help me to solve it out.Thank you."
2274,How do you classify books by genre (using deep learning) when some books have multiple genres?,"['python', 'machine-learning', 'deep-learning', 'text-classification']","I am trying to build a neural network that looks at the text of a book and guesses the book's genre. I can train a network fine when each book only has one genre. Is there a good way to train a network when a book is associated with multiple genres?I have tried using a basic SGDClassifier from sklearn. It works wonderfully with a data set where each one book/block of text is tied to one genre. Unfortunately, I do not know how to give it a data set where each book/block of text is associated with multiple genres. Here is the basic code I am using for context:Does anyone know of a good way to approach this problem? Can anyone link me to a place where a smart person has already solved it?"
2275,Retrieve the indices for only the resampled instances after oversampling using imbalanced-learn?,"['nlp', 'text-classification', 'indices', 'oversampling', 'imbalanced-data']","For a binary text classification problem with imbalanced data, I use imbalanced-learn library's function RandomOverSampler to balance the classes. Now, I want to retrieve only the instances that were oversampled (replicated) from the original data. For example, if ""item_1"" is the original data and item 2 to 4 are the replicas of ""item_1"", I require only the indices for ""item_2"", ""item_3"", ""item_4"" for further processing and leave out the index for ""item_1"". Here goes the my code:"
2276,Braces removing from a given path in tcl,"['tcl', 'text-classification']","Suppose I have a path like {\$ALLO/afg}.
I want to remove the starting and ending braces by using regsub or regexp."
2277,How to continue training after loading model on multiple GPUs in Tensorflow 2.0 with Keras API?,"['tensorflow', 'text-classification', 'tensorflow2.0', 'multiple-gpu']","I trained a text classification model consisting RNN in Tensorflow 2.0 with Keras API. I trained this model on multiple GPUs(2) using tf.distribute.MirroredStrategy() from here. I saved the checkpoint of the model using tf.keras.callbacks.ModelCheckpoint('file_name.h5') after every epoch.
Now, I want to continue training where I left off on same number of GPUs from the last checkpoint I saved. After loading the checkpoint inside tf.distribute.MirroredStrategy() like this-, it is throwing following error.Now I am not sure where the problem is. ALso, if I do not use this mirror strategy for using multiple GPUs, then the training starts from beginning but after few steps it reaches the same accuracy and loss value like before the model was saved. Although not sure if this behaviour is normal or not.Thank You!
Rishabh Sahrawat"
2278,How can i use the same code for different variables in a dataset in R?,"['r', 'function', 'classification', 'repeat', 'text-classification']","I am working in a email classification supervised model, the emails are classified in 20 different groups, I have finished the model for the first group (G1) (a very large code) and I would like to know if there's some function which can repeat the code but with the others groups as variable, because changing G1 for (G2...G20)manually would be so tedious.I have not idea how I could do it."
2279,How should I go about using TF-IDF for text classification on the data I collected?,"['python', 'machine-learning', 'text-classification', 'tf-idf']",I'm working on a personal project to build a text classifier. I scraped around 3000 news articles from 8 categories. I have every single word in every article with its article's category tag in a dataframe. The answers I saw online referred to using tfidf on entire articles/text blocks. Is there any way to analyze individual words?Here is an idea of what my data currently looks like:I apologize for the horrible formatting; I'm somewhat new to this.
2280,Text classification using Doc2vec and evaluated by cross validation,"['python', 'cross-validation', 'text-classification', 'doc2vec']","Recently I am doing a text classification problem and want to use Doc2vec for experiment. I googled and saw the tutorial post of @susanli2016. I was wondering how can I use cross validation to evaluate instead of train/test split in this case? 
P.s.: As I couldn't really find anyone who use cv for evaluation, is train/test split better here?Here is her code:Here is my thought:but I got the following error message:I don't know where did I do wrong and how to fix it. Since I am very new to python and NLP domain, I would be glad if someone can give me a hint on this so my work can proceed:) Thank you in advanced!"
2281,how to save an nlp classification model in order to use it later,"['nlp', 'sentiment-analysis', 'text-classification', 'oversampling']","My dataset is imbalanced. it contains messages as: Negative 34.34 %, Neutral 63.95 % and Positive 1.71 % The solution I took is oversampling with SMOTE. SMOTE doesn't work on the on the list of bare messages (like the corpus above): it requires the vectors instead.
Hence,before applying SMOTE, I applied ""TfidfVectorizer.fit_transform"" on my corpus. So now I've X and y with respective shapes: (5850, 41369) and (5850,)Applying SMOTE now creates X_smote and y_smote. My data distribution is now balanced: ""Counter({-1: 3741, 0: 3741, 1: 3741})""In the 17th line,what should the ""corpus"" variable in my case be ?
And after loading, or creating my model, the fit function is supposed to be applied to what variables ? (classifier.fit(X_smote, y_smote)) ???And then, what am I supposed to save so that the next time I open a fresh notebook I just launch ""my_model.predict()"" on a new messagecopied code from"
2282,Tensorflow retrain if it's wrong,"['python', 'tensorflow', 'artificial-intelligence', 'text-classification']","I'm new to Tensorflow and AI, so I'm having trouble researching my question. Either that, or my question hasn't been answered.I'm trying to make a text classifier to put websites into categories based on their keywords. I have at minimum 5,000 sites and maximum 37,000 sites to train with.What I'm trying to accomplish is: after the model is trained, I want it to continue to train as it makes predictions about the category a website belongs in.The keywords that the model is trained on is chosen by clients, so it can always be different than the rest of the websites in its category. How can I make Tensorflow retrain it's model based on corrections made by me if it's prediction is inaccurate? Basically, to be training for ever."
2283,"Multinomial naive bayes classification problem, normalization required?","['machine-learning', 'scikit-learn', 'text-classification', 'naivebayes', 'multinomial']","Classification using multinomial naive bayes is not working, see the codeOutput is [10], But I was expecting it to be [20].What is wrong here, my understanding?"
2284,Grouped Text classification,"['python', 'machine-learning', 'deep-learning', 'nlp', 'text-classification']","I have thousands groups of paragraphs and I need to classify these paragraphs. The problem is that I need to classify each paragraph based on other paragraphs in the group! For example, a paragraph individually maybe belongs to class A but according to other paragraph in the group it belongs to class B.I have tested lots of traditional and deep approaches( in fields like text classification, IR, text understanding, sentiment classification and so on) but those couldn't classify correctly.I was wondering if anybody has worked in this area and could give me some suggestion. Any suggestions are appreciated. Thank you.Update 1:Actually we are looking for manual sentences/paragraph for some fields, so we first need to recognize if a sentence/paragraph is a manual or not second we need to classify it to it's fields and we can recognize its field only based on previous or next sentences/paragraphs.To classify the paragraphs to manual/no-manual we have developed some promising approaches but the problem come up when we should recognize the field according to previous or next sentences/paragraphs, but which one?? we don't know the answer would be in any other sentences!!.Update 2:We can not use whole text of group as input because those are too big (sometimes tens of thousands of words) and contain some other classes and machine can't learn properly which lead to  the drop the accuracy sharply.Here is a picture that maybe help to better understanding the problem:
"
2285,Grouping of text in pandas,"['python', 'pandas', 'nlp', 'nltk', 'text-classification']",Hello guys i am new to NLP and am totally lost. All help is much appreciated.I Have data set of dimension 5000x3. So my first column is the a timestamp and the second column is Title of the essay. I need to group them into similar titles and introduce a third column with a group number based on the title similarity and newness and frequency of  the topics occurrance . Can anyone help me with this? This is what ive done : 1. NLP preprocessing(tokenization and stop word removal in the essay column). 2. tf_idf vectorize 3.Cosinesimilarity 4.k means clustering based off cosine similarity. I'm pretty sure what is doing is wrong. Help!!!! 
2286,How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?,"['python', 'text-mining', 'gensim', 'text-classification', 'lda']","I try to compare texts in form of 'text-files' concerning their content.
e.g.: I got 100 texts about animals and I want to analyze each text about what animals it discusses.
I am looking for an analysis output like: doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? Also I've seen a tutorial manipulating the topics via the eta-value in gensim but I don't know how I could use this in my favor. I am grateful for any advice that can lead me to the right direction. Thanks!"
2287,Are there any approaches/suggestiosn for classifying a keyword so the search space will be reduced in elasticsearch?,"['elasticsearch', 'nlp', 'text-classification']","I was wondering is there any way to classify a single word before applying a search on elasticsearch. Let's say I have 4 indexes each one holds few millions documents about a specific category. I'd like to avoid searching the whole search space each time. 
This problem becomes more challenging since it's not a sentence, 
The search query usually consists only a single or two words, so some nlp magic (Named-entity recognition, POS etc) can't be applied. I have read few questions on stackoverflow like:Semantic search with NLP and elasticsearchIdentifying a person's name vs. a dictionary wordand few more, but couldn't find an approach. are there any suggestions I should try?"
2288,How do I convert text to a MOA Instance?,"['text', 'weka', 'text-classification', 'moa']","I'm doing incremental learning in MOA for a text application. This requires creating an Instance object that represents the text numerically, such as TF-IDF scores for every stemmed word in the vocabulary. My MOA version is 2019.05.0.I looked for text processing tools in MOA, but couldn't find them.I saw that Weka has a class StringToWordVector, so I decided to try that. Weka's classes aren't the same as MOA's classes, but there's a class called WekaToSamoaInstanceConverter that I thought I could create a Weka Instance, run it through StringToWordVector, and convert it to a MOA Instance. Maybe this is the wrong track, or maybe this is the right track and I'm missing something in my syntax.wekaWordVectors.size() is the number of files in the subdirectories, so that's what I expect.The call to samoaInstances() fails. Line 220 tries to make a call to locateIndex(0). There is no class at 0, so that returns -1. This -1 is used as an array index, so I get an ArrayIndexOutOfBoundsException. I don't know what class 0 means but I know that an ArrayIndexOutOfBoundsException means I did something wrong."
2289,"Using training data to fine-tune BERT-base, the loss does not decrease","['nlp', 'text-classification']","I selected a total of 24478 scenic spot description data from 237 categories to fine-tune BERT-base, and the average loss remained about 5. Specifically, the amount of data for each category is less than or equal to 200, and learning rate: 10-6 or 10-7. Other hyper-parameters are consistent with the original experiment of BERT. What factors may cause losses to not converge?I selected 800 news data from 10 categories from the public news data set as training set. After 10 epochs of training with learning rate of 10-5, the model achieves 97% accurance on 400 test samples. After verifying the validity of the method, I selected a total of 3050 scenic spot description data from 14 categories to fine-tune BERT-base. After 10 epochs of training with learning rate of 10-6, the training loss converges to 0.5 and the model achieves 83% accurance on 230 test samples(the loss does not decrease with learning rate of 10-5)."
2290,Predictor prediction function to classify text using a saved BERT model,"['python', 'tensorflow', 'deep-learning', 'text-classification']","I have created a BERT model for classifying a user generated text string as FAQ or not FAQ. I have saved my model using the export_savedmodel() function. I wish to write a function to predict the output for a new set of strings, which takes as input a list of the strings. I tried using predictor.from_saved_model() method but that method requires passing key value pairs for input id, segment id, label id and input mask.
I am a beginner and I do not understand completely what to pass here."
2291,scikit-learn kmeans clustering text with jaccard distance,"['python', 'scikit-learn', 'k-means', 'text-classification']","I'm trying to use sklearn to cluster some tweets as a dictionary
I have 25 initial centroids id (tweet id)
I wrote it in my own functions, BUT I don't know how to implement it with sklearnI made a 2D matrix in which there are jaccard distances. I don't know how to fix init in kmeans method. it errors that's not ndarraywhat exactly should I  pass to it?"
2292,No batch_size while making inference with BERT model,"['python', 'tensorflow', 'machine-learning', 'deep-learning', 'text-classification']","I am working on a binary classification problem with Tensorflow BERT language model. Here is the link to google colab. After saving and loading the model is trained, I get error while doing the prediction.Saving the ModelPredicting on dummy textError batch_size param is present in estimator, but not in the loaded model params."
2293,How to make features for serving_input_receiver_fn BERT Tensorflow,"['python', 'tensorflow', 'nlp', 'feature-extraction', 'text-classification']",I have created a binary classifier with Tensorflow BERT language model. Here is the link to sample code. I am able to do predictions. Now I want to export this model. I am not sure if I have defined feature_spec correctly.Code to export model. Error 
2294,my train test splitting of data is not working. unigrams show same words for all category,"['python', 'machine-learning', 'scikit-learn', 'data-science', 'text-classification']","output of unigrams for both categoryWhen splitting the text-classifier data into test train and using chi2 to find unigrams it shows same unigrams for both of my categories and training is also not done.Using chi2 to find unigrams. Tried using another models to train but same result.
Expected results should be as follows:
I have two categories to classify cricket and football.
and in each of those unigrams only specific and distinct words should be allowed. but for each cricket and football unigram same set of words are coming."
2295,What does a “None” mean after compilation of keras model?,"['python', 'keras', 'lstm', 'text-classification']","I'm trying to implement a binary text classification model using keras layers. After compiling a model, in a summary, I am getting None at the bottom end and I don't exactly understand what does it mean?here is the code which I am using.This is the model summary and at the bottom end It is showing None."
2296,Algorithm for finding highly frequent patterns followed by a set of text messages,"['algorithm', 'nlp', 'cluster-analysis', 'text-classification', 'unsupervised-learning']","I have large amount of text messages. I want to find usual patterns followed by these messages (say 20 most common patterns). Example messages:You can see that msg1 and msg2 share same pattern/template (see below) while msg3 is different (there might be other messages sharing pattern with msg3). My requirement is to find such highly frequent templates in my data. For above example, the pattern/template would be like:I tried following:For top 20 clusters:i) Select 10 random messages.ii) Find the pattern followed by them using some string manipulation.Above method worked but clustering seems to be bottleneck and takes significant time. (approx. 10 min for 100000 messages on my system)Python function to find clusters:I get a feeling that the problem can be solved without using Machine Learning, but I don't know how to proceed for achieving results in less time. Worst case time complexity of DBSCAN seems to be O(n^2) (Average O(nlog(n))).I assume using Wagner-Fischer algorithm will result in longer time as it will have computations for every message with every other message (O(n^2) time complexity)."
2297,Encode sentence as sequence model with Spark,"['apache-spark', 'parallel-processing', 'pyspark', 'text-classification']","I am doing text classification and I use pyspark.ml.feature.Tokenizer to tokenize the text. However CountVectorizer transforms the tokenized list of words to bag of words model, not the sequence model.Assume that we have the following DataFrame with columns id and texts:What I want here is (for the row 1)So is there anyway that I can write custom function to run encoding in parallel? Or is there any other library that can do in parallel other than using spark?"
2298,Keras Movie Review Sentiment Classifier: What is the role of GlobalAveragePooling1D layer?,"['keras', 'text-classification']",I was reading some IMDB movie review sentiment classifier on top of Keras. Here is the model definition:What I don't understand is the role of GlobalAveragePooling1D here. 
2299,KeyError - Pandas,"['machine-learning', 'text-classification', 'sklearn-pandas']",I found a code for text classification in tensorflow and when I try to run this code: https://www.tensorflow.org/beta/tutorials/keras/feature_columns I get an error.I used the dataset from here: https://www.kaggle.com/kazanova/sentiment140When I printed df.index.name I got NONE. So is the dataset not correct or am I doing something wrong?I changen the dataframe.head() to print(dataframe.head()) and got this output:
2300,"when checking target: expected dense_2 to have shape (1,) but got array with shape (2,)","['python-3.x', 'encoding', 'deep-learning', 'sentiment-analysis', 'text-classification']","sentiment analyses with csv contains 45k with two cols[text,sentiment],trying to use sigmoid with binary_crossentropy but its return an error :Error when checking target: expected dense_2 to have shape (1,) but
  got array with shape (2,)i have tried to use LabelEncoder , but its return, bad input shape, how do i let the encoding label acceptable for Sigmond 1 dense ?"
2301,Not able to upload Dataset into AutoML Natural Language text classification GUI,"['google-cloud-platform', 'text-classification', 'automl']","I'm trying to perform custom text classification by using AutoML on Google Cloud Platform. I am using the official google documentation to help me get started. The link to the blog is https://cloud.google.com/blog/products/ai-machine-learning/no-deep-learning-experience-needed-build-a-text-classification-model-with-google-cloud-automl-natural-languageIn the above above blog they have used the 20 Newsgroup dataset. After preparing the dataset and following the instructions given here I am getting an error while uploading the dataset into GCP AutoML Text Classification GUI. I have also tried to upload a csv file with just one data entry that also doesn't seem to work. Every time I try to upload dataset I get the following error ERROR CODES:
4
Last error message
CSV file is empty"
2302,How to save a text classification model and test it later on a new unseen data,"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'natural-language-processing']","I am a newbie to python and working on a binary text classification problem. I have developed a text classification model. Now I want to save that trained model and reload it again to test it on a new test data file. I tried pickle, and joblib for this task and some other suggested methods here at stack overflow but unable to do this. With one method, I successfully saved my model but couldn't test it on a new test data file. Any help shall be highly appreciated. Apologies if I couldn't explain the problem well as I am new to python.I also tried this one.One more solution I tried.Despite trying multiple solutions, I couldn't get the required results. I may be doing some blunder due to my less expertise in machine learning and python. May someone please point out where I am doing mistake. Thanks in anticipation."
2303,invalid literal for int() with base 10 with GRU module,"['keras', 'deep-learning', 'text-classification', 'valueerror']","My input is simply a csv file with 50K rows and two columns for Arabic sentiment analyses  : but im keep getting error while trying to train my data on a stacked GRU modelkeep getting the below errorValueError: invalid literal for int() with base 10: 'اللهم اني احسن
  التدبير فادبر امري'"
2304,Is there a way to evaluate losses on the test sample using spacy model,"['nlp', 'spacy', 'text-classification', 'loss-function']",I am trying to create a binary classifier with spacy 2.1.3 and in order to perform an overfitting test I would like to evaluate losses on the test sample. In their tutorial losses are used as a parameter and somehow updated:https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py#L90 I cannot find any example of how to evaluate it on my test sample. Ideally I would like to produce plots as show here:https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/I tried digging into their code but I didn't find anything useful. Has anyone tried to produce similar plots? Thank you for your help and comments :) 
2305,ValueError: Classification metrics can't handle a mix of multiclass and multilabel-indicator targets,"['python', 'machine-learning', 'lstm', 'data-science', 'text-classification']",I have Multi class labeled text classification problem with 2000 different labels. Doing classification using LSTM with Glove Embedding.Getting below error after 1st epoch:Can you please tell me where I did mistake in my code?
2306,numpy refused to retrieve concatenated values,"['numpy', 'deep-learning', 'text-classification']","numpy refused to retrieve concatenated valuesafter searching i have found that This is a numpy version problem. using version > 1.11.2, which doesn't support float index.
i have change them to np.int() but still the problem existthis the code"
2307,Is there a fast way to train many models at the same time?,"['python', 'classification', 'text-classification', 'naivebayes', 'multiclass-classification']","I want to train a 2-way classifier, that is, assume I have 4 classes that I want to classify a text to. I don't want to group all the training data in one training set and the labels then would be 4 labels. Rather, I want to make a binary labels. For example, I have to first make 4 copies of the dataset, and then, I make label A and the rest Not A, and then the second dataset would be B and Not B and so on..After that, I have to make 4 models(naive bayes for example) and train every dataset I made. What I want is a method to do all of that without all of this work. Is that possible?"
2308,"How to get text Prediction accuracy from the saved model""","['python', 'tensorflow', 'machine-learning', 'text-classification', 'tensorflow.js']","New to tensorflow, html, and stuck badly in text classification. i'am trying to detect positive and negative polarity of text. trained the model in browser on manually filtered text (sentences) for both neagative and postive classes and saved it in .JSON and .BIN File.Loaded back the files by user inputsummary of model loaded, trained and saved from small portion of datastuck in Re_creating model ( ERROR:  Uncaught TypeError: Sequential model cannot be built: model is empty. Add some layers first.)What i need to do is LOAD MODEL, this model should predict the polarity of user input text as negative/positive, detection accuracy. Any one can help please in bit detailed, As learning it but not able to get it via tutorials from https://www.tensorflow.org/jsModel"
2309,How to predict with Word2Vec?,"['python', 'gensim', 'word2vec', 'text-classification']","I'm doing arabic dialect text classification and I've used Word2Vec to train the model, I got this so far:What do I do now to predict a new text if it's of any of the 5 dialects I have?
Also, I looked around and found this code: But it gives me this error when I run it and load my trained word2vec model:Actually, there's another code that I didn't post here, I wanted to use word2vec with neural networks, I have the code for neural network, but I don't know how to make the features I got from word2vec to be as an input to the neural net and with labels as output. Is it possible to connect word2vec to a deep neural net and how?"
2310,How to know the feature count of a specific class in text classification?,"['python', 'scikit-learn', 'nlp', 'text-classification', 'naivebayes']",I'm trying to calculate the prediction probabilities for a certain text. There're problems with the classification of those texts. The text should be classified as class B but it's assigned class A. So I want to calculate the probability manually. But I need the feature count for Class A and B. All what I have is the total feature count which is 60343. So how can I get the feature count for each class?I'm doing dialect text classificaiton. and using scikit learn.
2311,Google Cloud Natural Language API Classifying Plaintext vs Html,"['google-cloud-platform', 'text-classification', 'google-natural-language']","I want to use Google Natural Language API to classify query results: Classifying contentThe query results, which I want to classify, are available in HTML and plain text. The official documentation says that the API accepts both types Document.Type.PLAIN_TEXT and Document.Type.HTML. Because the HTML format has additional annotations like e.g. <b>important text</b>, I am wondering which format is better to achieve the best classification result possible?"
2312,Don't understand the HashingVectorizer from sklearn,"['python-3.x', 'scikit-learn', 'nlp', 'vectorization', 'text-classification']","I'm using HashingVectorizer function from sklearn.feature_extraction.text  but I do not understand how it works.My codeMy resultI read a lot of paper on the Hashing Trick, like this article https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f I understand this article but do not see the relationship with the result obtained above.Can you explain me with simple example how work HashingVectorizer please"
2313,How to have multioutput in text classification?,"['python', 'scikit-learn', 'nlp', 'text-classification']","I'm doing dialect text classification. The problem is some tweets, can be classified as both dialect A and B, how can I do that? I want to do it and then automatically calculate the accuracy, I don't want to do it manually. When I don't classify them as both A and B, it gives me many misclassified texts. In the training though, they're not classified as both dialect A and B. but separately. "
2314,How can I add features other than BOW to scikit-learn classification models,"['python', 'scikit-learn', 'text-classification']","I'm trying to build a text-classification model. My goal is to classify each paragraph in 10000 documents into whether it has information on 'Labor Strike Event' or not.First, I followed the basic preprocessing steps and transformed the paragraphs into a term-document matrix (CountVectorizer). Then feed this matrix to the models such as logistic regression and svm provided by scikit-learn.However, I found the information on the topic of documents, which is given by pre-trained LDA model would be helpful for paragraph level text-classification. Hence I want to also feed information on topics along with BOW information for each text.Following the answer to the similar question, I think that simply adding topic number at the last column of term-document matrix.  For example, let 'text_train' be my training corpus (say 40000 paragraphs) and 'topic' be the list of topic number given to each paragraph by LDA model.This returns the desired matrix whose last column has information on topics.However, since the topic numbers do not have cardinal (or ordinal) order like elements of term-document matrix does, I am worried that some linear machine learning model might not be proper for this work.Is is true? Then is there any other solutions that I can adopt?"
2315,How can I classify big text data with scikit-learn?,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","I have large database of 50GB in size, which consists of excerpts of 486,000 dissertations in 780 specialties.
For scientific purposes, it is necessary to conduct training on the basis of this data. But alas, resources are limited to a mobile processor, 16 GB of memory (+ 16Gb  SWAP) The analysis was carried out using a set of 40,000 items (10% of the base) (4.5 GB) and the SGDClassifier classifier, and the memory consumption was around 16-17 GB.Therefore, I ask the community for help on this. currently my code is similarTherefore, I ask for advice on how to optimize this process so that I can process the entire database."
2316,How to compute the perplexity in text classification?,"['nlp', 'text-classification', 'naivebayes', 'countvectorizer', 'perplexity']","I'm doing dialect text classification with scikit learn, naive bayes and countvectorizer. So far I'm only doing 3 dialects text classification. I'm going to add a new dialect(or actually, the formal language for those dialects). The problem is, the new text that I'm going to add, shares a lot of words with the other 3 dialects. So I read the following in a research document:We train an n-gram model for each dialect from the collected data. To
  train the MSA model, we select sentences from Arabic UN corpus and
  news collections. All the dialect and MSA models share the same
  vocabulary, thus perplexity can be compared properly. At
  classification time, given an input sentence, the classifier computes
  the perplexity for each dialect type and choose the one with minimum
  perplexity as the label.They mean by MSA(Modern Standard Arabic) which is the formal language for those dialects. How are they  calculating the perplexity? Are they just using naive bayes or there's more to it?"
2317,How to avoid overfitting by entities in short text classifcation?,"['python', 'scikit-learn', 'logistic-regression', 'cross-validation', 'text-classification']","I am binary classifying headlines. This headlines are between 1 to 7 words, and sometimes include the name of the person who created them or locations.
I am able to classify with 81% precision but when I examine the features (words) that influence to each classification group I notice that many names classify best a specific group because they by random were presence only on 1 group.I though about 2 solutions: 
1. Using NER to filter entities
2. Increasing regularization (still not succeeded with it)What is the best way to handle this overfitting? 
And if increasing the lambda value is the answer, so how it is done with sklearn LogisticRegressionCV?The relevant code: And some na,es example features:"
2318,Number of features of the model must match the input in python,"['python', 'text-classification']","I have created a model of text classifier using this link: https://stackabuse.com/text-classification-with-python-and-scikit-learn/ 
And then I tried to check it and use with my own data which is not it dataset. But, It says that number of features does not match.
Here my code for that:ValueError                                Traceback (most recent call last)
 in 
----> 1 y_pred = model.predict(X)~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in predict(self, X)
    541             The predicted classes.
    542         """"""
--> 543         proba = self.predict_proba(X)
    544 
    545         if self.n_outputs_ == 1:~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in predict_proba(self, X)
    581         check_is_fitted(self, 'estimators_')
    582         # Check data
--> 583         X = self._validate_X_predict(X)
    584 
    585         # Assign chunk of trees to jobs~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in _validate_X_predict(self, X)
    360                                  ""call fit before exploiting the model."")
    361 
--> 362         return self.estimators_[0]._validate_X_predict(X, check_input=True)
    363 
    364     @property~\Anaconda3\lib\site-packages\sklearn\tree\tree.py in _validate_X_predict(self, X, check_input)
    386                              ""match the input. Model n_features is %s and ""
    387                              ""input n_features is %s ""
--> 388                              % (self.n_features_, n_features))
    389 
    390         return XValueError: Number of features of the model must match the input. Model n_features is 1500 and input n_features is 86 "
2319,How to reduce the number of features in text classification?,"['python', 'nlp', 'text-classification', 'naivebayes', 'countvectorizer']","I'm doing dialect text classification and I'm using countVectorizer with naive bayes. The number of features are too many, I have collected 20k tweets with 4 dialects. every dialect have 5000 tweets. And the total number of features are 43K. I was thinking maybe that's why I could be having overfitting. Because the accuracy has dropped a lot when I tested on new data. So how can I fix the number of features to avoid overfitting the data?"
2320,Datasets for Document Classification problem,"['python', 'scikit-learn', 'nltk', 'text-classification']","I am doing a project to make a application that can take pdf and docx documents as input and classify them into various categories such as
- Financial
- Government and Political
- Sports and Entertainment
- Technology and Scientific
- Sensitive ( Personal and Government )
But I was not able to find enough data that can be used to train the ML model.Can you please point me to some places where I could find data sets that could be used for my project. I currently only have the BBC news dataset. I would prefer if the datasets are in .txt format or in a format that can be converted to .txt with some code or software."
2321,Am I having an overfitting problem with my text classification?,"['python', 'scikit-learn', 'nlp', 'text-classification', 'countvectorizer']","I'm doing dialect text classification with 4 dialects. I split the dataset(of size 20K) into 75% training and 25% testing. When I trained them with naive bayes, and tested on the test dataset I got 90% accuracy. But when I did validation with a new dataset of size 400 tweets, I got accuracy of 63%. Do you think that drop is caused by overfitting? If there's any other information you need please tell me. This is my code:Those are my setting for for naive bayes and countVectorizer. I didn't add any hyperparameters for naive bayes. I'm not sure what to add or it it's gonna make any noticeable difference. this is the confusion matrix:for trainingfor testingfor new dataImportant notes:"
2322,I get isnan error when I merge two countvectorizers,"['python', 'scikit-learn', 'nlp', 'text-classification', 'countvectorizer']","I'm going dialect text classification and I have this code:When I run this code I get this error:I was following the code in this post:
Merging CountVectorizer in Scikit-Learn feature extractionAlso, how can I train and predict afterwards?"
2323,Why WEKA considers data as attributes?,"['machine-learning', 'nlp', 'weka', 'text-classification']","I am trying to predict a category of given string. Below is the sample of arff file i have(REDACTED)I believe the attributes in the above set are Text and Class. However the program recognises each and every work in the Data as attribute. Is this expected?I am following the weka docs along with https://www.codingame.com/playgrounds/6734/machine-learning-with-java---part-5-naive-bayes
https://github.com/nsadawi/WEKA-API/tree/master/srcThanks, 
Jai"
2324,How to gain insights in text classifications with error analysis?,"['python', 'machine-learning', 'nlp', 'text-classification', 'countvectorizer']","I'm doing text classification for dialect. And I'm using naive bayes classifier with countVectorizer. I'm having many misclassified texts. Is there a method to analyse those errors to figure out where the classification is going wrong? For example, if I could know what words are being used to misclassify text as A, while it should be classified as B and then I can take those out words from the corpus of A.  Should switch to unsupervised learning using clustering? or neural networks and deep learning? if naive bayes classifier isn't working. Also, how can I know how countVectorizer classified the documents? And which words he used to classify a document."
2325,How to concatenate two countVectorizers?,"['python', 'concatenation', 'text-classification', 'n-gram', 'countvectorizer']","I'm using text classification to identify dialects. I'm using sklearn and countVectorizer, I want to train naive bayes classifier on both character-based ngrams and vocabularies. So I have the following settings on two countVectorizers:Where do I go from there? I tried this:as was suggested in this post:
How to use bigrams + trigrams + word-marks vocabulary in countVectorizer?but didn't work"
2326,Text classification method,"['python', 'algorithm', 'machine-learning', 'text-classification', 'text-database']","I'm trying to classify sentences on a given topic using machine learning. However, I can't seem to find the adequate algorithm / solution for this particular problem.Some details:I have tokenized, lemmatized and vectorized the sentences. So, given a sentence:It gets tokenized:It then gets lemmatized:And then based of a small dictionary (~100 words) I built, the sentenced gets transformed in a sequence of 0 or 1, signaling if the words appear in the dictionary or not:I've built myself a small dataset (~50 sentences split in 3 topics) and now I need an algorithm that will train on the dataset and that'll predict one of those 3 classes, given a new sentence.Deep learning is not efficient given the reduced size of the dataset. I've tried liniar regression but outputs random very big numbers. Any ideas on what should I try or if any mistakes were made?"
2327,How to train a classifier to detect vernacular from grammatical language?,"['python', 'machine-learning', 'nlp', 'text-classification']","I'm using text classification to classify Arabic dialects, so far I have 4 dialects. However, now I want the classifier to detect the formal(standard or grammatical) language of those dialects which is called MSA(Modern Standard Arabic). Should I use grammatical analysis? build a language model? or I do the same as I did with the dialects by collecting MSA tweets and then train them?"
2328,What is the most efficient way to extract tweets that has certain dialect?,"['python', 'twitter', 'dataset', 'text-classification']","I'm doing text classification for Arabic dialects, and I need to collect data. So I'm using Twitter API to do that.I need to find tweets that have the same dialect.Is to collect tweets based on certain keywords only one dialect haveWhen I test the data, of course the accuracy will be high. Because the test data will contain those keywords that I used to collect the dataset.Isn't there another way to circumvent this bias?"
2329,Is there a way to find the most representative set of samples of the entire dataset?,"['machine-learning', 'scikit-learn', 'nlp', 'data-science', 'text-classification']","I'm working on text classification and I have a set of 200.000 tweets.The idea is to manually label a short set of tweets and train classifiers to predict the labels of the rest. Supervised learning.What I would like to know is if there is a method to choose what samples to include in the train set in a way that this train set is a good representation of the whole data set, and because the high diversity included in the train set, the trained classifiers have considerable trust to be applied on the rest of tweets."
2330,How to extract manually annotated tweets using Twitter API?,"['twitter', 'text-classification', 'corpus', 'tagged-corpus']","I'm using text classification to classify dialects. First I need a large manually annotated tweets, and I have read a research paper that says:We have collected tweets that were published during June 2015. Arabic
  linguists manually annotated a small part of these tweets, so we got
  51,589 tweets with correct dialectal labels. These tweets were
  manually found in Twitter and annotated by the linguists.So this researcher was able to extract those tweets, I wanted to contact him but their emails weren't valid. He says those tweets were published during June 2015. How can I extract those tweets?"
2331,How to use bigrams + trigrams + word-marks vocabulary in countVectorizer?,"['machine-learning', 'nlp', 'text-classification', 'countvectorizer']","I'm using text classification with naive Bayes and countVectorizer to classify dialects. I read a research paper that the author has used a combination of :He means by word-marks here, the words that are specific to a certain dialect.How can I tweak those parameters in countVectorizer? So those are examples of word marks, but it isn't what I have, because mine are arabic. So I translated them.Those are used to classify a text.Also, in the this post:
Understanding the `ngram_range` argument in a CountVectorizer in sklearnThere was this answer :I couldn't understand the output, what does [1,1] mean here? and how was he able to use ngram with vocabulary? aren't both of them mutually exclusive? "
2332,How can I know the labels of my predicted classification?,"['python', 'machine-learning', 'text-classification', 'multiclass-classification']","I have trained my classifier on 3 dialects using text classification. And this was the confusion matrix and precision:confusion matrixThe precisionHow to know which row in the confusion matrix and which element in the precision belong to what dialect I have? I provided the training data to the classifier with the following labels :Here's the code, I used RandomForestClassifier:output:"
2333,Why scikit learn confusion matrix is reversed?,"['scikit-learn', 'text-classification', 'confusion-matrix', 'performance-measuring']","I have 3 questions:1)The confusion matrix for sklearn is as follows:While when I'm looking at online resources, I find it like this:Which one should I consider? 2) Since the above confusion matrix for scikit learn is different than the one I find in other rescources, in a multiclass confusion matrix, what's the structure will be? I'm looking at this post here:
Scikit-learn: How to obtain True Positive, True Negative, False Positive and False Negative
In that post, @lucidv01d had posted a graph to understand the categories for multiclass. is that category the same in scikit learn? 3)How do you calculate the accuracy of a multiclass? for example, I have this confusion matrix:In that same post I referred to in question 2, he has written this equation:ACC = (TP+TN)/(TP+FP+FN+TN)but isn't that just for binary? I mean, for what class do I replace TP with? "
2334,Mapping doc2vec paragraph representation to its class tag post-training,"['python', 'gensim', 'word2vec', 'text-classification', 'doc2vec']","I have trained Doc2Vec paragraph embeddings on text documents using the Doc2Vec module in Python's gensim package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see this link for details):However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:My question is as follows: I trained the model of n classes of documents, yielding n document vectors in model.docvecs. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?"
2335,How to make text classification gives a None category,"['python', 'machine-learning', 'text-classification', 'countvectorizer']","I'm doing text classification for dialects. After I trained it for 3 types of dialects, I tested it with the test data I have. However, now suppose I'm going to extract a tweet from twitter, and ask the classifier to output the corresponding dialect, but what if the tweet wasn't written in any of those 3 dialects? I assume that he will give a category regardless, but that would be false positive. Therefore, I want him to give a None category. How to do that? Should I also give training data with None labels?"
2336,Is Naive Bayes biased?,"['machine-learning', 'nlp', 'text-classification', 'naivebayes']","I have  a use case where in text needs to be classified into one of the three categories. I started with Naive Bayes [Apache OpenNLP, Java] but i was informed that the algorithm is biased, meaning if my training data has 60% of data as classA and 30% as classB and 10% as classC then the algorithm tends to biased towards ClassA and thus predicting the other class texts to be of classA. If this is true is there a way to overcome this issue? There are other algorithm that i came across like SVM Classifier or logistic regression (maximum entropy model), however I am not sure which will be more suitable for my use case. Please advise."
2337,Text classification issue,"['tensorflow', 'machine-learning', 'keras', 'text-classification']","I'm newbie in ML and try to classify text into two categories. My dataset is made with Tokenizer from medical texts, it's unbalanced and there are 572 records for training and 471 for testing.It's really hard for me to make model with diverse predict output, almost all values are same. I've tired using models from examples like this and to tweak parameters myself but output is always without senseHere are tokenized and prepared dataHere is script: GistSample model that I usedUnfortunately I can't share datasets. 
Also I've tired to use keras.utils.to_categorical with class labels but it didn't help"
2338,How do I group similar categories?,"['python', 'python-3.x', 'nlp', 'classification', 'text-classification']","I have about 1200 tv show categories .. like Drama, News, Sports, Sports-non event, Drama Medical, Drama Crime.. etcHow do I use NLP so that I get groups such that Drama, Drama medical and Drama Crime group together and Sports, Sports-non event etc group together and so on... basically the end goal is to reduce the 1200 categories to very few broad categories.Till now I have used bag of words to build a dictionary with 146 words.."
2339,How to use text classification with dataframe in python,"['python', 'dataframe', 'machine-learning', 'text-classification', 'countvectorizer']","I'm using text classification to classify dialects. However, I noticed that I have to use countVectorizer like so:what happens is that I have make a new text file for every line in my csv file. I have collected 1000 tweets from twitter. and they're labeled. and I have them as csv in one file.  I have 2 questions:"
2340,"Short string classification, high acc tons of false positives. ¿Are we on the right path? [closed]","['python', 'machine-learning', 'scikit-learn', 'classification', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed last year. TL;DR AT THE END I've been working on feature extraction of documents with multiple frameworks for a few months, and recently the project has found a dead end.I'm aiming to find any kind of identification string within the document.Thanks in advanceLet's say that the project is structured into ""modules"" that are called separately, and the latest one that has been in development is aiming to find identification numbers within the document as I've said.For example:To avoid applying the module to the entire document and enhancing the accuracy, I'm finding a label and extracting the text located near it either to its right or down below it (I'm following occidental reading order for the time being, left to right, top to bottom). THIS PART WORKSThe thing is that there's not a normalized way of assigning an identifier to something, so you can expect ANY kind of string.After that dead long introduction:What I've tried: 
* Applying one or multiple regexes to the extracted text.And the solution I'm currently trying (without results as the title says)
* Convert the extracted text to a single string replacing each type of character with a generic and then apply n-grams with scikit.With the following function, any given string is replaced so letters are ""a"", capital letters ""b"", digits ""c"", whitespaces ""d"" and so on...At first, it seemed like a good approach since we got an accuracy of 0.931280The dataset contains over 75k rows of manually labeled True and False identifiers. 55%~ of which are 1 (it's an identifier) and the remaining FalseWith the above setup, this are the results:Since this approach doesn't seem to work, I'm kind of at a loss.
Given that in the end it's binary text classification, what approach should I follow next? I imagine that it's hard to classify that kind of strings, but I'm ok even with a 65 - 75% of accuracy.I've also tried multinomial naive bayes and SVC.TL;DRWorking on a binary text classifier (either True or False) which aims to discriminate whether a string is an identifier or not.I got 93% accuracy with Naive Bayes and CounterVectorizer but a lot of false positives comes up.
I've already tried SVC and the Dataset contains ~40k rows of True strings and ~35k rows of False cases manually labelled.Hardware is not an issue.Can you give me any recommendations? Any kind of approach?Much appreciated."
2341,Finding The Most Relevant or Important Features for SVM using SGD (loss=hinge),"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'sklearn-pandas']","I am working on a text-classification problem and have found that SVM is performing best for my text-classification problem. However, I did my experiment using sklearn's SGD classifier (loss=hinge).LIME seems to provide a way to analyze the instance and show the analysis for each class given an instance. However, the problem with LIME is:in the explainer.explain_instance function, it requires a probability distribution as the second parameter (c.predict_proba) and there is no predict_proba available for SGD (loss=hinge). I have tried using CalibratedClassifierCV but I am getting different results with it. The objective here is to find the most relevant/important features for each class in a multi-class text-classification problem. The purpose of this is to analyze the results and discuss their differences in a research paper. 
Open to any suggestions or alternatives. Also, I could possibly use sklearn's SVC instead of SGD as it has the predict_proba attribute. However, the problem is with SGD I already have a set of parameters for which I am doing the analysis and I couldn't find a way to exactly convert the SGD's configuration to SVC's configuration. "
2342,how to deal with fasttext library to build a text classifier?,"['python', 'sentiment-analysis', 'text-classification', 'fasttext']","I am doing sentiment analysis on twitter dataset in Arabic , and finished the phase of preprocessing on data .I want to use fasttext tool to build a classifier but I do not know how , I need some clear steps to upload my data and build the classifier , any help ?"
2343,How to classify a text in a category using a decision Tree,"['r', 'decision-tree', 'text-classification']","From 2 RSS feeds, I have imported some text data, and then created 2 Document Term Matrixes (DTM's). I have been working on various stats on this DTM like most frequent terms etc, but now I want to use these two DTM's, and with 70% percent of the data train a model (Decision tree) that will be able to identify a news feed in one of the 2 categories (Education and Environment), then I will use the remaining 30% to test its performance. 
How can I do this? 
What I have done until now is:How will I procede from there? Should I join the 2 dataframes before training the model? How will I join them and what happens to the common terms if applicable? 
How about the requirement of 70% of samples being used for the training. Should I take 70% before joining the tables?"
2344,Finetuning BERT on custom data,"['tensorflow', 'deep-learning', 'nlp', 'text-classification', 'bert-language-model']","I want to train a 21 class text classification model using Bert. But I have very little training data, so a downloaded a similar dataset with 5 classes with 2 million samples.t 
And finetuned downloaded data with uncased pretrained model provided by bert.
And got about 98% validation accuracy.
Now, I want to use this model as pretrained model for my small custom data.
But I am getting shape mismatch with tensor output_bias from checkpoint reader error as the check-point model has 5 classes and my custom data has 21 classes."
2345,How to classify text data with hundreds of classes and less amount of samples in each class,"['machine-learning', 'nlp', 'data-science', 'text-classification']","I have a dataset that contains around 10000 small paragraphs and the paragraphs belong to classes. There are around 80 - 100 classes. The paragraphs can be organized in hierarchies. I want to build a classifier model that will predict the class of an unseen paragraph.Currently what I have done is, I have implemented a two step classification using FastText. First I classify the unseen text to a top level class and then using another classifier I classify it to a subclass of the identified top level class. This helped me to increase the accuracy.Is there a better way to do this? Is there any good hierarchical classifier like https://github.com/globality-corp/sklearn-hierarchical-classification out there for text classification? Or can this be improved using FastText itself in some way?"
2346,Replacing empty texts - text embedding,"['machine-learning', 'nlp', 'artificial-intelligence', 'text-classification', 'fasttext']","I am trying to embed texts, using pre-trained fastText models. Some are empty. How would one replace them to make embedding possible? I was thinking about replacing them with dummy words, like that (docs being a pandas DataFrame object):
        docs = docs.replace(np.nan, 'unknown', regex=True)However it doesn't really make sense as the choice of this word is arbitrary and it is not equivalent to having an empty string.Otherwise, I could associate the 0 vector embedding to empty strings, or the average vector, but I am not convinced either would make sense, as the embedding operation is non-linear."
2347,How can I test the model I created with Keras,"['python', 'keras', 'text-classification']","I was working on a text classification problem with Keras. But
I tried to test the model I created, but I cannot use the TfidfVectorizer to test the class.After installing the model I have prepared a test list to use.No problem so farBut..I get such an errorAnd I also triedbut I have received such an error, I learned that the fit () method should come before this can not be used.But I still can't test the model I'm training"
2348,How to use SHAP with a linear SVC model from sklearn using Pipeline?,"['scikit-learn', 'pipeline', 'text-classification', 'svc', 'shap']",I am doing text classification using a linear SVC model from sklearn. Now I want to visualize which words/tokens have the highest impact on the classification decision by using SHAP (https://github.com/slundberg/shap).Right now this does not work because I am getting an error that seems to originate from the vectorizer step in the pipeline I have defined - whats wrong here?Is my general approach on how to use SHAP in this case correct?This is the error message I get:
2349,How to determine most Important/Informative features using Linear Support Vector Machines (SVM) classifier,"['python', 'machine-learning', 'scikit-learn', 'svm', 'text-classification']",I am new to python and working on a text classification problem. I am interested in the visualization of the most important features of each class through a linear SVM classifier model. I want to determine which features are contributing towards the classification decision as Class-1 or Class-2 by classification model. This is my code.I have read all related questions available on this platform but I found the following useful code which I added in my code. This code works for naive Bayes and logistic regression and it gives the most important features but for SVM it gives me the error.I am getting this error.Any help shall be highly appreciated.
2350,Text Classification Approach,"['python', 'machine-learning', 'nlp', 'classification', 'text-classification']","I have data with 2 important columns, Product Name and Product Category. I wanted to classify a search term into a category. The approach (in Python using Sklearn & DaskML) to create a classifier was:I realized the OneHotEncoder (or any encoder) converts the text to numbers by creating a matrix keeping into account where and how many times a word occurs. Q1. Do I need to convert from Word to Vectors before train-test split or after train-test split?Q2. When I will search for new words (which may not be in the text already), how will I classify it because if I encode the search term, it will be irrelevant to the encoder used for the training data. Can anybody help me with the approach so that I can classify a search term into a category if the term doesn't exist in the training data?"
2351,Error while calculating probabilities using CalibratedClassifierCV python,"['python', 'scikit-learn', 'svm', 'text-classification']","I'm trying to classify text using SGDClassifier(loss='hinge'), for which I want to get their probabilities as well. Since for SGDClassifier(loss='hinge') doesn't have a predict_proba(), I went through this post and got to know that it could be achieved through CalibratedClassifierCVI have a dataframe with small text and their corresponding classGives ErrorDoes anyone has any idea about this?"
2352,How to test new word set against my NLP Naive bayes classifier,"['python', 'nlp', 'data-science', 'text-classification', 'naivebayes']","i build a NLP classifier based on Naive base  using python scikit-learn the point is that , I want my classifier to classify a new text "" that is not belongs to any of my training or testing data set"" in another model""like regression"" , I can extract the Theta's values so that i can predict any new value. however i know that,naive based is working by calculation the probability of each word to against every class . for examplemy data set include (1000 record of some text) as 
"" it was so good "" 
"" i like it "" 
"" i don't like this movie "" 
etc .. and each text is classified as either +ev or -ev i do separation to my data set into training and testing set. every thing is ok .now i want to classify a brand new text like "" Oh, I like this movie and the sound track was perfect"" how to make my model predict this text !here is the code now iam expecting to do some kind new text like ""good movie and nice sound track"" and ""acting was so bad"". and let my classifier predict was it good or bad ! Xnew = [[""good movie and nice sound track""], [""acting was so bad""]]
ynew = classifier.predict(Xnew)also I wonder if i can get all the probability for each word in my NLP Bag of my corpus.thank's in advance"
2353,Deal with imbalanced dataset in text classification with Keras and Theano,"['python', 'keras', 'binary', 'conv-neural-network', 'text-classification']","For ~20,000 text datasets, the true and false samples are ~5,000 against ~1,5000. Two-channel textCNN built with Keras and Theano is used to do the classification. F1 score is the evaluation metric. The F1 score is not bad while the confusion matrix shows that the accuracy of the true samples is relatively low(~40%). But actually it is very important to predict the true samples accurately. Therefore, want to design a custom binary cross entropy loss function to increase the weight of mis-classified true samples and make the model focus more on predicting accurately on the true samples.The sample code of the cost sensitive loss function focusing on the mis-classified samples is:Actually, a custom loss function for binary classification implemented with Keras and Theano that focuses on the mis-classified samples is of great importance to the imbalanced dataset. Please help troubleshoot this. Thanks!"
2354,how to text classification using LSTM,"['keras', 'deep-learning', 'nlp', 'lstm', 'text-classification']","I am doing text classification using LSTM model, I got 98% accuracy in validation data but when I  am submitting It gets 0 scores, please help me how to do, I am a beginner to  NLP.
 I have data like this  I am applying tokenizer here :I am applying  sequence padding here:Here my model:Here my epochs:after I will applied predict function to test data:
my submission file like this :my actual submission file like this :"
2355,How to identify text templates patterns in a string dataset?,"['python', 'algorithm', 'text', 'text-classification']","I am trying to find an efficient way to process a list of text records and identify the text templates commonly used in the records, only keeping the fixed part and abstracting the variable also counting the number of records matching each identified template.——My most successful attempt at tackling the challenge involves splitting text records into array of words, comparing arrays of the same size word per word to write up templates found into a list of template.As you might expect, it’s not perfect and struggles to run for datasets longer than 50,000 records.I was wondering if there were some text-classification libraries that would be more efficient or faster logic to improve the performances, my current code is very naive...——This is my first attempt in Python, using a very simple logic.Ideally, the code should take an input such as the below:and output a list of template as well as the number of records that they match."
2356,How can I convert a Pandas DataFrame of vectors and labels into input for an RNN in TensorFlow,"['python', 'tensorflow', 'machine-learning', 'text-classification']","I'm working on a text classifier using an LSTM in TensorFlow and can't figure out the format of the input data.
My input data is a Pandas Dataframe with one feature column and one label column.My feature column is a 2D array representing an array of vectors and my label column is a String, an example of my data input is below.How do I convert this Dataframe into a dataset that can be used as input to be used in a Tensorflow.Keras model?I've tried converting the Dataframe into a TensorFlow.Dataset dataset using tf.data.Dataset.from_tensor_slices but this produces a TypeErrorThis produces the following error:"
2357,“Number of features of the model must match the input” while trying to predict new unseen data,"['python', 'numpy', 'text-classification']","I trained a model on some Wikipedia articles divided by two categories (each category has 12 articles).Below is how I created the model, trained it and pickled it:Then, I loaded the pickle file and tried to predict the classification for a new unseen article:I got the following error while calling the predict function:Number of features of the model must match the input. Model n_features is 10 and input n_features is 47It seems like the new article got a numpy array of 47 features, while the trained model works with arrays of 10 features. I'm not sure I understood this correctly, I'd be glad if you can help me understand better and make it work.Thanks!"
2358,How do i input doc2vec vectors combined with categorical features in a CNN to predict a class?,"['python', 'keras', 'neural-network', 'nlp', 'text-classification']","I have a dataset with 2 features , one is a text feature and the other is a categorical feature. I want to predict the category based on these two features. I am using doc2vec to convert the text column into vectors and one hot encoding the categorical feature . After converting both the column, I don't know how to combine both of these features and feed into a neural network, say a CNN . Please guide me what approach should I take? I am new to this, please don't mind the shortcomings.My dataset and some of the code:"
2359,I want to predict the no.of updates for a new incident? how to do that in python?,"['python', 'knn', 'text-classification', 'naivebayes', 'workload']",I have a data set of n incidents which has some information to it. Information such as description (text is in either english or german) and no.of updates information(in intiger). I want to predict the no.of updates for a new incident regardless of the defined language(german or english).I just started learnig python. Please suggest the action plan libraies/algortims used in python?
2360,Error multiclass text classification with pre-trained BERT model,"['python', 'text-classification', 'multiclass-classification', 'transfer-learning']","I am trying to classify text in 34 mutually exclusive classes using Google's BERT pre-trained model. After preparing the ""train"", ""dev"" and ""test"" TSV files which BERT expects as input, I try to execute the following command in my Colab (Jupyter) NotebookI get the following errorIn the ""run_classifier.py"" script, I have modified the ""get_labels()"" function, originally written for a binary classification task, to return all my 34 classesAny idea what is wrong or if I am missing additional necessary modifications?Thanks!"
2361,Implementation of n-grams in python code for multi-class text classification,"['python', 'text-classification', 'n-gram', 'natural-language-processing', 'trigram']","I am new to python and working on the multi-class text classification of contract documents of the construction industry. I am facing problems in the implementation of n-grams in my code which I produced form by getting help from different online sources. I want to implement unigram, bi-gram, and tri-gram in my code. Any help in this regard shall be highly appreciated.I have tried bigram and trigram in my Tfidf part of my code but it is working.File ""C:\Users\fhassan\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 328, in 
    tokenize(preprocess(self.decode(doc))), stop_words)File ""C:\Users\fhassan\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 256, in 
    return lambda x: strip_accents(x.lower())File ""C:\Users\fhassan\anaconda3\lib\site-packages\scipy\sparse\base.py"", line 686, in getattr
    raise AttributeError(attr + "" not found"")AttributeError: lower not found"
2362,How to implement a multi-label text classifier in Keras?,"['keras', 'deep-learning', 'nlp', 'text-classification', 'multilabel-classification']","I've been trying to create a multi-label text classifier that uses GloVe embeddings using Keras. I'm currently experimenting with the TREC-6 data set available at http://cogcomp.org/Data/QA/QC/. I'm only considering the 5 broad labels for my classification problem and am ignoring the sub-labels. Since it's a multi-label classification problem, given a sentence, my neural network should output all labels with a probability greater than 0.1. Issue is that the network almost always classifies a question with only 1 label, which is fine when I'm asking a question that belongs to only one category. When I combine questions from different categories however, it still gives only one label with a high confidence most of the time, although I want all relevant labels to be identified.
I'm absolutely sure the pre-processing steps are correct. I get the feeling that there's some issue in my model.I started experimenting with only CNNs in the beginning by referring to the paper ""Convolutional Neural Networks for Sentence Classification"" at https://www.aclweb.org/anthology/D14-1181.pdf, but on a teacher's advice and after seeing that they fail for long questions with different relevant topics, I tried experimenting with LSTMs and BiLSTMS. I started with this approach https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568 and kept modifying some parameters / adding and removing layers hoping to get a good result but I've failed so far.
I tried copy pasting some code for Attention mechanism and adding that after my LSTM layers as well but it doesn't help.My current model looks somewhat like this. I'll paste most of the rest of my code for clarity. The model and training code is present in the sentence_classifier() function.As you can see, I've used a callback to print my predictions after each step.
I've tried to predict labels for all kinds of questions but I get good results only for questions that fall under one category. For instance,givesas the output. But a more complex question likegives something like as the output although labels like 'location' and 'numeric' are also relevant.
I used a callback to predict the prediction for that question after every epoch and I see something like this.I've tried varying my parameters hundreds of times, especially my network size, batch size and epochs to try and avoid over-fitting.I know my question is ridiculously long but I'm running out of patience and any help would be appreciated.Here's the link to my colab notebook - https://colab.research.google.com/drive/1EOklUw7efOv69HvWKpuKVy1LSzcvTTCk."
2363,Doc2Vec & classification - very poor results,"['python', 'classification', 'gensim', 'text-classification', 'doc2vec']","I have a dataset of 6000 observations; a sample of it is the following:The goal is to predict the job sector of each row based on the job title.Firstly, I apply some preprocessing on the job_title column:Then I do the following with Gensim and Doc2Vec:However, the result which I am getting is 22% accuracy.This makes me a lot suspicious especially because by using the TfidfVectorizer instead of the Doc2Vec (both with the same classifier) then I am getting 88% accuracy (!).Therefore, I guess that I must be doing something wrong in how I apply the Doc2Vec of Gensim.What is it and how can I fix it?Or it it simply that my dataset is relatively small while more advanced methods such as word embeddings etc require way more data?"
2364,"CountVectorizer values work alone in classifier, cannot get working when adding other features","['python', 'scikit-learn', 'classification', 'text-classification', 'countvectorizer']","I have a CSV of twitter profile data, containing: name, description, followers count, following count, bot (class I want to predict)I have successfully executed a classification model when using just the CountVectorizer values (xtrain) and Bot (ytrain). But have not been able to add this feature to my set of other features.ERROR:I did some debugging:Any help would be appreciated. "
2365,NLP - which technique to use to classify labels of a paragraph?,"['python', 'machine-learning', 'text', 'nlp', 'text-classification']","I'm fairly new to NLP and trying to learn the techniques that can help me get my job done. Here is my task: I have to classify stages of a drilling process based on text memos. I have to classify labels for ""Activity"", ""Activity Detail"", ""Operation"" based on what's written in ""Com"" column. I've been reading a lot of articles online and all the different kinds of techniques that I've read really confuses me. The buzz words that I'm trying to understand areI am given about ~40,000 rows of data (pretty small, I know), and I came across an article that says neural-net based models like Skip-gram might not be a good choice if I have small number of training data. So I was also looking into frequency based methods too. Overall, I am unsure which technique is the best for me.Here's what I understand:What approach & sequence of techniques should I use to tackle my problem? If there's any open source Jupyter notebook project, or link to an article (hopefully with codes) that did the similar job done, please share it here."
2366,Not able to load keras trained model,"['python', 'keras', 'deep-learning', 'text-classification']","I am using following code to train HAN Network.
Code LinkI have trained the model successfully but when I tried to load the model using keras load_model it gives me following error-
Unknown layer: AttentionWithContext"
2367,Load a plain text file into PyTorch,"['python', 'nlp', 'pytorch', 'text-classification', 'torchtext']","I have two separate files, one is a text file, with each line being a single text. The other file contains the class label of that corresponding line. How do I load this into PyTorch and carry out further tokenization, embedding, etc? "
2368,Predicting probability score of each classification bin for a given document,"['python', 'nlp', 'logistic-regression', 'text-classification']","I am creating a python model that will classify a given document based on the text. Because each document still needs to be manually reviewed by a human, I am creating a suggestion platform that will give the user the top n-classes that a given document belongs too. Additionally each document can belong to more than one class. I have a training set of documents filled with rich text and their tags.What I would like to do is perform a regression on each document to get a probabilistic score of each classification and return the top 5 highest scored classes.I have looked into Bayes classification models, and recommendation systems and I think a logistic regression will help be better as it returns a score. I am new to machine learning and would appreciate any advice or examples that is modeled after this kind of problem. Thank you.EDIT: Specifically, my problem is how should I parse my text data for ML modeling with logistic regression? Do I need to represent my text in a vector format using Word2Vec/Doc2Vec or a Bag-of-words model?"
2369,"Uneven K-means clustering, identical data in two clusters. Python","['python', 'machine-learning', 'cluster-computing', 'k-means', 'text-classification']","I am new to machine learning and want some help in text clustering.
Please suggest on code changes if you feel.My problem statement is to cluster the input data into multiple clusters. For this I am using tfidfvectorizer,steming, tokenizing and applying k-means algorithm.In output I am receiving identical data in two different clusters however I want them to be in same.Please find below sample data that I have and code that I have written.Data: ""Seat Allocation has been delayed...."" is going under two different clusters.
e.g.Data:is going into cluster 0 andis going into cluster 1.I have tried to reduce and increase the no. of clusters also. Still it is not working. The programming language I used is Python"
2370,categorize non-functional requirements,"['nlp', 'text-classification']","I am developing a machine learning project which analyzes requirement specification and categories the non-functional requirements in to categories like database, web socket, backend technology, etc. As I have researched Naive Bayes is the better way to categorize but due to lack of dataset I have planned to go with Seed LDA for topic modeling. Would it be okay to use LDA or should I use something else?"
2371,sentence classification in to predefined topics,"['nlp', 'text-classification', 'unsupervised-learning']","What unsupervised machine learning algorithms can be used to categorize sentences in to a fixed number of topics based on certain words in them? Like election and president words falls under politics category. I have already tried guided lda to implement this, any other suggestions? "
2372,One Category Text Classification on imbalanced data-set,"['tensorflow', 'machine-learning', 'keras', 'svm', 'text-classification']","I am having imbalanced dataset scraped from web pages text data and have manually classified it into positive class, while the other negative class can have any type of text data which I have marked as negative.
Looking at the dataset it was then clear that negative samples are very less approx. 1200 out of 6000.Negative = 1200Positive = 4800Initially with the imbalanced port stemmed dataset the model biased to majority class with high accuracy which was having worst performance in unseen data.So I took 1200 Negative and 1200 Positive and made it balanced.I implemented a Dense Model of 64 nodes in 4 layers with
  regularization of 0.5 using Keras and was able to achieve 60% accuracy
  in cross-validation while train accuracy goes as high as up to >95%.Looking at the val_acc and acc I feel that it is totally overfitting after around 20 epochs. In addition to that, it is also not able to generalize due to less number of data rows in the balanced dataset."
2373,Weka: how to convert the test data attibutes consistent with the train data attibutes?,"['weka', 'text-classification']","I am doing the text classification task.I build a classifier with train text data, has 1700+ attributes(words). However, my test data only has 500+ attributes(words), when i run test data on above model, it throws a Train and test set are not compatible exception. How could i convert the test data attributes consistent with train data's?"
2374,Text classification beyond the keyword dependency and inferring the actual meaning,"['python', 'nlp', 'text-classification', 'natural-language-processing']","I am trying to develop a text classifier that will classify a piece of text as Private or Public. Take medical or health information as an example domain. A typical classifier that I can think of considers keywords as the main distinguisher, right? What about a scenario like bellow? What if both of the pieces of text contains similar keywords but carry a different meaning. Following piece of text is revealing someone's private (health) situation (the patient has cancer): I've been to two clinics and my pcp. I've had an ultrasound only to be told it's a resolving cyst or a hematoma, but it's getting larger and starting to make my leg ache. The PCP said it can't be a cyst because it started out way too big and I swear I have NEVER injured my leg, not even a bump. I am now scared and afraid of cancer. I noticed a slightly uncomfortable sensation only when squatting down about 9 months ago. 3 months ago I went to squat down to put away laundry and it kinda hurt. The pain prompted me to examine my leg and that is when I noticed a lump at the bottom of my calf muscle and flexing only made it more noticeable. Eventually after four clinic visits, an ultrasound and one pcp the result seems to be positive and the mass is getting larger.
[Private] (Correct Classification)Following piece of text is a comment from a doctor which is definitely not revealing is health situation. It introduces the weaknesses of a typical classifier model: Don’t be scared and do not assume anything bad as cancer. I have gone through several cases in my clinic and it seems familiar to me. As you mentioned it might be a cyst or a hematoma and it's getting larger, it must need some additional diagnosis such as biopsy. Having an ache in that area or the size of the lump does not really tells anything bad. You should visit specialized clinics few more times and go under some specific tests such as biopsy, CT scan, pcp and ultrasound before that lump become more larger.
[Private] (Which is the Wrong Classification. It should be [Public]) The second paragraph was classified as private by all of my current classifiers, for obvious reason. Similar keywords, valid word sequences, the presence of subjects seemed to make the classifier very confused. Even, both of the content contains subjects like I, You (Noun, Pronouns) etc. I thought about from Word2Vec to Doc2Vec, from Inferring meaning to semantic embeddings but can't think about a solution approach that best suits this problem.Any idea, which way I should handle the classification problem? Thanks in advance. Progress so Far:
The data, I have collected from a public source where patients/victims usually post their own situation and doctors/well-wishers reply to those. I assumed while crawling is that - posts belongs to my private class and comments belongs to public class. All to gether I started with 5K+5K posts/comments and got around 60% with a naive bayes classifier without any major preprocessing. I will try Neural Network soon. But before feeding into any classifier, I just want to know how I can preprocess better to put reasonable weights to either class for better distinction."
2375,NLP data preparation and sorting for text-classification task,"['python', 'nlp', 'dataset', 'text-classification', 'spacy']","I read a lot of tutorials on the web and topics on stackoverflow but one question is still foggy for me. If consider just the stage of collecting data for multi-label training, what way (see below) are better and whether are both of them acceptable and effective? For instance, I have articles about war, politics, economics, culture. Usually, politics tied to economics, war connected to politics, economics issues may appear in culture articles etc. I can assign strictly one main theme for each example and drop uncertain works or assign 2, 3 topics. I'm going to train data using Spacy, volume of data will be about 5-10 thousand examples per topic.I'd be grateful for any explanation and/or a link to some relevant discussion."
2376,Why am I getting almost same top 10 features using Multinomial Naive Bayes classifier for positive and negative class?,"['machine-learning', 'text-classification', 'feature-selection', 'naivebayes', 'tfidfvectorizer']","After running MultinomialNB multiple times I'm getting same features for +ve and -ve class BoW, TfIdf.
I even tried it on bi-grams, tri-grams still the same features for both classes.This is the code for getting top 10 features for positive and negative classes of text data Tf-Idf.
feats_tfidf contains the features of categorical, numerical and text data.For Positive classOutput:For negative classOutput:Please help me someone is it correct way of doing."
2377,Factorizing text features for classification,"['python', 'text-classification']","I  have a dataframe, df consisting of both text and numerical features similar to one shown below. Currently I convert the text features into numerical features using factorize function and then use the new dataframe for classification.After running the above code my dataframe looks like thisThe factorize function is reading 'keywords' and 'keyword' as different words, so is there any function which will read words similar to 'keywords' and 'keyword' as same words ?The output dataframe should actually look like this"
2378,BERT multilingual model - For classification,"['nlp', 'stanford-nlp', 'lstm', 'text-classification', 'multilabel-classification']","I am trying to build multilingual classification model with BERT.I'm using a feature-based approach (concatenating the features from top-4 hidden layers) and building a CNN classifier on top of that.After that I'm using different language (say chinese) from the same domain for testing, but accuracy for these languages is near zero.I am not sure that I understand paper well, so here is my question:Is it possible to fine-tune BERT multilingual model on one language
(e.g. English) or use feature-based approach to extract the features and build classifer, and after that use this model for different languages (other
languages from the list of supported languages in documentation of
BERT)?Also, is my hypothesis, ""regarding BERT that it maps I think that it's embedding layer maps words from different languages with same context to similar clusters"", correct?"
2379,How to recognize entities in text that is the output of optical character recognition (OCR)?,"['nlp', 'recurrent-neural-network', 'text-classification', 'named-entity-recognition', 'named-entity-extraction']","I am trying to do multi-class classification with textual data. Problem I am facing that I have unstructured textual data. I'll explain the problem with an example.
consider this image for example:I want to extract and classify text information given in image. Problem is when I extract information OCR engine will give output something like this:Now target classes here are:Problem I am facing is that input text is not separable, meaning ""multiple lines can belong to same class"" and there can be cases where ""single line can have multiple classes"".So I don't know how I can split/merge lines before passing it to classification model. Is there any way using NLP I can split paragraph based on target class. In other words given input paragraph split it based on target labels."
2380,"How to balance topical, two classes dataset when one of the topics is too broad and the other is very narrow?","['python', 'nltk', 'prediction', 'text-classification']","I have simple prediction, where The dataset is composed from 2300 samples for each class e.i. total = 4600 (binary classification). The first class encompasses all news types except the other class, which is very narrow topic. I have used The Naive-Bayes classifier of NLTK to perform the task, where the classifier takes the samples one-hot-encoded. Although the classifier performed well in test portion of the dataset (94% accuracy), it fails miserably when it classify crawled news (in production).

My thinking the problem is due to that the two classes are imbalanced in the real word. If this is the reason, then, how to overcome this problem? how to balance my dataset? Assume that I can collect many samples additionally for the broad class(A), but very a few samples for the narrow class(B)."
2381,Predict “user-input” reviews with Naive Bayes trained model,"['python', 'scikit-learn', 'user-input', 'text-classification', 'naivebayes']","I am using a dataset with textual Yelp restaurant reviews and their ""star"" rating. 
My data is a df and looks like this:I have built the MultinomialNB model which predicts the ""star"" (1-stands for negative, 5 stands for positive; using only these two categories) for the review.What I'm trying to do is to predict ""star"" rating for the user provided restaurant review. Here are my attempts:I am not sure whether the output that I am getting is correct since I get an array of scores. Can someone help me interpret these scores? Do I just take the average and that will be my ""star""rating for the review?ps I realize that the sample data is poor and my model is biased towards positive ratings!
Thanks beforehand!"
2382,sklearn Vectorizer (NLP task) : Generating Custom NGrams which are capable of scaling up for n >= 3,"['scikit-learn', 'nlp', 'tokenize', 'text-classification']","I would like to build a vectorizer in sklearn which can scale up for higher values of n. Here n is the number of different words considered as single vocab element.My idea is that for n = 1 and n = 2, my custom vectorizer remains the same as sklearn vectorizers, but for n>=3, I would like to replace ""I am good"",""Harry will play"" with ""I x good"" and ""Harry x play"".Example: Let's consider that I want to build a vectorizer which scales upto n = 4. Now, take an example sentence ""Harry will play tommorow"".Then, ""Harry will play tommorow"" can break as:-All 1,2 length vocab words, ""Harry x play"", ""will x tommorow"" and ""Harry x x tommorow"".Since, the order of elements in this vocabulary is same as that for n = 2, and words of form ""A x B"" will not be any rarer than ""A B"", I believe that this model may scale better and give performance benefits.I searched over the net to find a method to do this and while there are many tutorials for building custom vectorizers all of them end up using there pre-implemented n-gram method."
2383,Why does append() always return None in Python? [duplicate],"['python', 'list', 'append', 'nonetype']",I'm learning Python and I'm not sure if this problem is specific to the language and how append is implemented in Python.
2384,Intent classification with large number of intent classes,"['python', 'tensorflow', 'nlp', 'text-classification']","I am working on a data set of approximately 3000 questions and I want to perform intent classification. The data set is not labelled yet, but from the business perspective, there's a requirement of identifying approximately 80 various intent classes. Let's assume my training data has approximately equal number of each classes and is not majorly skewed towards some of the classes. I am intending to convert the text to word2vec or Glove and then feed into my classifier.I am familiar with cases in which I have a smaller number of intent classes, such as 8 or 10 and the choice of machine learning classifiers such as SVM, naive bais or deeplearning (CNN or LSTM).My question is that if you have had experience with such large number of intent classes before, and which of machine learning algorithm do you think will perform reasonably? do you think if i use deep learning frameworks, still large number of labels will cause poor performance given the above training data?We need to start labelling the data and it is rather laborious to come up with 80 classes of labels and then realise that it is not performing well, so I want to ensure that I am making the right decision on how many classes of intent maximum I should consider and what machine learning algorithm do you suggest?Thanks in advance..."
2385,SVM and NN Model overfitting on large data,"['python', 'machine-learning', 'scikit-learn', 'neural-network', 'text-classification']","I have trained SVM and NN model using sklearn for two class. One class have 24000 tweets and another 32000 tweets. When I do validation it gives like this For - For When I change alpha value in NN model from 0.001 to 0.00001When I test few records, it is always biased to one class. For example SVM was predicting every input to non-disaster and NN does it to disaster class. Any idea or suggestion how can I fine tune this model?"
2386,Keras evaluate() and predict() results are way too off,"['python', 'keras', 'lstm', 'recurrent-neural-network', 'text-classification']","Im working on a binary classification model using keras. See data set up belowIm using LSTM, activation is 'sigmoid' and 'binary_crossentrophy' is my loss function.At end of 10 epochs, training accuracy is 0.97 and validation accuracy is around 0.72.Every thing seem to be good till this point and it goes south when i run predict() function on the test dataIm not able to understand why evaluate() and predict() results are way too off. Can you please point of what's wrong? Im running this on a GPU EC2 instance. Version of software below.Keras 2.2.4
Tensorflow 1.12.0Let me know if ay other detail about the model is needed. Thanks"
2387,Data augmentation for text classification,"['machine-learning', 'nlp', 'classification', 'data-science', 'text-classification']","What is the current state of the art data augmentation technic about text classification?I made some research online about how can I extend my training set by doing some data transformation, the same we do on image classification.
I found some interesting ideas such as:Synonym Replacement: Randomly choose n words from the sentence that does not stop words. Replace each of these words with one of its synonyms chosen at random.Random Insertion: Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random place in the sentence. Do this n times.Random Swap: Randomly choose two words in the sentence and swap their positions. Do this n times.Random Deletion: Randomly remove each word in the sentence with probability p.But nothing about using pre-trained word vector representation model such as word2vec. Is there a reason?Data augmentation using a word2vec might help the model to get more data based on external information. For instance, replacing a toxic comment token randomly in the sentence by its closer token in a pre-trained vector space trained specifically on external online comments.Is it a good method or do I miss some important drawbacks of this technic? "
2388,Spacy text classification scores,"['nlp', 'spacy', 'text-classification']","I'm quite new to NLP text classification and trying to apprehend the basics. It seems that Spacy is more suitable for my tasks and experience. I've read through all the docs and run the example code from https://spacy.io/usage/training#example-textcat with default plac parameters with my own output folder. Then, I wrote a testing file:and got results:Is it OK for Spacy model, or have I done something wrong? I mean there's quite narrow frontier between ""positive"" and ""negative"" labels. Even definitive ""This is a bad film"" earned 0.46 of ""positive"" rating. ""I love this movie"" got only 0.75 while ""Very involving work with developed characters"" got 0.83. At the same time, suggested in the original Spacy usage docs phrase ""This movie sucked"" got 0.65 ""positive"" score!Thank you in advance for your answer"
2389,Predicting iteratively with trained classifier in sklearn,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","I have trained a classifier model with 120 features, for predicting I am using this code.It is a binary classifier. When I run this, the features for the new text are generated on the fly and then fed to the model. And the output I get is[1 1 1 1][1 1 1 1 1 1 1][1 1 1 1 1 1 1 1][1 1 1 1 1 1 1 1 1 1]The results are getting appended. How to avoid this or get results one by one."
2390,Find best machine learning for predicting category of products,"['python', 'python-3.x', 'machine-learning', 'text-classification']","I have a dataframe that contains product and in this dataframe I have some features like: brand, cat1, cat2, cat3, city, desc, image_count, mileage, price, title, year.
The goal is predicting category of products. I have 1 billion training data and important features for prediction are title and description that are text type.
I like to know what algorithm is best for my prediction? I'm a beginner in machine learning and confused among different algorithms. Thanks"
2391,Is there a difference in computation according to input shape? (CNN in Python with Tensorflow),"['python', 'tensorflow', 'classification', 'conv-neural-network', 'text-classification']","I am solving a text classification problem by reference to the paper(Kim, 2014).
And then I found between below two models, the model on the left(Model 1) takes about 2.5 times more time than the model on the right(Model 2).
I think the number of weight parameters of the two models is the same.
Why is there the difference of time between the two models?
*The input data's contents of the two models are the same. Simply changed the shape.  
I used tf.nn.conv2d. And the filter shapes and the stride are as following
model 1 : 3x9x1xthe number of filters, stride 3
model 2 : 1x9x3xthe number of filters, stride 1
And the other things are the same
*On above image, width means 'self.embedding_dim' and height means 'self.max_length'.  "
2392,Predict multi class in svm,"['python', 'machine-learning', 'scikit-learn', 'svm', 'text-classification']","I have user review dataset likereview-1 is user review and 0,1,1,0,0 is review categories. one review can have multiple categories. I want to predict categories to reviews. so I implement the code thatBut I'm getting error likeCould anyone suggest any good solution to solve this?"
2393,Classify Multiple Documents and store them in different folders Via Flask (python),"['python', 'flask', 'document', 'text-classification']",I want to do is I want my web app to take multiple documents as input and classify them using my model and store those classified documents into different folders.I have developed a model which classifies documents. Model is ready and have accuracy of about 0.96 f-score. I want to implement it in flask. I already implemented on text input which shows accurate results.I want to do is I want my web app to take multiple documents as input and classify them using my model and store those classified documents into different folders. When placed a query it will generate the result.
2394,Classification based on list of words R,"['r', 'text-classification', 'stringr']","I have a data set with article titles and abstracts that I want to classify based on matching words. ""This is an example of text that I want to classify based on the words that are matched from a list. This would be about 2 - 3 sentences long. word4, word5, text, text, text""Given that that text above matches words in Topic 2, I want to assign a new column with this label. Preferred if this could be done with ""tidy-verse"" packages."
2395,Different accuracy for the same code in text classification in keras,"['python', 'tensorflow', 'keras', 'nlp', 'text-classification']","I'm training a recurrent neural network based on LSTM for text classification and I have a strange behaviour. With the same code and same training set I obtain very different level of accuracy. I know it's normal to have different value but sometimes i get value of 40% accuracy and others 90%. How is it possible?Moreover sometimes I get ""stuck"" on the accuracy level, I mean that the loss and the accuracy doesn't change during the different epochs so both values remain the same. which is the  explanation for this?Another aspect that I'm not sure that i truly understand is the fact of the padding for my input text (I'm using training in batch). I think, since I'm using a RNN, should be better use a left padding. Do you know how to pad the input? Is better left or right padding?Last question is how to choose the number of layer and the number of node. I know that for someone the best approach is by experiment but some suggestion could be useful.This is the implementation of my RNN:I'll appreciate any hint of the question above and also some suggestion to improve my neural network."
2396,Unify text and image classification (Python),"['python', 'machine-learning', 'classification', 'text-classification']","I am working on a code to classify texts of scientific articles (using the title and the abstract). And for this I'm using an SVM, which delivers a good accuracy (83%). At the same time I used a CNN to classify the images of these articles. My idea is to merge the text classifier with the image classifier, to improve the accuracy.It is possible? If so, you would have some idea how I could implement it or some kind of guideline? Thank you!"
2397,Email classification using word2vec,"['tensorflow', 'keras', 'word2vec', 'recurrent-neural-network', 'text-classification']","My goal is to classify email, so each email have to correspond to a specific category like sport, clothes and so on.My idea is to use a Recurrent Neural Network with one layer based on embedding (by word2vec). I'm using the followed schema ""emojify v2"" with some changing:https://github.com/enggen/Deep-Learning-Coursera/blob/master/Sequence%20Models/Week2/Emojify/Emojify%20-%20v2.ipynband the word2vec pretrained model downloaded from:
http://hlt.isti.cnr.it/wordembeddings/The problem is that this neural network does not work properly. In fact, trying to train the network on only 2 categories (20 sample). I get an accuracy of 50-60% on the training set that is of course too low.Do you have any hint to improve my result?What could be the problem?I'm using keras (tensorflow).edit1: insert code"
2398,Retrain production model with labeled + predicted data?,"['machine-learning', 'text-classification', 'training-data']","Let's say that I'm currently doing text classifying with two different classes. The labeled data I have now is the one I have manually classified as either X or Y. The dataset is atm kind of large with a dataset with size 7000 (3500 X, 3500 Y).The thing is that I have 2000 which are currently not labeled, but they belong to either X or Y (there is no other class). My model's accuracy, recall, and f1-score is around 95-98 depending on the model I use.The goal is not to be needing this manual categorization of either X or Y anymore, and just let the ML model do it for me (ofc it gets it wrong sometimes and its all okay).The question is, can I use the model's predictions together with the manually categorized data on training and validation when I later retrain my model?I know this is a kind of hard question, due to you don't have all the information, etc. But I guess I'm not the only one that wants to replace something that's currently done manually with an ML model."
2399,Can i use CountVectorizer on both test and train data at the same time or do I need to split it up?,"['machine-learning', 'scikit-learn', 'text-classification', 'word-count']","I currently have an SVM model that classify text into two different classes. I'm currently using CountVectorizer and TfidfTransformer to create my ""word vector."" The thing is that I think I maybe do it in the wrong order when I'm doing the conversion of all the text first and then split it up.My question is, will there be any difference if I do train_test_split first and then do the fit_transform only on the train data and then transform on the test data? What is the correct way to do it? Big thanks in advance, happy coding! "
2400,Naive Bayes in Quanteda vs caret: wildly different results,"['r', 'r-caret', 'text-classification', 'supervised-learning', 'quanteda']","I'm trying to use the packages quanteda and caret together to classify text based on a trained sample. As a test run, I wanted to compare the build-in naive bayes classifier of quanteda with the ones in caret. However, I can't seem to get caret to work right.Here is some code for reproduction. First on the quanteda side:Not bad. The accuracy is 80.8% percent without tuning. Now the same (as far as I know) in caret Not only is the accuracy abysmal here (49.6% - roughly chance), the pos class is hardly ever predicted at all! So I'm pretty sure I'm missing something crucial here, as I would assume the implementations should be fairly similar, but not sure what.I already looked at the source code for the quanteda function (hoping that it might be built on caret or the underlying package anyway) and saw that there is some weighting and smoothing going on. If I apply the same to my dfm before training (setting laplace = 0 later on), accuracy is a bit better. Yet also only 53%."
2401,Multiple input parameters during text classification - Scikit learn,"['machine-learning', 'scikit-learn', 'nlp', 'text-classification']","I'm new to machine learning. I'm trying to do some text classification. 'CleanDesc' has the text sentence. And 'output' has the corresponding output. Initially i tried using one input parameter which is the string of texts(newMerged.cleanDesc) and one output parameter(newMerged.output)This works fine. But the accuracy is very low. I wanted to include one more parameter(newMerged.type) as the input, along with the text to try improving it. Can I do that? How do I do it. newMerged.type is not a text. It just a two character string like ""HT"". I tried doing it as follows, but it failed,"
2402,Spacy text categorization: getting the error massage “'float' object is not iterable”,"['python', 'text-classification', 'spacy']","I work on text categorization project using spaCy. I follow spaCy code example very closely. The only important difference is that I'm using two categories instead of one in the example. I don't understand what is wrong, as I checked and the data that I'm loading is in the same format as in the original example. Here is the relevant code (the full code attahced below):and this is the log:Any ideas why I'm getting this error? The full code for reference:"
2403,Adding an extra dimension to text classification,"['tensorflow', 'neural-network', 'recurrent-neural-network', 'text-classification', 'tensor']","I want to do text classification using a neural network in Keras. I have setup a simple test sample using the following network:This network accepts tokenized padded sequences of text. E.g. I tokenize the text ""hello world"" = [0,1,0,0,0..]. It train & evaluates fine.Now my issue is that I do not want to enter a single sequence into the network, but rather a collection (let's say 500) sequences into the network and get a category out. So instead of an input with shape (100) it's now (500, 100). I'm unsure how to best create the network architecture, ie:1) Should I flatten the input or try to reduce the dimensions? What layers could I use for that job?2) Should I just create one large sequence with all the text?3) Does it even make sense to have a LSTM with 4 dimensions?4) Does examples exist for classification with an array of array of tokens?The text is collected text from different sources, so the different sequences in each batch is not necessarily related in relation to anything else than date."
2404,Meaning of “ Gold sentiment distribution”?,"['nlp', 'sentiment-analysis', 'text-classification']","What is the mean by "" Gold sentiment distribution""?I read few papers and they have mentioned about "" gold sentiment distribution:
I tried to find it on google but I cannot find the explanation or meaning of gold sentiment distribution.can anybody explain in simple words what is the meaning?Thanks"
2405,Text classification: training based on long documents and applying it for short sentences,"['python', 'text-classification', 'supervised-learning']","I'm a beginner in NLP and have a general question (maybe too general). I have two datasets. The first is agricultural patents documents with a given classification into classes. The second is a list of words/short sentences (agricultural tasks such as ""Apply manure"" or ""Disk""). My goal is to classify each task into one of the patent classes. At this stage, I don't want to start from the fundamentals and program all steps of the algorithm. I thought to use this algorithm from spaCy, with small changes. This algorithm train the model to classify reviews into categories and then apply the model on new reviews. The only difference I see in my case is that the data I want to classify with the model is not at the same length as the training data (short sentences versus a few paragraphs). My questions are:Thanks!"
2406,fasttext keeps predicting one label,"['text-classification', 'fasttext']","am trying to use fasttext to label some data [url]or[PN] just to test it
after training on 6k of each label and upon predicting it keeps predicting [PN]training commandsample training data sample test data"
2407,Classify movies based on their ratings using their subtitles-accuracy very bad,"['nlp', 'tokenize', 'text-classification', 'naivebayes', 'svc']","I have a data set of 130 movies and their subtitles.I have to classify them based on their ratings (R,NR,PG,PG-13,G).(language used python)
I did the following:
1)tokenized the data using treebank whitespace and wordpunc tokenizers.
2)lemmatized the data.(lemmatization gave more accuracy when pos tags were included. )
3)removed stop words and punctuation.
4)for movies belonging to each class, performed tfidf vectorization and picked the top 1000 words using max_features and constructed a data frame of size 125 *5000.
5)I applied several classification and clustering algorithms and they gave me the following accuracy:
SVC: test accuracy :0.325 and train accuracy :0.63
Naive Bayes: test acc:0.25 and train accuracy :0.33
knn: test accuracy : 0.41
kmeans:test accuracy:0.162
Logistic regression: test accuracy:0.53 and training accuracy of : 0.96What should I do to improve my accuracy?
Am I making any mistakes or missing out on something important?"
2408,How to balance data in a multi class text classification problem?,"['machine-learning', 'classification', 'text-classification']","I have a multi class text classification problem with 29 output classes. This is the distribution of records across the 29 classes in training dataset.I want to know how should I go about balancing my data using upsampling? For upsampling, should I upsample all classes to 1337 rows?"
2409,How can I implement multiple hidden layers in an RNN (PyTorch)?,"['lstm', 'pytorch', 'recurrent-neural-network', 'text-classification']","My Pytorch RNN for name classification does not allow me to choose multiple hidden layers. If I choose more than 1 layer I get the following error message:Traceback (most recent call last): File ""TRAIN2.py"", line 63,line 104, in training: y_hat = y_hat.view(batch_size, n_categories)
  RuntimeError: shape '[26718, 6]' is invalid for input of size 320616I don't know exactly what the problem is but the input size is doubled when I choose 2 layers, tripled with 3 layers and so on.. My output size is 6 and when I choose 1 layer it works but with two layers there are twice as many inputs and I get the error and with 3 layers there are 3 times as many inputs. Shouldn't the input size stay the same regardless of the number of layers?I am attaching the main file (where the error appears in the loss computation) and my network architecture.And this is my modelI am aware that using multiple layers does probably not increase performance but I want to use a non-zero dropout rate for which I'm being told that it is only active in all layers but the last and with only 1 layer therefore useless...Thank you in advance for your help."
2410,Bad performance While training Lstm for Text Classification on Amazon Fine Food Reviews Dataset?,"['machine-learning', 'deep-learning', 'lstm', 'text-classification']","I am trying to train a Lstm model for Text classification on Amazon Fine food review problem , I am using same dataset as provided by kaggle , I am using tokenizer to convert text data into tokens , but while I am training I am getting same accuracy for all the epochs .Like thisMoreover when I am plotting my confusion matrix None of the Negative classes are predicted only positive classes are predicted.
I think I am mostly doing wrong when converting Labels i.e. 'Positive' and 'Negative' to some numerical representation for classification.
Please see my code for more details.I have tried increasing number of Lstm units and epochs also tried increasing length of sequences but none worked , please note that I have done all pre-processing on reviews as required.I would like my lstm model to generalise well and predict negative reviews which are minority class in this case as well."
2411,Is there a problem with my data set for language identification using sklearn?,"['python-3.x', 'scikit-learn', 'logistic-regression', 'feature-extraction', 'text-classification']","I've trained my language identification model on huge training data set for 53 languages. For some languages the precision is spot on but some languages are failing pretty bad, English is one of them. This leads me to think that data-set is still messed up.I've under-sampled the data to 500 records, removed special characters, extra spaces and different punctuation from the text and have limited the length of the text field to 140 chars. The data set for English after clean up looks like this:
https://drive.google.com/file/d/1lv4ZqTPUCRV2s4G9-LWlXXVxtpW2s9Hb/viewSadly, the precision is barely .5. Some of the lines still contains different language texts, however, I'm not sure how to take care of this.Also, I'm using TFIidfVectorizer with ngram_range of 1 to 3 to normalize the text feature and logistic regression classifier on top of that. I've also tried MultinomilaNB but in vain. Below is my code:Here's the classification report:"
2412,Spacy text classification using minibatch,"['python', 'text-classification', 'spacy', 'mini-batch']","I have a question regarding the minibatching that is used in the example  train_textcat.py
The main training loop looks like this:I was woundering why do all batches of the minibatch get consumed in one interation instead of consuming one batch per iteration of the main loop? Following code should expain what i mean.Thanks in advance!"
2413,How to resample text (imbalanced groups) in a pipeline?,"['python', 'pipeline', 'text-classification', 'resampling', 'oversampling']","I'm trying to do some text classification using MultinomialNB, but I'm running into problems because my data is unbalanced. (Below is some sample data for simplicity. In actuality, mine is much larger.) I'm trying to resample my data using over-sampling, and I would ideally like to build it into this pipeline. The pipeline below works fine without over-sampling, but again, in real life my data requires it. It's very imbalanced. With this current code, I keep getting the error: ""TypeError: All intermediate steps should be transformers and implement fit and transform.""How do I build RandomOverSampler into this pipeline? "
2414,"Text Classification, How to convert text strings to vector representation","['java', 'machine-learning', 'classification', 'svm', 'text-classification']","I am working on a text classification program. My training data is 700+ text categories and each categories contains 1-350 text phrases. 16k+ phrases total. The data that needs to be classified are text phrases. I want to classify the data so it gives me 5 most similar categories. The training data shares a lot of common words.My first attempt was using Naive Bayes Theorem using this library on github because this library was very easy to use and allowed me to load my training data as strings. But other users reported issues and when I tried to classify my data, my input is either classified wrong or not classified.  https://github.com/ptnplanet/Java-Naive-Bayes-ClassifierSo I think the library was the issue, so Im going try different libraries and look into k means clustering since my data is high variance.So when I looking at other libraries, they all require input and training data as a vector matrix. I looked at word2vec and td-idf to convert text vectors. I understand tf-idf, and that I can get the weight of the word compared to the rest of the documents. But how can I use it classify my input data to categories? Would each category be a document? Or would all categories be a single document?edit:data sampleSEE_BILL-see billSEE_BILL-bill balanceSEE_BILL-wheres my billSEE_BILL-cant find billPAY_BILL-pay billPAY_BILL-make paymentPAY_BILL-lower balancePAY_BILL-remove balancePAST_BILL-last billPAST_BILL-previous billPAST_BILL-historical billPAST_BILL-bill last year"
2415,How to fix tuple object error in feature union & pipelines (while using sklearn)?,"['machine-learning', 'scikit-learn', 'classification', 'text-classification']","I have a pandas data frame with 56 columns. Around half of the columns are float and the others are string(textual data) and finally col56 is the label column. The dataset looks something like thisI want to use both numeric and textual data to run classification algorithms. A quick google search told that the best way to proceed is by using Feature UnionThis is the code so farThen I get an errorI don't understand since the exact same method is used here and here
And there doesn't seem any error. What am I doing wrong? How can I fix this?"
2416,Feeding LSTMCell with whole sentences using embeddings gives dimensionality error,"['python', 'tensorflow', 'machine-learning', 'lstm', 'text-classification']","So currently i'm sitting on a text-classification problem, but i can't even set up my model in Tensorflow. I have a batch of sentences of length 70 (using padding) and i'm using a embedding_lookup with an embedding size of 300. Here the code for the embedding:So now inputs should be of shape [batch_size, sentence_length, embedding_size] which is not surprising. Now sadly i'm getting a ValueError for my LSTMCell since it is expecting ndim=2 and obviously inputs is of ndim=3. I have not found a way to change the expected input shape of the LSTM Layer. Here is the code for my LSTMCell init:The error is triggered in the call function of the cell, that looks like this:Similar question but not helping: Understanding Tensorflow LSTM Input shape"
2417,How can I get around Keras pad_sequences() rounding float values to zero?,"['python', 'numpy', 'keras', 'lstm', 'text-classification']","So I have a text classification model built with Keras. I've been trying to pad my varying length sequences but the Keras function pad_sequences() has just returned zeros.I've figured out that if you have a numpy array like the one below, it works just fine. But once the elements become floats or decimals like the second array it just turns to zeros.outputs:Butoutputs:And this:outputs:So I guess this function just ignores floats/decimals. Is there a way I can get around this?"
2418,unusual language text clustering / classification,"['nlp', 'classification', 'cluster-analysis', 'data-mining', 'text-classification']","Briefing:
What would be your approach to clustering similar text from unusual language.Details:
I'm scraping a classified ads website trying to group similar ads(same product). The text has often misspelling, written in 2 languages (a bit of kind of 1ee7) and some text written phonetically in different alphabet (ex. Diànshì for 电视 or velosiped for велосипед) or different dialect.Then how would you proceed to manage such an unpredictable input?"
2419,How to handle text classification problems when multiple features are involved,"['python', 'nlp', 'feature-extraction', 'text-classification']","I am working on a text classification problem where multiple text features and need to build a model to predict salary range. Please refer the Sample dataset
Most of the resources/tutorials deal with feature extraction on only one column and then predicting target. I am aware of the processes such as text pre-processing, feature extraction (CountVectorizer or TF-IDF) and then the applying algorithms. In this problem, I have multiple input text features. How to handle text classification problems when multiple features are involved? These are the methods I have already tried but I am not sure if these are the right methods. Kindly provide your inputs/suggestion.1) Applied data cleaning on each feature separately followed by TF-IDF and then logistic regression. Here I tried to see if I can use only one feature for classification.   2) Applied Data cleaning on all the columns separately and then applied TF-IDF for each feature and then merged the all feature vectors to create only one feature vector. Finally logistic regression. 3) Applied Data cleaning on all the columns separately and merged all the cleaned columns to create one feature 'merged_text'. Then applied TF-IDF on this merged_text and followed by logistic regression.All these 3 methods gave me around 35-40% accuracy on cross-validation & test set. I am expecting at-least 60% accuracy on the test set which is not provided.Also, I didn't understand how use to 'company_name' & 'experience' with text data. there are about 2000+ unique values in company_name. Please provide input/pointer on how to handle numeric data in text classification problem."
2420,"OpenNLP-Document Categorizer- how to classify documents based on status; language of docs not English, also default features?","['java', 'nlp', 'text-classification', 'naivebayes', 'opennlp']","I want to classify my documents using OpenNLP's Document Categorizer, based on their status: pre-opened, opened, locked, closed etc.I have 5 classes and I'm using the Naive Bayes algorithm, 60 documents in my training set, and trained my set on 1000 iterations with 1 cut off param.But no success, when I test them I don't get good results. I was thinking maybe it is because of the language of the documents (is not in English) or maybe I should somehow add the statuses as features. I have set the default features in the categorizer, and also I'm not very familiar with them.The result should be locked, but its categorized as opened.
Can someone make a suggestion for me how to categorize my documents well, like should I add a language detector first, or add new features?Thanks in advance"
2421,Using Keras to build a LSTM+Conv2D model,"['python', 'tensorflow', 'machine-learning', 'keras', 'text-classification']","I want to build a model similar to this architecture:-
My current LSTM model is as follows:-How to use the Conv2D layer after the BiLSTM later with 2D Max Pooling layer ?"
2422,How dictionary is created when making dictionary-based text classifications? How values are determined?,"['dictionary', 'sentiment-analysis', 'text-classification']","I'm trying to create sentimental analysis of about 1 million twits I've collected from Twitter. I've found a lot of dictionary related to text categorization. The dictionaries I found were rated words between -4 and +4. For example,fan 3angry -2revenge -2bad -3calm 2celebration 3What I wonder is how numbers are given to words. How can I sure that numbers are valid? How dictionaries are created?"
2423,How can I reduce the RAM utilization in my program that use deeplearning4j (Memory-mapped files and WorkspaceConfiguration)?,"['java', 'neural-network', 'deep-learning', 'text-classification', 'deeplearning4j']","I'm using deeplearning4j but when i load pre-trained model for text-classification I don't have enough RAM on my pc.I tried to change eclipse.ini file and add more memory changing Xms and Xmx. Unfortunately it doesn't work for me.https://deeplearning4j.org/docs/latest/deeplearning4j-config-memoryIn this link seems there is a possible solution to use less RAM even though it cost more time of corse, but I don't care now.From that link:Memory-mapped files ND4J supports the use of a memory-mapped file
  instead of RAM when using the nd4j-native backend. On one hand, it’s
  slower then RAM, but on other hand, it allows you to allocate memory
  chunks in a manner impossible otherwise.Can I add this in a code like this (follow the link)?https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.javaOf cours if there is another way (or a better way) write it. I'll appreciate any advice.Thanks in advance."
2424,MutiLabel classification,"['python', 'machine-learning', 'deep-learning', 'text-classification']","I have some 1000 news articles related to science and technology. I need to train a classifier which will predict say 3(computer science, electronics, electrical) confidence scores for each article. 
Each score represents how much the article belongs to each field.
The confidence score will be a value between zero and one.But the data set doesn't have a training label.  How do I proceed from here? What kind of data do I need?
How do I train such a model?"
2425,Keras give less accuracy than any classifier,"['python', 'keras', 'deep-learning', 'text-classification']","I use python to multi-class text classification , my data set contains 25000 Arabic tweets divided into 10 classes[sport, politics,....]
When I use Accuracy: 0.9525099601593625When I use keras:0.28127490039840636where the mistake???update
I change the code to:0.7201593625498008
still bad accuracy!!!"
2426,Question Classification (Input Classification) using python,"['python', 'scikit-learn', 'nlp', 'svm', 'text-classification']",I have 4 categories of Questions I have a training questions for all of them (maybe 500 questions for each categories). I want to classify the user's input (or the Question) with respect to these 4 Categories on the basis of training data.What would be the best way to do that? I looked up and found many ways to perform it. I am looking for the way that works the best with more accuracy.P.S: Very short explanation with a short sample code would be helpful if possible.Thanks
2427,"Is there a library to take a string, and classify it into a category based on whether it matches a group of strings?","['java', 'text-classification']","So, I have four lists of strings, each list which corresponds to a specific category. Each string is a job title, such as ""web-developer"", which corresponds to the category ""IT"". The input string is going to be another job title, and the idea is to sort that job title into the appropriate category based on how well it matches the list of stringsDoes anyone know a good library to accomplish this? Sadly, I do not have enough source material to properly train a machine learning system... All the libraries I've found so far seem to be based on machine learningAlternatively, if no such library exists, do anyone have any suggestions on how to accomplish this? My best idea so far have been to just... search through all the strings and do a string.contains(searchString) and just match it like that. I dunno how to handle multiple matches though...Ideally the library should be java, but this is not a necessity. "
2428,neutral label for NLTK,"['nltk', 'sentiment-analysis', 'text-classification', 'multilabel-classification', 'nltk-trainer']","I have similar problem like belowWhy did NLTK NaiveBayes classifier misclassify one record?In my case, I queried positive feed and built positive_vocab and then queried negative feed and built negative_voca. I get the data from feed clean and built the classifier. How do I build the neutral_vocab. Is there a way I can instruct NLTK classifier to return neutral label when the given word is not found in the negative_voca and positive_vocab. How do I do that?In my current implementation, if I give a word which is not present in the both sets it tells positive by default. Instead it should tell, neutral or notfound "
2429,ArrayIndexOutOfBoundsException in instance classification in java using weka,"['java', 'weka', 'text-classification', 'j48']","I am writing a program to classify a given test instance in to ""positive"" or ""negative""; using j48 algorithm. 
I have created the model using Weka tool, and using it in my java program. My Java programMy training data fileI am getting an ArrayIndexOutOfBoundsException from result = cls_co.classifyInstance(data.firstInstance());I tried many examples found on the internet. But still I am getting the same error.Thanks in advance. "
2430,keras parametes for multilabel text classification,"['python-3.x', 'keras', 'text-classification', 'keras-layer']","I am using keras in my multiclass text classifcation, the dataset contains 25000 arabic tweets with 10 class labels
I use this code :Summary:but i get error:
 could not convert string to float: 'food'
where food is a class namewhen i change loss to categorical_crossentropy  i get the error
 Error when checking target: expected activation_24 to have shape (10,) but got array with shape (1,)Update"
2431,Making a prediction after training and fitting a RNN Sequential model,"['python', 'tensorflow', 'keras', 'neural-network', 'text-classification']","I am trying to get predictions from my sentiment analysis models that classify 500 worded News articles. The models validation loss and training loss is in are about the same and their scores are relatively high. However when I try to make predictions with them I get the same classification result in all of them regardless of the text input. 
I believe that the problem might be on the way I am trying to make a prediction (I pad my string with spaced characters). I was hoping that someone here could shed some light on this issue (my code below). Thank you for your helpThe output of this script is below. Both prediction and predicted classes, are regardless of the text input always 0 for some reason:[[0]] [[0.00645966]]"
2432,LSTM model weights to train data for text classification,"['keras', 'lstm', 'text-classification']","I built a LSTM model for text classification using Keras. Now I have new data to be trained. instead of appending to the original data and retrain the model, I thought of training the data using the model weights. i.e. making the weights to get trained with the new data. 
However, irrespective of the volume i train, the model is not predicting the correct classification (even if i give the same sentence for prediction). What could be the reason?
Kindly help me."
2433,python - How do I extract the id from an unsupervised text classification,"['python-3.x', 'k-means', 'pca', 'text-classification', 'unsupervised-learning']","So I have the following dataframe:And the following code:Now, how can I see which id falls in which cluster, is this something that can be done? Also is my approach correct in order to ""clusterize"" these text documents?Please not that I might have skipped some code in order to keep the question short"
2434,NLP ML How to know the weight of words used in text classifier?,"['python', 'machine-learning', 'nlp', 'nltk', 'text-classification']","I am building a tweet classifier where I try to train different ML models to classify tweets from 2 different tweeter accounts. So far I have train Logistic Regression model, K Neighbors Classifier and decision tree classifier.Is there a way to know what words in the tweets those classifiers used to predict the account? like the weight of words in the classification process?? I am open to train new classifiers that can do that as well.Already did some ngram analysis on the tweets like word frequency.thanks in advance!   "
2435,Improving Perfomance of LSTM on text classification (3-class problem),"['machine-learning', 'deep-learning', 'lstm', 'sentiment-analysis', 'text-classification']","My problem is a 3 class sentiment analysis classification problem with 4000 reviews of average around 500 words length each. The dataset distribution of sentiments are 1800 negative 1700 neutral and 500 positive. I am trying the following LSTM but as i was searching on how to improve perfomance by changing the parameters i didnt find any specific rules on how to choose them, most of the answers i found was ""it depends on the problem"", but as i am a noobie on the subject of deep learning i dont really understand from where to start. My model achieves around 63% accuract, tested with k=5 cross val. Thank you in advance. This is the code i have so far:"
2436,How to Make Predictions of Data Using Sklearn's RandomForestClassifier,"['python', 'machine-learning', 'scikit-learn', 'text-classification']","I followed this website here https://stackabuse.com/text-classification-with-python-and-scikit-learn/ and have successfully completed the model and saved it using my own data, however I don't know how to test a new document on the model. I have a bunch of documents in a string format like so: string = ""Whatever and more of whatever"" and I just need to know what code I need to run to test these documents through my model. My code is the exact same as the website with the only difference being the files I have loaded and to solve my problem I tried to use classifier.predict(string) and it gave me the error ValueError: could not convert string to float. Any help would be appreciated.Update:
I tried to convert my document to the proper format using this codeAnd this is the error I got 
                                                                  ValueError: Number of features of the model must match the input. Model n_features is 897 and input n_features is 149 "
2437,how to differentiate sentences with antonyms using word2vec,"['word2vec', 'text-classification']","Say I have two sentences, which are similar except there is only one different word with opposite meaning. e.g. ""I like her"" vs. ""I hate her"".
word2vec is used in my classification project. As far as I know, word2vec seems unable to figure out differences between antonyms. Is there any way to solve this?"
2438,Reusing an sklearn text classification model with tf-idf feature selection,"['python', 'machine-learning', 'text-classification', 'tf-idf', 'tfidfvectorizer']","I'm relatively new to sk-learn & machine learning here so forgive any possible ignorance. I'm making a model to classify assets based on a text description (in python). There is only one predictor (the text) and one predicted (the category) variable. For the labels, I am factorizing the categories, there are about 30, so each is represented by a number from 0 to 29. For the features, I'm using a tf-idf score. The modeling and accuracy are fine and I'm saving the model using a pickle dump.However, the model needs to be reusable, so it must be able to load again at some point in time in order to label a new set of data. Please see the code for the saving/loading of the model below.Please note that the tfidf vectorizer settings are exactly the same as what the model training/validation was done with. The cat_dict is the initial factorization of the categories and here I'm just making sure that the text categories are converted to the same numbers as what the model was trained/validated on.When I attempted this, I came to this error:which is understandable because the tf-idf of the new dataset does not produce the same number of features as the training/validation dataset initially used.So I was wondering if there a workaround for this? Should I not use tf-idf in the first place when training the model? If not, what are alternative ways for feature selection that would not lead to this problem later on?Thanks in advance and again sorry if I'm missing something obvious."
2439,Text clustering/NLP [closed],"['python', 'machine-learning', 'nlp', 'text-classification']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.Imagine there is a column in dataset representing university. We need to classify the values, i.e. number of groups after classification should be as equal as possible to real number of universities. The problem is that there might be different naming for the same university. An example: University of Stanford = Stanford University = Uni of Stanford. Is there any certain NLP method/function/solution in Python 3?Let's consider both cases: data might be tagged as well as untagged.Thanks in advance."
2440,How to prevent overfitting in Keras sequential model?,"['python', 'machine-learning', 'scikit-learn', 'keras', 'text-classification']","I am already adding dropout regularization. I am trying to build a multiclass text classification multilayer perceptron model.
My model:My model.summary():I am getting: The validation accuracy is ~20% less than the accuracy, and the validation loss is way higher than the training loss. I am already using dropout regularization, and using epochs = 1000, batch size = 512 and early stopping on val_loss.Any suggestions?"
2441,Keras: tweets classification,"['python', 'machine-learning', 'keras', 'text-classification', 'tweets']","Hello dear forum members,I have a data set of 20 Million randomly collected individual tweets (no two tweets come from the same account). Let me refer to this data set as ""general"" data set. Also, I have another ""specific"" data set that includes 100,000 tweets collected from drug (opioid) abusers. Each tweet has at least one tag associated with it, e.g., opioids, addiction, overdose, hydrocodone, etc. (max 25 tags).My goal is to use the ""specific"" data set to train the model using Keras and then use it to tag tweets in the ""general"" data set to identify tweets that might have been written by drug abusers.Following examples in source1 and source2, I managed to build a simple working version of such model:In order to move forward, I would like to clarify a few things:Any other relevant feedback is also greatly appreciated.Thanks!Examples of ""general"" tweets:Examples of ""specific"" tweets:"
2442,"Getting different accuracy on each run of Random Forest, Non-Linear SVC and Multinomial NB in python for text classification","['python', 'machine-learning', 'classification', 'random-forest', 'text-classification']","I am working on a binary text classification problem in python, and have developed models in Random Forest, Non-Linear SVC & Multinomial NB.But on each run, of these respective models, am getting different accuracy & confusion matrix parameters on the test set. I have used random_state parameter in train_test_split and while initializing each of these models. Random.Seed is also added in the code.Is there anything else I am missing? Thanks.Code Sample:"
2443,Extract most important features (per Class) using mutual_info_classif,"['scikit-learn', 'text-classification']","I'm using mutual_info_classif to determine the most important words for a binary text-classification task as:but the above gives an array of feature scores without reference to the corresponding classesIs there a way to get the most important features per class using MI? P.s., I've already tried Chi2 but it gives the same feature rank for both classes"
2444,Tensorflow - Sparse embedding lookup that remains sparse,"['python', 'tensorflow', 'conv-neural-network', 'text-classification']","I'm implementing a text classifier with a CNN similar to Kim 2014 with Tensorflow. Tensorflow provides tf.nn.embedding_lookup_sparse, which allows you to provide the word IDs as a sparse tensor. This is nice, especially for enabling variable length sequences. However, this function requires a ""combination"" step after the lookup, such as ""mean"" or ""sum"". This coerces it back to the dense tensor space. I don't want to do any combination. I want to keep my vectors in the sparse representation, so I can do other convolutions afterwards. Is this possible in TF?EDIT: I want to avoid padding the input prior to the embedding lookup. This is because Tensorflow's embedding lookup generates vectors for the pad value, and its a kludge trying to mask it with zeros (see here)."
2445,How to use class Imbalance technique (SMOTE) with Java Weka API?,"['java', 'weka', 'text-classification']","I am trying to build classification model using Java Weka API. My training data set have class imbalance problem. For this reasons, I want to use class imbalance techniques like SMOTE to reduce the class imbalance problem. Source Code are below:My code is work well without class imbalance techniques. But, I need to use class imbalance techniques to mitigate class imbalance problem. But, I do not know how to use it in Java Weka API."
2446,What is the numpy way to conditionally merge arrays?,"['python', 'numpy', 'text-classification']","I have two numpy arrays (1000,) filled with predictions from two models:model_1 is attractive due to extremely low FP, but consequently high FN.model_2 is attractive due to overall accuracy and recall.How can I conditionally apply predictions to take advantage of these strengths and weaknesses?I'd like to take all positive (1) predictions from the first model, and let the second model deal with the rest.Essentially I'm looking for something like this:This fails: The truth value of an array with more than one element is ambiguous.What is the numpy way to combine these arrays as above?"
2447,Maybe Overfitting trying to implement rnn with keras,"['machine-learning', 'neural-network', 'keras', 'text-classification', 'rnn']","I am trying to create a RNN for sentiment classification (-1(neg) or 1(pos)).My dataset file looks like this:
text,polarity
""this is a line"",1
""this is  asecond line"",-1
.....
etc.
with 2603 rows like these.
This is what i got so far and it reaches maximum accuracy very fast, so i assume it is wrong. What am i doing wrong though? Code:    "
2448,how to avoid calculate all distances in cluster to get most related posts in text clustering?,"['python', 'cluster-computing', 'cluster-analysis', 'text-classification']","In k-means clusteringI have large number of samples in each cluster, so when i have new sample and i want to get nearest 10 posts to it,I have to calculate distance using euclidean or cosine or whatever which take a lot of time to calculate with this large number of samples within the cluster.
Is there is any way to set radios and use the new sample as center and get only values within this radios?"
2449,Improving Accuracy on Text Classification,"['python', 'machine-learning', 'neural-network', 'text-classification']","I am trying to find a model for a text classification task. The number of my samples are around 4500 sentences of each sentence being approx 50 words length. The classes that i want to classify my text are 3, positive, negative and neutral. I used machine learning (SVM,RF,LR) and i got not more than 75% accuracy (i ve done the preprocessing part aswell). I would like to work a little bit with Deep Neural Nets and reccurent maybe but i dont know where to start. What are your suggestions in order to achieve maximum accuracy? and how much accuracy should i expect? (p.s I use python) "
2450,“ValueError: The shape of the input to ”Flatten“ is not fully defined” with variable length LSTM,"['python', 'keras', 'lstm', 'text-classification', 'variable-length']","Here's my code:I'm building an LSTM with variable input through 
this stackoverflow question. But now my model is saying ValueError: The shape of the input to ""Flatten"" is not fully defined (got (None, 20). How can I fix this?Thanks in advance"
2451,How to find similar noun phrases in NLP?,"['nlp', 'text-classification', 'synonym', 'ner', 'pattern-synonyms']","Is there a way to identify similar noun phrases. Some suggest use pattern-based approaches, for example X as Y expressions:Usain Bolt as Sprint KingLiverpool as Reds"
2452,Select top n TFIDF features for a given document,"['python', 'scikit-learn', 'sparse-matrix', 'text-classification', 'tf-idf']","I am working with TFIDF sparse matrices for document classification and want to retain only the top n (say 50) terms for each document (ranked by TFIDF score). See EDIT below.I have tried following the example in this post, although my aim is not to display the features, but just to select the top n for each document before training. But I get a memory error as my data is too large to be converted into a dense matrix.Is there any way to do what I want without working with a dense representation (i.e. without the toarray() call) and without reducing the feature space too much more than I already have (with min_df)?Note: the max_features parameter is not what I want as it only considers ""the top max_features ordered by term frequency across the corpus"" (docs here) and what I want is a document-level ranking.EDIT: I wonder if the best way to address this problem is to set the values of all features except the n-best to zero. I say this because the vocabulary has already been calculated, so feature indices must remain the same, as I will want to use them for other purposes (e.g. to visualise the actual words that correspond to the n-best features).A colleague wrote some code to retrieve the indices of the n highest-ranked features:But from there, I would need to either:There is a post here explaining how to work with a csr_matrix, but I'm not sure how to put this into practice to get what I want."
2453,Using sparse_softmax_cross_entropy() with unequal number of examples in each class,"['tensorflow', 'machine-learning', 'neural-network', 'text-classification']","I am working on convolutional neural network based text classifier. I am using the sparse_softmax_cross_entropy() on my logits tensor to get the loss but it is producing the following error:Based on what I could gather this is due to unequal number of examples across my classes(I tried running the classifier with equal number of examples for each class, but this gives a final accuracy of just 0.11). The difference of examples is not that profound given that I have 16542 examples spread across 9 classes and with 6 classes consisting of 2001, and three with 1452, 1300 and 1787 examples. How can I get the classifier to work without reducing the number of examples I have for training."
2454,I need to automatize the extraction of a logical statement (SWRL) from sentences in English,"['python', 'nlp', 'text-classification', 'spacy']","(Excuse me for my English, I'm also new at this, so be gentle, thank you)I'm trying to extract a logical statement(SWRL) from any possible sentence that contains actions and conditions This is the kind of logical statement I'd like to obtain:IF (CONDITION) THEN (ACTION | NOT ACTION | ACTION OR NOT ACTION)I've been trying to apply some NLP techniques with Spacy and Stanford NLP library, but my lack of knowledge about grammatical English structures makes it almost impossible for me.I'd like to know if someone could help me with this research, either with ideas or with unknown libraries for me.For example:Obtaining the root:Out: (is, 3)Obtaining the subject:Out: (limit, 2)Obtaining the childrens (dependencies of the word):Out: 'The speed limit 'Doc ents + root:Out: 'is 90 kilometres per hour 'Extracting the action:Applying all the funcionsOut: 'A traffic light with signal indicates '"
2455,"Image Classification using openCV, feature extraction and model building","['opencv', 'image-processing', 'deep-learning', 'text-classification']","I am currently working on a project, where the problem statement is to detect handwritten text from a image of a particular form. As a pre-processing step I have extracted texts in the form of bounding boxes, and I have around 1500 images of texts extracted from the image form, out of which 50 of them are handwritten.The problem is how do I now use these extracted images to train a classifier model which will classify the images as printed or handwritten text. I have no prior knowledge of Deep learning. Any help will be appreciated. I am uploading the image and the extracted images, as well as the code to extract the texts from the images.Images:
"
2456,"Text classification + NLP + Python : Warning: The least populated class in y has only 23 members, which is too few","['python', 'machine-learning', 'nlp', 'text-classification']","I am working on a text classification problem for which I am using 30 fold cross validation. Before starting experiment I made sure that each class had at least 30 members. Then I did necessary text processing and split my dataset into test and train sets.Test set consists of 20% of total data. Now when I run my model for training, I get this warning:Apparently, it seems after splitting my data into test set and train set, I have at least one class in my train set which has as few as 23 members. Am I correct?"
2457,tf-idf sickitlearn separate “word” from word,"['python', 'text-classification', 'tf-idf']","I am working with a problem in text classification where If a word was found in this format ""word"" it will have a different importance from if found in this format word  so I tried this code The result was Where I wished it would be"
2458,Multiclass Text Classification in Python,"['python', 'text-classification', 'multiclass-classification']","I am trying to create a Multiclass Text Classifier as explained here. However, my code is breaking at line:Below is the error which I am getting:I tried to find out what train[category] returns and I got same error. 1) X_train is a dataframe with one column and contains customer feedback. 2) train is a dataframe with two columns; first column contains customer review(same as X_train) and second column contains one of the 5 categories (Systems Error, Proactive Communication, Staff Behaviour, Website Functionalities, Others).3) category is one of the above mentioned categories.Below is the sample train dataframe:"
2459,Text Classification + NLP + Data-mining + Data Science: Should I do stop word removal and stemming before applying tf-idf?,"['nlp', 'data-mining', 'data-science', 'text-classification', 'tf-idf']","I am working on a text classification problem. The problem is explained below:I have a dataset of events which contains three columns - name of the event, description of the event, category of the event. There are about 32 categories in the dataset, such as, travel, sport, education, business etc. I have to classify each event to a category depending on its name and description.What I understood is this particular task of classification is highly dependent on keywords, rather than, semantics. I am giving you two examples:If the word 'football' is found either in the name or description or in both, it is highly likely that the event is about sport.If the word 'trekking' is found either in the name or description or in both, it is highly likely that the event is about travel.We are not considering multiple categories for an event(however, that's a plan for future !! )I hope applying tf-idf before Multinomial Naive Bayes would lead to decent result for this problem. My question is:Should I do stop word removal and stemming before applying tf-idf or should I apply tf-idf just on raw text? Here text means entries in name of event and description columns."
2460,"Text classification + Naive Bayes + Python : Input contains NaN, infinity or a value too large for dtype('float64')","['python', 'numpy', 'scikit-learn', 'text-classification', 'naivebayes']",I am trying to do text classification with Naive Bayes. This is my code:Error:The type of x_train_counts is scipy.sparse.csr.csr_matrix.The type of y_train is pandas.core.series.Series.
2461,Online LSTM classification model giving very high number of wrong predictions,"['python', 'tensorflow', 'machine-learning', 'lstm', 'text-classification']","I am trying to implement an online classification model using the 20 news groups data-set to classify the posts into relevant groups.pre-processing: I am going through all the posts and making a dictionary with the words.Then I am indexing the words starting from 1. I then iterate through all the posts and for each word in a post I am searching the vocabulary and putting the relevant index number into an array. Then I padded all the arrays by putting 0s at the end so that they are all the same size ( 6577). I am then creating am embedded layer (embed size=300). and each input will go through this embedded layer before being fed to the LSTM layer (LSTM input shape= (1,6577,300)).In my model I have a LSTM layer (size = 200) and a hidden layer (size= 25). I am using the dynamic_rnn  cell in tensorflow for this and I am setting the sequence length parameter to the actual length of the post (length without padded 0s) to avoid analyzing the padded 0s. Then from the output of the LSTM layer I am feeding only the relevant output to the hidden layer.From there onward, it is like an ordinary LSTM implementation. I have done everything I know of to improve the accuracy of the model but the number of wrong predictions is very high:Number of data points: 18,846 
   Errors:  17876
  Error Rate: 0.9485301920832007Note : During the back propagation I am training the embedded layer and the hidden layer.Question: I want to know what I am doing wrong here or any thoughts to improve the model. Thank you in advance.My complete code is shown below:EDIT"
2462,Text Classification + Naive Bayes + Scikit learn,"['scikit-learn', 'text-classification', 'naivebayes']","I am going to do Text classification first time with Naive Bayes.
This code I found on http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html :I want to resolve one doubt about the parameters X_train_tfidf, twenty_train.target passed to the function fit().X_train_tfidf is the tfidf vector representation of all the documents in the train set.twenty_train.target is the corresponding labels of documents in the exact order as they appear in the X_train_tfidf set.Am I correct?"
2463,Text Pre-processing + Python + CSV : Removing special characters from a column of a CSV,"['python', 'csv', 'text-processing', 'text-classification', 'python-textprocessing']","I am working on a text classification problem. My CSV file contains a column called 'description' which describes events. Unfortunately, that column is full of special characters apart from English words. Sometimes the entire field in a row is full of such characters, or, sometimes, few words are of such special characters and the rest are English words. I am showing you two specimen fields of two different rows:In the first one the entire field is full of such unreadable characters, whereas in the second case, only few such characters are present. Rest of them are English words.I want to remove only those special chars keeping the English words as they are, as I need those English words to form a bag of words at a later stage.How to implement that with Python ( I am using a jupyter notebook) ?"
2464,Prediction giving same value in every Iteration in an online multiclass classification using LSTM,"['python-3.x', 'tensorflow', 'machine-learning', 'lstm', 'text-classification']","I have developed a code to do online multi-class classification using the 20 news groups data-set. In order to eliminate the effect of the padded 0s of the text fed into the LSTM, I added the 'sequence_length' parameter to the dynamic_rnn passing the length of each text being processed. After I added this attribute, the prediction (the code shown blow) gives the same prediction for all the iterations except the very 1st one.Shown below are the predictions I received for the 1st, 2nd, 3rd and 4th iterations :1st: [[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05
  0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05]]2nd: [[0.04994956 0.04994956 0.04994956 0.04994956 0.04994956
  0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.0509586  0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956]]3rd: [[0.0498649  0.0498649  0.0498649  0.05072384 0.0498649 
  0.0498649
    0.0498649  0.0498649  0.0498649  0.0498649  0.05170782 0.0498649
    0.0498649  0.0498649  0.0498649  0.0498649  0.0498649  0.0498649
    0.0498649  0.0498649 ]]4th: [[0.04974937 0.04974937 0.04974937 0.05137746 0.04974937
  0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.05234195 0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.05054148 0.04974937 0.04974937]]After the 2nd iteration the prediction doesn't change (the argmax of the prediction always comes as 10).Question: What am I doing wrong here? 
Thank you in advance!Shown below is my complete code:"
2465,What's the accurate dataset for text classification of tntsearch package,"['php', 'machine-learning', 'dataset', 'text-classification']","I have found a package for text classification in PHP in which the method for the classifier accepts the sentence and the category like this:What dataset is best for this approach? 
And also I have a dynamic category which means I can add additional category. My problem is I have to give examples in every category added, which means I need more data in this category."
2466,h2o automl max_runtime_secs not stopping execution,"['h2o', 'text-classification', 'automl']","To Whom It May Concern,The code below is being run in a Docker container based on jupyter's data science notebook;
however, I've install Java 8 and h2o (version 3.20.0.7), as well as exposed the necessary ports. The docker container is being run on a system using Ubuntu 16.04 and has 32 threads and over 300G of RAM.
h2o is using all the threads and 26.67 Gb of memory. I'm attempted to classify text as either a 0 or a 1 using the code below.
However, despite setting max_runtime_secs to 900 or 15 minutes, the code hadn't finished executing and was still tying up most of the machine resources ~15 hours later. As a side note, it took df_train about 20 minutes to parse. Any thoughts on what's going wrong?"
2467,Text classification - randomForest. variables in the training data missing in newdata,"['r', 'r-caret', 'text-classification']","I'm completely new to statistical learning etc but have a particular interest in text classification. I was following a lab I found on the topic here: https://cfss.uchicago.edu/text_classification.html#fnref1. Unfortunately the lab ends before the trained model could be used on new data, so I tried to figure out how to complete it myself. I have my model trained, Im using random forest. When I try to use predict() on new data it throws an error: Error in predict.randomForest(modelFit, newdata) : 
  variables in the training data missing in newdataWhich in my mind doesn't make sense as the test data is literally a subset of the original data. I assume this error has something to do with how I built my model vs the data structure of the test data but I'm honestly not competent enough to figure out how to solve the error or where it is actually even stemming from (though I assume Im making some ridiculous error). There are other posts with the same error but I think the source of their errors are different to mine, I've tried to find a fix for this all day! Complete code I'm using below: The last line (final_predictions <- predict(congress_rf, newdata = test) is where the error appears, no error messages occur before that. "
2468,How can a machine learning model handle unseen data and unseen label?,"['machine-learning', 'scikit-learn', 'nlp', 'text-classification', 'naivebayes']","I am trying to solve a text classification problem. I have a limited number of labels that capture the category of my text data. If the incoming text data doesn't fit any label, it is tagged as 'Other'. In the below example, I built a text classifier to classify text data as 'breakfast' or 'italian'. In the test scenario, I included couple of text data that do not fit into the labels that I used for training. This is the challenge that I'm facing. Ideally, I want the model to say - 'Other' for 'i like hiking' and 'everyone should understand maths'. How can I do this? I consider the 'Other' category as noise and I cannot model this category. "
2469,Keras - Wrong input shape in LSTM dense layer,"['python', 'keras', 'lstm', 'text-classification']","I am trying to build an lstm text classifier using Keras.This is the model structure:Where y_tr_word2vec is a 3-dimensional one-hot encoded variable.When I run the code above, I get this error:I suppose that the issue could be about y_tr_word2vec shape or the batch size dimension, but I'm not sure.Update:I have changed return_sequences=False, y_tr_word2vec from one-hot to categorical, 1 neuron in dense layer, and now I am using sparse_categorical_crossentropy instead of categorical_crossentropy.Now, I get this error: ValueError: invalid literal for int() with base 10: 'countess'.Therefore now I suppose that, during fit(), something goes wrong with the input vector X_tr_word2vec, which contains the sentences."
2470,Classifying data into 3 classes with a binary classifier model,"['python', 'machine-learning', 'svm', 'text-classification']",Hi I am new to Machine Learning. I have a task of classifying data into 3 classes. I have data for only 2 classes. I have used SVM Linear Classifier for the classes for which I have data (2 classes say A and B). Now there is another third class C for which I don't have any data. is there a way that I can use this binary classifier and predict if a sample doesn't belong to any of the 2 classes(A and B) then it must belong to C ?
2471,Create ML Text Classifier probabilities,"['swift', 'text-classification', 'coreml', 'createml']","I am creating model with Create ML. I am using a JSON file.The JSON file looks like thisFollowing a tutorial I am trying to do a text detection on the ""text"" and return the possible ""author""I can make that but I would like to have also the probability.Creating the model with Create ML, as a Text Classifier I get only output a label: Author. Is there a way with Create ML to have also probability in Text Classification?Thanks"
2472,R - How to apply terms from training document-term-matrix (dtm) to test dtm (both unigrams and bigrams)?,"['r', 'text', 'nlp', 'tm', 'text-classification']","I am training a simple text classification method on 1,000 training examples and would like to make predictions on unseen test data (about 500,000 observations).The script is working fine, when I work only with unigrams. However, I am not sure how to use control = list(dictionary=Terms(dtm_train_unigram)) when working with unigrams and bigrams as I have two separate document-term-matrices (one for unigrams, one for bigrams, see below):To ensure that the test set has the same terms as the training set, I use the following function:How do I feed the terms of both the dtm_train_unigram and the dtm_train_bigram to the dtm_test?Thank you!"
2473,in which situation different methods in multi-label classifications in scikit learn should be applied,"['python', 'scikit-learn', 'text-classification', 'multilabel-classification']","I have read this to learn about various method in multi-label classifiers.
I learned that there are 3 techniques to do multi-label classifications:In the category of Problem transformations there are more three sub categories:I know that when we want better result we should apply the ensemble model. 
I would like to know in which situations the other different algorithm we should use.I know how they differently work, but I do not know when I should use each of them.And Also there is only two method implemented for Adapted Algorithm.
what if I want other methods but implemented in adapted algorithm approach?Please let me know if my statements are not clear. Thanks,"
2474,Keras: How to display attention weights in LSTM model,"['python', 'keras', 'lstm', 'text-classification', 'attention-model']","I made a text classification model using an LSTM with attention layer. I did my model well, it works well, but I can't display the attention weights and the importance/attention of each word in a review (the input text).
The code used for this model is:"
2475,How to classify the text which are not belong to the classes which I have to “Unknown” in text classifcation?,"['nlp', 'text-classification']","I am working on a NLP problem to classify the text to four classes. 
1. Sports
2. Entertainment
3. Astrology
4. UnknownI have created a training dataset for Sports, Entertainment, Astrology. But How to create a training dataset for ""Unknown"" category or how to classify the text which are not belong to first three category to the last category i.e ""Unknown category"""
2476,Sklearn Pipeline ValueError: could not convert string to float,"['python', 'scikit-learn', 'nlp', 'text-classification']","I'm playing around with sklearn and NLP for the first time, and thought I understood everything I was doing up until I didn't know how to fix this error. Here is the relevant code (largely adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html):The data loaded into the DataFrame is preprocessed text with all stopwords, punctuation, unicode, capitals, etc. taken care of. This is the error I'm getting once I call fit on the classifier where the ... represents one of the documents that should have been vecorized in the pipeline:I first thought the TfidfVectorizer() is not working, causing an error on the SVD algorithm, but after I extracted each step out of the pipeline and implemented them sequentially, the same error only came up on XGBClassifer.fit().Even more confusing to me, I tried to piece this script apart step-by-step in the interpreter, but when I tried to import either read_files or build_data_frame, the same ValueError came up with one of my strings, but this was merely after:I have no idea how that could be happening, if anyone has any idea what my glaring errors may be, I'd really appreciate it. Trying to wrap my head around these concepts on my own but coming across a problem likes this leaves me feeling pretty incapacitated."
2477,"Classify strings having centers already found, python [closed]","['python', 'classification', 'cluster-analysis', 'text-classification']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 2 years ago.I have a list of binary strings and two center strings which are not in the list.
I would like to classify that list around the center strings in order to create two clusters. A string of the list will be assigned to the cluster whose center is nearest to that string (hamming distance as metric).
I've seen that there are alghoritms like Neighbours Classifier, k-medoids, Affinity propagation, but all these procedure calculate centroids on their own; I have to use my center strings instead.Any suggestion?"
2478,Is it possible to update existing text classification model in tensorflow?,"['python', 'tensorflow', 'text-classification', 'resuming-training']","I am new to Python and have been performing text classification with tensorflow. I would like to know if this text classification model could be updated with every new data that I might acquire in future so that I would not have to train the model from scratch. Also, sometimes with time, the number of classes might also be more since I am mostly dealing with customer data. Is it possible to update this existing text classification model with data containing more number of classes by using the existing checkpoints?"
2479,RNN: Get prediction from a text input after the model is trained,"['python', 'tensorflow', 'machine-learning', 'keras', 'text-classification']","I am new to RNNs and I have been working on a small binary label classifier. I have been able to get a stable model with satisfactory results. However, I am having a hard time using the model to classify new inputs and I was wondering if any of you could help me. Please see my code below for reference. Thank you very much.When I run my code I get the following error:ValueError: Error when checking input: expected inputs to have shape
  (39,) but got array with shape (1,)"
2480,How do I determine the binary class predicted by a convolutional neural network on Keras?,"['python', 'machine-learning', 'keras', 'deep-learning', 'text-classification']","I'm building a CNN to perform sentiment analysis on Keras.
Everything is working perfectly, the model is trained and ready to be launched to production.However, when I try to predict on new unlabelled data by using the method model.predict() it only outputs the associated probability. I tried to use the method np.argmax() but it always outputs 0 even when it should be 1 (on test set, my model achieved 80% of accuracy). Here is my code to pre-process the data:And here is my model:I also tried to change the number of activations on the final Dense layer from 1 to 2, but I get an error: "
2481,How do you treat multi-class classification use case?,"['text-classification', 'multiclass-classification']","I have a list of labelled text. Some have one label, others have 2 and some have even 3. Do you treat this as a multi-class classification problem?"
2482,Classification of Latin Text,"['python', 'classification', 'text-classification']","I am following a tutorial on classification of text - https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/.My text domain is notarial deeds in Latin and the corresponding deed type. This is an extract of my data:I only have 150 deeds at present and the text is stripped from any punctuation in a previous step.When I try the classifiers such as Naive Bayes, after extracting the features, I am getting very low accuracies compared to the example:Any idea why? Is the training set too small?I have 53 different labels - which I am noticing that there are spelling variations I can correct...but still it is a large number of labels. These are all the labels:and this is all the dataframe:
https://drive.google.com/open?id=1D6MxfoOLbHzld86Rw1Cso-mpiUpKv7rQ"
2483,LSTM Text Classification Bad Accuracy Keras,"['keras', 'lstm', 'text-classification', 'rnn', 'multilabel-classification']","I'm going crazy in this project. This is multi-label text-classification with lstm in keras. My model is this: Only that I have too low an accuracy .. with the binary-crossentropy I get a good accuracy, but the results are wrong !!!!! changing to categorical-crossentropy, I get very low accuracy. Do you have any suggestions?there is my code: GitHubProject - Multi-Label-Text-Classification"
2484,Classification with one file with entirely the training and another file with entirely test,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'text-classification']","I am trying to make a classification in which one file is entirely the training and another file is entirely the test. It's possible? I tried:I have set test_size to zero because I do not want to have a partition in those files. And I also applied Count and TFIDF in the training and test file.My output error:Traceback (most recent call last):File ""classif.py"", line 34, in 
      X_train, y_train = train_test_split(X, y, test_size = 0, random_state = 100)ValueError: too many values to unpack (expected 2)"
2485,ValueError: cannot use sparse input in 'SVC' trained on dense data,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'text-classification']","I'm trying to run my classifier but I get this errorMy output error:ValueError: cannot use sparse input in 'SVC' trained on dense dataI can not execute my code because of this problem and I am not understanding anything of what is happening.all output errorTraceback (most recent call last):File ""classification.py"", line 42, in 
      predicted = classifier.predict(test_tfidf)File ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 584, in predict
      Y = self.decision_function(X)File ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 614, in decision_function
      for est, Xi in zip(self.estimators_, Xs)]).TFile ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 614, in 
      for est, Xi in zip(self.estimators_, Xs)]).TFile ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 548, in predict
      y = super(BaseSVC, self).predict(X)File ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 308, in predict
      X = self._validate_for_predict(X)File ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 448, in _validate_for_predict
      % type(self).name)ValueError: cannot use sparse input in 'SVC' trained on dense data"
2486,NameError: name 'fit_classifier' is not defined,"['python', 'python-3.x', 'scikit-learn', 'classification', 'text-classification']","I'm trying to make a text classifierBut I have the following error and I can not understand. Traceback (most recent call last):File ""classificacao.py"", line 37, in 
      fit_classifier(X_train, y_train)NameError: name 'fit_classifier' is not definedBut fit is not always defined by default?"
2487,Keras LSTM predict two features from one input in Text classification?,"['python', 'keras', 'text-classification']","I have X as text, with two different labels(columns) to train.Here my input X will be content. I have converted it to sequence matrix. I need both category and rate to be trained along with content. I couldn't figure out how to pass this inside the layers.Y_train contains only category. I want to add rate to the training. Does any one know?
I want two results. One should be about category and another is Rate. 
Currently its returning only the label. Not with the rate. I don't know the way to add a layer for the Rate column."
2488,enwiki-latest-categorylinks.sql.gz. sub-category fiels is not in readable format,"['python', 'text-classification']",I'm doing keyword classification using Wikipedia category and sub-category. I downloaded dump file enwiki-latest-categorylinks.sql.gz. The super-category fields are readable but the sub-category field is not readable. can anyone help me out?Thanks
2489,Text classification with Naive Bayes,"['python', 'nlp', 'nltk', 'text-classification', 'textblob']",I am leaning NLP and noticed that TextBlob classification based in Naive Bayes (textblob is Build on top of NLTK) https://textblob.readthedocs.io/en/dev/classifiers.html works fine when training data is list of sentences and does not work at all when training data are individual words (where each word and assigned classification).Why?
2490,sklearn How to use MultiOutputClassifier with multi-label text classification,"['python', 'scikit-learn', 'classification', 'text-classification']","I am trying to do multi-output multi-label multi-class text classification. The below example works, but I know it's not properly using the MultiOutputClassifier. I believe the point to it is to only have to train once and fit once, even for multiple outputs. How can I do this with a single pass through my data?The output from running it:"
2491,NLTK NaiveBayesClassifier classifier issues,"['python', 'nltk', 'text-classification', 'naivebayes']","I am experimenting with NaiveBayesClassifier and have following training data:I then classify following sentence: bad Awesome movie, I liked itHere is what I get for each word:bad:neg
awesome:pos
movie,:pos
i:pos
liked:pos
it:posHow/why decision is made to classify words not in the training set (such as I Liked It, Movie) as positive?thanks"
2492,Predict a text with bag of word approach,"['python', 'machine-learning', 'scikit-learn', 'keras', 'text-classification']","I am trying text classification using the bag of word model. Everything works fine till I use the test set for testing and evaluation of accuracy but how we can check the class of a single statement.I have a data frame with 2 classes labels and body. Now I want to predict this statement using my model. How to do this 
I tried converting my statement to vector using the count vectorizer but according to the bag of word approach, it is just an 8 dimension vector. "
2493,Machine Learning/NLP text classification: training a model from corpus of text files - scikit learn,"['machine-learning', 'scikit-learn', 'nlp', 'text-classification']","I am very new to machine learning and I was wondering if somebody could take me through this code and why it is not working. It is my own variation of the scikit-learn tutorial found at: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html which is basically what I am trying to do. I need to train a model with a labelled training set so that when I use my test set, it can predict the label of the test set. Also it would be really useful if somebody could show me how to save and load the model. Thank you very much. This is what I have so far:I saw online people using csv files to input the data so I tried that too, i may not need it so i apologise if that is incorrect. error being shown: Thank you very much for your help, please let me know if you need further explanation.sample of two entries in csv:"
2494,which class is positive and how to determine real alarm in preparing data for ML,"['machine-learning', 'classification', 'text-classification', 'confusion-matrix']","I am preparing some data for Machine Learning.
This question would be very easy, but I am little bit confusing.Let's suppose there are system that have about 100 alarm every 1 hour and only 1 or 2 alarm are real alarm of them. Fake alarm will be ignored by human. I've collected some datas that looks like feature and gave label 0 or 1 to fake or real alarm.In this case, is real alarm 0 or 1? By this, TP, TN, FP, FN, chance level would be changed. What we have interest in is real alarm and we don't want to miss it even though all alarms are checked by human manually.Almost alarm is fake, so the chance level would be over 95%. Then the major class  and positive class would be 1 and fake alarm? But our interest is not fake alarm.
How should I set the label in this case?"
2495,Text-Classification RNN - LSTM - Error checking target,"['python', 'machine-learning', 'lstm', 'text-classification', 'rnn']","i'm developing a LSTM - RNN text classification with Keras
This is my Code.My model is this:My error is:
Error when checking target: expected activation_1 to have 3 dimensions, but got array with shape (750, 1)I try to reshape all my array, but i not found the solution.
Can someone help me??? thank you :D
Sorry for my bad english."
2496,convolutional neural network (CNN) for text classification,"['text', 'keras', 'deep-learning', 'conv-neural-network', 'text-classification']","I am using CNN for text classification, in my model after flatten layer I used output layer directly without using dense layer. is that correct or must use dense layer? just as example:"
2497,Does similarity based algorithms outperform SVM/Tree algorithms in text classification ?,"['machine-learning', 'nlp', 'svm', 'text-classification', 'cosine-similarity']","What performs better for text classification similarity (cosine distance) based algorithms or regular classification methods like SVM or decision trees in terms accuracy and performance? 
Does SVM resolves in finite time with 4GB-8GB RAM systems, while training with large text data ?"
2498,Getting same output using Naive Bayes Classifier Python for Text Classification,"['python-3.x', 'machine-learning', 'spyder', 'text-classification', 'naivebayes']","I am trying to do text classification in Python using Naive Bayes classifier, and it works well when there are two labels ""negative"" and ""Positive"" as the results. I have sample data of about 300 sentences, and I want to label them based on Moore Bygrave model, so essentially I want to map these 300 sentences to around 10 labels. So when I train the model and try to predict the answer is always just one label ""Commitment"". I am confused, can anybody please guide and give me some references as to how I handle complex text classification problems using Python. The code has been shown below:Where as the label for any of these is not Commitment, please advice as to how I should fix my code, so that proper text classification can be achieved, please help out. Thanks."
2499,Removing irrelevant information of online articles,"['nlp', 'text-classification', 'data-cleaning']","I am doing text classification to detect political leaning in online news article. The problem is that the articles are very noisy with media attributes  such as media tag line, articles copyright, information of publication, author/reporter names, related-article links, etc, and there is no separator between the main information and the noise (html tags were already removed).
I've read several papers about how to clean irrelevant information from crawled online articles; however, all of them was doing the cleaning process in collection stage by using HTML tag. My research is purely NLP so it's out of my project's scope.I've studied about removing stopwords based on IDF and Information gain, also by using outlier detection techniques (distance-based, clustering based) . But I don't think they can work in my case.
Any suggestions how to automatically remove those irrelevant contents of news articles?
Thank you for any comments and answers."
2500,Cannot freeze Tensorflow models into frozen(.pb) file,"['python', 'python-3.x', 'tensorflow', 'text-classification', 'tensorflow-serving']",I am referring  (here) to freeze models into .pb file. My model is CNN for text classification I am using (Github) link to train CNN for text classification and exporting in form of models. I have trained models to 4 epoch and My checkpoints folders look as follows: I want to freeze this model into (.pb file). For that I am using following script:I am using following argument parser to run the above code It is giving errorMy model is CNN for text classification. What should I write in output_node_names ? to produce a successful .pb file in the output
2501,Cloud ML Engine and Scikit-Learn: 'LatentDirichletAllocation' object has no attribute 'predict',"['python', 'machine-learning', 'scikit-learn', 'text-classification', 'google-cloud-ml']","I'm implementing simple Scikit-Learn Pipeline to perform LatentDirichletAllocation in Google Cloud ML Engine. Goal is to predict topics from new data. Here is the code for generating pipeline:Now (if I have understood correctly) to predict topics for test data I can run:However, when uploading pipeline to Google Cloud Storage and trying to use it to produce local predictions with Google Cloud ML Engine I get error that says LatentDirichletAllocation has no attribute predict.Lack of predict-method can be seen also from docs, so I guess this isn't the way to go with this.
http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.htmlNow the question is: What is the way to go? How to use LatentDirichletAllocation (or similar) in Scikit-Learn Pipelines with Google Cloud ML Engine?"
2502,How use custom features in Keras for text classification,"['python', 'keras', 'classification', 'data-analysis', 'text-classification']","I'm working for a text classificator in Python using Keras. For now I tried so make model only with the words of my dataset, using bag of words. Now I would use in my classifier other custom features (like polarity) but I don't know how to add there in my code. My dataset is like:The middle two colums are my custom features that i want add to my classifier but I don't know how.In this example i use only bag of words feature of the content f first column and i want add other 2 column like features (polarity, number of words). Someone has an idea how add these? Thanks in advance."
2503,Box plots in Python using Seaborn - creating duplicates for bigrams and trigrams,"['python', 'seaborn', 'boxplot', 'cross-validation', 'text-classification']","I am using Spyder as part of Anaconda and trying to classify tweets (text) by event type. To do this, I am using the package cross_val_score, having already vectorised my tweets using TfidVectorizer and then transforming my training data using fit_transform for unigrams, bigrams and trigrams, as per the below:Now I perform cross validation using the package cross_val_score to calculate the average accuracy for unigrams, bigrams and trigrams. Once complete, I am trying to produce and save a boxplot for the accuracies achieved. This is completed for 4 different models:The output of the unigrams is exactly what I want:Now when I run the code for bigrams and trigrams (highlight ALL code and hit 'play'), I get the following:Bigrams:[Trigrams:The code for each of these is identical, except they use 'cv_bigrams' and 'cv_trigrams' for the data input for the box plots. Code for each is below.Bigram code:Trigrams code:Here is what happens if I select the below code only and run:Same for trigrams:Output:Any idea why I am getting duplicate boxplots overlapping each other when I run all of the code at once (which I need to do when I put this code into production), rather than highlighting the snippets and running separately?"
2504,Dictionary not influenced by input?,"['r', 'text-classification', 'fasttext']","There's a get_dictionary() function  in the fastrtextpackage, and I thought it would return all the words in the dictionary. However, when I set wordNgrams to 2 or 3, it returned exactly the same list of words as what I got when setting wordNgrams to 1. Can someone tell me what's going on here? Thanks!"
2505,scikit-learn - Using a single string with RandomForestClassifier.predict()?,"['scikit-learn', 'text-classification']","I'm an sklearn dummy...  I'm trying to predict the label for a given string from a RandomForestClassifier() fitted with text, labels.It's obvious I don't know how to use predict() with a single string.  The reason I'm using reshape() is because I got this error some time ago ""Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.""How can I predict the label of a single text string?The actual learning data text file is 43663 lines long but a sample is in small_list.txt which consists of lines each in the format: <label>: <long text string>"
2506,Word embedding decreasing classification precision,"['scikit-learn', 'nlp', 'nltk', 'gensim', 'text-classification']","I am currently trying to classify text into 7 classes. Up to now, I have been able to reach a 90% precision score using Majority Voting (with SVM, Multinomial NB, Random Forest and KNN).I wanted to try to increase a little more this precision by using word embeddings and thus getting less dimensions for my samples. I use gensim word2vec to create my model and the NLTK list of stop-words and tokenizer:The model seems fine, I get satisfying results when I use similarity between words.I use this class to get a mean representation of my samples:and Finally, I build a common sklearn pipeline and let sklearn perform a GridSearchCV on my data:The issue is that I get random precision scores (around 0.5).Maybe dimensionality reduction is not always able to increase precision but I think I don't understand something or I did something wrong, do you have an idea of what is going wrong ?Thank you in advance"
2507,How to plot Vector space model for text clustering? [duplicate],"['c#', 'winforms', 'cluster-analysis', 'k-means', 'text-classification']","I'm using K-Means based text clustering technique. I have a vector space model which is a n-dimensional array of float. I'm running dry of ideas to plot it in the graph. Also my final need after plotting is to have each data points in the plot labelled( mostly with the file name of text document). My work is inspired by https://www.codeproject.com/Articles/439890/Text-Documents-Clustering-using-K-Means-Algorithm
If anyone could help in plotting for text clustering , it would be really a great help."
2508,Multi-Class Classification: SMOTE oversampling for multiple columns in a row,"['python', 'text-classification', 'oversampling']","I have an imbalanced dataset contained in a dataframe called city_country that is made up of 5 columns:In the dataframe called city_country, the class (event_id) is imbalanced. Before testing the predictive power of different text classifiers, to predict the event_id from the content of a tweet (preprocessed), I want to oversample the minority classes.It is important that when I duplicate the entries that belong to minority classes that I duplicate all 5 columns.What I have done so far (incorrectly) is only oversample the tweet content (preprocessed and the event_id. In the below code, I convert the tweets into vectors (which I do not want to do but to my knowledge, I have to) and then overrepresent minority classes. This only oversamples the vectorised tweet (x_words) and the event_id (y). To my knowledge, the use of SMOTE in imblearn.over_sampling requires you to feed real values (not strings) and only 2 values - an 'x' and a 'y'. In this case, 'x' is my vectorised training set of tweets and 'y' is my event label.Is there a way for me to simply split my dataframe into training and test sets, then oversample all 5 columns for the minority classes, so that the output is a larger dataframe that contains all 5 columns? I can then use this to predict the event_id and hopefully perform the equivalent of a vlookup, so that I can join the tweet with its respective lat and long values. "
2509,expected dense to have shape but got array with shape,"['python', 'keras', 'shape', 'text-classification']",I am getting the following error while calling the model.predict function when running a text classification model in keras. I searched the everywhere but it isn't working for me.My data has 5 classes and has a total of 15 examples only. Below is the datasetThis is the code of my modelI am not able to figure out where the problem lies and how to fix it.
2510,Text classification on feature vector X with multiple vs. merged columns,"['text', 'nlp', 'classification', 'text-classification']","I am working on text classification problem where I have around 95 data points and the data looks like this (only two dummy entries shown):In my current approach, I have merged the data in .csv, space delimited (shown below) and I am using only one column to do text-classification.This approach seems to work for me and I am getting around 70% of accuracy with test data.Now I am thinking of performing text classification on multiple columns of my feature vector (X) instead of merging all columns of X vector into one column (like by performing feature engineering on individual columns of X vector and then concatenating the transformed vectors. The approach has been mention in this article as well : https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308). Now my question is : When it comes to NLP, are both approaches equivalent theoretically. Should the later approach yield any better/worst results than my former approach.
Thanks. "
2511,How can I use regular expressions in my vocabulary for CountVectorizer?,"['python', 'scikit-learn', 'nlp', 'text-classification', 'countvectorizer']","How do I make ""First word in the doc was [target word]"" a feature? Consider these two sentences:If I were trying to measure relationship commitment, I'd want to be able to treat the phrase ""at the moment"" as a feature only when it shows up at the beginning like that.I would love to be able to use regex's in the vocabulary...But that doesn't seem to work. I have also tried this, but get a 'vocabulary is empty' error...What's the right way to do this? Do I need a custom vectorizer? Any simple tutorials you can link me to? This is my first sklearn project, and I've been Googling this for hours. Any help much appreciated!"
2512,How to change the classification with “yes” or “no” to the score of “yes” or “no” by using tensorflow?,"['python', 'tensorflow', 'conv-neural-network', 'text-classification']","I'm novice to deep learning.I use tensorflow to construct my TextCNN model(two categories) referring this tutorial.
This model can predict the categories of the text. But I want a score (continuous value in [0,1]) rather than the discrete value. For example, If the model give 0.77, the text is more likely one of the category; if it gives 1.0, the text is actually that category.
This is the part of my code.  Thanks in advance."
2513,Latent Semantic Analysis: How to choose component number to perform TruncatedSVD,"['scikit-learn', 'text-classification', 'svd', 'lsa']","I am practicing to use LSA to classify Enron dataset (all emails). My understanding is to successfully perform any further classification or clustering, I need to perform a lower rank approximation using TruncatedSVD to maximize the variance.I have done all the pre-processing i could think of including 1) removing all punctuation 2) removing words less than 2 characters 3) remove documents with text size less than 1500 byte (tfidf works better with longer text) 4) remove stop wordsHowever, if i set component to 100 per SKlearn suggests for LSA, i can only get 35% of variance (svd.explained_variance_ratio_.sum()). I tried with component = 2000, and can get 80%. ( i read somewhere saying one needs to get 90% variance as recommended?)So my question is to perform a successful LSA, 1) how to test and pick the number of component 2) is high component number normal? 3) anything i can do to increase variance while keeping component number low?"
2514,NLTK Classifier - “dict” object not callable in NLTK Naive Bayes Classification,"['python', 'nltk', 'text-classification']","I am trying to write a small classifier to classify male vs female names and keep getting ""dict object not callable"" error. I have replaced dict() with my_dict() to see if it was due to reserved words, but then the error becomes ""my_dict() is not defined"". Please help.My code is:Error is:"
2515,How to update trained Naive Bayes model saved with joblib,"['python', 'machine-learning', 'text-classification', 'naivebayes']","I am trying to update my already trained model so that it can correct the errors it is making. For that, I am partially fitting the new data for which it was giving incorrect labels with the new correct labelsI have saved my Bayesian model in a file like this:and then loading it another file like this:now it should use the updated model and if new_features_matrix is given to predict, it should predict new_label_matrix with high accuracy but the model is not. It gives the same label matrix as it was giving before refitting. Could it be that I have trained my initial model with similar data many times with different labels that it is not able to learn from fewer data? "
2516,How I can get the vectors for words that were not present in word2vec vocabulary?,"['python-3.x', 'pandas', 'word2vec', 'gensim', 'text-classification']","I have check the previous post link but it doesn't seems to work for my case:-I have pre trained word2vec model:Now I have a pandas dataframe with keywords:All I want to add the vectors for each keyword in its corresponding columns but
when I use model['cambodia'] it throw me error as KeyError: ""word 'cambodia' not in vocabulary""so I have update the keyword as:But this won't work out for me, when I use 
model['cambodia'] it still giving an error as KeyError: ""word 'cambodia' not in vocabulary"". How to update new words into word2vec vocabulary so i can get its vectors? Expected output will be:-"
2517,Precision recall datasets,"['information-retrieval', 'text-classification']","Can anyone point me at datasets of precision and recall generated by binary classifiers at different confidence levels?I'm not trying to develop a classifier. I'm doing an empirical investigation into common patterns that emerge when these data sets are evaluated.Ideally, what I'd like are three columns: precision, recall, confidence threshold. Alternatively, precision and recall could be replaced by the four underlying tallies: true positives, true negatives, false positives and false negatives.I could also work from lists of ranked results where each result is flagged as true or false.The actual classifiers and source data sets aren't that important but I would like to know what they are.Many thanks in advance."
2518,Custom estimator using sparse matrix,"['python', 'tensorflow', 'machine-learning', 'text-classification']","I'm trying to do almost the same thing as Tensorflow DNN with tf-idf sparse matrixHowever, I'm trying to make a custom estimator. 
I've followed most of what he has done, input:feature columns:getting sparse tensor:input function:getting feature column keys:input layer (effectively lifted off the tensorflow site):Lambda the input function and initialise:the error I get:I'm not too sure what I'm doing wrong, I've only just started using tensorflow. Train accepts a tuple of tensor values/labels, which was provided, and feature columns are a list of _featurecolumns."
2519,how can I use arabic light stemmer lib using java,"['java', 'text-classification']",I have a search classification of Arabic text and I try use the Arabic light stemmer to do text processing but does not know how i do you can help me?
2520,"SMOTE, Oversampling on text classification in Python","['python', 'machine-learning', 'nlp', 'text-classification', 'resampling']","I am doing a text classification and I have very imbalanced data like Now I want to over sample Cate2 and Cate3 so it at least have 400-500 records, I prefer to use SMOTE over random sampling, Code It does not work as it can't generate the sample synthetic text, Now when I covert it into vector like I am not sure if it is right approach and how to convert vector to real text when  I want to predict real category after classification "
2521,Is it normal that accuracy decreases when advancing to the next epoch?,"['pytorch', 'cnn', 'data-augmentation']",I’m training a CNN to predict digits using the MNIST database. I’m doing Data Augmentation and for some reason accuracy sharply decreases when advancing to next epoch (iteration 60 in the image)It has to do with data augmentation (transform = my_transforms in the code) because when I deactivate augmentation (transform = None) accuracy doesn't decrease when advancing to next epoch. But I can't explain why. Does anyone have an idea why this happens?
2522,Adding AdditiveGaussianNoise to a single image - AssertionError: Expected boolean as argument for 'return_batch',"['python', 'error-handling', 'data-augmentation']",I would like to add AdditiveGaussianNoise (link: https://imgaug.readthedocs.io/en/latest/source/overview/arithmetic.html#additivegaussiannoise) to a single image which I resized before.This is my code:And I get this error message:How do I have to amend my code?Thank you very much!
2523,how to fix this issue ? cv2.error: OpenCV(4.1.2) … error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize',"['numpy', 'opencv', 'operating-system', 'rotation', 'data-augmentation']","I am trying to do rotation on multiple images in a folder but I am having this error when I put values of fx, fy greater than 0.2 in the resize function
(cv2.error: OpenCV(4.1.2) ... error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize')Although, when I try to rotate a single image and put values of fx and fy equal to 0.5, it works perfectly fine.Is there a way to fix this issue because it is very hectic to augment images one by one? Plus the multiple images which are rotated by the code attached here, with fx and fy values equal to 0.2, have undesirable dimensions i.e the photos are very small and their quality is also reduced.the part of code for rotation of multiple images is given below:"
2524,What are the mostly used Augmentation techniques provided by Tensorflow for Activity Recognition? [closed],"['python', 'tensorflow', 'deep-learning', 'data-augmentation']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 3 days ago.I'm doing a sports activity recognition task and for this, I have a total of 5 classes containing 100 videos each. In total, I have a dataset of 500 video dataset.I've extracted 20 frames from each video. So, in total, I have 10,000 image data. I want to apply data augmentation techniques while training the dataset. I have a thought to use the horizontal_flip, width_shift, height_shift and rotation techniques though not sure about their effectiveness on my dataset.So can anyone suggest me the most commonly used augmentation techniques for activity recognition task?"
2525,Data Augmentation on selected data [closed],"['python', 'keras', 'deep-learning', 'data-science', 'data-augmentation']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 5 days ago.I have a dataset directory where few classes are having less number of images.
Now i want to perform data augmentation on only those few classes.
How can i do that and how can i concat both augmented and not augmented images to give them as input to the model??????
I tried the below codeIt works but now how can I concat it to the not augmented images"
2526,Index out of bounds error in images displays,"['python', 'mnist', 'data-augmentation']","let us consider following code :it gives me following error :it should be noted that previous code for generating mnist  dataset  works finehere is its result :
"
2527,How to import ImgAug 0.4.0 in Colab?,"['python', 'import', 'google-colaboratory', 'data-augmentation']","I would like to use some augmentation techniques of the package imgaug 0.4.0 (link: https://github.com/aleju/imgaug) in Google Colab. Currently, there is version 0.2.9 imported by default. Is there a way to import 0.4.0?I already tried to install it using !pip install imgaug.Thanks a lot!"
2528,How to know which parameters were used with imgaug Sequential,"['python', 'conv-neural-network', 'data-augmentation']","I want to use the imgaug module to augment a set of data offline, for later use in a convolutional neural network. To easily apply several transformations, imgaug offers the Sequential object with a list of transformations to apply:The parameters inside each function are randomly generated in whatever range I give. A more complex sequence can be made by randomizing the application of functions as well:There is also SomeOf(f1, f2..) which will apply one (or more) of the functions passed as arguments, and several others that randomize which augmenters are used. All the random part is made internally. However, as part of the work done with images later, I need to retrieve and save, for each image, what function and what parameter was used. Since the call to seq only returns the images, I don't have access to these values. The most relevant discussion I found about this issue would be this one. The first answer could be a solution for the parameters, but it doesn't seem to be a universal solution for functions such as SomeOf, Sometimes etc., and the suggested method for SomeOf doesn't look like something I'm comfortable with and want in my code, especially since I plan to use quite a lot of transformations.Is there an easier, universal way to retrieve:"
2529,How to generate labels when performing data augmentation on an image dataset,"['computer-vision', 'dataset', 'object-detection', 'yolo', 'data-augmentation']","I have taken my image dataset from 'Open Images Dataset v6' which is labeled. It is small, so I want to perform data augmentation on it.How can I do it in a way that the new/changed images are produced with their respective labeled '.txt' files. So that I don't manually have to label them.Also I am Using Yolo and darknet for object detection."
2530,Include AdditiveGaussianNoise to customized ImageDataGenerator(sequence),"['python', 'keras', 'data-augmentation', 'data-generation']",I constructed the following DataGenerator for image data.x_set are my input data (images; the file paths to this images) and y_set the output data (labels). I now want to add ImageAugmentation and I would like to use AdditiveGaussianNoise (https://imgaug.readthedocs.io/en/latest/source/overview/arithmetic.html#additivegaussiannoise) to add Gaussian Noise to the input data.How can I include this to my DataGenerator and extend the Generator by ImageAugmentation?Thanks a lot!
2531,Mapping dataset to augmentation function does not preserve my original samples,"['tensorflow', 'tensorflow2.0', 'tensorflow-datasets', 'data-augmentation']","How should I implement an augmentation pipeline in which my dataset gets extended instead of replacing the images with the augmented ones, that means, how to use map calls to augment and preserve the original samples?threads I've checked: 1, 2I was expecting from the code above that by iterating through batches I would get the original image and its cropped version, besides that i guess that i haven't properly understand how the cache method behaves.Then I have used the code below to exhibit the images, that plots random cropped images."
2532,Is it possible to use non-pytoch augmentation in transform.compose,"['python', 'deep-learning', 'pytorch', 'artificial-intelligence', 'data-augmentation']","I am working on a data classification problem that takes images as an input in Pytorch. I would like the use the imgaug library, but unfortunatly I keep on getting errors. Here is my code.I am aware that the input to the imgaug transformer must be a numpy array, but I am not sure how to incorporate that into my transform.compose (if I can at all that is.). When the imgaug seq is not in the transform.compose it works properly.Thank you for the help!"
2533,defining the steps_per_epoch if apply image augmentation,"['machine-learning', 'conv-neural-network', 'data-augmentation']","what should I do to solve the problem of defining the steps_per_epoch, because I have apply image augmentation but it seems that the traditional way for define steps_per_epoch is to use total_train which is the lenght of the data before applying data augmentation. I think it is not correct to use the total_train, instead of it, should I use the lengt of the variable train_data_gen, which has the data that has been modified by the data augmentation process.This is the colab code: https://colab.research.google.com/drive/1J0BwFh6lMMXcOQRHFh-TuZuSxkyVMNvl?usp=sharing"
2534,Data augmentation by adding noise in python regression model,"['python', 'machine-learning', 'statistics', 'regression', 'data-augmentation']","I am building a regression model for a target variable which is heavy tailed. I want to augment data so that the model gets enough training samples in the region where it's a long tail. Accuracy of prediction for the rare data points is important.I am currently augmenting data by adding noise to the training samples. After splitting into train and test, I do MinMaxScaler on all the features(X), but no scaling on the target variable(y).
Then, I add noise to both X and y with different mean and std since X is scaled to [0,1] but y isn't.
Here is the code for augmenting by adding noiseI invoke this using something like add_noise(0,0.005,X_train) and add_noise(0,1,y_train)
X_train is normalized/scaled so I can use a small std deviation. Now I have to decide what std deviation of y_train will cause only a small perturbation that corresponds to the perturbation to X_train.Questions"
2535,data augmentation for feature vectors?,"['python-3.x', 'tensorflow', 'machine-learning', 'data-processing', 'data-augmentation']","How do I artificially expand my training dateset with values similar to, but not exactly the same as data I already have? I have time series data which is featurized into a vector of key values, for instanceI would like to be able to generate features which are similar to existing vectors, but are not exactly the same."
2536,The effect of data augmetation on classification model,"['deep-learning', 'neural-network', 'conv-neural-network', 'data-augmentation']","I want to study the effect of data augmentation on a medical classification model..what are the preferred models for this study and what criteria should be followed. As for the models, do I use Transfer learning or not, Is it desirable to put random seed?"
2537,Keras ImageDataGenerator - multiple generators flowing from directory,"['python', 'image', 'keras', 'cnn', 'data-augmentation']","I have a folder with 15,000 unique images and I'm taking an un/semi-supervised triplet loss siamese CNN approach.I need to produce a set of anchor, positive and negative reference images for each of the 15,000 images where the anchor and negative cannot be the same image, and the positive reference image is an augmented version of the anchor. In essence, training set = [15,000 anchor, 15,000 positive, 15,0000 negative images].How can you use ImageDataGenerator like this to ensure anchor != negative while augmenting the anchor on the fly to produce a positive reference image?like the following code (though this doesn't work):a and n are shuffled with different seeds - so hopefully the same two images are never within the same set, and p uses the data augmentation generator as opposed to the basic generator.The obvious issue with this implementation is that it just repats the same images within the range of b_size since .next() is being applied to a and n.If I remove the .next() - all images are loaded in resulting in OOM (since the generators are being iterated through.Ideally, I'd like to be able to pass batches of size b_size that iterate through the entire directory to model.fit. In other words; cycle through .next()?How could this be achieved?Any help appreciated!"
2538,Images not getting stored and augemented properly using Keras,"['python-3.x', 'matplotlib', 'keras', 'deep-learning', 'data-augmentation']","I am working on an image classification code and I was using ImageDataGenerator from Keras. I am facing some problems storing images in batched and even if I solve it (by using list instead of array), I still face problems when datagen part comes. Here's the code for storing images:Where, imgs is initialized as imgs = np.zeros((10, 300, 300, 3))When I display the images, I get the following:However, when I initialize imgs as list instead of array, store it in list and then convert it array, then it works:Then, when I reach the data augmentation stage, the same thing happens to the training images. matplotlib doesn't plot the image.Am I missing something? Or should I just continue with the training?"
2539,Training loss curve decrease sharply,"['python-3.x', 'tensorflow', 'conv-neural-network', 'data-augmentation']","I am new at deep learning.
I have a dataset with 1001 values of human pose upper body. The model I trained for that has 4 Conv layers and 2 fully connected layers with ReLu and Dropout. This is the result I got after 200 iterations. Does anyone have any ideas about why the curve of training loss decreases in a sharp way?
I think probably I need more data, as my dataset concludes numerical values what do you think is the best data augmentation method I have to use here?"
2540,Data Augmentation in Retraining a object detection model,"['artificial-intelligence', 'object-detection', 'data-annotations', 'yolo', 'data-augmentation']","I am working on the object detection model. Being new into AI learning, I could not figure out that is data augmentation is included in the retraining model such as yolov3 and yolov4? Their paper mentioned a set of data augmentation techniques.Is it included in the model that we retrain or we need to do it before we retrain the model of our requirement? Also, if possible, suggest some of the On-the-fly data augmentation techniques with anchor/Bounding box dataset."
2541,Error in handling indices during CutMix Augmentation,"['python-3.x', 'numpy', 'computer-vision', 'data-augmentation']",I am getting an IndexError: too many indices for array while fixing the bounding boxex at image_batch_updated during the generate_cutmix_image function call :Here's the bounding box generator that I had used:The image_batch in a NumPy array of pixels with 4 random images and the labels are randomly generated.
2542,"Expected boolean as argument for 'return_batch', got type <class 'PIL.Image.Image'>","['pytorch', 'data-augmentation']",AssertionError                            Traceback (most recent call last)
2543,ImageDataGenerator is not saving the actual output to the disk,"['python', 'tensorflow2.0', 'data-augmentation', 'image-preprocessing']","I have a training set of images with structure like this:I am trying to augment the images and save them to disk for future use. I want to implement the below code:When I generate the image, the preprocessing_function (i.e. mobilenet.preprocess_input) is applied properly and I am getting the correct result.But the 'output from ImageDataGenerator' (the actual output) image is not saved in save_path. This is how the image is saved in save_path after the data augmentation.Am I doing something wrong? Why is the correct output not saved to the disk?Please let me know if any further details are required.Note: I am getting the same output from datagen.flow i.e. the image is not saved properly.Any help or suggestion is appreciated.Thank you very much"
2544,Data Augmentation: What exactly does steps_per_epoch mean?,"['keras', 'deep-learning', 'cnn', 'data-augmentation']","I am newbie to Deep Learning. I have one basic doubt. It might sound stupid for you.
I am working on Road Extraction from Satellite Images. I have 1080 sample images only. That's why I applied Data Augmentation.Following is the code for Data AugmentationAll these 3 properties namely rotation_range, zoom_range and horizontal_flip will apply separately. I mean I will get one rotational image, one zoomed image and one horizontally flipped image. Am I guessing it right?Now, I am fitting my training data on my modelMy output:My question is, what does this 218 denotes? I know that, it denotes total number of sample(or image in my case) in general.But what it denotes when we apply Data Augmentation? Does it taking 218 images or it is taking 218 * 3(applied properties in data augmentation) = 654 images?Pixel size of my dataset image is 10m. Then how should I augment the data? Which properties should I apply?I would be more than happy for your help!Thanks in advance!"
2545,Using Autoencoder for Data Augmentation of numerical Dataset in Python,"['python', 'keras', 'scikit-learn', 'autoencoder', 'data-augmentation']","i have CSV-File with three labels. Now i am looking for a way to increase the amount of Data. So i thought i could just take one class seperatly and use a Autoencoder to increase the data of that class. If i do that for all three classes and combine everything i optain a bigger Dataset.Do you think that way could work? My Dataset has 27 Dimensions.
If it is possible, does maybe anyone has an example code for that with sklearn or keras? I know a bit of coding Neural networks, but i do not realy know how to start on this problem.Best Regards
Marvin"
2546,Poisson/non-negative gaussian noise data augmentation in Keras,"['python', 'tensorflow', 'keras', 'noise', 'data-augmentation']","I'm using Keras to do some data augmentation before passing 'images' of energy distribution to a CNN. I would like to add some noise to the images, but this noise can't be less than 0.The data is in a numpy array of shape (5000,29,29), and the code for the data augmentation is currently:Is there a way to add either poissonian noise, which is obviously already non-negative, or gaussian noise with the negative values truncated to the data augmentation layer, or do I have to add it manually to the data before parsing it to the NN?Edit: Following the advice given here: How to add a noise with uniform distribution to input data in Keras?I created a class like this:And I add it to my data augmentation like this:But I get the error:
The added layer must be an instance of class Layer. Found: Tensor(""noise_layer_8/Identity:0"", shape=(None, 29, 29, 1), dtype=float32)I don't understand why the class that's defined as a keras.layer isn't actually considered a layer.Can someone help me fix this issue?Edited a second time to remove the tf.keras, as I've read that the problem could be compatibility between tf.keras layers and keras layers, however I'm still getting the same error"
2547,How much data is increased by augmentation of Keras ImageDataGenerator?,"['python', 'tensorflow', 'keras', 'data-augmentation']","I have a confusion regarding the implementation of augmentation techniques of ImageDataGenerator Keras.I'm doing a classification task and for this I've total 5 classes containing 1200 images each.In total, I have a dataset of 6000 image dataset.If I apply 5 augmentation techniques, whether the techniques are applied altogether to an image or applied separately?Will I get (6000X5)=30,000 data or will I get(6000X2)=12,000 data generated by augmentation method?I want to apply the following augmentation techniques:Can anyone help me to figure it out?"
2548,Custom ImageDataGenerator keras,"['keras', 'data-augmentation']","I've been trying to implement Keras custom imagedatagenerator so that I can do hair and microscope image augmentation.This is the Datagenerator class:Below Code is for albumentations augmentation (Just trying albualbumentations augmentation to test if the data generator works or not):Now creating DataGenerator object :A NonImplementedError comes when i
run model.fit_generator(generator=train_datagen,steps_per_epoch=30,epochs = 30,validation_data=val_datagen,validation_steps=15) I have shared my kernel here and
I was taking help from here.
I have also looked for other ways to augment which were all the same.I will be thankful if someone can tell why and where is the problem ? and Is there is any other good way to do custom image augmentation in keras."
2549,In Colab doing image data augmentation with “imgaug” is not working as intended,"['computer-vision', 'google-colaboratory', 'object-detection', 'data-augmentation', 'keypoint']",I am augmenting my image data-set which also contains key-points. For this reason I am using imgaug library. Following is the augmentation code:But while reviewing the augmented images I found following problems:But the weird thing is that the same code when I run it on my PC it runs completely okay. But when I run it on Google-Colab it creates these unwanted outputs. Why this is happening?
2550,Keras ImageDataGenerator : how to use data augmentation with images paths,"['python', 'tensorflow', 'keras', 'deep-learning', 'data-augmentation']","I am working on a CNN model and I would like to use some data augmentation, but two problems arise :How can I properly deal with these two issues? For what I've found, there might be two solutions :Is there an easiest way to overcome this simple problem? A mere trick would be to force ImageDataGenerator.flow() to work with a nparray of images paths rather than a nparray of images, but I fear that modifying the Keras/tensorflow files will have unexpected consequences (as some functions are called in other classes, a local change can soon result in a global change in all of my notebook library)."
2551,Train and test split set using ImageDataGenerator and flow,"['keras', 'data-augmentation', 'train-test-split']","I'm trying to make a network using augmentation.First I use ImageDataGenerator with validation_split=0.2.Then I tried to create a augmented training data end a not augmented validation data.
I have to use flow instead of flow_from_directory.I get this error menssage.What I'm doing wrong?The model.fit code is something like this"
2552,How is data augmentation done in each epoch?,"['python', 'pytorch', 'data-augmentation']",I'm new to PyTorch and want to apply data augmentation to the datasets on each epoch. II got the code from an online tutorial. So from what I understand train_transform and test_transform is the augmentation code while cifar10_train and cifar10_test are where data is loaded and augmentation is done at the same time. Does this mean data augmentation is only done once before training? What if I want to do data augmentation for each epoch.
2553,How to batch process with multiple Bounding Boxes in imgaug,"['image-processing', 'data-processing', 'data-augmentation', 'module-augmentation']","I'm trying to set up a data augmentation pipline with imgaug. The transformation of the images works and does not throw any errors. In the second attempt I tried to transform the N Bounding Boxes for each image and I get a persistent error. In the following line the following error occurs:
aug_bbox = seq_det.augment_bounding_boxes(bbox)I have already tried several different approaches but I can't get any further. Furthermore, I haven't found any information in the docs or other known platforms that would help me to get the code running."
2554,Data Augmentation in Keras for 3D “images”,"['keras', 'data-augmentation']","I have a DNN coded in keras that trains events in a particle physics detector that looks something like this:I take these coordinates and convert them into an ""image"" by first converting a specific x-y plane into a 2D array and then flattening it. So for instance, if my event has only 3 points with coordinates: (1, 1, 1); (1, 3, 1) and (2, 3, 2). I take the first two coordinates (with same z value) and convert it into an array:and then for the next coordinate:I then combine the two into a 1D array like so and feed it into the network:I then have the network try and predict the center of mass of the whole event. However, my model doesn't give a very good accuracy and I think I can improve it by having more data by using data augmentation, but as far as know, Keras doesn't have that for 3D data like the one I have. Any suggestions on how to fix that?"
2555,Black pixels outside the border when using keras.layers.experimental.preprocessing,"['python', 'tensorflow', 'keras-layer', 'data-augmentation']","I am trying to implement data augmentation using the preprocessing layers in Keras, imported as:I read in the documentation that there are three types of fill_mode, 'constant', 'wrap' and 'reflect'. I tried all of them but what I get is always black pixels outside the border of the image, while on other tutorials I saw it working correctly. What am I doing wrong?This is the code snippet I'm using:This is the original image.And this is what I get when trying to augment it  (I tried all the possible fill_mode and interpolation parameter).Versions of the modules I use:TF --> 2.2.0Keras --> 2.3.0-tf"
2556,Keras: Plot Image augmentations samples for training data using flow_from_directory,"['python', 'keras', 'computer-vision', 'data-augmentation']","I am using Keras flow_from_directory to perform data augmentation:How do I plot a sample image with examples of its data augmentations?For example, my directory is ../input/train/cats/ and ../input/train/dogs/"
2557,How to account for discount in sell likelihood prediction without enough data,"['python', 'model', 'data-augmentation']","I have a small datasets of ~10K offers, and whether they were accepted by the client or not.Each offer has a date, several products, how many of each, their catalog price and the discount offered, and some client features.The problem is I don't have the same offer (=same combination of products) with different discounts, so the models I build don't take into account the fact that the same offer with higher discount is more likely to be accepted by the client (even thought the accuracy for the test data is very high).I thought about data augmentation, but how to synthesize data that accurately represents reality?Any ideas how to build a model with logical discount-likelihood proportion?Thanks a lot."
2558,how to show results of data augmentation before and after keras.preprocessing.image.ImageDataGenerator,"['python', 'tensorflow', 'keras', 'data-augmentation', 'data-generation']","
i am currently training a CNN with the ASL dataset https://www.kaggle.com/datamunge/sign-language-mnist. To optimize my accuracy I used the ImageDataGenerator from Keras. I wanted to print out the results of the Data Augmentation (image before and after the Data Augmentation). But I don't understand how to plot the results from datagen. This is my code:train_data is a numpy array of shape (20, 28, 28, 1) and train_label(20, 1) as they are 20 images with 28*28 pixels and the third dimension for the usage in a CNN. I would like to plot it with matploit lib but also happy with anything else (np array of the pixels).If someone could also tell me how I can print the amount of data the datagen generated would be awesome. Thank you in advance for your help."
2559,How do I augment images using keras ImageDataGenerator? [closed],"['python', 'machine-learning', 'keras', 'deep-learning', 'data-augmentation']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 months ago.I have training set of 364 images stored in numpy array also its labels stored in a different numpy array(there are 8 labels to classify).The dataset being small I want to use augmentation but can only find resources which augment images if they are stored in specific folders according to the labels. So how can I augment images in real time using ImageDataGenerator. Thanks! "
2560,How to augment all the images in a folder using TensorFlow,"['python', 'image', 'google-colaboratory', 'tensorflow2.0', 'data-augmentation']",To upload a folder to colab and augment the images
2561,Image data augmentation in TF 2.0 — rotation,"['tensorflow', 'deep-learning', 'data-augmentation']",I am training a tensorflow model with with multiple images as input and a segmentation mask at the output. I wanted to perform random rotation augmentation in my dataset pipeline.I have a list of parallel image file names (input output files aligned) for which I convert into tf dataset object using tf.data.Dataset.from_generator and then and use Dataset.map function to load images with tf.image.decode_png and tf.io.read_file commands.How can I perform random rotations on the input-output images. I tried using random_transform function of ImageDataGenerator class but it expects numpy data as input and does not work on Tensors (Since tensorflow does not support eager execution in data pipeline I cannot convert it into numpy as well). I suppose I can use tf.numpy_function but I expect there should be some simple solution for this simple problem. 
2562,How do RandomBrightness and HueSaturationValue in albumentations work?,"['python', 'python-3.x', 'image-processing', 'deep-learning', 'data-augmentation']","I'm applying some data augmentation techniques on image data by using albumentations. But I don't understand how RandomBrightness (Randomly change brightness of the input image.) and HueSaturationValue (Randomly change hue, saturation and value of the input image.) work inside the library. Any mathematical explanation or step-by-step explanation of the way these two techniques work."
2563,How do i apply Data Augmentation on entire data-set,"['python', 'data-augmentation']","enter image description hereHello Guys, I am new to Machine Learning and trying to learn it. I tried data augmentation on my data-set. i got this code from keras website but this code is just picking 1 image at a time. I want this code to pick image one by one from the data-set and apply augmentation techniques on it. I am confused on what to change in it. I will be very thankful if someone helps me."
2564,"how to turn off automatic data augmentation of ImageDataGenerator and flow_from_Directory in tensor flow, keras","['python', 'tensorflow', 'keras', 'data-augmentation']","I am using the code below. I would NOT like to have any data augmentation so I am setting all the variables in ImageDataGeberator and flow_from_directory to None but still I see that the directory I set in flow_from_dierctory (save_to_dir) is becoming filled with more and more image during training so does this mean that data is augmented? I have 81 images for training but the number of images in this directory even reaches to 500. That's why I think one of these two are augmenting data . How do I stop data augmentation ? 
I am using model.fit_geenrator. "
2565,Augmented Images consists of empty arrays whereas Original images consists of arrays with values. Why is it so?,"['python', 'opencv', 'image-processing', 'deep-learning', 'data-augmentation']","I change the extensions of all .png images to .jpeg inside a folder and also renamed them into using this code,I successfully got images in right format like this inside a separate folder like this.
NORMAL_IMG_0
NORMAL_IMG_1
NORMAL_IMG_2
.
.
.But after augmenting and reading the images using OpenCV  and imgaug I got empty arrays like this using this code,I got this,While the original image is like this,Can anyone help me why this is happening? I also want to augment all my images. I didn't notice this until I tried to augment my images which are in JPEG format inside a directory."
2566,Getting Bad Images After Data Augmentation in PyTorch,"['python', 'matplotlib', 'pytorch', 'data-augmentation', 'semantic-segmentation']","I'm working on a nuclear segmentation problem where I'm trying to identify the positions of nuclei in images of stained tissues. The given training dataset has a picture of the stained tissue and a mask with the nuclei positions. Since the dataset was small, I wanted to try data augmentation in PyTorch, but after doing that, for some reason, when I output my mask image, it looks fine, but the corresponding tissue image is incorrect. All my training images are in X_train with shape (128, 128, 3), the corresponding masks in Y_train with shape (128, 128, 1) and similarly the cross-validation images and masks in X_val and Y_val respectively. Y_train and Y_val have dtype = np.bool, X_train and X_val have dtype = np.uint8.Before data augmentation, I check my images like this:The output is as follows:
Before Data AugmentationFor the data augmentation, I define a custom class as follows: Here I have imported torchvision.transforms.functional as TF and torchvision.transforms as transforms. images_np and masks_np are the inputs which are numpy arrays.This is followed by:I have used from torch.utils.data import DataLoaderAfter this step, I try to check on my first set training image and mask using this:I get this as my output:
After Data Augmentation 1When I change the line axis_1.imshow(img.astype(np.uint8)) to axis_1.imshow(img), I get this image:
After Data Augmentation 2The images of the mask are correct, but for some reason, the images of nuclei are wrong. With the .astype(np.uint8), the tissue image is completely black. Without the .astype(np.uint8), the positions of the nuclei are correct, but the color scheme is all messed up (I expect images like those seen before data augmentation, either gray-ish or pink-ish), plus 9 copies of the same image in a grid are displayed for some reason. Can you please help me get the correct output of the tissue images?"
2567,Why Data Augmention amounts are not the same as expected,"['tensorflow', 'machine-learning', 'keras', 'classification', 'data-augmentation']","I have 500 images in one class, and when I used ImageDataGenerator with 4 process, it didnt give 2000 images, instead it was just around 1900 images?"
2568,Offline Data Augmentation in Google Colab - Problems with RAM,"['deep-learning', 'google-colaboratory', 'tensorflow-datasets', 'data-augmentation']","What would be most efficient way to perform OFFLINE data augmentation in Google Colab? Since I am not from US I cannot purchase Google Chrome for bigger RAM, so I am trying to be ""smart"" about it. For example, when I finish loading 11000 images, first as NumPy arrays and then creating pandas DataFrame from them, it occupies around 7.5GB of RAM. Problem is, I tried to del every object (NumPy array, tf.data object etc) in order to check if RAM changes, and RAM does not changed. Is it better to try and be smart about RAM or maybe write to disk any time I augment image and do not keep anything in RAM? If this is the case, is using TFRecords a smart approach for this?"
2569,How to avoid augmenting data in validation split of Keras ImageDataGenerator?,"['python', 'tensorflow', 'keras', 'data-augmentation']",I'm using the following generator:Now the problem is that the validation data is also being augmented which I guess is not something you'd want to do while training. How do I avoid this? I don't have two directories for train and validation. I want to use a single dataframe to train the network. Any suggestions?
2570,increase the margin between classes in CIFAR10,"['neural-network', 'classification', 'transformation', 'data-augmentation']","Is there anyway to process images (e.g., transforming to another space) to have increased margin between classes in CIFAR10, and train the network on the processed images?"
2571,TF2 Data Augmentation Randomness,"['tensorflow', 'random', 'tensorflow2.0', 'custom-function', 'data-augmentation']","I've been trying to add custom augmentation to a model. However, while library functions such as tf.image.random_flip_left_right or tf.image.random_brightness actually work and create random variations for every image in every sample, my custom functions always have the same effect for ALL images in ALL batches.For instance, these are my functions for ""random"" gaussian and motion blurs:As I said, my images get random brightness, flip, etc. But they ALL get the same motion and Gaussian blur, which is not what I wanted.EDIT: To clarify, I call all augmentation functions sequentially in an augment(image, label) function, which is called via dataset.map(augment). This works just fine for the other augmentations."
2572,Why do i become less pictures after data augmentation than it should be?,"['python', 'deep-learning', 'data-augmentation']","I try to become more data through data augmentation in my Python projekt. I habe 672 original pictures. Through my transformations i want to get from 1 picture 30 pictures. When i try to use my code on 1-5 images i get what i expect 30-150 images, but when i use my code on more pictures, than i get less. For example by using 100 picture i get 294 instead of 300, by using 200 pictures i get 574 pictures... By using all of my pictures (672), i get 8.672 pictures instead of 20.160. My code:"
2573,Adding shear to polygons,"['python', 'transform', 'polygon', 'shapes', 'data-augmentation']","I'm trying to create a varied set of poylgons. To create this set I apply some transformations to regular polygons starting with some rotations. To add diversity I want to add shear on each shape. My issue is that my shear function doesn't work for some polygons which have been rotated to be inline with the x-axis (the scale of the outputs is way off).
More specifically it doesn't work for polygons with more than 4 sides that have been rotated to be inline with x axis, it does work for rotated polygons with 4 or less sides.My questions are does anyone know of a more robust method of adding shear to polygons?
If not, could you adivse on how I could make my current shear function more robust?Here's the code to recreate the issue if anyone needs it:Switch between shear_dict and the commented out sl_shear_dict to see working and non working examples. 
The index/key for both dicts is the number of sides of the polygon."
2574,Why do we need test_generator and val_generator for data augmentation,"['machine-learning', 'keras', 'deep-learning', 'neural-network', 'data-augmentation']",Data augmentation is applied for training only.  I'm wondering why several tutorials create test_generator and val_generator.  Why don't we create only train_generator.
2575,Data Augmentation multi target regression,"['neural-network', 'regression', 'data-augmentation']","I am dealing with multi target regression (strongly non-linear) problems.
I am facing them using ANNs. To train them I have to generate a database that requires computationally expensive and time-consuming simulations. The database generally has n rows =~ 1000 (each row is a simulation), 15 < n parameters (x) < 30 and 10 < n targets (y) < 25.I tried some dimensionality reduction techniques (e.g. PCA) but they did not lead to any satisfactory result. I'd like to know if there are any data augmentation methods to reduce the number of simulations I have to do.
Any thought or advice is welcome.Edit 1: "
2576,Tensorflow 2.0 ImageAugmentation using tf.keras.preprocessing.image.ImageDataGenerator and tf.datasets: model.fit() is running infinitely,"['tensorflow-datasets', 'data-augmentation']","I am facing issue while running the fit() function in TensorFlow with augmented images(using ImageDataGenerator) passed as a dataset. The fit() function is running infinitely without stopping. I tried it with the default code which was shared in Tensorflow documentation.
Please find the code snippet below:"
2577,Validation set augmentations PyTorch example,"['image', 'deep-learning', 'computer-vision', 'pytorch', 'data-augmentation']","In this PyTorch vision example for transfer learning, they are performing validation set augmentations, and I can't figure out why.As far as I know, data augmentations are to be made solely on the training set (and sometimes on the test set, in what is called Test Time Augmentations).  Why is it done here as well?
Also, why not just resizing straight to 224?  "
2578,Augmenting images using different focal points in Python,"['python', 'data-augmentation']","There is a data augmentation algorithm I'm trying to implement from this paperThe algorithm is described in the image below. From my understanding, the algorithm rescales a spectrogram at different focal lengths using a convex lens. However, I have no idea how to simulate a convex lens in Python. Is anyone aware of any library that does this, or can give me a nudge in the right direction to try and implement this myself?"
2579,How do I double data for classes which have less number of images compare to other classes?,"['image-processing', 'deep-learning', 'classification', 'data-augmentation', 'imbalanced-data']","My training data is imbalanced. So I decided to resample my dataset. I want to do slightly changes while resampling. I'd like to apply a horizontal flip and Gaussian filter to minority classes to make all classes equal. To do so, I'd like to use pure image processing techniques to increase the number of my samples with the minority. To do that, I run this code in my classes which have less imagesHowever, I Have seen some tutorials are using Keras libraries to do image augmentation. Like following blog post:https://www.pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/In my case can I use the first technique (pure image processing= manual copy-pasting data with slight changes)? or should I use the libraries that are available in Keras or PyTorch?"
2580,Keras fit_generator gives a dimension mismatch error,"['python-3.x', 'machine-learning', 'keras', 'deep-learning', 'data-augmentation']","I am working on MNIST dataset, in which X_train = (42000,28,28,1) is the training set. y_train = (42000,10) is the corresponding label set. Now I create an iterator from the image generator using Keras as follows;which works fine.Then I train the model using;Here it gives the following error;I tried but failed to find the mistake. Also I searched here but there was no answer:expected dense_218_input to have 2 dimensions, but got array with shape (512, 28, 28, 1)BTW this is the summary of my model
Please help me.Update:"
2581,The effect of batch_size in ImageDataGenerator,"['python-3.x', 'machine-learning', 'keras', 'deep-learning', 'data-augmentation']","I am new to Keras and am trying to do data augmentation but I am stuck at the start itself. I am having an image and I am trying to make augmentations of it as follows;So I understand that datagen is a generator and iter is an iterator to iterate upon it but my doubt is regarding the batch_size. Here, the batch_size=2, it means during each iteration, a batch of 2 images is created. Now I am able to see the first image in the batch using batch[0] as shown above but not able to see the second image of the batch using batch[1]. When I check the batch.shape, it displays (1,399,640,3)which means there is only one image in the batch. I am not able to understand it. Where is the second image?  How can I display the second image of the batch."
2582,Data Augmentation For Object Detection with train_label.csv,"['tensorflow', 'object-detection', 'data-augmentation']","I like to enlarge my dataset by using the Data Augmentation For Object Detection1, but I dont know how to use it with my train_labels.csv. Converting the Annotations for each Images to pd seems difficult to me.Has anyone created a solution or can help me out?Thanks in advance!"
2583,How to implement and understand Pre-processing and Data augmentation with tensorflow_datasets (tfds)?,"['python', 'tensorflow2.0', 'tensor', 'tensorflow-datasets', 'data-augmentation']","I'm learning segmentation and data augmentation based in this TF 2.0 tutorial that uses Oxford-IIIT Pets.For pre-processing/data augmentation they provide a set of functions into a specific pipeline:This code brought me several doubts given the tf syntax. To prevent me from just doing a ctrl C ctrl V and actually understanding how tensorflow works, I would like to ask some questions:1) In normalize function, the line tf.cast(input_image, tf.float32) / 255.0 can be changed by tf.image.convert_image_dtype(input_image, tf.float32)?2) In normalize function it's possible to change my segmentation_mask values in tf.tensor format without changing to a numpy? What I desire to do is to only work with two possible masks (0 and 1) and not with (0, 1 and 2). Using numpy I made something like this:It's possible to do this without a numpy transformation?3) In load_image_train function they say that this function is doing data augmentation, but how? In my perspective they are changing the original image with a flip given a random number and not providing another image to the dataset based in the original image. So, the function goal is to change a image and not add to my dataset an aug_image keeping the original? If I'm correct how can I change this function to give an aug_image and keep my original image in the dataset?4) In others questions such as How to apply data augmentation in TensorFlow 2.0 after tfds.load() and TensorFlow 2.0 Keras: How to write image summaries for TensorBoard they used a lot of .map() sequential calls or .map().map().cache().batch().repeat(). My question is: there is this necessity? Exist a more simple way to do this? I tried to read tf documentation, but without success.5) You recommed to work with ImageDataGenerator from keras as presented here or this tf approach is better?"
2584,How to balance data with keras.ImageDataGenerator(),"['tensorflow', 'keras', 'computer-vision', 'data-augmentation']","Having unbalanced data, how can I use ImageDataGenerator() to generate enough augmented data for shorter sample to balance all categories? "
2585,Resize Vs CenterCrop Vs RandomResizedCrop Vs RandomCrop,"['pytorch', 'data-augmentation']","Can anyone tell me in which situations the above functions are used and how they affect the image size?
I want to resize the Cat V Dogs images and i am a bit confuse about how to use them."
2586,How to augment object detection data in TFRecord format,"['python-3.x', 'tfrecord', 'data-augmentation', 'tensorflow2.x']","I have a small dataset(500 images, 10000 labels, 16 classes) that I'm using for training a yolov3 model I'm getting acceptable results after training however I want to improve the confidence scores and the detection accuracy and of course overfitting is not an option. I have the training data stored in TFRecord format. I'm interested in using imgaug library since it has a decent documentation and support for working with bounding boxes. Here is the sequence of steps that I use to handle the dataset:I want to add another train_dataset = train_dataset.map(some_augmentation_pipeline) and I don't know how this can be achieved.Here are the reading functions:I'm thinking maybe I pre-augment the images and store them within the same TFRecord along with their adjusted labels/bounding boxes but I don't think this is the best way to do it ... what do you think?"
2587,Data Augmentation with Cross Validation in Python,"['python', 'cross-validation', 'data-augmentation']","I am trying to use cross_val_score to have an intensive test to get the overall accuracy of my solution. but my case is a bit different than having a normal X and y. I am using data augementation to generate more data for the training part i.e. each sample is going to have 30 new generated data. In otherwords, x1 is going to have 30 smaples, x2 is going to have 30 samples, and so now cross validation is going to be useless if I pass the real data + augmented data since the augmented data is meant only for training part and not for testing part as I need only to use real data for testing. Is there a way, I can address this issue? I hope I was able to convey the question"
2588,How do I correctly apply data augmentation to a TFRecord Dataset?,"['python', 'tensorflow', 'tensorflow-datasets', 'data-augmentation']","I am attempting to apply data augmentation to a TFRecord dataset after it has been parsed. However, when I check the size of the dataset before and after mapping the augmentation function, the sizes are the same. I know the parse function is working and the datasets are correct as I have already used them to train a model. So I have only included code to map the function and count the examples afterward. Here is the code I am using:In both cases, num_ex = 324 instead of the expected 324 for non-augmented and 648 for augmented. I have also successfully tested the flip function so it seems the issue is with how the function interacts with the dataset. How do I correctly implement this augmentation?"
2589,The third column of cv2.getRotationMatrix2D,"['python', 'opencv', 'data-augmentation']","I have been learning how to rotate images without cropping recently. But I am slightly confused with the third column of cv2's getRotationMatrix2D. It returns a matrix of size 2 by 3. Although my code is working based off Rotate an image without cropping in OpenCV in C++ and Python 2.7.3 + OpenCV 2.4 after rotation window doesn't fit Image, I would be glad and keen to know what exactly the third column of this transformation matrix does."
2590,How do I perform Data Augmentation for landmarks localization?,"['keras', 'deep-learning', 'data-augmentation']",I'm building a neural network using Keras to perform landmarks localization on a grayscale images.I saw that for classification task there is a Keras function to perform Data Augmentation. But for localization task I have not found a function to perform Data Augmentation since in the labeled data there are points to be changed.Any idea to perform Data Augmentation for landmarks localization task?Thanks!
2591,Training design / sequential loading of images for Mask-RCNN,"['python', 'keras', 'deep-learning', 'data-augmentation']","I am training a deep learning model using Mask RCNN from the following git repository: matterport/Mask_RCNN. I rely on a heavy augmentation of my dataset (original dataset: 59 images of 1988x1355x3 with each > 80 annotations), which I store locally (necessary to evaluate type/degree of augmentation vs validation metrics). The augmented dataset counts 6000 images. This dataset varies in x and y dimensions of the image, because of reducing resolution and affine transformations - I assume the different x,y-dimensions will not affect the final tests. However, my Python kernel crashes whenever I load more than 'X' images to train the model.Hence, I came up with the idea of splitting the dataset in sub-datasets and iterate through the sub-dataset, using the 'last' trained weights as starting point for the new round. But I am not sure if the results will be the same (read: same, taken the stochastic nature of 'stochastic gradient descent' into account)? I wonder, if the results would be the same, if I don't iterate through the sub-datasets per epoch, but train Y epochs (eg. 20 for 'heads' only, 10 for 'all layers')?Yet, I am sure this is not the most efficient way of solving this issues. Ideas for improvement are welcome.Note, I am not using keras.preprocessing.image.ImageDataGenerator(), as I have understood it, it randomly generates data and feeds it to the model by replacing the input for the epoch, whereas I would like to feed the whole dataset to the model.If my specs are of any help: Python 3.6, Tensorflow 1.9.0, Keras 2.2.4, CUDA 10.0, GPU Nvidia Geforce RTX 2060 Super (8 GB)"
2592,Should we train the original data point when we do data augmentation?,"['deep-learning', 'computer-vision', 'pytorch', 'transform', 'data-augmentation']","I am confused about the definition of data augmentation. Should we train the original data points and the transformed ones or just the transformed? If we train both, then we will increase the size of the dataset while the second approach won't. I got this question when using the function RandomResizedCrop. If we resize and crop some of the dataset randomly, we don't actually increase the size of the dataset for data augmentation. Is that correct? Or data augmentation just requires the change/modification of original dataset rather than increase the size of it?Thanks."
2593,data augmentation in Numpy,"['python', 'numpy', 'machine-learning', 'computer-vision', 'data-augmentation']","I have a limited dataset of cat/no cat images and am trying to augment it by rotating the images and adding a greyscale. Sadly it seems they don't improve the accuracy when I add them back to the dataset, so I am thinking of more augmentation techniques. The following functions are being used to change the dataset. I would appreciate any suggestions regarding the functions themselves and other augmentation strategies. As a side note, I cannot for the sake of the model that I have to stick to change the dimensions of the images so the shape cannot change, this is why I am redistributing my grey back into the 3 RGB channels. If you could suggest other techniques or improve my existing ones I would be very much grateful. Since they do not seem to be making much of an impact/difference.In the code below I am redistributing the grey back into the other 3 channels because I want to keep the dimensions the same."
2594,ValueError: Could not find a format to write the specified file in single-image mode,"['python', 'image-processing', 'computer-vision', 'scikit-image', 'data-augmentation']","I am trying to read an image using skimage package, then crop it and then save it.
Till cropping it works fine. While saving, it throws the below errorValueError: Could not find a format to write the specified file in
  single-image modeBelow is my code. Any help is highly appreciated.
thanks"
2595,Keras generator return augmented data with different range of pixel,"['python', 'machine-learning', 'keras', 'deep-learning', 'data-augmentation']","I'm trying to use data augmentation based on Keras, the problem that I get the range of pixel on the data augmented is not the same as the original image. I need to preserve the range of pixel of my images, any available parameter on ImageDataGenerator ?code : "
2596,Fast Dataset Augmentation in Python- Deep Learning,"['python', 'numpy', 'machine-learning', 'deep-learning', 'data-augmentation']","I am working on a project where there is a need for data augmentation. I wanted to flip the image horizontally and add that to the training data array. The problem is that there is over 10,000 images. This is the code for manually flipping each image (a 2d numpy array) in the array train_images of length 'size'.This is taking quite a long time. Is there any library function or faster way to compute the new images and add them to the array without multi-threading?Thank you in advance for your comments."
2597,Tensorflow random numbers in Data Augmentation map function,"['python', 'tensorflow', 'tensorflow-datasets', 'data-augmentation']","I want to use the crop_central function with a random float between 0.50-1.00 for data augmentation. However, when using numpy.random.uniform(0.50, 1.00) and plotting the images the crop is constant. I debugged this by using 4 images and plotting 8 rows, the images are identical.In general the question might be formulated as follows: How to use random numbers in the Dataset map functions?"
2598,is there a way to apply data augmentation on a numpy array of images?,"['python', 'numpy', 'computer-vision', 'data-augmentation']","I was wondering if it's possible to apply data augmentation on my NumPy array of images instead of using it with the help of image data generator
I'm currently working on Keras framework"
2599,not able to pass Keras Generator image to face recognition,"['python', 'keras', 'face-recognition', 'data-augmentation']","I am using Keras to generate pictures to feed face_recognition package.the following code I used to read and prepare the picture to be passed to the generatorthen for the generated image is used as follow:RuntimeError: Unsupported image type, must be 8bit gray or RGB image.I tried to solve the issue by using squeeze before passing the generated picture; but also it did not work"
2600,Passing single image throwing error through imgaug,"['python', 'image', 'data-augmentation']","I found an example on:https://github.com/aleju/imgaug
You can check the example under this  :https://github.com/aleju/imgaug#example-augment-images-and-bounding-boxesI am passing this image through it:
This is code:But it is throwing this error:Explanation:
I have only changed code to read this image. But it seems to be throwing some error regarding bounding boxes. Im unsure of how to fix this "
2601,How to use a cv2 image augmentation function with tensorflow tf.data.Dataset?,"['tensorflow', 'deep-learning', 'data-augmentation']","I am using tf.data.Dataset to create my dataset and training a CNN with keras. I need to apply masks on the images, and the mask depends on the shape of the image, there are no predefined pixel coordinates. When looking for an answer on the internet, I found that there are 2 ways of accessing shapes of images in TensorFlow (in training time):Using eager execution (which is not enabled by default in my case, I'm using tf v 12.0)Using a sessionI do not want to use eager execution because it slows down training, and cannot use a session because I train and test the CNN using Keras (I feed the data to model.train() using iterators of tf.data.Dataset).As a consequence, I have no way of knowing the shapes of images, and thus cannot access specific pixels for data augmentation.I wrote a function using OpenCV (cv2) that applies the masks. Is there a way to integrate it with the TensorFlow data pipeline?EDIT : I found a solution. I used tf.py_func to wrap the python functions"
2602,Create custom datagenerator in Keras using my own dataset,"['python-3.x', 'keras', 'deep-learning', 'data-augmentation', 'data-generation']","I want to create my own custom DataGenerator on my own dataset. I have read all the images and stored the locations and their labels in two variables named images and labels. I have written this custom generator:What I want to know is how can I add other augmentations like flip, crop, rotate to this generator? Moreover, how should I yield these augmentations so that they are linked with the correct label. Please let me know. "
2603,How to apply data-augmentation on acoustic datasets?,"['python', 'data-augmentation', 'acoustics']","I have a small acoustic dataset of human sounds which I would like to augment and later pass to a binary classifier.I am familiar with data augmentation for images, but how is it done for acoustic datasets?I've found 2 related answers regarding autoencoders and SpecAugment with Pytorch & TorchAudio
but I would like to hear your thoughts about the audio-specific ""best method""."
2604,Error during reading an image using skimage,"['python', 'anaconda', 'scikit-image', 'data-augmentation']","I have been trying to read a bunch of images from a specific path , the code I am using is ~when I am trying with a single image its working fine but when I am using the image path to pick up a random image it is showingAny idea what I am doing wrong , any help would be appreciated."
2605,Keras Training and Validation ImageDataGenerator for point localization,"['python', 'keras', 'deep-learning', 'conv-neural-network', 'data-augmentation']","I want to perform point localization using CNN. I have a dataset containing images and point coordinates (target) in a separate XML files. I transformed and saved images and target in a .npy files.I want to perform Data Augmentation and then split training set and validation set.Is this the correct way to perform this data augmentation? If no, how can I do that?Later, I want to split these augmented data in a training set and validation set using this:"
2606,Combine two data generator to train a CNN,"['tensorflow', 'keras', 'dataset', 'generator', 'data-augmentation']","I am trying to train a model using a dataset that i split in two parts, for each part i create a different ImageDataGenerator using keras and tensorflow.my question is , how to combine the data from both of my generators to train the model. I don t want t use each one separatelytnx for all"
2607,Non-image data augmentation,"['python', 'machine-learning', 'data-augmentation']","I am looking for an algorithm and-or tutorial about data augmentation but all of them belong to image augmentation , is it possible to do that in other datasets ? 
I am working on parkinsons data set (https://archive.ics.uci.edu/ml/datasets/parkinsons) and want to create an example of data aug with python , is this possible ? or should i use smt like mnist/fmnist ?"
2608,How do i disable the flipping of images on Darkflow YOLOv2?,"['python', 'yolo', 'data-augmentation', 'darkflow']","I am currently training a model based on orientation.  I want to be able to train a model that can tell the estimated orientation of an object. Currently, i am at around 1000 epoch and the accuracy is not good. My theory is that the flip operation seems to lead to an inaccurate model as an orientation of 90 degrees may be flipped to -90 degrees. Hence, the 2 separate classes would be confused with one another.These are the codes related to the data augmentation during training. I would like to consult your advice if my theory is right? And if so, how can i disable the flip operation but keep the rest of the data augmentation? Thank you!"
2609,why does data augmentation not improve my performance (cnn)?,"['python', 'tensorflow', 'keras', 'data-augmentation', 'cnn']","I'm relatively new to deep learning. I am trying to train a CNN model to classify spectograms of EEG data. When applying data augmentation, the model performs worse than without... What am I missing? Normally our model runs with an accuracy of 0.84 and a loss of 0.5 for both training and validation. after training with generated data"
2610,ImageDataGenerator - trained with model.fit instead of model.fit_generator,"['python-3.x', 'keras', 'conv-neural-network', 'data-augmentation']","I am a beginner in using the ImageDataGenerator from Keras and I accidentally used model.fit instead of model.fit_generator.Is that a glaring mistake, do I have to re-train everything with fit_generator?Thanks for every helpUpdate I have forgotten the code for gen_Image_data()"
2611,Image Augmentation to make ANN robust to MRI artifacts,"['deep-learning', 'computer-vision', 'image-segmentation', 'medical', 'data-augmentation']","I am training a model to segment mouse-brain MR images.The problem is that my training data is already bias-field-corrected, but I would like my ANN to be invariant to the bias field (meaning that it should output the same mask for the same image with, and without bias field).To do this I would like to add something like an artificial bias field on top of my training data, but I don't really know how.How can I create an artifical bias-field for my training data?Below are examples of an image with bias-field, together with the (pretty bad) predictions of my current model. 
"
2612,Should I use over/undersampling or just data augmentation?,"['data-augmentation', 'oversampling', 'cnn', 'imbalanced-data', 'smote']","I have a dataset that contains 43 classes (the data are images). This dataset is imbalanced, the minority class has ≈200 images and the majority class has ≈2000 images. I read that SMOTE isn't optimal for images because it loses local features. And I think 200 images per class isn't optimal either because isn't enough (I think) to build a CNN with good results.What should I do in that case?
Should I just do data augmentation and ignore the imbalance? Use SMOTE? What you suggest?Obs: I searched a lot about this, but every scenario that I found that use SMOTE or other kind of oversampling, aren't images and the imbalance ratio it's like 100:1 or even 1000:1."
2613,"RNN (LSTM) extend input features by their derivatives gives better results, how far can we go?","['lstm', 'derivative', 'seq2seq', 'data-augmentation']","I got a question regarding LSTMs for seq2seq prediction.Basic physical problem:
Consider we have a basic physical task of a heated spring with air flowing around it, decreasing its temperature by only convective heat transfer. If we apply a force to compress the spring, the area around it is reduced, so the heat transport due to convection decreases. We have the training data for a random motion of the spring and the related convective heat transfer.
Now we want to predict the heat transfer for another random motion. So for training input we have say 2000 timesteps describing the motion of the spring and training output would be the heat transfer, so with shape (sequences, timesteps, features):train_x.shape = (1, 2000, 1)
train_y.shape = (1, 2000, 1)I've used a single layer LSTM with 32 units with quite okay results.Now I thought, if I calculate the derivative of the springs motion, so of the training input signal as it is so far, and insert it as a new feature as input I might get better results. One might see it as a trick of data augmentation.So new shape is:
train_x.shape = (1, 2000, 2)
train_y.shape = (1, 2000, 1)And I got some better results, with everything else staying the same.So I repeated the procedure and inserted the second derivative of the motion to the input signal, so that the shape was :train_x.shape = (1, 2000, 3)
train_y.shape = (1, 2000, 1)And again, I got some better results!
Although I should mention that due to more information feeding into the LSTM I had to increase the number of units to 50.So here comes the question:How far could I go with that? And why isn't everybody else doing at least the first derivative as an input? I've never seen that anywhere, and the results were way more accurate and training took only a bit longer.
Of course it's always a deal between increasing computational cost and increasing accuracy but I wonder if someone with a more powerful computer than me has investigated this behavier and can tell something about the results. "
2614,keras ImageDataGenerator interpolates binary mask,"['python', 'keras', 'tensorflow2.0', 'data-augmentation']","I am training a neural network to predict a binary mask on mouse brain images. For this I am augmenting my data with the ImageDataGenerator from keras.But I have realized that the Data Generator is interpolating the data when applying spatial transformations.This is fine for the image, but I certainly do not want my mask to contain non-binary values.Is there any way to choose something like a nearest neighbor interpolation when applying the transformations? I have found no such option in the keras documentation.(To the left is the original binary mask, to the right is the augmented, interpolated mask)Code for the images:"
2615,Keras iterator with augmented images and other features,"['python', 'keras', 'conv-neural-network', 'data-augmentation']","Say you have a dataset that has images and some data in a .csv for each image. 
Your goal is to create a NN that has a convolution branch and an other one (in my case an MLP).Now, there are plenty of guides (one here, another one) on how to create the network, that's not the problem.The issue here is how do I create an iterator in the form of [[convolution_input, other_features], target] when the convolution_input is from a Keras ImageDataGenerator flow that adds augmented images. More specifically, when the nth image (that may be an augmented one or not) is fed to the NN, I want it's original features inside other_features.I found few attempts (here and here, the second one looked promising but I wasn't able to figure out how to handle augmented images) in doing exactly that but they do not seems to take into account the possible dataset manipulation that the Keras generator does."
2616,Data augmentation for Image Segmentation with Keras,"['python', 'keras', 'deep-learning', 'data-augmentation']","I am training a neural network to predict a binary mask on mouse brain images.
For this I am augmenting my data with the ImageDataGenerator from keras.For my purpose it is important that the network is invariant to intensity shifts in the input image, meaning that it needs to predict the same mask for the same input image even if its intensities got linearly shifted.To teach this invariance, I want to augment my data such that for each image that is created with the keras ImageDataGenerator I have a corresponding mask that is unchanged.
Is there any way to do this?Also I would like to apply other spatial transformations where it is important that the mask receives the same transformation as the input image.I am currently augmenting my data with:"
2617,Can the procedure of applying image transformations on the whole dataset after oversampling be called data augmentation?,"['machine-learning', 'computer-vision', 'data-augmentation', 'cnn']",I have used oversampling method on the minority classes on an imbalanced dataset. After that I have used ImageDataGenerator() class provided by Keras to randomly apply geometric transformations on all the images. Can this procedure be called Data Augmentation?
2618,How to convert Tensor to numpy array inside a map function using tf.py_function,"['python', 'tensorflow', 'tensorflow2.0', 'tensorflow-datasets', 'data-augmentation']","I am trying to create an image augmentation pipeline for an object detection network, where my training examples are augmented as they go into the network. The images and the bounding boxes need to be augmented, but the standard tf.image methods don't work with bounding box data.All the easy augmentation libraries that work with bounding boxes need numpy arrays but I don't know how to convert my Tensors into numpy arrays inside my .map() function. Even when I wrap my augment function in a tf.py_function call I still get the error AttributeError: 'Tensor' object has no attribute 'numpy' when I try to convert my image via image = image.numpy().my dataset is loaded via this:this calls my parsing function:which calls my augment function:But no matter which parts of the code I wrap in a tf.py_function or where I try to convert to a numpy array, I always get the same error.What am I doing wrong?"
2619,Data Augmentation for Image Data Having Landmarks,"['data-augmentation', 'cnn']",I am currently building a CNN model which can predict landmarks on a facial image. But the problem is I have images ranging from various shapes. The landmark coordinates are stored in a csv file. I want to perform the following transformations on these images such that these changes also change the landmark positions (where applicable):Again randomly apply the following transformations (maintaining some range):I have already tried imgaug library but finding it difficult to get the documentation. Basically I need an easy illustration of these operations on some demo images how these operations can be performed. You can assume all of my images are stored in a single folder and annotations to them is contained in some csv file.
2620,Data-augmentation generators not working with TensorFlow 2.0,"['tensorflow2.0', 'keras-2', 'data-augmentation']","I am trying to train model with image data-augmentation generators on TensorFlow 2.0, after downloading Kaggle's cats_vs_dogs dataset using below code.But on first epoch, getting this error: How should I modify the above code base for TensorFlow 2?"
2621,It is possible to call a fit-generator for many times to do Data augmentation?,"['keras', 'neural-network', 'conv-neural-network', 'python-3.6', 'data-augmentation']","Im new in the CNN field. I have a dataset, I need to fine-tune a VGG model. My dataset is so small that I would like to do data augmentation on the training set to overcome the overfitting generated problem. I would use multiple augmentations with different degrees of rotation (10°, 20°, etc). But I don't know how can I do this in python using Keras: should I call many times fit_generator (after each imageDataGenerator)! see code 1, or should I call many times imageDataGenerator and finally call the fit-generator! see code 2.code 1:code 2:"
2622,How to do Data Augmentation In CT Data,"['python', 'deep-learning', 'data-augmentation']","I'm doing a deep learning study using CT data. However, there is a lack of data, so data augmentation is required.The number of 'normal' is about 300 and the 'abnormal' is about 80.However, since this data is CT, the general augmentation method that cuts, flips, and reverses is not good.Therefore, I think that the method using the rotation is optimal, but because it can not rotate much, I think 5 degrees to 10 degrees is appropriate.However, I do not know if this works in terms of augmentation, so I would like to ask.(https://arxiv.org/abs/1411.4389, this is study about model)this is one of my data"
2623,How can I apply data augmentation using Sequence API on Keras?,"['keras', 'data-augmentation']","I found out online this Sequence API (don't remember where, sorry):And int the load_image function, we have this:It seems that I can use data augmentation there, but I can't figure it out how.I thought about using DataImageGenerator from Keras and use flow to get images augmented, but I couldn't make this work.What's the best approach to deal with it?"
2624,Keras: Altered ImageDataGenerator gives error “__init__() got an unexpected keyword argument”,"['tensorflow', 'keras', 'data-augmentation', 'image-preprocessing']","I am using keras=2.3.1 and I wanted to use my own version of zca_whitening. For that, I made changes directly into the ImageDataGenerator class in the keras file /home/user/.local/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py. The file, including my changes is this one. My neural network file nn_script.py that uses this altered image_data_generator.py can be seen here. There are no problems, as long as nn_script.py uses the default keras augmentaions like:But if I decide to switch on my custom version of zca_whitening called zca_whitening_fast in this wayI get the error messageYou can reproduce my error message by substituting your image_data_generator.py file with the one I posted above and trying to create a generator with the option zca_whitening_fast=True.First I thought the problem is, that I have forgotten to include zca_whitening_fast in def __init__() in the file image_data_generator.py, but it is there, as you can check. I also tried deleting the __pycache__ folder, because I thought, that some older files are getting executed in this folder, but the same error appeared. Now I wonder if the error message comes from an __init__() that is in a different file, not in image_data_generator.py. I think the solution is something obvious, I probably need to make a change in another file in the keras preprocessing folder, but I don't know which. I don't think the problem is because my keras version is old and because of compatibility reasons with my graphic card I can't upgrade keras.Any ideas why I get this error message? Thanks"
2625,tensorflow - Data augmentation pipeline increase train loss value,"['python', 'tensorflow', 'deep-learning', 'tensorflow-datasets', 'data-augmentation']","I've implemented the following data augmentation pipeline to my tensorflow (1.14)  code:Even though I get coherent images from it, the training loss curve has a very weird behavior that I'm still not able to figure it out. (You can see it here)It is something wrong with my implementation? I keep reading tutorials and other questions here but nothing works.Thank you in advanced!"
2626,3D Data Augmentation with Keras Image generator,"['python', 'machine-learning', 'keras', 'tensorflow2.0', 'data-augmentation']",I was wondering if anyone knew about 3D data augmentation for movies and such?Is there a way to augment the data during a keras.flow_from_directory(...) function without consuming too much memory?Thanks!
2627,Single image augmentation using different magnitudes of noise,"['machine-learning', 'image-processing', 'keras', 'computer-vision', 'data-augmentation']","Is it practical to create a dataset with noise off of just one image? Recently, I have asked here with regards to adding noise to images. I am aware that convolutional neural networks require datasets with thousands of images. However, my goal is to train a model off of just one image.I intend to create a dataset of about 50 photos just by adding different levels of noise to a single image. Will I be able to get useful results out of it?I hypothesize that this may not be viable for scratch training a CNN, but I think it'll work if I wanted to use Facenet. For those who may not know what Facenet is, it is trained using triplet loss. It receives an image input and outputs an embeddings. This embedding can be used to compute distance metrics (specifically L2/Euclidean distance), wherein smaller measurements correspond to similarity and larger ones are different faces. It is trained on the LFW dataset to generalize facial features. As I said I think that using my method as a substitute for datasets like LFW is stupid, but then again I haven't tried. But might it work for creating different but similar embeddings? I am in the process of trying it out but I want to hear what you guys think."
2628,Python: time stretch wave files - comparison between three methods,"['opencv', 'machine-learning', 'scikit-image', 'librosa', 'data-augmentation']","I'm doing some data augmentation on a speech dataset, and I want to stretch/squeeze each audio file in the time domain.I found the following three ways to do that, but I'm not sure which one is the best or more optimized way:However, I found that librosa.effects.time_stretch adds unwanted echo (or something like that) to the signal.So, my question is: What are the main differences between these three ways? And is there any better way to do that?"
2629,What kinds of data augmentation methods in image detection task are robust to weather variations?,"['image-processing', 'data-augmentation']","I am collecting the dataset from outdoor cameras. For the reliable accuracy of my classification model, I need to collect a sufficient dataset, which includes weather and illumination variation. But, it takes a long time to collect this data.In particular,
weather variations: sunny, cloudy, misty rain and haze
illumination variation by time: AM 10~12, PM 12~2, PM2~4, PM4~6So, as a solution, what data augmentation can be applied to solve it??Could you recommend some data augmentation methods for weather and illumination variations?
Any reference is also fine.Thank you,"
2630,What is the proper way of applying augmentations consecutively in Tensorflow Object Detection API?,"['python', 'tensorflow', 'object-detection', 'object-detection-api', 'data-augmentation']","I want to augment dataset by rotating 90, 180, and 270 degrees and apply same rotations to the horizontal or vertical flipped version of the image. So, in total, there will be 8 images for a single image (4 from original image rotations, 4 from flipped image rotations).There is only 90 degree rotation in the API (random_rotation90). So, I suppose this function should be applied consecutively in order to obtain 180 and 270 degrees rotations. What is the proper way of doing this in config file?As far as I understand from the preprocessor.py file, above configuration applies augmentations directly to original image, in this case augmentations will not be applied consecutively. So is the following correct configuration for my purpose? "
2631,Tensorflow - how to augment images and bounding boxes during training with tfrecord files,"['python', 'tensorflow', 'tensorflow2.0', 'tensorflow-datasets', 'data-augmentation']","I currently have a training pipeline using images and labels in a tfrecord format. Augmenting them using tf.image has been super easy but I am limited to augmentations that don't effect that location of the objects because the tf.image functions don't seem to effect the bounding boxes.Currently I have this:But I would really love to add some random flipping, rotating, skewing, and cropping to make it robust. 
The tf.image.random_flip functions would be ideal but I dont know how to deal with flipping the bounding boxes as well.I was looking into using the imgaug package, but I don't know how to handle the tfrecord bounding box data, or if it would be efficient to do during training because it uses numpy.Is there a simpler way of augmenting the data during training? Or is there even a simple way of using imgaug with tfrecords so that I could do all the augmentation before training and save them as new images?"
2632,Will the augmented data set really improve the ML model,"['tensorflow', 'machine-learning', 'data-augmentation', 'machine-learning-model']",We previously had 411 size data set. With that we got 70 % of the objects detected. We augmented it to 5000 by implementing rotations and different levels of brightnesses. But the accuracy dropped to 40%. We don't know where it went wrong.
2633,How to create noisy images for data augmentation,"['python', 'numpy', 'image-processing', 'data-augmentation', 'gaussianblur']","I followed the most upvoted answer to a question regarding adding noise to an image. However it doesn't work for me. I just want to observe different noise effects on image while using Python
How to add noise (Gaussian/salt and pepper etc) to image in Python with OpenCVFrom what I know, images are something of uint8 type? I'm not certain if this type can take decimals.The salt and pepper part don't work eitherThe last line is to just see what is output by the terminal. It's output is Commenting target = Image.fromarray(target) gives me:I thought I'd also rewrite it, e.g. from image.shape to np.shape(image)Any help would be appreciated. "
2634,Image augmentation with S&P and Gaussian blur,"['computer-vision', 'noise', 'data-augmentation', 'gaussianblur', 'noise-generator']","Let's say I had two identical images. If I used Gaussian blurring on one and salt and pepper on the other, I get two different images (at least on the pixel level). However, would I get two similar images if:
  the blurred image is added S&P
  and the s&p image is added blurringWould they be any different? Or will they be similar?I'm asking for an image augmentation project."
2635,SVM Scikit-learn - Data Augmentation possible?,"['keras', 'scikit-learn', 'conv-neural-network', 'svm', 'data-augmentation']","are there any functions in Scikit-learn or another library which support Data Augmentation for SVMs?
Anything like the ImageDataGenerator in Keras, just for SVMs?It is really an important question for me which I have been trying to answer for a long time, so I would be very grateful for any tip."
2636,trouble creating and saving augmented images using imgaug,"['python', 'machine-learning', 'data-augmentation']","Using python v 3.7.3, pytorch v 0.4.1, imgaug 0.3.0, windows 10, Jupyter NotebookI am trying to iterate through several folders containing images, augment each image 6 times, then save a hard copy of each augmented image inside that folder.  I am using the imgaug library to augment the images.I am able to iterate through the folders, and augment and display the images inside the folders with this code:But, I would like to ultimately augment each image 6 times and create 6 new hard files per image. I am trying to use this tutorial to make these changes.
Right now, I am just trying the step to save a hard copy of the augmented images.  Using this code:While the augmented images show up normally when I print them in Jupyter labs, they are being saved as a hard copy as completely flat.  It's also saving hundreds of these images:Why would my image show up correctly augmented in Jupyter Labs, but be saved in that format when I try to save a hard copy?"
2637,imgaug: is it possible to keep the augmenter at the current transformation?,"['python', 'data-augmentation']","I'm working at augmenting a dataset made of sequences of frames: i'd like to apply a particular transformation to all the frames in a sequence, while normally imgaug applies a random transformation (within the given parameters) for each image fed into it.
Say the sequences of augmenters is rotate, flip and zoom:
I'd like to store a particular transformation once applied to the first image, like rotate 30°, flip horizontally, and zoom 0.15%, and apply it to all other images in the sequence.
Is this possible?"
2638,Caching for a custom pytorch dataset,"['python', 'audio', 'deep-learning', 'pytorch', 'data-augmentation']","I am creating a custom pytorch dataset to train an audio classification system.
Since some of the transformations applied during training as data augmentation are computationally demanding (e.g. pitch shift audio data), I would like to add a cache to my dataset to speed up the training. HDF5 seems to be the recommended option for this task. Basically, in each training loop, I need to search the cache. If a sample with certain augmentation parameters has already been calculated the sample will be loaded from the cache, otherwise it will be calculated and cached. How should I structure the HDF5 file to do that? Thanks in advance for any suggestion.    "
2639,getting error while performing data augmentation whien i try to import image_to_array in keras,"['keras', 'deep-learning', 'data-augmentation']","ImportError: cannot import name 'image_to_array' from 'keras.preprocessing.image' (C:\Users\shkatta\New folder\lib\site-packages\keras\preprocessing\image.py)
 i am not able to import the above error when i am importing image_to array in keras
please help"
2640,flow_images_from_directory to save augmented images,"['r', 'keras', 'data-augmentation']","I'm trying to user Keras package in R just to augment my images and use it for training. I could not find example or walk through of how to use it. I'm a R-user with very limited python knowledge.
I did:I understand that this is just a iterator, but how should I run it to be able to save my file? There are python examples, but couldn't find any with R."
2641,CSV File Dataset Augmentation using Keras,"['python', 'machine-learning', 'keras', 'neural-network', 'data-augmentation']","I am working on an already implemented project in Kaggle which has to do with Image Classification. I have 6 classes to predict on in total, which are Angry, Happy, Sad etc. I have implemented a CNN model and I am currently using only 4 classes(the ones with highest number of images), but my model is overfitting, my validation accuracy is going 53% at maximum, therefore I have tried several things but not seemingly improving my accuracy. Now I saw people mentioning something called Data Augmentation and thought to give it a go as it seems a potential to increase the accuracy. However I am stuck with an error which I cannot figure out. Distribution of dataset: Error: ValueError: Input to .fit() should have rank 4. Got array with
  shape: (28709, 2305)I have already reshaped my data into a 4d array but for some reason in the error it appears as my data is 2d. 
This is the shape of print(x_train.shape) => (28709, 48, 48, 1)x_train is where the dataset is, x_train[1:2] accessing one image.P.s Is there any other approach that you would recommend to improve my accuracy according to this dataset. For further questions about my dataset please let me know if you don't understand something in this partial code."
2642,Increasing dataset size using imgaug,"['python', 'machine-learning', 'data-augmentation']","I am merging two different datasets containing images into one dataset.  One of the datasets contains 600 images in the training set.  The other dataset contains only 90-100 images.  I want to increase the size of the latter dataset by using the imgaug library.  The images are stored in folders under the name of their class.  So the path for a ""cake"" image in the training set would be ..//images//Cake//cake_0001.  I'm trying to use this code to augment the images in this dataset:Right now there's not output, even if I put print(img) or imshow(img) or anything.  How do I ensure that I got more images for this dataset?  Also, what is the best spot to augment images?  Where do the augmented images get stored, and how do I see how many new images were generated?"
2643,Augmentation in Keras - Generating Images,"['python', 'image', 'image-processing', 'keras', 'data-augmentation']","I have an unbalanced data set, so I decided to use the Keras Image generator.I created a piece of code so three images are combined together to form one image,This code works fine when giving only one image as an input, and it generates three images.The folder has 100 input images, so 300 images in total are to be generated.But, this code does not generate exactly 300 images. Instead, it generates 295 or 288 images every time this code is executed.What is the issue, any help?Thanks"
2644,Image Data Generator for a folder of images,"['python', 'keras', 'directory', 'data-augmentation']","I have 7 labeled classes all with varying quantities of images in them (ranging from 2000-20000). I know in keras when using the model.fit I can change how many times each folder of labeled images is read in. Instead, I would like to compare the results if I augmented the images in the folders with fewer images. I only know how to do this image by image, how would I augment all the images in the folder instead of 1 at a time?"
2645,How to use 3D CNN with data augmentation using Keras and Python?,"['video', 'classification', 'data-augmentation']","I am working to classify videos using 3D CNN and I want to augment the videos but I do not know how can I augment the videos in training set?the shape of training data :The samplesNum is the number of videos.
The frame_depth is the number of frames that I take from each video. 
The shape of input is:The question is how can I create samples and what is the shape of it?also, what is the shape of it ?Can you please help me?"
2646,GridSearchCV with Data Augmentation,"['keras', 'scikit-learn', 'conv-neural-network', 'gridsearchcv', 'data-augmentation']","I try to optimize hyperparameters (like dropout rate, learning rate etc.)  for a cnn architecture by using GridSearchCV (Scikit-Learn). Since the data set is rather small, I would also like to do Data Augmentation.
And here the question arise if there is a way to combine GridSearchCV with Data Augmentation by using ImageDataGenerator (from Keras)?
Is it advisable to combine GridSearchCV with Data Augmentation at all? Or is it better to first optimize the hyperparameters via GridSearchCV without Data Augmentation and then use Data Augmentation when training the final CNN with the optimized hyperparameters?"
2647,how to do data augmentation for text (object detection) in image,"['python', 'deep-learning', 'image-recognition', 'data-augmentation']","I'm doing object detection for texts in image and want to use Yolo to draw a bounding box where the text is in the image.Then, how do you do data augmentation? Also, what is the difference between augmentation (contrast adjustment, gamma conversion, smoothing, noise, inversion, scaling, etc.) in ordinary image recognition?If you have any useful website links, would you tell me plz :)"
2648,Keras: losing axis with brightness_range during image augmentation,"['tensorflow', 'multidimensional-array', 'keras', 'deep-learning', 'data-augmentation']","I'm training a U-net based segmentation network and using keras' ImageDataGenerator for inline augmentation of my grayscale images. Everything works as expected unless I include brightness_range in my arguments. When this happens my 512,512,1 image seems to turn into a 512,512 image and messes things up. How do I fix this?Here's my augmentation code:And here's my error message:"
2649,How to make the TensorFlow map() function return multiple values?,"['python', 'tensorflow', 'data-augmentation']","I am trying to write a function that will augment images from a dataset. I am able to successfully augment an existing image and return it, but I want to be able to do multiple augmentations on a single image and return those augmented images individually and then add them to the original dataset.Augmentation function:Map function:train_ds is tf.data Dataset with with the following shape:How can make the map function return multiple value in such a way that I could, for example, return both the h_flipped_image and the v_flipped_image and them to the train_ds dataset?"
2650,Adding noise to Features of high dimensional input data,"['python', 'linear-regression', 'pca', 'data-augmentation']","I have a dataframe with 2170 records and post vectorization I have got 6000+ columns.Upon carrying out PCA, in order to consider 0.5 Variance, I need to have minimal 1500 columns. Thus resultant Data is 2170 * 1500. Ofcourse When I run Neural net ( simple 2-3 layers, 16 neurons each) or a linear regression as well, i face severe Overfitting.Source dataset on which I perform PCA is as follows.How can I add noise to my Data in order to augment data."
2651,Cast ImageDataGenerator Data Output,"['python', 'keras', 'casting', 'image-segmentation', 'data-augmentation']","I'm writing a network for Image Segmentation. I have my ImageDataGenerator for my masks (which are RGB images with only 0 and 255 as values, black and white) which is: And flow_from_directory: The code works fine, the only problem is that, when i'm applying data augmentation to the masks, i won't have binary images anymore, but i get some values between 0 and 1 (normalized). For example, if i print my output matrix (the image) i get something like this: Which contains also those ""extra"" values due to augmentation. If i don't apply any augmentation i get binary images as i wanted.How can i embedd the casting to integer? (in order to get values which are only 0 or 1)
I tried to use the field dtype=int in the ImageDataGenerator, but it doesn't do anything, i keep getting the same results. "
2652,Plot images from Image Generator,"['python', 'image-processing', 'keras', 'data-augmentation']","I'm trying to plot the images created by my image generator. So far this is the code of my data given to the generator: I followed this question: Keras images, but without any success.How can i plot (for example) the first n images generated by my imageGenerator?EDIT :I added the code used in the above mentioned question, but i get this error:"
2653,Data Augmentation: What proportion of training dataset needs to be augmented?,"['python', 'speech-recognition', 'librosa', 'data-augmentation']","I am currently working on a speech classification problem. I have 1000 audio files in each class and have 7 such classes. I need to augment data to achieve better accuracy. I am using librosa library for data augmentation. For every audio file, I am using the below code.That is I am augmentating each audio file (pitch-shifting and time-shifting). I would like to know, is this the correct way of augmentation of training dataset?
And if not, what is the proportion of audio files that need to be augmented?"
2654,Augmentation with tf.data with the cross product to two datasets,"['python', 'tensorflow', 'tfrecord', 'data-augmentation']","I would like to write a data augmentation step to my input pipeline, conceptually I have two datasets which can be fed to a generator as a pair where they will yield a bunch of output examples.I have managed to achieve such a thing by doing the following:This produces:as expected. My issue here is that gen is (in my real case) a computationally expensive operation so I would like to use parallel calls where possible. My attempts to add num_parallel_calls so far has failed to yield performance gains.Also, if it's important, my input dataset is coming from a TFRecordDataset which gives even more opportunity to add num_parallel_calls options, i.e."
2655,How to extract all tf.data.Dataset object into features and labels and pass into ImageDataGenerator's flow() method?,"['python', 'tensorflow-datasets', 'tensorflow2.0', 'tf.keras', 'data-augmentation']","I am acutally working on a mini-project based on cifar10 dataset. I have loaded the data from tfds.load(...) and practicing image augmentation techniques. As I am using tf.data.Dataset object, which is my dataset, real-time data augmentation is quite unachievable, hence I want to pass all the features into tf.keras.preprocessing.image.ImageDataGenerator.flow(...) to gain the functionality of real-time augmentation.But this flow(...) method accepts NumPy arrays which in no way related to tf.data.Dataset object.Can somebody guide me in this regard (or any alternative) and how do I proceed further?Are tf.image transformations real-time? If not, what can be the best aproach other than ImageDataGenerator.flow(...)?My code:"
2656,"Keras Image Preprocessing .flow(x, y, save_to_dir) only saves the augmented x images and not the y's","['python', 'keras', 'tensorflow2.0', 'data-augmentation']","I am using Keras data augmentation for both my x_train and my y_train images for an image segmentation task.For this I am using following code:But this only saves my augmented x images and not their labels. How can I save both the images and their labels to visualize them?I have already tried the example on the Keras Image preprocessing page Example of transforming images and masks together without success.I get the error AttributeError: 'zip' object has no attribute 'shape'.The code:Also, is there any way to increase the data_augmentation parameters during training? "
2657,Changing the elements of numpy.ndarray of Keras ImageDataGenerator,"['python', 'keras', 'data-generation', 'data-augmentation']","I am using Keras ImageDataGenerator for data augmentation. 
After generating ImDatagenerator as follows,I am trying to change one image's pixels of DG by simply assigning a numpy.ndarray with the same size as follows While it does not give any error, but I do not observe any change in the DG."
2658,"Error when checking target: expected dense_34 to have 2 dimensions, but got array with shape (64, 10, 2)","['keras', 'conv-neural-network', 'data-augmentation']","I see that similar questions have been answered and this has helped me to realize that the input is not what the model expects, but nowhere have I been able to find how to correct this.  My question is why is it expecting 2 dimensions and what can I do to my code to make this work to classify into 10 different classes?"
2659,Data augmentation function not correct,"['python', 'numpy', 'machine-learning', 'data-augmentation']","Just a head's up: I'm new to this, so please go gentle.I'm trying to create a function that will shift every image in the MNIST dataset and add the shifted image to the original dataset, effectively doubling the dataset size.My code (a warning, it might be a hot mess, I'll have to eventually learn how to write more elegant functions):I've examined the outputted dataset, and it doesn't seem to be applying a shift. Can anyone guide me past this?"
2660,Randomly Generate Synthetic Noise in an Image Text Document,"['image', 'machine-learning', 'image-processing', 'random-seed', 'data-augmentation']","I'm working on denoising dirty image document. I want to create a dataset wherein  synthetic noise will be added to simulate real-world, messy artifacts. Simulated dirt may include coffee stains, faded sun spots, dog-eared pages, lot of wrinkles and many more. How shall I do that?Sample Clean Image :After Adding Synthetic Noise:


How can I randomly achieve images shown above?"
2661,Method of visualizing fit of cross validated model,"['python', 'keras', 'cross-validation', 'data-augmentation']","How do I go about writing code to visualize the progress of my accuracy and loss development over training when using cross validation? Normally I would assign the variable name 'history' to the fit function when training the model, but in the case of cross validation it does not display the validation curves. I assume this is the case because I am not calling validation_data within the fit function (below). Normally I would use code such as below, but since I do not have the validation data within fit, I am not sure how to approach it.
]1]1"
2662,Is it okay if we augment the data first then randomly choose the data and split the data afterward?,"['validation', 'machine-learning', 'training-data', 'data-augmentation']","I am doing a science project about classifying medical images but I do not have a lot of data so, is it okay if I augment the data first then randomly select the data to keep and split the kept data afterward? At first, my teacher told me to augment the data first then split the data into train, validation, and test. But I think my proposed method will make the training dataset collide with the testing dataset which will cause the accuracy to be unrealistic(way too high), so I thought my method that randomly chooses the files after doing data augmentation should help the augmented dataset to not be too similar to each other and solve the imbalanced amount of dataset problem."
2663,Keras `ImageDataGenerator` image and mask augments differently,"['tensorflow', 'machine-learning', 'keras', 'semantic-segmentation', 'data-augmentation']","I'm training a semantic segmentation model using Keras with TensorFlow backend. I adopted ImageDataGenerator to do the image augmentation, including rotation, flip and shift. By following the documentation, I created a dictionary maskgen_args and used it as arguments to instantiate two ImageDataGenerator instances.The training data generator is done as follows, by setting seed to the same value, the mask will match the image.So far, there is no problem occurred. But as I need to do some extra preprocessing (eg. normalization) only for the image but not for the mask, I created another imagegen_args dictionary and used it as the arguments when instantiating the ImageDataGenerator.When I check the output of the training_data_generator, problem occurred: seems the image and mask are generated separately: they surely have random rotation, but they are rotated in different angle, unlike before. Here is an example of a food image and the mask for the food.I checked the id of image_datagen and mask_datagen, both cases their id are different. I wonder why the first case they can rotate the image and mask with the same random angle, but not in the second case? What should I do to make them behave like the first case when I indeed need to give extra arguments to image_datagen?"
2664,Is it feasible to do random cropping as a data augmentation technique in the context of multi-label image classification?,"['machine-learning', 'data-science', 'kaggle', 'data-augmentation']","I have read 2 top-ranking solutions in kaggle concerning multi-label image classification. In both of the competitions I read, random cropping was performed. To me, this seems like a bad move to make because we could have a mismatch between the labels and the cropped images. Here are the two links:1.human-protein-atlas-image-classification2.iMet Collection 2019 - FGVC6If the reason for cropping is an input size image constraint for the used model architecture, then isn't it better to resize the image instead of cropping it?"
2665,What are some good data augmentation techniques for document images?,"['python', 'deep-learning', 'data-augmentation']",I have 1000 resume in png format and I am implementing MaskRcnn for object detection. What data augmentation techniques can I use to improve the mask Rcnn performance?
2666,How do I augment data after spliting traininng datset into train and validation set for CIFAR10 using PyTorch?,"['deep-learning', 'conv-neural-network', 'pytorch', 'torch', 'data-augmentation']","When classifying the CIFAR10 in PyTorch, there are normally 50,000 training samples and 10,000 testing samples. However, if I need to create a validation set, I can do it by splitting the training set into 40000 train samples and 10000 validation samples. I used the following codesNormally, when augmenting data in PyTorch, different augmenting processes are used under the 
transforms.Compose function (i.e., transforms.RandomHorizontalFlip()). However, if I use these augmentation processes before splitting the training set and validation set, the augmented data will also be included in the validation set. Is there any way, I can fix this problem?In short, I want to manually split the
  training dataset into train and validation set as well as I want to
  use the data augmentation technique into the new training set."
2667,PyTorch Data Augmentation is taking too long,"['pytorch', 'data-augmentation']","For the task that involves regression, I need to train my models to generate density maps from RGB images. To augment my dataset I have decided to flip all the images horizontally. For that matter, I also have to flip my ground truth images and I did so.But here is the problem : For some reason, PyTorch transforms.RandomHorizontalFlip function takes only PIL images (numpy is not allowed) as input. So I decided to convert the type to PIL Image.And yes, this operation need enormous amount of time. Considering I need this operation to be carried out for thousands of images, 23 seconds (should have been under half a second at most) per batch is not tolerable.I would appreciate any suggestions to speed up my augmentation process"
2668,Keras with data augmentation does (almost) not converge,"['machine-learning', 'keras', 'data-augmentation']","I'm currently playing around with data augmentation in Keras. My model looks as follows:The lambda layer basically scales the image.Training worked ok-ish, however, I do not really have enough data, hence, generalisation was lousy.
Hence, I tried data augmentation.But now the fitting is not converging anymoreI inspected the generated images manually and they look good"
2669,Rotate image and points with Tensorflow,"['tensorflow', 'keras', 'image-rotation', 'data-augmentation']","I modify this code to work with not square image. But something is going wrong ! When the image is square, the result is good! Thnanks to comment"
2670,Why 'imgaug Flipud' is not working for polygons?,"['python', 'data-augmentation']","Here, augmentor is giving same polygon coordinates as the original coordinates. It works in case of Fliplr and Affine but somehow is not working for Flipud. I have also tried Flipud with bounding box too and it is working fine."
2671,Data augmentation imgaug (transformation on z-axis),"['python', 'data-augmentation']","I am trying to do data augmentation with the Python library imgaug. I would like to apply some transformation on the ORIGINAL image so that the TRANSFORMED image is the outcome.
Does anybody know which transformation at the Python library imgaug I could apply to achieve this?ORIGINALTRANSFORMEDMany thanks!!"
2672,How to append images to a list inside a Keras network,"['python-3.x', 'tensorflow2.0', 'keras-2', 'data-augmentation', 'image-preprocessing']","I would like to append the images which are procurred by the ImageDataGenerators to two different lists.  I believed I could do that with a lambda layer but I am getting an error message.  For a toy example see the code below.  You can use any set of images to run the code.  I used the cats and dogs dataset found here: ""https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"" "
2673,How to save images after data augmentation in a new folder without looping,"['python-3.x', 'keras', 'deep-learning', 'infinite-loop', 'data-augmentation']","I am trying to save my augmented images in a folder. But the loop is executing infinity times. I have 5000 images in the folder, but the number of augmented images I am getting is infinity. My aim is to get the same number of augmented images i.e., 5000.Thank you"
2674,Why dont generate enough images in this image augmentation?,"['python', 'python-3.x', 'image-processing', 'keras', 'data-augmentation']","I want to generate 1800 image using ImageDataGenerator. I created a loop from 200 images and want to generate 9 images from each image, totality 1800 images.
But when I append images to list, and print len(list), I see the len is 1630 till 1700.
What happended?"
2675,Object Detection on small test images with medium train image bounding boxes,"['python', 'tensorflow', 'object-detection', 'faster-rcnn', 'data-augmentation']","I want to train a model with tensorflow faster rcnn that can detect animals from a ""far"" distance where the objects are relativly small (example: https://cdn1.spiegel.de/images/image-830326-breitwandaufmacher-bfrb-830326.jpg ). Much of my training data is was captured closer (example: https://www.welt.de/img/reise/nah/mobile139849438/0572506457-ci102l-w1024/Schafe-in-bei-Clifden-irische-Kleinstad.jpg ) How can my model detect smaller objects better that look pretty similar to my training data but are smaller? I already tried to augment my Data in the pipeline (faster rcnn config). This improved my results lot but its still not perfect.I also tried to lower the scales of my anchors to detect the smaller objects in my dataset better, but this had a bad impact on the model"
2676,Keras - CNN - adjust dataset - remove biased class and data augmentation attempt,"['python', 'tensorflow', 'keras', 'deep-learning', 'data-augmentation']","I have a a predicament with the model I am developing based on a popular dataset of skin-cancer images. 
I have to points I'd like some guidance on - A.The original dataset is over +10K images with which almost 7000 images belong to one of the seven classes. I've created a subset of 4948 random images with which I ran a function to convert the images into a list of lists - first list contains the image and the latter the class as well as dismiss any images that are of class (5 - the class with the +6800K images). Thought process was to normalise the distribution across the classes.Re-running the original model with an output (Dense layer of 6 neurons instead of 7) - retrieves an error.Am I missing a step to 'indicate' to the model that there are only six possible classes? The model runs only when the output layer has seven neurons.error:B.I'm attempting to add Data Augmentation as the dataset is relatively small taking into account the number of classes and the sparsity of the images across the classes. Once I try to run the generator I receive the error message below which suggests that there is something wrong with one of the variables in the validation_data tuple. I cannot understand what the issue is.Example values of the test set look like:Error:Code:"
2677,Tensorflow object detection api: how to use imgaug for augmentation?,"['tensorflow', 'object-detection-api', 'data-augmentation']","I've been hand-rolling augmenters using imgaug, as I really like some of the options that are not available in the tf object detection api. For instance, I use motion blur because so much of my data has fast-moving, blurry objects.How can I best integrate my augmentation sequence with the api for on-the-fly training? E.g., say I have an augmenter:Is there some way to configure the object detection api to work with this?What I am currently doing is using imgaug to generate (augmented) training data, and then creating tfrecord files from each iteration of this augmentation pipeline. This is very inefficient as I am saving large amounts of data to disk rather than running augmentation on the fly, during training."
2678,Is it the right way for data augumentation for training a model?,"['keras', 'deep-learning', 'data-augmentation']","I'm new to keras and deep learning. I'have tried to use data augmentation for for training my model, but not sure if i'm doing it the right way. Can anyone assure me it my approach is correct? here is my code:"
2679,How to use augumented data when using transfer learning?,"['tensorflow', 'keras', 'deep-learning', 'transfer-learning', 'data-augmentation']",I have used VGG16 for transfer learning and got very low accuracy. Is it possible to use data augmentation technique to increase the accuracy when using transfer learning? 
2680,How to do object detection on high resolution images?,"['deep-learning', 'object-detection', 'yolo', 'data-augmentation', 'faster-rcnn']","I have images of around 2000 X 2000 pixels. The objects that I am trying to identify are of smaller sizes (typically around 100 X 100 pixels), but there are lot of them.I don't want to resize the input images, apply object detection and rescale the output back to the original size. The reason for this is I have very few images to work with and I would prefer cropping (which would lead to multiple training instances per image) over resizing to smaller size (this would give me 1 input image per original image).Is there a sophisticated way or cropping and reassembling images for object detection, especially at the time of inference on test images?For training, I suppose I would just take out the random crops, and use those for training. But for testing, I want to know if there is a specific way of cropping the test image, applying object detection and combining the results back to get the output for the original large image."
2681,Custom image data augmentation causes memory leak in TensorFlow2.0,"['python', 'memory-leaks', 'tensorflow2.0', 'tf.keras', 'data-augmentation']","I am working on Region Proposal Network for object detection. If I want to augment my data I need to augment both images and corresponding bounding boxes (e.g. scale, rotate, ...) + I have also image mask as an input, which I need to augment too. So I made custom function(s) for data augmentation.The pipeline is as follows:code continues with some additional expansion but this part is important because when I add augmentations it starts to cause memory leak.I have also tried a version with tf.py_function, where I used combination of OpenCV and numpy for resizing but it did not work. Does anyone encountered such a problem?Thank you for your help."
2682,How to perform 10 Crop Image Augmentation at training time using Tensorflow 2.0 Dataset,"['python', 'tensorflow', 'tensorflow-datasets', 'tensorflow2.0', 'data-augmentation']","I am using Tensorflow Dataset API and reading data from TFRecord files. I can use the map function and use method like random_flip_left_right, random_crop for data augmentation.However when I am trying to replicate AlexNet paper I am facing an issue. I need to flip each image and then take 5 crops ( left, top, bottom, right & middle).So the input dataset size will increase by 10 times. Is there anyway to do this using tensorflow dataset API? The map() function just returns the one image and I am not able to increase the number of images.Please see the code I have now."
2683,How to augment more images then the input,"['python', 'image', 'data-augmentation']","I am trying to make some augmented images using imgaug and when I input an array size (40, 600, 600, 3) it outputs (40, 600, 600, 3). Hence it created 40 new images. However I want to create more images (let's say 4000 images) is there a way to create 4000 images at once?I tried using a for loop, but I am not sure how to save and append the images in memory efficient way. The code I use for the image augmentation is the following: Thank you in advance! Kind regards, Leslie"
2684,Balancing an Unbalanced Dataset with K-Fold Cross Validation,"['python', 'deep-learning', 'pytorch', 'data-augmentation']","I'm trying to train/validate a CNN using Pytorch on an unbalanced image dataset (class 1:250 images, class 0: 4000ish images), and right now, I've tried augmentation solely on my training set (thanks @jodag). However, my model is still learning to favor the class with significantly more images.I want to find ways to compensate for my unbalanced data set.I thought about using oversampling/undersampling using the imbalanced data sampler (https://github.com/ufoym/imbalanced-dataset-sampler), but I already use a sampler to select indices for my 5-fold validation. Is there a way I could implement cross-validation using the code below and also add this sampler? Similarly, is there a way to augment one label more frequently than the other? Along the lines of these questions, are there any alternative easier ways that I could address my unbalanced dataset that I haven't looked into yet?`Thank you for your time and help!"
2685,Using AutoAugment and Cutout for CIFAR100,"['python', 'deep-learning', 'conv-neural-network', 'data-augmentation']","I've been training the cifar 100 dataset on my own implementation of googlenet. I achieved around 71% top-1 accuracy in 200 epochs at first, then I boosted that to 78.5% with warmup training, sgd with nesterov momentum, and learning rate scheduling.I've also been using some standard augmentations like padding with 4 zeros, 32x32 random crop, random horizontal flip and rgb channel normalizations.Now I wish to see if I can further boost that accuracy to 79-80 or even beyond 80% using more augmentation techniques. My questions are:1) Can I apply Cutout along with the standard augmentations I mentioned earlier? Or are they mutually exclusive?2) Autoaugment apparently finds the 'best' augmentation policies for a dataset. If I apply autoaugment, do I still need to use cutout and my earlier augmentations? Or can all of them be used together?3) I've been training 200 epochs (with batchsize 128) since I've seen it being done several times in various papers, and in my own implementations there is no significant improvement in training past 200 epochs. After applying cutout and/or autoaugment, should I train more epochs than 200? My learning rate starts at 0.1,  and decreases in the 60,120,160th epoch (with a decreasing factor of 0.2). Should I add more learning rate milestones and train up to 300 epochs?"
2686,Keras data augmentaion changes pixel values for masks (segmentation),"['keras', 'image-segmentation', 'data-augmentation']","Iam using runtime data augmentation using generators in keras for segmentation problem..Here is my data generatorMy labels are in a numpy array with data type float 32 and value 0.0 and 1.0.However, the data generator seems to modifies pixel values as shown below:- It is supposed to have same values(0.0 and 1.0) after data geneartion..
Also, the the official documentation shows an example using same augmentation arguments for generating mask and images together.However when i remove shift and zoom iam getting (0.0 and 1.0) as output.
Keras verion 2.2.4,Python 3.6.8UPDATE:-I saved those images as numpy array and plotted it using matplotlib.It looks like the edges are smoothly interpolated (0.0-1.0) somehow upon including shifts and zoom augmentation. I can round these values in my custom generator as a hack; but i still don't understand the root cause (in case of normal images this is quite unnoticeable and has no adverse effects; but in masks we don't want to change label values )!!! Still wondering.. is this a bug (nobody has mentioned it so far)or problem with my custom code ??"
2687,How to perform data augmentation on a set of images and its masks in keras?,"['python', 'keras', 'data-augmentation']","I want to perform the augmentations of both my original images and its segmented masks, but I always end up with some error like:Here is my code: Can someone correct me where I am going wrong? Thanks for your help.EDIT: I have a set of images and it's maskings. I want to obtain the same augmentation/transformation for both of them."
2688,"How do I set up training using augmentation, but ensuring the actual images unaugmented were trained as well?","['python', 'keras', 'data-augmentation']","I am using data augmentation for a model and would like to include the original unaugmented images as well as the augmented images, in training. I have used the following code so far: Please let me know if anyone is able to help! Thanks :)"
2689,Data transformation cropping with Normal Distribution using Pytorch,"['python', 'image-processing', 'pytorch', 'data-augmentation']","I running a model on unsupervised learning for images. Before the images go in for learning I am performing various data augmentations, such as rotate, random erasing, flips, etc. However, I would like to also crop my images based on their Normal/Gaussian Distribution. I am also using the external library called Augmentator which also does not have cropping based on the normal distribution. Its basically just random center or just simply random. All my transforms go into a list like this:Any help greatly appreciated. I found no external libraries that do this so If i have to implement it on my own I need a way to integrate into Pytorch so any guidance would be helpful. "
2690,"Data augmentation in python throws an error “int() argument must be a string, a bytes-like object or a number, not 'dict'”","['python', 'affinetransform', 'gaussianblur', 'data-augmentation']","I am trying to load all the images present in a folder, augment each of them and save it in a different repositoryI am able to augment an by hard coding the path and image name however, when I am trying to store the path of all the images and then processing the loop, it is not working and throwing an error mentioned in the title (also : int() argument must be a string, a bytes-like object or a number, not 'dict'
). In the later part after augmenting the image, I am storing the outputs in a different folder. The code is also as follows:It would be really helpful if anyone can provide a solution to this problem."
2691,Perform Augmentation (using ImageDataGenerator) and save augmented image as original name,"['python', 'image-processing', 'keras', 'deep-learning', 'data-augmentation']","I am applying augmentation to 493 classes and each class has 1 or 2 or 3 or 4 images (its not known 1 class may have only 1 image other may have 2 images). When I apply augmentation using ImageDataGenerator I get the augmented images but the name of the images are generated randomly , I want the augemnted image name as the original image name.I tried some code:What I am getting is and images also belong to different classes.What I want is , generate the folder same as class and also the augmented images should be save in the same class and name should be same as original image."
2692,What exactly the shear do in ImageDataGenerator of Keras?,"['python', 'keras', 'data-augmentation']","I cant understand what is the effect of shear parameter in ImageDataGenerator of kerasI had tried to use an image to apply the shear by apply_transform member function in ImageDataGenerator. I can see the image seems to be rotated and stretched out after apply this function. But I cant understand what exactly it did.The image does have some change, but I cant understand the effect."
2693,Rotating image and its key points label in tensorflow2.0,"['tensorflow2.0', 'data-augmentation']",I am trying to add rotation to my dataset of images where the labels have some facial keypoints. tf.contrib is removed from tensorflow 2.0 and any other library like PIL does not work as I am using tf.data.Dataset.I need angle rotated to be random while the same rotation needs to be applied to both an image and its keypoint labels as well. Is there a way to do this in tensorflow 2.0?Below is the function I used:Here I used PIL but it is not working when I try to map a tf.data.Dataset containing image paths to load_and_preprocess_data function.
2694,Correct usage of ImageDataGenerator flow function,"['python', 'tensorflow', 'machine-learning', 'keras', 'data-augmentation']","I am trying to use data augmentation for a regression model in Keras. Therefore I want to use the ImageDataGenerator class from Keras. Nearly all tutorials I can find on that task have a classification approach and thus use the method flow_from_directory. But with a regression task, this doesn't work.Then I stumbled across the flow method, but sadly there are no good examples for using it. The only thing I can find is that people are using it to output augmented data directly to the hard drive. What I want to do is (like with flow_from_directory) use the generator and put it in the fit_generator function. But the results I got are not very good and I am not sure if it's the augmented data or if I am using the flow method wrong. Here is what I did:EDIT:I noticed something else. If I setup data_gen like the followingor if the data isn't normalized alreadyThe results are far from what I tested without the data augmentation, even though ImageDataGenerator should not have transformed any image. How is that possible?"
2695,How to fit Keras ImageDataGenerator for large data sets using batches,"['python', 'keras', 'data-augmentation']","I want to use the Keras ImageDataGenerator for data augmentation.
To do so, I have to call the .fit() function on the instantiated ImageDataGenerator object using my training data as parameter as shown below.However, my training data set is too large to fit into memory when loaded up at once.
Consequently, I would like to fit the generator in several steps using subsets of my training data.Is there a way to do this?One potential solution that came to my mind is to load up batches of my training data using a custom generator function and fitting the image generator multiple times in a loop. However, I am not sure whether the fit function of ImageDataGenerator can be used in this way as it might reset on each fitting approach.As an example of how it might work:"
2696,How to use the python method “ImageDataGenerator” and save the augmented images in a variable?,"['python-3.x', 'keras', 'data-generation', 'data-augmentation']","I'm with a little trouble...
I'm constructing an augmented database to improve my CNN. The scheme is:
- I send an image, one per time, to generate others 40 images.
- The quoted method save the augmented images in a directory, but I want to save it in a variable without saving firstly in my computer. That is, I want to save directly in a variable. The code above shows what I'm telling. Take a look at the parameter ""save_to_dir""... If I neglect it, the processing is made but the data isn't saved anywhere. 
Can anyone help me?"
2697,CIFAR-10 Keras image data augmentation effect for one image only,"['tensorflow', 'keras', 'data-augmentation']","I want to show the effect of different data augmentation (randomly scaling, rotating and translating) on just one image. I plot the first image from x_train, however the second plot does not appear to have any changes.I guess I am using datagen.flow wrongly, please advise on it. Thanks.The output shape of x2 is (32, 32, 32, 3) which is why i'm unable to plot it. Why are the dimensions like that and what can i do?"
2698,"using flow_from_directory for training and validation, without augmentation","['keras', 'data-augmentation']","I am training a simple CNN with Nt=148 + Nv=37 images for training and validation respectively. I used the ImageGenerator.flow_from_directory() method because I plan to use data augmentation in the future, but for the time being I don't want any data augmentation. I just want to read the images from disk one by one (and each exactly once, this is primarily important for the validation) to avoid loading all of them in memory.But the following makes me think that something different than expected is happening:(*) Lastly I found that the batch size I used for both is 19, so I expect that I am providing 19*7=133 or 19*8=152 training images per epoch and 19 or 38 images as the validation set at each epoch end. By the way: is it possible to use the model.fit_generator() with generators built from the ImageGenerator.flow_from_directory() to achieve:
- no data augmentation
- both generators should respectively supply all images to the training process and to the validation process exactly once per epoch
- shuffling is fine, and actually desired, so that each epoch runs differentMeanwhile I am orienting myself to set the batch size equal to the validation set length (i.e. 37). Being it a divider of the training set numerosity, I think it should work out the numbers. But still I am unsure if the following code is achieving the requirement ""no data augmentation at all"""
2699,Keras ImageDataGenerator with center crop for rotation and translation shift,"['python', 'tensorflow', 'keras', 'crop', 'data-augmentation']","I need to do data augmentation but not with any fill modes, constant, reflect, nearest, wrap. Instead everytime the image is rotated or translated, I would like to have it center-cropped (shown below) so as not have any black, white, reflected, or constant edges/borders as explained here.How do I extend the ImageDataGenerator class (if that's the only way to do it and no center crop is available out of the box) with these points taken into account?Keep existing parts of the ImageDataGenerator other than the augmentation part, and write a custom augmentation functionIt would be efficient to retain the images of original size without resizing  before augmentation happens because center crop would result in huge loss of data after resize. Translate/Rotate -> Center crop -> Resize should be more efficient than Resize -> Translate/Rotate -> Center crop"
2700,how to increase number of images with data augmentation,"['python-3.x', 'pytorch', 'data-augmentation']","I'm trying to apply data augmentation with pytorch. In particular, I have a dataset of 150 images and I want to apply 5 transformations (horizontal flip, 3 random rotation ad vertical flip) to every single image to have 750 images, but with my code I always have 150 images."
2701,Data augmentation in training only with tensorflow,"['tensorflow', 'conv-neural-network', 'data-augmentation']","I want to do some random augmentation , only at train time.I've combined the augmentation as part of the graph - which I think is kind of mistake since the same graph is used for testing also - and I don't want the test images to be augmented.I want the above augmentations to be applied only at test time - how can it be done ?"
2702,How to combine two RGB images along the depth axis to prepare 6-channel input data while using Keras flow_from_directory?,"['tensorflow', 'keras', 'numpy-ndarray', 'image-preprocessing', 'data-augmentation']","The train_generator reads batches of RGB image data from disk using Keras flow_from_directory (example code below). But in my case, I have two directory of images such that I want to read a pair of images and stack them along the depth-axis to form a 6-channel image (i.e 2x R, 2x G, 2x B channels) before it goes to fit_generator.So, my question is how to combine two RGB images along the depth axis to prepare 6-channel input data while using Keras flow_from_directory?I'm following an example CNN code for classification from here: https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d"
2703,Is there a python code for image data augmentation for the whole data set?,"['python', 'tensorflow', 'keras', 'data-augmentation']","I'm working on CNN(Convolution Neural Network) for image classification problem. I have a data set of 1000 images, this images are not enough to fit the model I have designed and I want to increase the number of images before training the CNN. How to augment my data set by using python."
2704,Data Augmentation with Keras,"['keras', 'deep-learning', 'data-augmentation']","I am learning data augmentation with Keras (using jupyter notebook, python=3.7). I know that scipy no longer supports ndimage.imread() function but I have used two different alternatives to this with imageio.imread() and matplotlib.pyplot.imread(). But none of them worked. They both showed huge number of errors instead. They both produce arrays but of different types. Is there anyway this can work ?These are the code pieces that I've already tried:This is how I initially started"
2705,Failed to convert object of type <class 'function'> to Tensor,"['tensorflow', 'deep-learning', 'data-augmentation']",I am trying to randomize the flip augmentation using tensorflow's left_right and up_down augmentation function. I am getting error mapping the function based on the boolean condition via tf.cond()TypeError: Failed to convert object of type  to Tensor. Contents: . Consider casting elements to a supported type.ROR:Let me know what am I missing or if more info is needed.
2706,Can choose some data in training data after doing data augmentation?,['data-augmentation'],"I am training a UNET for semantic segmentation but I only have 200 labeled images. Given the small size of the dataset, it definitely needs some data augmentation techniques.I have question about the test and the validation set.I have custom data generator which keep feeding data from folder for training model.So what I plan to do is:do data augmentation for the training set and keep all of it in the same folder""randomly"" pick some of training data into test and validation set (of course, before training).I am not sure if this is fine, since we just do some simple processing (flipping, transposing, adjusting brightness)Would it be better to separate the data first and do the augmentation for the rest of data in the training folder?"
2707,"Mask tensors parts in tensorflow, data augmentation","['python', 'tensorflow', 'speech-recognition', 'tensor', 'data-augmentation']","I am trying to implement the following paper: https://arxiv.org/abs/1904.08779 in order to achieve better results in Speech to Text.
I am trying to implement it using the mozilla DeepSpeech repo. 
It uses the tensorflow dataset model to load the data. The audio is converter to a spectrogram and mfcc are calculated, so when the data arrives at the augment_spec function it has a shape of (?, 26). ? is the result of a reshape of a variable audio length.
I am trying to mask certain parts of the images, to do that I thought of multiplying to tensors, one being a mask of ones and zeros, using some code like thisThe problem is that these instructions:do not work since the when the graph is being built the tensors has not defined shape, so I do not know how to implement this.
Could you help me with this?
Thanks a lot!!UPDATE: Following @kempy answer my code now looks like this:But now I am getting this error:  I do not know how to solve this, thank you for your help"
2708,data augmentation error in fit generator function,"['tensorflow', 'machine-learning', 'keras', 'data-augmentation']","I am trying to train a cnn model for face expression detection and to reduce the imbalance in the input classes, I use ImageDataGenerator from keras
to enlarge my dataset.this is my code : I generated a lot of images and fed it into the model to train it but it gives me this error :
could not broadcast input array from shape (28709,128) into shape (28709)How can I identify the reason for this error?"
2709,How to properly perform data-augmentation on MRI and prepare them for deep learning?,"['tensorflow', 'multidimensional-array', 'medical', 'data-augmentation']","I'm trying to using deep learning (3D CNN) to perform brain disease classification. Currently, the input size is set to be 96*96*96. This is due to the original scan have a size of 256*256*256. I first removed the background by resizing to 192*192*192 then downsampled by a factor of 2.
However, my dataset only contains 825 subjects. I want to augment the dataset to sufficient size but it troubled me a lot.
First of all, 96^3 result in 884k voxels for input. From my past experience, the number of training samples should be a lot more than the number of input units. So my first question is: Am I right about the training data should be more than input units (in this case, more than 884k).
Secondly, to perform data augmentation, what techniques are recommended? So far I tried rotation around 3 axes with 10-degree interval. But that only augments the data size by a factor of 100.
Thirdly, when training models, I used to append input data to a list and used sklearn's train-test-split function to split them. Another way is to use keras' ImageDataGenerator.flow_from_directory. However, now I'm dealing with 3D data, and no memory could afford loading thousand of 3d arrays altogether. And ImageDataGenerator does not support nifti file format. Is there any way I could prepare all my 3d arrays for my training without exhausting my memory? (I would imagine something like ImageDataGenerator. Of course, this is under my understanding that data generator sends data into the model one batch at a time. Correct me if I'm wrong)"
2710,What are the disadvantages of mirroring CNN training images that contain anchored data?,"['python', 'opencv', 'tensorflow', 'conv-neural-network', 'data-augmentation']","I am training various CNNs (AlexNet, InceptionV3 and ResNet). The dataset consists of screen captures of a game and an array of 4 classes representing the input for that given capture as [w,a,s,d].To reduce the data I need to gather, I've looked into mirroring captures with classes that appear less frequently. If I was mirroring a left-turning capture, for example, I would also change the labels so [0,1,0,0] would become [0,0,0,1]. I'm unsure if mirroring will work as the minimap in the bottom-left corner of original images contains a GPS route.I haven't trained any models yet.I am mirroring the images and adjusting the labels via opencv:What impact on the CNN will a mirrored training dataset cause?
I.e. Will it fail to see the minimap in the bottom-left corner as it was only there in half of the training examples?Example capture and its mirrored counterpart"
2711,TensorFlow Object Detection API augmentations,"['tensorflow', 'crop', 'image-resizing', 'object-detection-api', 'data-augmentation']","I'm curious about the order of resizing and augmentations in the TensorFlow object detection API. For example, I'm using the config file ssd_mobilenet_v2_oid_v4.config. This uses fixed_shape_resizer and ssd_random_crop. So what is the interaction between these two modules? Does the ssd_random_crop take a crop of the size defined in fixed_shape_resizer? If resizing happens first, then what size are the crops after resizing? And I assume they all need to be the same exact size in order to create proper batches?"
2712,"Tensorflow 2.0 CNN training: Image augmentation function shifts pixel values outside of the [0, 1] range. Is this a problem?","['python', 'numpy', 'tensorflow', 'tensorflow2.0', 'data-augmentation']","I am working on my specific data augmentation function to train a CNN in TensorFlow 2.0. The image data I'm using are stored in a numpy multidimensional RGB array; all its pixel values are floats in the [0, 1] range.While playing with function tensorflow.image.random_brightness (documentation) I found that its pixel values are shifted outside of the [0, 1] range (etiher above or below). When I try to visualise them using matplotlib.pyplot.imshow() I get the following message:Clipping input data to the valid range for imshow with RGB data
  ([0..1] for floats or [0..255] for integers).I also found that when I try to re-normalize the data in that range, the image comes back to its original look (making the whole process useless).Are pixel values outside of the [0, 1] range a problem for CNN training? And if this represents a problem, what can I do to have pixel values in the correct range, without giving up adjusting brightness?"
2713,PyTorch transforms on TensorDataset,"['python', 'pytorch', 'data-augmentation']","I'm using TensorDataset to create dataset from numpy arrays.How do I apply data augmentation (transforms) to TensorDataset?For example, using ImageFolder, I can specify transforms as one of its parameters torchvision.datasets.ImageFolder(root, transform=...).According to this reply by one of PyTorch's team members, it's not supported by default. Is there any alternative way to do so? Feel free to ask if more code is needed to explain the problem."
2714,Data augmentation with Keras produces almost white images,"['python', 'keras', 'data-augmentation']","I'm trying do do data augmentation with keras ImageDataGenerator.
I'm pulling the images from a Dataframe containing the paths to the image in one column and the label in another one. For now, i'm only trying to flip the image horizontaly. But when I plot the images, the images look like the brightness was pushed to the max. I wonder what's going on here... Any thoughts?"
2715,"How to augment long string containing (0,1,2,3,4,5,6,?) by replacing a '?' with the the largest neighboring number?","['python', 'string', 'loops', 'data-augmentation']","I'm trying to augment a lengthy string that can contain multiple number of digits (0,1,2,3,4,5,6,?)
Consider a string ""000000000004?0??100001??2?0?10000000"".
I'm trying to replace all the question marks (?) by the neighbouring largest digit. The comparison should be done from both left character and right character to the question mark (?).Input String:  ""000000000004?0??100001??2?0?10000000"" 
Output String: ""000000000004401110000122220110000000""I wrote a function that ends up replacing them during the first iteration of the loop itself which results in replacing the ? by the highest number i.e, 4 in this case. Check the code snippet below.Wrong Output: ""000000000004404410000144240410000000""Input String:           ""000000000004?0??100001??2?0?10000000""Expected Output String: ""000000000004401110000122220110000000""Actual Output String:   ""000000000004404410000144240410000000""There are multiple strings like this with different combinations of digit and ?. I hope I have explained it well. Please help. Thanks."
2716,Do images with large dimensions (e.g. 2000 x 2000) be auto-scaled to 300 x 300 when using them for training data in AWS Sagemaker?,"['object-detection', 'amazon-sagemaker', 'data-augmentation']","I'm working on a project that trains an ML model to predict the location of Waldo in a Where's Wally? image using AWS Sagemaker with the underlying object detection algorithm being Single Shot Detection, but I am thinking that using an actual puzzle image with dimensions like 2000 x 2000 as training data is not possible and that SSD will auto-resize the image to 300 x 300 which would render Waldo a meaningless blur.  Does SSD re-size images automatically, or will it train on the 2000 x 2000 image?  Should I crop resize all puzzles to 300 x 300 images containing Waldo, or can I include a mix of actual puzzle images with dimensions 2000+ x 2000+ and the 300 x 300 cropped images?I'm considering augmenting the data by cropping these larger images at locations that contain Wally so that I can have 300 x 300 images where Wally isn't reduced to a smudge on a page and is actually visible - is this a good idea?  I am thinking that SSD does train on the 2000 x 2000 image, but the FPS will reduce by a lot - is this wrong?  I feel like if I don't use the 2000 x 2000 image for training, in the prediction stage where I start feeding the model images with large dimensions (actual puzzle images), the model won't be able to predict locations accurately - is this not the case?"
2717,steps_per_epoch for keras model.fit with data augmentation,"['tensorflow', 'keras', 'data-augmentation']","I am using tensorflow+keras. I am unsure about the steps_per_epoch parameter in model.fit when using data augmentation. My data augmentation is done using map function in tfrecord, not the image generator in keras. In this way, if I augment my data by 4 fold, will my steps_per_epoch also increase by 4 fold?"
2718,Display examples of augmented images in PyTorch,"['python', 'deep-learning', 'computer-vision', 'pytorch', 'data-augmentation']","I want to display some samples of augmented training images.My transform includes the standard ImageNet transforms.Normalize like this:However, because of the Normalise, the images display in weird colours.This answer says I'd need access to the original image, which is difficult when the transforms are applied at load time:How would I go about displaying a few sample augmented images in their usual colours while using the normalised ones for calculation?"
2719,Multiprocessing unable to automatically recycle zombie children processes for my on-the-fly data augmentation code,"['python-multiprocessing', 'data-augmentation']","System info:I am doing on-the-fly data augmentation for medical image segmentation. Inspired by line 158~161 of faustomilletari/VNet, the code example for data augmentation is as below:The code above was running well on one server, which is now not available, but failed on another server, where lots of zombie children processes came out after a while and the training process just hang there indefinitely.It seems that some children processes were killed by the system but we don't know why."
2720,Data augmentation in cross-validation,"['deep-learning', 'conv-neural-network', 'cross-validation', 'data-augmentation']","Am I understanding correctly that data augmentation in an object classification task should only be done on the training set?If so, how do you implement 10-fold cross-validation with augmented data? Is the augmented data created every time a test fold changes (i.e. 10 times)?Bonus question: can you direct me to a resource that shows how to do this in Tensorflow?"
2721,How to apply data augmentation in TensorFlow 2.0 after tfds.load(),"['python', 'tensorflow', 'tensorflow-datasets', 'data-augmentation', 'tensorflow2.0']","I'm following this guide.It shows how to download datasets from the new TensorFlow Datasets using tfds.load() method:The next steps shows how to apply a function to each item in the dataset using map method:Then to access the elements we can use:ORHowever, the guide doesn't mention anything about data augmentation. I want to use real time data augmentation similar to that of Keras's ImageDataGenerator Class. I tried using:and other similar augmentation functions in format_example() but, how can I verify that it's performing real time augmentation and not replacing the original image in the dataset?I could convert the complete dataset to Numpy array by passing batch_size=-1 to tfds.load() and then use tfds.as_numpy() but, that would load all the images in memory which is not needed. I should be able to use train = train.prefetch(tf.data.experimental.AUTOTUNE) to load just enough data for next training loop."
2722,Display augmented image using Pytorch,"['python', 'python-3.x', 'numpy', 'pytorch', 'data-augmentation']","This is a snippet mostly provided by @ptrblck in Pytorch forum for data augmentation on some of images. The task is segmentation, so I assume image and its correspond mask need to be augment. I'm wondering how can I display some images and correspond mask after transformation in order to find out how they look like? Here is the script:currently it's causing an error path = os.fspath(path)  
  TypeError: expected str, bytes or os.PathLike object, not tuple"
2723,list augmentation in python with numpy or other library [duplicate],"['python', 'list', 'numpy', 'data-augmentation']","I would like to augment list fromtoIf I want to augment likewise n times (like 100 or 500 times), how can I do it? I do not want to do it with regular loop, but using some library like numpy. Any helps?Many thanks."
2724,Visualizing augmented train images [tensorflow object detection api],"['tensorflow', 'tensorboard', 'object-detection-api', 'data-augmentation']","It is possible to augment images in tensorflow object detection api config files, e.g.: How can I visualize the training images to inspect the results of the augmentation?Thank you for your help."
2725,Replace group of similor colored pixels with image - iOS,"['ios', 'pixel', 'mask', 'data-augmentation']","I am using the fritz person model and from that, I am getting one mask image of a person. Now, I want to add one different image on that mask. So, basically, I want to replace mask with some different image or in other words you can say I want to replace a group of common colored pixels with some image.I researched a lot but didn't find anything.
I am also adding the mask image which I have got after using fritz person model for reference.Thanks in advance."
2726,Data Augmentation with torchvision.transforms in pytorch,"['pytorch', 'data-augmentation']","I found out data augmentation can be done in PyTorch by using torchvision.transforms. I also read that transformations are apllied at each epoch. So I'm wondering whether or not the effect of copying each sample multiple times and then applying random transformation to them is same as using torchvision.transforms on original data set(unique images) and just training it for a longer time(more epochs).
Thanks in advance."
2727,Read training files one by one in datagenerator,"['python', 'keras', 'data-augmentation']","I have a very large dataset that does not fit into memory 
I split it into files and I want to use them in a data generator for
training I use the following codecreate the label binarizer for one-hot encoding labels, then encode
the testing labels construct the training image generator for data augmentation, 
initialize both the training and testing image generatorsand then I use but fit_generator reads the first file only"
2728,tensorflow shift image and interpolate,"['python', 'tensorflow', 'image-processing', 'data-augmentation']","I am trying to shift all images in a batch and have 'NEAREST' interpolation. For some reason, the resulting images are interpolated with black pixels... Is this a bug?"
2729,Image augmentation options in config for Object detection and high loss value using SSD MobileNetv2,"['deep-learning', 'computer-vision', 'object-detection', 'data-augmentation']",I noticed that when I include the image augmentation options like below to train my object detection model the loss value is incredibly high  like 30K and 65K unlike when I dont use these optionsWhy is that so? note that I have only observed this for first few hundred steps and haven't baby sit my model for too long65K loss value with these~30K plus loss value with these
2730,Keras: poor performance with ImageDataGenerator,"['python', 'image', 'keras', 'computer-vision', 'data-augmentation']","I try to augment my image data using the Keras ImageDataGenerator. My task is a regression task, where an input image results in another, transformed image. So far so good, works quite well.Here I wanted to apply data augmentation by using the ImageDataGenerator. In order to transform both images the same way, I used the approach described in the Keras docs, where a transformation of an image with a corresponding mask is described. My case is a little bit different, as my images are already loaded and don't need to be fetched from a directory. This procedure was already described in another StackOverlow post.To verify my implementation, I first used it without augmentation and using the ImageDataGenerator without any parameter specified. According to the class reference in the Keras docs, this should not alter the images. See this snippet:Unfortunately, my implementation seems to have some issues. I do not get the performances expected. The validation loss is somehow stable around a certain value and only slightly decreasing (see the image below). Here I expect, as I did not use any augmentation, the same loss as the non-augmented baseline.In comparison, my training without the ImageDataGenerator looks likeI guess I got somehow mixed up with the usage of the ImageDataGenerator, the flow and the fit function. So my questions are:Update (2019-01-23 & cont.):
What I have already tried so far (in responses to answers):Neither of these approaches helped to make the results better."
2731,How to calculate the amount of generated images from the image augmentation API of keras,"['python', 'keras', 'conv-neural-network', 'data-augmentation']","I want to calculate the amount of generated images from the image augmentation API from Keras. It is unclear to me if each setting is also used in combination with other settings.I have the following settings:Rotation range: 0.2,
Width shift: 0.05,
Height shift: 0.05,
Shear range: 0.05,
Zoom range: 0.05,
Horizontal flip: True,
Fill mode: NearestFor example, does it do horizontal flips for all augmentated images as well or only for the input images? And what would then be the total amount of generated images with the parameters above?"
2732,"in the python imgaug library, when applying affine transformations to image, how do I set cval to RGB?","['python', 'data-augmentation']","from the documentation, the cval parameter is described as follows:The constant value used to fill up pixels in the result image that didn’t exist in the input image (e.g. when translating to the left, some new pixels are created at the right).I'm trying to set it's value to RGB (specifically green).here is an example picture
I want to set the white background in the r.h.s picture to green"
2733,Keras ImageDataGenarator: Inconsistency in flow functions parameter,"['python', 'keras', 'deep-learning', 'data-augmentation']","I am struggling with the image augmentation in Keras.The concept, where I define an ImageDataGenerator to modify the data and a flow function to apply it to the data is (or seems) clear to me.But why are the flow functions (flow, flow_from_dataframe, flow_from_directory) different from each other? Their purpose is clear to me: they handle data with different types of sources. I mean the difference in the parameters to pass. Especially, one difference comes to my mind: for the flow (where I augment data that is already loaded)I don't have a possibility to state an interpolation mechanism. But don't I need one then?"
2734,What does ImageDataGenerator.fit() function does in Keras Image preprocessing?,"['keras', 'image-preprocessing', 'data-augmentation']","There is ImageDataGenerator.flow() function, in which we pass our training images and it returns augmented images. But what does ImageDataGenerator.fit() function does?"
2735,Keras data augmentation with change in outputs,"['python-3.x', 'keras', 'data-augmentation']",I want to do regression with images. There are images of roads and the associated steering angle. As I want to apply data augmentation in Keras I would like to flip the input images horizontally but that would imply that the steering angle has to change its sign if the image is flipped. As far as I can see the documentation does not cover this problem. Is there a tutorial explaining how this can be achieved?
2736,Data augmentation in validation,['data-augmentation'],"I am a little bit confused about the data augmentation. If I perform data augmentation in train dataset, validation dataset should have the same operations?
For exampleWhy do we take the 'resize' and 'CenterCrop' operations in 'val' dataset?"
2737,Do data augmentations by Tensorflow Object Detection API result in more samples than original?,"['tensorflow', 'object-detection-api', 'data-augmentation']","So let's say my original raw dataset has 100 images. And I apply random_horizontal_flip data augmentation, which by default horizontally flips with 50% probability. So just for the sake of example, lets say it flips 50 of the 100 images. So, I read as much official documentation as possible, and looked into preprocessor code, but couldn't find my answer."
2738,How many images are generated by keras fit_generator?,"['keras', 'data-augmentation']","I use keras to do image augmentation and segmentation. I want to investigate the number of images generated, so I test the following setting of arguments:
(1) set batch_size as 1 in flow_from_directory when define the generator:(2) When training, I set epochs = 1 and steps_per_epoch=1:After training finished, I expect only 1 image and 1 mask are in the directory './view', but I actually found 11 pairs there.What is wrong here? Did I fail to setting some arguments, or did I do something wrong?"
2739,Tensorflow Object Detection API Data Augmentation Bounding Boxes,"['tensorflow', 'object-detection', 'bounding-box', 'data-augmentation']","For Object Detection via the Tensorflow API using the model_main.py, when I use i.e. random_horizontal_flip in the data_augmentation_options in the train_config of my pipeline.config, are my bounding boxes also affected? This is very important, as otherwise these options are not applicable. This is the same question, but it was not properly answered."
2740,TensorFlow Object Detection API: specifying multiple data_augmentation_options,"['tensorflow', 'object-detection', 'data-augmentation']","I'm wondering if there's any difference between specifying the data augmentations like this:Or like this:In the object detection pipeline file?All the samples in the models repo use the first format, but the second format is accepted as well."
2741,How to rotate images at different angles randomly in tensorflow,"['python', 'tensorflow', 'image-rotation', 'data-augmentation']",I know that I can rotate images in tensorflow using tf.contrib.image.rotate. But suppose I want to apply the rotation randomly at an angle between -0.3 and 0.3 in radians as follows:So far this will work fine. But the problem arises when the batch size changes on the last iteration and I got an error. So how to fix this code and make it work in all case scenarios? Please note that the inputs images are fed using tf.data.Dataset api.Any help is much appreciated!!
2742,What does “color ordering” mean when used with distorted inputs with inception model,"['tensorflow', 'image-preprocessing', 'data-augmentation']","I have been looking for pre-processing techniques of input images. After looking into the implementation of the inception model at: https://github.com/awslabs/deeplearning-benchmark/blob/master/tensorflow/inception/inception/image_processing.py#L164, I found that they have used color_ordering parameter which could be 0 or 1. What does color ordering mean in this context?"
2743,Automatically make a composite image for cnn training,"['python', 'tensorflow', 'dataset', 'conv-neural-network', 'data-augmentation']","i would like to train a CNN for detection and classification of any kind of signs (mainly laboratory and safety markers) using tensorflow. 
While I can gather enough training data for the classification training set, using e.g. The Bing API, I‘m struggeling to think about a solution to get enough images for the object detection training set. Since these markers are mostly not public available, I thought I could make a composite of a natrual scene image with the image of the marker itself, to get a training set. Is there any way to do that automatically? 
I looked at tensorflow data augmentation class, but it seems it only provides functionality for simpler data augmentation tasks."
2744,Python Google Translate API error : How to translate a large amount of data,"['python', 'dataset', 'translation', 'google-translate', 'data-augmentation']","I would like to use a kind of data-augmentation method for NLP consisting of back-translating dataset.Basically, I have a large dataset (SNLI), consisting of 1 100 000 english sentences. What I need to do is : translate these sentences in a language, and translate it back to English.I may have to do this for several language. So I have a lot of translations to do.I need a free solution.I tried several python module for translation, but due to recent changes in Google Translate API, most of them do not work. googletrans seems to work if we apply this solution.However, it is not working for big dataset. There is a limit of 15K characters by Google (as pointed out by this, this and this). The first link show a supposed work-around.Even if I apply the work-around (initializing the Translator every iteration), it is not working, and I got the following error :I tried using proxies and others Google translate URLs :But it's not changing anything.My problem might come from the fact that I am using multi-threading : 100 workers for translating the whole dataset. If they work in parallel, maybe they use more than 15k characters together.But I should use multi-threading. If I don't, it will take several weeks to translate the whole dataset...How do I fix this error so I can translate all sentences ?If it's not possible, is there any free alternative, to get machine translation in Python (not mandatory to use Google Translate), for such a big dataset ?"
2745,How to replace words with their synonyms of word-net?,"['python', 'sentiment-analysis', 'wordnet', 'pos-tagger', 'data-augmentation']",i want to do data augmentation for sentiment analysis task by replacing words with it's synonyms from wordnet but replacing is random i want to loop over the synonyms and replace word with all synonyms one at the time to increase data-size
2746,batch image augment in tensorflow,"['python', 'tensorflow', 'data-augmentation']","I want to do image augmentation, for example, rotate random angle in tensorflow. In each batch, I wanna rotate different random angles for every single image. I can do it by using tf.contrib.image.rotate to image_batch with randomly generated angle tensor:However, if I build batch with allow_smaller_final_batch=True, the batch_size is useless because the image_batch will not have fixed batch size. And the rotate will fail because the N dimension of radian and image_batch is not the same. How can I fix it?"
2747,Use keras ImageDataGenerator with multiple preprocessing functions,"['python', 'keras', 'deep-learning', 'data-augmentation']","Let say I wanted to train an image database with Keras, and I want to automatically generate new images using Keras ImageDataGenerator, the thing is that some functions are not available with the classical settings (flip, shift etc..)Is it possible to add not only one but a list of functions as ""preprocessing function"" ? I also tried to apply my functions before calling the datagen, but I had so many functions that I had MemoryErrors"
2748,Calculate new Position of coordinates with numpy,"['python', 'numpy', 'data-augmentation']","I have a dataset of images for Key point detection. Each Image got labeled with one keypoint (x|y). I use numpy to flip images for data augmentation. I flip an Image horizontal with this code: And vertical with this codeSo far so good. But I have to also recalculate the keypoints (labels) ( [85 35])
I know its basic math but i havent campe up with a solution.Thanks in Advance."
2749,Why do some images save properly and others don't when using ImageDataGenerator and .flow()?,"['python', 'image-processing', 'keras', 'data-augmentation']","I have been trying to augment some training data and the corresponding labels using ImageDataGenerator.Here's the way I've approached it (apologies if the formatting is a bit off)This code works for me, in the way that I do get flipped images, but the problem I am having is that it will only flip the first two images but not the rest of the images in my imgdatas and imglabels arrays. The rest come out blank. See here for an example. I've looked into this post and this one about iterating over .flow(), but still not sure why only 2 of the images work when I iterate over .flow(). Any ideas? Also I'm unsure about what the names of the images mean, it looks like it's a randomly generated number, but not sure where that's been defined.Thanks for your help"
2750,Inconsistent augmentation results for masks and images while flipping in tensorflow,"['python', 'tensorflow', 'data-augmentation']","I am currently trying to implement my own function to do paired data augmentation in tensorflow. To do this, I need to apply random image transformations to the input image and relevant transformations to the output mask (i.e. rotations and flipping). However, image flipping/reversing is not working as appropriate. A code example of what I have at the moment is as follows:What I was expecting this to do was to flip all images/masks in the same way if flip_lr > 0.5 or flip_ud > 0.5, but what is actually happening is that some images will be flipped without the masks getting flipped and v/v. Has someone experienced this and knows how to solve it?Thanks in advance "
2751,Tensorflow ImageDataGenerator Number of Outputs,"['tensorflow', 'neural-network', 'keras', 'data-augmentation']","Please forgive me if this is a silly question, I only recently started using tensorflow's data augmentation capabilities and I want to know how tell how many different variation the ImageDataGenerator produces for each inputed image. A copy of the specific code I'm using is below.Thanks for any help you can give or resource you can point me towards."
2752,Data augmentation by zooming in,"['python', 'image-processing', 'conv-neural-network', 'scikit-image', 'data-augmentation']","I want to augment my data set of images by creating new, synthetic images. One of the operations I want to try is zooming in, i.e. take a subsection of the original image (say 80% of the original size), and intelligently increase that subsection so that it still has the exact same dimension in pixels as the original image. In other words, create new image by zooming in on a portion of an existing image, so that the size of the new images is same as the size of the original.How do I do this in skimage? Or anything else in Python?"
2753,How to use different data augmentation for Subsets in PyTorch,"['python', 'deep-learning', 'pytorch', 'data-augmentation']",How to use different data augmentation (transforms) for different Subsets in PyTorch?For instance:train and test will have the same transforms as dataset. How to use custom transforms for these subsets?
2754,Data Augmentation hurts accuracy Keras,"['keras', 'deep-learning', 'transfer-learning', 'data-augmentation']","I'm trying to adapt Deep Learning with Python section 5.3 Feature extraction with Data Augmentation to a 3-class problem with resnet50 (imagenet weights).Full code at https://github.com/morenoh149/plantdiseaseQuestions:UPDATE:
This may be an issue with keras itself"
2755,Data Augmentation in PyTorch,"['python', 'image-processing', 'dataset', 'pytorch', 'data-augmentation']","I am a little bit confused about the data augmentation performed in PyTorch. Now, as far as I know, when we are performing data augmentation, we are KEEPING our original dataset, and then adding other versions of it (Flipping, Cropping...etc). But that doesn't seem like happening in PyTorch. As far as I understood from the references, when we use data.transforms in PyTorch, then it applies them one by one. So for example:Here , for the training, we are first randomly cropping the image and resizing it to shape (224,224). Then we are taking these (224,224) images and horizontally flipping them. Therefore, our dataset is now containing ONLY the horizontally flipped images, so our original images are lost in this case. Am I right? Is this understanding correct? If not, then where do we tell PyTorch in this code above (taken from Official Documentation) to keep the original images and resize them to the expected shape (224,224)?Thanks "
2756,How to perform data augmentation in Tensorflow Estimator's input_fn,"['tensorflow', 'machine-learning', 'training-data', 'data-augmentation']","Using Tensorflow's Estimator API, at what point in the pipeline should I perform the data augmentation?According to this official Tensorflow guide, one place to perform the data augmentation is in the input_fn:If I perform data augmentation inside input_fn, does parse_fn return a single example or a batch including the original input image + all of the augmented variants? If it should only return a single [augmented] example, how do I ensure that all images in the dataset are used in its un-augmented form, as well as all variants?"
2757,Using ssd_random_crop_pad operation in Tensorflow's Object Detection API,"['tensorflow', 'object-detection', 'object-detection-api', 'data-augmentation']","I am using Tensorflow's Object Detection API to train an Inception SSD object detection model on Cloud ML Engine and I want to use the various data_augmentation_options as mentioned in the preprocessor.proto file.The one that I am currently interested in using is ssd_random_crop_pad operation and changing the min_padded_size_ratio and the max_padded_size_ratio. The documentation mentioned in preprocessor.proto says the following:And the documentation mentioned in preprocessor.py defines the function as such: Where the arguments for min_padded_size_ratio and max_padded_size_ratio are a tuple of the form (h, w).However when I supply these arguments in the config file in the format given below:I run into the following error:Can anyone help me with the format to pass the arguments to min_padded_size_ratio and max_padded_size_ratio?Edit 1: Changed the config file with the following arguments:Now running into the following error:"
2758,using Keras' flow_from_directory with FCNN,"['python', 'keras', 'data-augmentation']","I trained a constitutional neural net for image segmentation with Keras successfully. Now I am trying to improve performance applying some data augmentation to my images. To do so I use the ImageDataGenerator and then flow_from_directory to load only batches into memory (I tried without but I get memory error). Code example is:However when I run the code I get the following error (I am using Tensorflow back-end):In the error it complains about incompatible shapes 14400000 (400x400x9) vs. 4800000 (400x400x3). I am using a custom loss function here (and if you look at the error it says something about the loss) that is the Dice coefficient, defined as follows:Here I use (400,400,3) images with masks for 1 class of shape (400,400,1). My NN has inputs defined as Input((img_rows, img_cols, 3)) and output as Conv2D(1, (1, 1), activation='sigmoid', name='out')(conv9) (but this was working fine when training without data augmentation)."
2759,"How to solve the problem of word segmentation, pos tag and sentiment analysis in SnowNLP?","['python', 'sentiment-analysis', 'pos-tagger', 'wordsegment']","I'm using SnowNLP package to execute the program, then I used the following github website to download this tool:https://github.com/isnowfy/snownlpIn this tool, I saw it can execute 3 programs. It includes word segmentation, pos tag and sentiment analysis.The following is the code of word segmentation.The following is the code of pos tag.The following is the code of Sentiment analysis.These programs can train. I can add more word segmentation data and pos tag data to train, but I got some problem.I referenced the following website to do it.https://www.itread01.com/content/1541222413.htmlThe first time I added some data can be train it. However, when I want to add >30,000 data to train it, I got the following error information:For Pos Tag:For Word SegmentationI don't know why I got these problems.Can anyone help me? Thanks."
2760,Export list from lapply to csv in R,"['r', 'export-to-csv', 'lapply', 'sentiment-analysis', 'corpus']","I am trying to script R to take the output of lapply and export it as a .csv with the following header:
score, file.This is how I have imported the files and created a corpus of .txt files:I am running this lapply function over the corpus of .txt files i created above:This produces a list of scores that look like this in the console:Now I want to export/print this list to a .csv file that lists the scores AND their corresponding file name. Ideally, I want the .csv to look like this:I have tried working with write.csv but I have a hard time getting the .csv in the format mentioned above. Any help would be greatly appreciated!"
2761,how to display the count and percentage of positive and negative record in excel using python?,"['python', 'excel', 'dataframe', 'percentage', 'sentiment-analysis']","I have a python script that classify tweets positive or negative  using dataframe to read csv file, and  using the logisticRegression model with tfidf , cross validation , bigram and GridSearchCV, to create a AI project, at the end it create an excel file that handel all the records with the sentiment.Now i want to add to the exported excel file  the number of the positive and negative records with percentage over all records existing in the excel file.I have a function that highlight the records based on their sentiment, i want to add to this function the ability to count and  calculate the percentage of the positive and negative sentiment  and display the result in the exported excel file."
2762,Is it possible to edit R (analyzeSentiment)?,"['r', 'sentiment-analysis', 'sentimentr']","I'm using analyzeSentiment in R and it's not performing as well as I want.As an example, I have the sentence:
""Incredibly professional representative helping with the loan.""This is clearly a positive sentence but its getting tagged with a 0 (neutral).Here's the syntax I used in case it's relevant (where Comments.csv has a column called ""Phrase"" that includes the above sentence):This results in all the Sentiment (SentimentHE, SentimentGI, SentimentLM, and SentimentQDAP) giving this sentence a score of 0.So my question is can I add to the dictionaries or change something about these packages to help them perform better for my data?Thanks!"
2763,what is better to train your own model for sentiment analysis or to use pre trained model like vader and textblob?,"['python', 'logistic-regression', 'sentiment-analysis', 'pre-trained-model']","I have python script that trained a dataset for sentiment analysis and create a model using  logisticRegression model with tfidf , cross validation , bigram and GridSearchCV. With performing the pre-process phase for the text.And i tried to use pre trained model like VaderSentiment in order to compare between the 2 models.the result on real data was :so where is the error in my trained model ? or its better to use the vaderSentiment for twitter sentiment analysis?Note that in my trained result i got :"
2764,how to add neutral sentiment to the trained dataset ? using sklearn and nltk,"['python', 'logistic-regression', 'sentiment-analysis']","I have a python script that works on sentiment analysis in order to predict the sentiment of the text.The problem is that the trained and tested dataset  includes only 2 class of classification positive & negative.but when i run this model on real data i detect neutral classification that is not existing in my trained data and this cause a problem because the system predict  it as positive or negative.I am using the logisticRegression model with tfidf , cross validation , bigram  and GridSearchCV."
2765,"Error: argument “string ” is missing, with no default Sentiment analysis","['r', 'text', 'sentiment-analysis', 'mining']","I'm trying to do some, sentiment-analysis.
And I can get a Score from my text.But if I try to get a list of the matching Words I get an Error.I get Error: argument ""string "" is missing, with no default.
And I have no Idea why.Additional InformationsHere is a Linkt to the Text im using: https://pastebin.com/CZ1qkXVA
And thats the complete Code."
2766,Comparison of deep learning models,"['machine-learning', 'deep-learning', 'comparison', 'lstm', 'sentiment-analysis']","I've applied different deep learning models for sentiment analysis. BERT, CNN, BiLSTM, BiLSTM-CNN give fine results with 10 epochs and 64 batch size while LSTM accuracy remains constant for 10 epochs and 64 batch size.
I want to ask that in case of comparison, do the parameters need to be same or can they be different? e.g. running LSTM for 100 epochs.
Would the comparison be valid then?"
2767,Sentiment results are different between stanford nlp python package and the live demo,"['python', 'stanford-nlp', 'sentiment-analysis']","I try sentiment analysis of tweet text by both stanford nlp python package and the live demo, but the results are different. The result of the python package is positive while the result of the live demo is negative.and the result is:screenshot of the live demo result"
2768,"Creating a dataframe on Date from two incomplete, not same size dataframes","['python-3.x', 'pandas', 'dataframe', 'sentiment-analysis', 'stock']","I am trying to put together a big dataframe wih dates, average sentiment scores (from Twitter), and closing stock price.Here is what I have so far.Desired outputMy plan is to then save this dataset as a csv.Issues I've run into:
Df and ts are NOT the same size. I'd need to go through ts to make all the weekend close prices the same as Friday. How do I do that?
Not knowing how to write a loop that can assign the score for a date in one dataframe to a column in another dataframe.I use pandas-3."
2769,Keras model for multiclass classification for sentiment analysis with LSTM - how can my model be improved?,"['python', 'tensorflow', 'keras', 'sentiment-analysis']","So I want to do predict the number of stars a product gets on Amazon through keras, I have seen other ways of doing this, but I have used the universal sentence encoder with one-hot encoding (I have followed a Youtube tutorial to embed the reviews). Now without using an LSTM layer and using the following layers:I am able to get an accuracy of around 0.55 and a loss of 1, which isn't great. However when I reshape my X_train and X_test data to be 3D input for an LSTM layer and then put it into a model such as:I get an accuracy of around 0.2 which is even worse, with a loss of close to 2.00.I have no idea whether an LSTM is necessary as I am new to neural networks but I have been trying to do this for my project.So I am asking should I stick with the first model without an LSTM or is there a way of changing the second neural network with LSTM to have an accuracy of 0.2 whilst using the embedding methods that I have used?Thanks for your time!"
2770,stanford corenlp sentiment analysis live demo does not work,"['stanford-nlp', 'sentiment-analysis']","Stanford nlp sentiment analysis demo does not work:
http://nlp.stanford.edu:8080/sentiment/rntnDemo.htmllive demo screenshotI could access it a week ago, but now it is not available. I do not know why. Please help, thanks!"
2771,How to assign labels/score to data using machine learning,"['python', 'pandas', 'machine-learning', 'sentiment-analysis']","I have a dataframe made by many rows which includes tweets. I would like to classify them using a machine learning technique (supervised or unsupervised).
Since the dataset is unlabelled, I thought to select a few rows (50%) to label manually (+1 pos, -1 neg, 0 neutral), then using machine learning to assign labels to the other rows.
In order to do this, I did as follows:Original DatasetSplit the dataset into 50% train and 50% test. I manually labelled 50% of data as follows:Then I extracted words'frequency (after removing stopwords):....I would like to try to assign labels to the other data based on:I know that there are several ways to do this, even better, but I am having some issue to classify/assign labels to my data and I cannot do it manually.My expected output, e.g. with the following test datasetshould be something likeProbably a better way should be consider n-grams rather than single words or building a corpus/vocabulary to assign a score, then a sentiment. Any advice would be greatly appreciated as it is my first exercise on machine learning. I think that k-means clustering could also be applied, trying to get more similar sentences.
If you could provide me a complete example (with my data would be great, but also with other data would be fine as well), I would really appreciate it."
2772,AFINN LEXICON SENTIMENT ANALYSIS ON R,"['r', 'twitter', 'sentiment-analysis']","im a newbie and im trying to get to work with Rstudio for my final project. i am using the AFINN sentiment dictionary, i found this code online but I keep getting an error and i am not sure where i am going wrong.here is the codehere is the errorenter image description herei hope everyone can help, thankyou in advance!"
2773,"AFINN, BING LIU, AND NRC ON R USING TIDYVERSE/TIDYTEXT (LEXICON) (SENTIMENT ANALYS)","['r', 'tidyverse', 'sentiment-analysis', 'tidytext']","im a newbie and im trying to get to work with Rstudio for my final project. I am using tidyverse and tidytext library to get the sentiment score but its only available in english. My problem is, I still dont know how to get the sentiment score in Indonesia language, is there any way to add the indonesian dictionary? I hope everybody can help, thank you in advance!here is the code:"
2774,How to improve accuracy of sentiment analysis project using ML,"['machine-learning', 'scikit-learn', 'sentiment-analysis', 'naivebayes']","I am trying to implement sentiment analysis using naive bayes. So I took the dataset from internet with 15K records and did the pre-processing, vectorisation and trained the model using Naive Bayes (MultinomialNB) and then predicted the accuracy. Then I took tweets from twitter and predicted their sentiment. But the accuracy is only 61% .Can you please suggest me how to increase the accuracy of sentiment analysis of tweets.Could you please suggest me if I should go with another classifier model or any other cross validation technique? Or are there any more ways that i can improve the accuracy of the below code using the same Naive Bayes to 90% ?"
2775,Hybrid neural network architecture for sentiment analysis,"['machine-learning', 'deep-learning', 'sentiment-analysis', 'oversampling', 'smote']","I'm new in the field of Machine Learning. I'm trying to implement Sentiment Analysis using different deep learning models. I've implemented LSTM, BiLSTM, CNN and BERT. Now, I'm trying to implement CNN-BiLSTM. Some papers have used CNN-BiLSTM architecture while some have used BiLSTM-CNN.
What I'm doing is CNN-BiLSTM. Using three convolution layers with filter sizes 3, 4 and 5. Their max_pooling layers are concatenated and the concatenated tensor is fed to BiLSTM. The error is:ValueError: Input 0 is incompatible with layer bidirectional_12: expected ndim=3, found ndim=4As discussed in here, when conv2d is followed by LSTM, Lambda is used to reshape from 3d to 4d as conv2d takes 4d tensor.(or Reshape layer can also be used)My question is, if BiLSTM is followed by conv2d, how can I reshape concatenated tensor(maxpool layers) from 4d to 3d so that it is used as input to BiLSTM layer?Here is the code:"
2776,Calling a function while iterating through a list in Python,"['python', 'list', 'for-loop', 'iteration', 'sentiment-analysis']","I have a csv file with about 10,000 records of string.I am trying to call an API on azure to run through each one of those records and applying some function and returning a set of values.When I call the API on an individual string, it works fine, now I want to call that function for the 10,000 records stored in the csv file. So I used the csv.reader function in Python to read the file and stored it in a variable.
The problem is that now I want to create a for loop to go through each record in the List and call the function and store the returned values in another List and then output that as a csv file.when I try to make a for loop and call the function from within, I get an Index Out of Range, Error.This is the code which I am using, can you please tell me what I could be doing incorrectly?"
2777,"ValueError: blocks[0,:] has incompatible row dimensions","['python', 'machine-learning', 'scikit-learn', 'sentiment-analysis', 'feature-extraction']","Im trying to extract few text features(word_count.char_count...) & tf-idf from a twitter dataset for sentiment analysis. Using sklearn's featureUnion to combine them and give them to a classifier in a Pipeline.Im getting the following error ValueError: blocks[0,:] has incompatible row dimensions. Got blocks[0,8].shape[0] == 7920, expected 1. Here is the code:Here is the complete error logdata set sample:dataset shape - (7920, 3)Any immediate help on this would be grateful."
2778,Unable to create app in twitter developer account,"['twitter', 'tweepy', 'sentiment-analysis', 'data-extraction', 'twitterapi-python']","Why is it that I am unable to create a twitter application ? I have created a developer account and ever since, i can see the option to create new application but the page reloads and lands on the dashboard. I'm trying to extract data using Tweepy for sentiment analysis. Any suggestions would help - Thanks in advance !!"
2779,Sentiment analysis in R by row,"['r', 'sentiment-analysis']","I am performing a sentiment analysis of a dataframe. Each row of the data frame has a ""text"" variable that consists of several words. I wish to perform a sentiment analysis by counting the positive and negative words through the ""bing"" lexicon. However, when I use the code:I get the count of all the entries in the dataframe.Is there any way to get the count by row of the dataframe.For example, the following are the rows of the data frameWhen I use the previous code it would count the number of positive and negative entries of the text variable in both entries. I want it to only count the positive and negative of each row."
2780,How to Loop through sentiment analysis with Textblob,"['python', 'sentiment-analysis', 'textblob']","I am trying to use Textblob to perform sentiment analysis on abstracts retrieved from the New York Times APIs. Eventually, I want to extract this data into an excel file using Pandas. How would I go about performing sentiment analysis on all 20 abstracts at once?This is what I have so far:"
2781,Why has sentiment analysis been turned off in our LUIS apps?,"['sentiment-analysis', 'luis', 'sentimentr', 'luis.ai']","Support help needed: A few weeks ago we discovered the shape of the sentiment json object had changed without notice, breaking our code. Now it's breaking again because the entire sentiment object is missing. After investigating in our LUIS app, it turns out the sentiment setting has been turned off in our manage portal, and it is no longer accessible (I can't turn it back on).  What is happening with sentiment?"
2782,Word2Vec - Model with high cross validation score performs incredibly bad for test data,"['machine-learning', 'cross-validation', 'word2vec', 'sentiment-analysis']","While working on sentiment analysis of twitter data, I encountered a problem that I just can't solve. I wanted to train a RandomForest Classifier to detect hate speech. I, therefore, used a labeled dataset with tweets that are labeled as 1 for hate speech and 0 for normal tweets. For vectorization, I am using Word2Vec. I first performed a hyperparametrization to find good parameters for the classifier.
During hyperparametrization I used a repeated stratified KFold cross-validation (scoring = accuracy)
Mean accuracy is about 99.6% here. However, once I apply the model to a test dataset and plot a confusion matrix, the accuracy is merely above 50%, which is of course awful for a binary classifier.
I successfully use the exact same approach with Bag of Words and had no problems at all here.
Could someone maybe have a quick look at my code? That would be so helpful. I just cannot find what is wrong. Thank you so much!(I also uploaded the code to google collab in case that is easier for you: https://colab.research.google.com/drive/15BzElijL3vwa_6DnLicxRvcs4SPDZbpe?usp=sharing )First I preprocessed my data:Now train has the labels in column 0 and the preprocessed tweets in column 1.Next I defined the Word2Vec Vectorizer:I split the dataset into test and training set and vectorized both subsets using W2V as defined above:Now I carry out the hyperparametrization:This results in the following output:Next, I wanted to draw a confusion matrix with the test data using the Model:This outputs:for the RandomForestClassifier we receive the following values:
Accuracy: 57.974%
Precision score: 99.790%
Recall score: 15.983%
F1 score: 27.552%"
2783,How can I build my search query with tweepy,"['twitter', 'tweepy', 'sentiment-analysis']","hope you're fine!I want to build a query using the Twitter search API.
Am using tweepy trying to use API.search()I am very new to this but need to get this work now.According to the doc, the search() function should specify a value for q and other parameter. I don't know how to March my query exactly.
For example I want to return tweets containing certain keywords like where some names are mentioned, how do I march this in the q parameter... Any help will be deeply appreciated."
2784,Removing all of the stop words in each tweet,"['python', 'python-3.x', 'jupyter-notebook', 'data-mining', 'sentiment-analysis']","I want to remove all of the stop words in each tweet, but i'a, getting this error below.
I try finding solutions to figure it out, however i didn't find any."
2785,Facebook Sentiment Analysis dashboard in Power BI,"['facebook', 'powerbi', 'sentiment-analysis']","I want to apply Sentiment Analysis on COVID 19 posts, likes, and how much research about COVID 19 to add in my dashboard using Power BI? Im New in Data Analysis and Power BI so Please help me to learn and give me the steps to.Thanks in Advance,"
2786,Oversampling Using SMOTE Removes a Label Category from y_train,"['machine-learning', 'deep-learning', 'sentiment-analysis', 'oversampling', 'smote']","I'm using LSTM for Sentiment Analysis by using imbalanced dataset having 86% positive class and 14% negative class samples. It's a very small dataset with 472 sentences but they're in regional language. Train_test_split ratio is 0.3.
I'm having two issues in implementation:1: Training and Validation accuracy is constant throughout the process (Without SMOTE).
2: While using SMOTE for oversampling, y_train shows only 1 label in oversampled y_train.shapeThe results for y_train:However, the actual shape of data is as follows:Hence, LSTM training gives error message"
2787,Feature Selection in Twitter Sentiment Analysis,"['python', 'machine-learning', 'twitter', 'sentiment-analysis', 'feature-selection']","I'm currently working on a twitter sentiment analysis project. In this project, one requirement is to perform Feature selection for a better prediction. But I'm fairly confused about the techniques to do feature selection:I assumed that removing stopwords, @mentions, punctuation etc. are all steps for the pre-processing of the tweets. Now I discussed that matter with a group member and I was told that those steps are already part of the feature selection. And on top of that, there are also statistical methods to select the features even more.Now I wanted to make sure if that's true because then I'm not sure what exactly belongs to the pre-processing?"
2788,"sentimental analysis only for one review.. here's the code what supposed to be second argument for classifier.fit(new_X_test, )?","['python', 'machine-learning', 'sentiment-analysis', 'naivebayes']","this is the code for sentimental analysis only for one review, as we don't have dataset i am not able to figure out what would be the second parameter for classifier.fit method in naive bayes model?"
2789,Twitter sentiment analysis: Approach to generate multiple features for feature selection,"['python', 'twitter', 'sentiment-analysis', 'plotly-dash', 'feature-selection']","Currently, I'm working on an interactive feature selection project in combination with a Twitter sentiment analysis. In this project, we want to use Dash so that the users can decide which features they want to remove/keep to improve the classification into positive or negative tweets.I got a general analysis running. Now I wanted to extend the analysis with additional features so that the user has a choice which features should go into the prediction. (In this analysis we use the Naive Bayes classifier and the dataset is already labelled) The additional features that should be available are the ones from this thread here:Twitter Sentiments Analysis useful featuresBut I don't really know how exactly I should add these features to the analysis. My initial idea was:I'm not quite sure if this approach is suitable or even functional. And when I want to follow an approach like this: Are there any dictionaries/libraries that help with the assessment of emojis or negation words? And how could I include unigrams and bigrams?I'm grateful for every input, hint, advice or another idea :)"
2790,Classification Problem: iterate different K for KNN,"['python', 'machine-learning', 'classification', 'sentiment-analysis']","I am currently working on a classification problem (tweet sentiment analysis) and I would like to include a for loop for different K-values (KNN) in the classifiers list below.I know that I could just go with:
KNeighborsClassifier(3), KNeighborsClassifier(5)... But I am trying to implement the rather elegant solution with a for loop.Unfortunately, trying to create an empty list and add the different K values to it and then including it in the classifiers = [] list does not work properly. Do you have any good recommendations?My code:If you need any more info, just let me know :) Thank you very much in advance!"
2791,"Using a trained sentiment analysis model, TF-IDF and logistic regression","['python', 'machine-learning', 'scikit-learn', 'sentiment-analysis', 'tf-idf']","I'm doing a sentiment analysis project on a Twitter dataset. I used TF-IDF feature extraction and a logistic regression model for classification. So far I've trained the model with the following:This logistic regression model was trained on a dataset of about 1.5 million tweets. I have a set of about 18,000 tweets and I want to use this model to predict the sentiment scores for the tweets in this new dataset. I'm at a loss of how to actually apply this trained model to new data. The head of this new dataframe df_chi looks like this:which has shape (18393, 7). I want to take the trained model I already have, apply it to the text column, and create a new sentiment column with those predicted scores in the df_chi dataframe. (Note: the image doesn't show cleaned text, but I'll do that.)I'm a ML noob and I've never taken a trained model and applied it to new data. My confusion starts with extracting features from the df_chi text with TF-IDF. I attempted to do this (total guess):which gives the following ValueError:Pretty sure my whole approach to applying the trained model on the new data is incorrect. What's the right way to do this?"
2792,R: SentimentAnalysis Package,"['r', 'grouping', 'sentiment-analysis']","Does anyone know why aggregate argument in function analyzeSentiment from SentimentAnalysis package in R is not grouping sentiment scores? Here is a simple reproducable example:Which gives me the following:The help on function analyzeSentiment  says that "" aggregate: A factor variable by which documents can be grouped. This helpful when joining e.g. news from the same day or move reviews by the same author"".The question I have is that why scores are not the same within each group?"
2793,How to build a dataset for a twitter sentiment analysis?,"['python', 'twitter', 'data-science', 'tweepy', 'sentiment-analysis']","I'm doing a twitter sentiment analysis and I've created two versions of my project so far. One using the twitter API to get a user timeline tweets, but the API only gets tweets from within the last week, so there's only 200 tweets, which is a small sample size. I've tried using the streaming API, as that would get me much more tweets to have a larger sample size, but the issue is that is the retweet counts and fav counts are zero, which could hinder the performance of my data later on, so I'm wondering which would be the best way to go about with this project? Should I get tweets from the API or stream the tweets? Thanks"
2794,Issue applying textblob to a dataframe series,"['python', 'sentiment-analysis']","After splitting my dataset into train, test, and validation sets I have a x_validation set which is a set of strings. Calling x_validation.head() gives:It has something like 15,000 strings total. I'm trying to create a new list tbresult containing the sentiment polarity scores of each string as calculated by TextBlob:This gives me the following error:I'm confused because when I do the following,it works, I get 0.5. I'm confused where this float type is coming from in the error. How do I do this properly?"
2795,"Impossible to find path, what should I put in the folder to make it work?","['python', 'python-3.x', 'stanford-nlp', 'sentiment-analysis']","I want to load a specific folder for the Stanford Sentiment Treebank and I want to convert it into tabular format.I already downloaded it, but I don't know whether I should put it in the Python project folder or not.The code is:But it says it is impossible to find the path:FileNotFoundError: [WinError 3] Impossibile trovare il percorso specificato: './raw_data\\trees\\raw_data\\train.txt' -> './raw_data\\train.txt'andI don't know what should I put in the folder to make it work?"
2796,sentiment analysis using python pandas and scikit learn,"['python', 'pandas', 'scikit-learn', 'sentiment-analysis']","I have a dataset of product review.I want to count words in a way that instead of counting all the words I want to count some specific words like ('Amazing','Great','Love' etc) and put this counting in a column called 'word_count'.Now our goal is to create a column products[‘awesome’] where each row contains the number of times the word ‘awesome’ showed up in the review for the corresponding product.we will use the .apply() method to iterate the the logic above for each row of the ‘word_count’ column.First,we have to use a Python function to define the logic above. we have to write a function called awesome_count which takes in the word counts and returns the number of times ‘awesome’ appears in the reviews.Next, we have to use .apply() to iterate awesome_count for each row of ‘word_count’ and create a new column called ‘awesome’ with the resulting counts. Here is what that looks like:
products['awesome'] = products['word_count'].apply(awesome_count)Can anyone please help me with the code need for the problem mentioned above.
Thanks in advance."
2797,NRC Emolex (Lexicon) in R,"['r', 'sentiment-analysis', 'lexicon']","im a newbie and im trying to get to work with Rstudio for my final project. As i know that NRC Emolex is available in 40+ languages including Indonesian language. My problem is, I still dont know how to get the sentiment score with NRC Emolex in Indonesia language. I hope everybody can help, thank you in advance!code that i trieddata that i want to import and use https://drive.google.com/file/d/1tzdZlfS-mf2Y6W-bYQC4qMR9n7AEwgfn/view?usp=sharing"
2798,Sentiment Analysis Stanford NLP in Google Colab,"['stanford-nlp', 'sentiment-analysis']","Please, I'm trying to use sentiment analysis using Stanford NLP on Google Colab, but I dont know how to 'redirect' the server, as explained here:How to Run standford corenlp server on Google Colab?I don't know about servers in generally. I would like to do this in Colab:
To do this,this code worked in my computer:1-I've downloaded the Stanford NLP library;2-The files containing the text that I want to process with sentiment analysis are in a folder ( what I've call as ""directory_path""(input) in the above code);3- I want to generate(output) a xml file with the output/answer generated by the serverAny advice can help me! So, my objective is just generate the output using this code on Google Colab - but this code doesn't work on Colab, and I know nothing about servers and don't have much experience .I didn't find working examples of code for sentiment analisys with Stanford NLP in Colab.Note: as far as I know, Stanza library doesn't support sentiment analysis. I don't mind about the version of the library used - it can be older version of Stanford NLP too."
2799,How to get a sentiment scale from a dfm,"['r', 'sentiment-analysis', 'quanteda']","For a university project (using Quanteda in R) I am trying to calculate the sentiment score of a corpus generated with a kwic function. I started by creating the wanted corpus with kwic:I think that worked alright, I can look at texts and the summary looked realistic.To start my sentiment analysis I then used the the Lexicoder dictionary by Young and Soroka:To create a sentiment measure I then tried to use this logit scale, but it produces only NAsBecause it is a university project, I have to use this sentiment measure, so is there any way I can make it work?"
2800,"Error in nchar(Terms(x), type = “chars”) : invalid multibyte string, element 4123","['r', 'text-mining', 'sentiment-analysis', 'naivebayes', 'term-document-matrix']","I've got an error when I tried to inspect a 'Term Document Matrix'. Here is the source code that I used.And when I run the 'inspect(tdm)', the error message was:
Error in nchar(Terms(x), type = ""chars"") :    invalid multibyte string, element 4123Can anyone help me with this issue? Thanks before."
2801,Getting the average sentiment from Youtube comments,"['python', 'google-api', 'youtube-data-api', 'sentiment-analysis', 'google-api-python-client']","I'm trying to get the average sentiment from comments on two Youtube videos. When I do the sentiment analysis for all of the text from one video the sentiment is 1 which I believe is because the text is so long. Therefore I want to separate the comments, do individual analysis on them and then get the average. Any clues of how to solve my problem?"
2802,need to merge a sparse matrix and two columns from data frame in python,"['python', 'sparse-matrix', 'sentiment-analysis', 'tf-idf']","I have a sparse matrix with (881900, 76656) .Sprase matrixDataframe with column(highlighted) of interestBoth my dataframe and sparse matrix has same no of rows, but no columns in common
sparse matrix is my output from tf-idf process of sentiment analysisCan you please guide me if i can merge these two objects in python, when merging, i think i should ensure that every entry from data frame is aligned with corresponding entry of sparse matrix."
2803,Problem while feeding LDA Topics as Input to supervised classifier,"['python', 'sentiment-analysis', 'lda', 'zomato-api']","I am doing topic modeling on restaurant reviews in python. I have collected the LDA Topics and fed as input to a supervised classifier. This is my code,The above code 'train_vecs' variable is a list of each review distribution on the LDA topics.
Here I am getting output as 0 for all the three classifiers mentioned above.
Could anyone help me with the solution?"
2804,I am trying to construct TF-IDF Matrix in order to compute sentiment score of amazon reviews,"['python', 'machine-learning', 'scikit-learn', 'sentiment-analysis', 'feature-extraction']","I am trying to construct TF-IDF Matrix in order to compute sentiment score of amazon reviews
I have preprocessed the data, data is a list of all the reviews, where every element in the list is a reviewx2 is a list of reviews (output after applying the lemmatizationnow i need to build a TF-IDF matrix, i am facing error , i am not sure how to feed the data to built TF-IDF Matrixpackages used : sklearnerror(sorry about the indentation , this is my first post)"
2805,"Using np.where, langdetect in pandas","['python', 'pandas', 'numpy', 'dataframe', 'sentiment-analysis']","I want to add a new column in dataframe, which will paste the data from another column if it is written in English, and paste nothing if it is not in English using langdetect library.I hope, the meaning is clear. But I have Error like this.If I typethere is again the same Error, smth linked with AppData. What can I do?"
2806,Is there any way to detect English words in a string in pandas,"['python', 'pandas', 'dataframe', 'sentiment-analysis', 'non-english']","I have a dataframe of Songs, its singers and lyrics. There are songs which lyrics are not in English language, but letters are latin letters. Is there any way to separate English words (which have meanings in English) from non English words (which are written by latin letters, but have no English meaning). Any Python library or sort of code? My main goal is to do sentiment analysis by the lyrics."
2807,How to map aspects to reviews in sentiment analysis?,"['python', 'sentiment-analysis', 'zomato-api']","I am doing sentiment analysis on the aspects of food reviews in python . If the review is  Eg : ""Biryani was good and the atmosphere was bad""  , it should be tagged as {[food,1],[ambience,-1]} (i.e) Food should be mapped to briyani and ambience to atmosphere . Could you please help me to solve this ?? Is there any pre defined dictionaries in python to handle these food reviews to map them ??"
2808,How to find intensity from TextBlob sentiment analysis,"['python', 'python-3.x', 'sentiment-analysis', 'textblob']","I am using the sentiment analysis tool in the TextBlob package on Python 3.7. I am familiar with it and understand that it works on a basis of 3 values: polarity, subjectivity, and intensity. Polarity and subjectivity are standard output from TextBlob('string').sentiment, however TextBlob('string').intensity was sadly not successful. Any clues on this? "
2809,How to save my keras sentiment classification model and use it,"['python', 'tensorflow', 'keras', 'word2vec', 'sentiment-analysis']","I am training a sentiment classification model using pre-trained word2vec model And an LSTM architecture.
How can i save this model and use it on a input text, i found a lot of ways to do it but i dont know which is the right one ?
Thanks in advance."
2810,why a simple python MLP model can outperform complex pytorch RNN-LSTM model tremendously,"['python', 'pytorch', 'lstm', 'sentiment-analysis', 'mlp']","I have compared simple MLP (in fact even just one layer with 10 nodes!) using python (followed Andrew Trask instruction) versus RNN-LSTM model defined in Pytorch for the same movie sentiment binary classification problem. To my surprise the simple model outperforms pytorch LSTM overwhelmingly; it takes less than a minute for the first model to be trained on CPU (20,000 reviews) and generates 85% of test accuracy while for the latter it took me ~ 75 minutes to reach more or less the same accuracy! That makes me curious is there any guideline when to use just simple models over complex ones? I mean, should the demanding models like LSTM just be used for huge datasets? As a complementary point, I know data pre-processing is important but not sure if it makes such a mind-blowing difference!  Here is the github link: https://github.com/mghanava/SentimentAnalysisPractice.git
Please check Sentiment_Classification_Comparison.ipynb or its pdf version.Thanks"
2811,How to get specific amount of tweets using tweepy?,"['python', 'twitter', 'tweepy', 'sentiment-analysis']",I am working on a sentiment analysis project on twitter.Trying to get specific amount of tweets on a related keyword. But my code keeps giving tweets with random count. How can i fix that ?
2812,Trouble fitting my model on sklearn from svm,"['python', 'typeerror', 'svm', 'sentiment-analysis', 'sklearn-pandas']","I am trying to perform sentimental analysis using sklearn. I have loaded the data and created word vectorization, for the classification, here am trying to fit X and Y on svm model, but it throughs up a type error  Below you can see my code and error message.please let me know a workaround for this issue.thanks in advance"
2813,TypeError during normalization in sentimental analysis,"['python', 'normalization', 'sentiment-analysis', 'python-unicode']","I have printed a small part of the df_stock.loc[date,'articles']the stack trace is given belowFrom what i understand the issue seems to be the unicode.normalize function, but I can't figure out what the exactly is the issue. "
2814,How to create a customized trade/law lexicon for r text analysis,"['r', 'nlp', 'text-mining', 'sentiment-analysis', 'lexicon']","I am planning to do text analysis in R just as sentiment analysis with an own custom dictionary following a ""trade"" versus ""law"" logic.I have all the required words for the dictionary in an excel file. Looks like this:What steps do I have to pursue in order to transform this in an R-suitable format and apply it to my text corpus?Thank you for your help!"
2815,NLP sentiment analysis: 'list' object has no attribute 'sentiment',"['python', 'nlp', 'sentiment-analysis', 'textblob']","For a current project, I am planning to perform a sentiment analysis for a number of word combinations with TextBlob.When running the sentiment analysis line polarity = common_words.sentiment.polarity and calling the results with print(i, word, freq, polarity), I am receiving the following error message:Is there any smart tweak to get this running? The corresponding code section looks like this:Edit: Please find below the full solution for the situation (in accordance with discussions with user leopardxpreload):"
2816,Swift NLP sentiment analysis doesn't work in Turkish. Why?,"['swift', 'nlp', 'swiftui', 'sentiment-analysis', 'createml']","I calculate the emotion analysis of the string variable entered in the code below. It works correctly in German and English languages, but it returns 0.0 double value in Turkish language. What is the reason of this ?"
2817,Sentiment_by function,"['r', 'twitter', 'sentiment-analysis', 'sentimentr']","i am collecting the sentiment data of some twitter information. Moreover I have been using the function sentiment_by in order to group the information by a column. 
The problem that now I need to group by two columns (screen_name and created_at) I have tried this but doesn't work "
2818,replace_emoticon function incorrectly replaces characters within a word - R,"['r', 'regex', 'data-cleaning', 'sentiment-analysis', 'emoticons']","I am working in R and using the replace_emoticon function from the textclean package to replace emoticons with their corresponding words:As seen above, the function works but it also replaces characters that look like an emoticon but are within a word (for example the ""xp"" in ""experience""). I have tried to find a solution for this issue and found the following function-overwrite that claims to fix this issue: However, while it does solve the issue with the word ""experience"", it creates a whole new issue: it stops replacing the "":P"" - which is an Emoticon and should normally get replaced by the function. Furthermore, the error is known with the characters ""xp"", but I am not sure whether there are other characters except for ""xp"" that also get replaced incorrectly while they are part of a word.Is there a solution to tell the replace_emoticon function to only replace ""emoticons"" when they are not part of a word?Thank you!"
2819,Replace Emojis in R with replace_emoji() function does not work due to different encoding - UTF8/Unicode?,"['r', 'unicode', 'emoji', 'data-cleaning', 'sentiment-analysis']","I am trying to clean my text data and replace Emojis with words so that I can perform a sentiment analysis later on. Therefore, I am using the replace_emoji function from the textclean package. This should replace all emojis with their corresponding words.The dataset I am working with is a text corpus, that is also the reason why I used the  VCorpus function from the tm package in my sample code below:Although the function itself works, it does not replace emojis in my text as it seems that the Encoding within the ""hash_emojis"" dataset is different than the one I have in my data. Thus, the function does not replace the Emojis into words. I have also tried to convert the ""hash_emojis"" data by using the iconv function but unfortunately did not manage to change the encoding.I would like to replace the Unicode values are shown in my dataset with words. "
2820,How to dynamically load models for Sentiment analysis if they have different column names in ML .net?,"['c#', 'sentiment-analysis', 'ml.net']","I use mapping now like this.If another trained model has column names that differ from the names in the code, there will be an exception in the CreatePredictionEngine method like this: 
System.ArgumentOutOfRangeException
Could not find input column 'SentimentText'.Also if I use the Column Name attribute it expects a constant.The use of such a model is not allowed since the Column Name expects a constant.How to dynamically map column names in a model for Sentiment Analysis to a model in code. Are there other ways to dynamically load models for Sentiment Analysis?"
2821,Attention mechanism for RNN,"['machine-learning', 'neural-network', 'recurrent-neural-network', 'sentiment-analysis', 'attention-model']","I m try to desing a rnn model to make a sentiment analyze.
So RNN to predict the sentiment (negative, neutral, positive, mixed) of giventext. The model iterates over the given word sequence using RNN and predicts the sentiment from the final hidden state. What would be the drawbacks of this architecture for the sentiment classification task and how would I improve this architecture using the attention mechanism? For examples: 
”It is a very snowy day.  There are lots of traffic accidents.  At least children seem.  happy.”
”Yesterday turned out to be a terrible day.  I overslept my alarm clock, and to make matters worse, my dogate my homework.  At least my dog seems happy..."""
2822,How to use smote on kfold,"['python-3.x', 'scikit-learn', 'sentiment-analysis', 'imbalanced-data', 'smote']","I'm trying to use smote for imbalanced data, but I dont know where I can applied smote function, can anyone help me? Below is my code for kfold and accuracy "
2823,AttributeError: 'int' object has no attribute 'lower',"['machine-learning', 'flask', 'pickle', 'sentiment-analysis', 'joblib']","I am trying to pass a tweet from a flask UI and be able to make a prediction of the type of the tweet if its a donation, disaster etc.Here is a working code from Jupyter notebook:The results May someone help me look at the code and correct me where i am doing it wrong?Stack trace:"
2824,Sentiment Analysis: Unknown SSL protocol error in connection to api.twitter.com:443,"['r', 'ssl', 'twitter', 'sentiment-analysis']","I'm trying to run a code to get tweets, but I'm always finding the same error with the SSL protocol.
I'm following a youtube tutorial from a class in sentiment analysis; that's where the code is from.
There's all the libraries I'm using for it.When I run the code above, everything works except for the handshake. I get this error:I don't know what to do. I've already updated my R and Rstudio, and also all my packages. I tried some stuff from turning down internet security and whatnot. If any of you have an answer for this or another code I could try, it would help a lot! It's for my term paper and I'm kinda desperate hahahaThanks in advance!!"
2825,How to generate tweets for a specified date?,"['data-analysis', 'tweepy', 'sentiment-analysis', 'twitter-streaming-api', 'textblob']","I have been able to use the tweepy API to generate tweets.
I stored the generated tweets from a given handle (user) in a dataframe(pandas) with columns representing the information I need from the tweets like text, likes, retweets,date, geo, etcWhat I want exactly is to generate tweets text from a specified region (a particular country) and in a range of time (say March to December, 2019).Any help will be appreciated."
2826,Rating videos based on various parameters - including sentiment analysis of comments,"['python', 'nlp', 'sentiment-analysis']","I am trying to come up with an algorithm for rating videos. Now there are few parameters (we store for every video) which can predominantly determine the popularity (liked by many) of any given video :-Other than that we can analyse sentiments in comments as well (sentiment score).I came up with following algorithm :-Now normalize all above parameters to come up in range 0 - 1. It seems this step is necessary otherwise views will, in general, be most predominant one. Thereafter, multiply each of above parameter (normalized) with their respective weight and sum up the result. This result when sorted will reflect the popularity.Is this the correct way to accomplish this?"
2827,Clean up the tweets for sentiment analysis,"['r', 'sentiment-analysis']",I used the following code to clean the corpus of the Username in the Tweet. But some names are in the word cloud. Why are they wrong?
2828,Sentiment analysis of a certain paragraph from a website,"['python', 'scope', 'sentiment-analysis', 'article', 'textblob']","I have url of multiple websites in an xlsx file. I ran a loop on the xlsx file and passed the urls as an argument to the following sentiment analysis code.
Now the code is providing me with the analysis of the whole website (the websites only contain text and numbers) but the problem is that I want to run the analysis only on the paragraph that starts with ""Managerial function"". How may I do the same?
Here's my code:"
2829,Tweepy not letting me access historical data,"['python', 'tweepy', 'sentiment-analysis']","I am trying to access some tweets from the past but twitter is generating blank file. While if I put most recent date its fetching datafrom textblob import TextBlob
import matplotlib.pyplot as pltclass SentimentAnalysis:Can somebody say why is it happening & how can I resolve. It is happening for any keyword that I search ."
2830,Extracting all possible user comments for Reddit,"['python', 'sentiment-analysis', 'reddit']","I am trying to do sentiment analysis for a particular reddit link1) The code works, but the comments look to be too few. I wonder if I am doing something wrong?
2) It also seems like there is a natural limit, something like 1000 comments? How do I extract all the data? "
2831,IBM-Watson Sentiment Analysis,"['python', 'ibm-watson', 'sentiment-analysis']",I am using IBM Watson for Sentiment Analysis and I get Forbidden Error 403. I contacted the IBM customer support mentioning the error and they asked me to check here.The Error received is given below:
2832,How to visualize aggregate VADER sentiment score values over time in Python?,"['python', 'pandas', 'plot', 'sentiment-analysis', 'vader']","I have a Pandas dataframe containing tweets from the period July 24 2019 to 19 October 2019. I have applied the VADER sentiment analysis method to each tweet and added the sentiment scores in new columns. Now, my hope was to visualize this in some kind of line chart in order to analyse how the averaged sentiment scores per day have changed over this three-months period. I therefore need the dates to be on the x-axis, and the averaged negative, positive and compound scores (three different lines) on the y-axis. I have an idea that I need to somehow group or resample the data in order to show the aggregated sentiment value per day, but since my Python skills are still limited, I have not succeeded in finding a solution that works yet. If anyone has an idea as to how I can proceed, that would be much appreciated! I have attached a picture of my dateframe as well as an example of the type of plot I had in mind :) Cheers,
Nicolai"
2833,Saving the file output,"['python', 'twitter', 'sentiment-analysis']","I am new to the coding world and I found this code on open source.
I am running a twitter API streamer and I wish to save the outputs in csv file. can someone add few lines to my code here to save the output?I have marked the required places where codes need to be entered"
2834,Exception: Cannot load model.bin,"['python', 'sentiment-analysis', 'fasttext']","I have got the following error message trying to run a model: The code I have used is the following: I have tried to follow this answer: FastText - Cannot load model.bin due to C++ extension failed to allocate the memory but probably I am doing something wrong as the error is still there. 
Any idea on how to fix it?"
2835,Sentiment analysis and fasttext: import error,"['python', 'sentiment-analysis', 'fasttext']","I want to run some sentiment analysis using FastText. However, I have always got errors during the declaration of libraries and no example and tutorial within the web seems to be able to fix this. I have tried to follow the steps described here: https://github.com/facebookresearch/fastText/tree/master/python#installationbut since the beginning, i.e. since I have been getting the following error:I am using Python 3.7 in Jupyter Notebook. I would need FastText to analyse the sentiment of some Italian texts. 
I went here: https://fasttext.cc/docs/en/supervised-models.html but I have not understood what I should download. I really hope you can help me with this. "
2836,"ValueError: Input 0 of layer bidirectional_16 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 64]","['tensorflow', 'machine-learning', 'keras', 'recurrent-neural-network', 'sentiment-analysis']","history = model.fit(train_dataset,epochs=5,validation_data=test_dataset,validation_steps=30)I was trying to make a sentiment analysis program with an inbuild dataset from tensorflow. But after adding this layerI started getting this particular error. Otherwise the program runs just fine. What could it be?
 I did not know the shape of the input hence it was not mentioned.Solved!
Will adding more layers increase accuracy of the model and is there a chance of overshooting while training for just 5 epochs?"
2837,"Adding a new column in a Dataframe, with certain conditions","['python', 'pandas', 'nlp', 'sentiment-analysis']","I am very new to python,and have started working on text data.I want add a column in the dataframe, compare it with a condition mentioned in a different column and fill it accordingly.The dataset was of 10000 rows, I shortened it by taking out random sample of 2000 rows.I want to include new column named "" Review Sentiment "" and fill the cells in it as  1 if review.rating is >3 and  0 if review.rating is =< 3. Here is what I have tried.Code:Error:ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().Dataset: Extract from the dataset. Kindly help using these columns from the dataset. The logic would remain the same."
2838,Is there a pre-trained German sentiment analyzer in flairNLP?,"['python', 'machine-learning', 'nlp', 'sentiment-analysis']","I'm currently looking at a sentiment analysis task on text messages in German. The Flair library for Python seems to have a pretty powerful pre-trained model for English, but I can't find any comprehensive answer to whether it also contains a similar thing in German. I've tried googling the issue but got nowhere, and also attempted exchangingfor and the same with 'ger-sentiment' and 'ge-sentiment' just in case that would work by some lucky fluke. However, I have so far gotten nowhere. I am aware of textblob-de and will try that, but wanted to ask whether I missed something in Flair."
2839,Getting error of list indices must be integer or slices not string after applying lambda function,"['python', 'nlp', 'sentiment-analysis']",In the below code I'm applying a positive and negative sentiment and neutral sentiment as per polarity score using a function fetch_sentiment_using_SIABut it is not working how to resolve it?
2840,Exception Value: only integer tensors of a single element can be converted to an index,"['python', 'machine-learning', 'sentiment-analysis', 'fast-ai', 'machine-learning-model']","I am trying to predict the sentiment of a sentence by using learn.predict(message). But I am getting the following error -File “/usr/local/lib/python3.7/site-packages/fastai/basic_train.py” in predict
382. y = ds.y.reconstruct(pred, x) if has_arg(ds.y.reconstruct, ‘x’) else ds.y.reconstruct(pred)File “/usr/local/lib/python3.7/site-packages/fastai/text/data.py” in reconstruct
341. return Text(t[idx_min:idx_max+1], self.vocab.textify(t[idx_min:idx_max+1]))File “/usr/local/lib/python3.7/site-packages/fastai/text/transform.py” in textify
134. return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]File “/usr/local/lib/python3.7/site-packages/fastai/text/transform.py” in
134. return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]Exception Type: TypeError at /test_sent/
Exception Value: only integer tensors of a single element can be converted to an indexCan someone please help me with this?"
2841,Tensorflow - Failed to convert a NumPy array to a Tensor (Unsupported object type float),"['python-3.x', 'pandas', 'tensorflow', 'sentiment-analysis']","I'm trying to convert a dataframe into a tf.data format. I'm using the following code:and my input dataframe is :But whe I'm trying to run it, I'm facing this error:I've tried googling the error myself, but its gone in vain. Any help would be truly appreciated. Thanks !"
2842,Extracting tweets using Python and Tweepy,"['python', 'twitter', 'tweepy', 'sentiment-analysis']","I am trying to look at President Trump's tweets on immigration and do some sentiment analysis on it. My code is:The code works perfectly fine. However, I have 2 questions:I have been trying to figure out a way but unsuccessful. "
2843,Trouble Loading qdap Package In R,"['r', 'data-science', 'sentiment-analysis', 'qdap']",I am having trouble loading the qdap package for sentiment analysis  in R every time I try and library the package in I get this error Please help me fix this problem your help will be greatly appreciated 
2844,Data Collection for NLP Project,"['pandas', 'nlp', 'python-requests', 'sentiment-analysis', 'data-collection']","I'm trying to do an NLP project on Sentiment analysis for movie reviews of 'Lion King' and unable to gather the reviews. Here's my code:Creating headers for our request:Initial payload parametersCreating a Session Object which is persistent to load multiple-page reviews:
Note that it is initialization operation, hence will be done only once irrespective of how many pages you want to read from the serverThe code to fetch one-page reviews (each page has 10 reviews) by using GET call on the required URL with our header and payload parametersThe response type is JSON so we can get the one page of reviews data in JSON format by calling its JSON functionI need to collect all the requested attributes from the ‘data’ object and populate a DataFrame with their values for each page. the fully populated DataFrame will be your train data.To get the next page reviews, update the payload parameters startCursor & endCursor with their values from the data object, and make a new GET call. Repeat the process until you collect 3000 reviews.And, I have tried to collect the reviews, but I had to do it for each and every page rather than create a  loop. And my question is how to collect 30 pages of 3000 reviews from the website."
2845,SparkStreaming and SentimentAnalysis,"['twitter', 'pyspark', 'spark-streaming', 'sentiment-analysis']","We are receiving a stream of tweets from ""localhost:10002"". We want to take each line of that stream, and analyze it. But first we want to print the incoming tweets, and we are having some problems.We have our input stream:And I have dug into the documentation, so stream is a DStream, which is an abstraction over multiple rdds. Now, knowing that i have tried:and I get this error:java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supportedAny ideas?In the end, I just want to print the result of the positive and negative bias of each tweet using TextBlob or Vader, so it would be something like:"
2846,Error with tune_grid function from R package tidymodels,"['r', 'data-science', 'sentiment-analysis', 'hyperparameters', 'tidymodels']","I've been reproducing Julia Silge's code from his Youtube video of Sentiment Analysis with tidymodels for Animal Crossing user reviews (https://www.youtube.com/watch?v=whE85O1XCkg&t=1300s). In minute 25, she uses tune_grid(), and when I try to use it in my script, I have this warning/error: Warning message:
All models failed in tune_grid(). See the .notes column. In .notes, appears 25 times: How can I dix this? I'm using the same code that Julia uses. My entire code is this:"
2847,google search API connect to sentiment analysis in R,"['r', 'api', 'google-api', 'rstudio', 'sentiment-analysis']","i`m trying to set up a text mining project in R where i want to search the web for text documents (news, blogs, social media) regarding organizations in combination with buzzwords (e.g. coca cola + (healthy OR sugar)  and afterwards analyze these documents regarding sentiment. Which one is the right API and how do i link to Natural language Processing API? Thank you"
2848,sentimentr negative sentiment for positive comment,"['sentiment-analysis', 'sentimentr']","Sentimentr package. I may be doing something wrong but cant quite figure it out. I have two examples of text and getting sentiment using sentimentr, example code below. In both examples of NI and NI2 I am getting negative sentiment for ""no improvements"" and ""too good"". I am wondering if its a simple matter of me getting something totally wrong as I am using simple code. "
2849,Multiple query as a search word for q in tweepy.Cursor,"['python-3.x', 'tweepy', 'sentiment-analysis']","Is it possible to search multiple tags/words using iterator as a argument to q intweepy.Cursor(q=""one string at a time OR multiple"")"
2850,"Finding the optimal unique number of words to estimate a predictive model with target x, using cross-validation","['r', 'text-mining', 'cross-validation', 'sentiment-analysis', 'predictive']","I was wondering if anyone could tell me how to find the optimal number of unique words with text mining, to use for predictive models. This is done by conducting a sentiment analysis (which is completely fine for a pre determined number of words). However, I have to find a way that enables me to test the accuracy with n number of words, eventually choosing the number that yield the highest result. Is there a metric that one could use to do so? The assignment mentioned something about cross validation, however, I am pretty sure that that was referring to the predictive models. Could someone help me out with this problem?"
2851,Is there a way to do the opposite of unnest_tokens? I want to combine words into a row based on a unique ID,"['r', 'sentiment-analysis']","I am currently trying to do some sentiment analysis and I want to revert each word back into its original format. So I want each word belonging to a unique ID to be combined in a single row. So I want the opposite of unnest_tokens function. I have tried the following: However, I simply get all the words combined into 1 row, instead of a row for each unique ID. Can anyone help me out here? Below is a screenshot of what my data frame looks like and a subset of my data. "
2852,Generating confusion matrix for keras model - Sentiment analysis,"['python', 'keras', 'sentiment-analysis', 'confusion-matrix']","I am testing a Sentiment Analysis model using LSTM. I need to add a Confusion Matrix to the classifier results and if possible also Precision, Recall and F-Measure values. I have only accuracy so far. Movie_reviews data has pos and neg labels.Using the above code for generating the confusion matrix, I am getting the following error:How do we exactly get the confusion matrix?"
2853,How to translate a pandas column,"['python', 'json', 'pandas', 'sentiment-analysis', 'translate']","I'm working on arabic sentiment analysis project. In my unsupervised learning approach I'm trying to translate the lexicon to arabic using googletrans library.
the lexicon['word'] column contains all the English words.
I tried the following methods:they all keep giving me the same error: **JSONDecodeError: Expecting value: line 1 column 1 (char 0)**I don't understand why I'm getting this error, although I copied it exactly as I found it on stackoverflow.
My lexicon contains almost 7000 words, and I read that it can take up to 15k words at the same time to translate. can someone help? Thanks in advance!"
2854,Flask raises this error ' TypeError: The view function did not return a valid response.' [duplicate],"['flask', 'sentiment-analysis']","I am getting the below ERROR message :- 
 'The view function did not return a valid response. The'
TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.my code is :-
'''
     app = Flask(name)if name=='main':
    app.run(host=""127.0.0.1"", port=5000)
'''"
2855,Twitter-Sentiment-Analysis in python 3 using tweepy and Textblob,"['python-3.x', 'tweepy', 'sentiment-analysis', 'textblob']",When I execute this code it terminates without any error but there is no any output. I have attached the right twitter authentication credentials.what would be the issue
2856,Require a dataset for multi class movie review analysis,"['dataset', 'sentiment-analysis', 'kaggle', 'multiclass-classification', 'imdb']","We have a project and want to implement a sentiment analysis for movie reviews. We are using Rotten Tomatoes dataset (https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews) which is a quite large dataset with 5 classifications. For implementation, we are using Random Forest. The other good dataset is IMDB with 50k entries but it is for binary classification. I want to if there is any other dataset that is large and suitable for multiple classifications.Thanks for your replies in advance.  "
2857,Aspect Based Sentiment Assignment,"['python', 'nlp', 'spacy', 'sentiment-analysis']","I was working on Aspect based sentiment analysis using Spacy dependent parser . In simple sentences such as "" Car is good"" it works properly , but when a slightly complex sentences arrives such as "" Car is good in terms of mileage "" it fails to assign positive sentiment for mileage and assigns it to car instead. Iam not able to relate mileage with good in following spacy parser. How to establish such a relation , If somebody could give a high level idea. 
Good and mileage are not at all related here"
2858,save gensim doc2vec trained model on google colab,"['nlp', 'google-colaboratory', 'gensim', 'sentiment-analysis', 'doc2vec']","I am training text data using gensim doc2vec model on google colab repository GPU runtime, and want to save trained model in test.d2v file. following is code snippetFollowing error is generated in colab notebook.** /usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL"
2859,Save Output Penntree in txt file?,"['stanford-nlp', 'sentiment-analysis', 'penn-treebank']","input (command line) :output:I would like to know if it worked well ?
Knowing that when I reduce the number of lines it works.
How to save it as a txt file ?Thanks in advance !"
2860,Printing Tamil Tweets in Python,"['python', 'twitter', 'sentiment-analysis', 'tamil']","I am doing a Sentiment Analysis. I am using 'Twint' for extracting the Tweets from the Twitter.
The Extracted Tweets has both English & Tamil Tweets. The Tamil Characters are Printed as some unknown characters. Eg. à®•à¯à®ªà¯€à®°à¯à®©à¯ à®šà®¿à®°à®¿à®šà¯à®šà®¿à®Ÿà¯à®Ÿà¯ à®‡à®°à¯à®•à¯à®•à¯‡à®©à¯..!! ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜
What Encoding are these? How can i print the Tamil Tweets as it is? "
2861,Arguments should have same length error in R,"['r', 'data-science', 'lapply', 'sentiment-analysis', 'tapply']",I'm trying to create a key-value store with the key being entities and the value being the average sentiment score of the entity in news articles.I have a dataframe containing news articles and a list of entities called organizations1 identified in those news articles by a classifier. The first row of the organization1 list contains the entities identified in the article on the first row of the news_us dataframe. I'm trying to iterate through the organization list and creating a key-value store with the key being the entity name in the organization1 list and the value being the sentiment score of the news description in which the entity was mentioned. I can get the sentiment scores for the entity from an article but I wanted to add them together and average the sentiment score. 
2862,How does AutoML sentiment analysis train the model?,"['sentiment-analysis', 'google-cloud-automl', 'automl']","I have been using AutoML for sentiment Analysis. I provide my csv file which contains my texts with their scores and AutoML starts training the model. everything is fine but does anyone know what is the method that it uses for pre-processing, like, does it use bag of words, tf-idf or word2vec to process the data? I searched a lot but didn't find anything! does anyone have any ideas?Thanks in advance!"
2863,"How to iterate an action across all files in a directory, and then store the results in a different file using Python/Pandas?","['python', 'pandas', 'loops', 'sentiment-analysis']","I have a total of 100 .db files stored on my Google Drive. All rows in these files contain a text column, for which I need to calculate the Sentiment Polarity Scores for each row, using vaderSentiment package. I have managed to do so for one of these .db files, but I want to repeat this calculation for all 99 other files in the directory. Additionally, I want to store the mean of this '[sentiment_ compound_polarity']' column of every file in my Google Drive directory in a .csv file. This file should have just two columns with 100 rows (1 for each file), containing the file name and df['sentiment_compound_polarity'].mean() for every single .db file stored in '/content/drive/My Drive/Database Files/'.I've managed to get some code to work, which prints the average sentiment scores for each file but I don't know how to store it in a .csv (of course I can just copy/paste it, but I want to know how to do it).Any help is much appreciated!"
2864,Feature Selection using Rough Set Theory,"['python', 'sentiment-analysis', 'feature-selection']","I have a python coding for feature selection using rough set theory. This is a quick reduct algorithm. However, I can't produce the output since it runs very slow for high dimensional dataset. It can takes days to run the coding but still I cannot produce the output. Can anyone give any suggestions to solve this problem ?"
2865,how to convert dataframe to list and then to corpus?what is differnce between Vcorpus and Corpus in R?,"['r', 'twitter', 'nlp', 'text-mining', 'sentiment-analysis']","i am new to r learning sentiment analysis using twitter tweets using R.
when i extract tweets in r they are in list class type but, then i need to convert them to dataframe to consider only one column with text type only because other types are irrelevant for my study like screenname, created, replyToSID, and others, and hence, i created dataframe and kept only one column with text. but, when i convert the dataframe in list to further convert it into corpus for data cleaning using tm. i am getting output i expect in single list to do my study. below are my code lines..output is not satisfactory.. when i use view function i can see 1 with 2 list i.e. content and metadata. 
I just want only text in the output in a list form..if any further information needed please, free to ask me..or explain me how i simply convert the tweets into dataframe then to list followed by creating corpus and cleaning data using tm. Much appreciated in advance.attached, the screenshotsoutput of df tweets
code i ran"
2866,how to use Vader in NLP project for non-english dataset?,"['python', 'sentiment-analysis', 'non-english', 'vader']","i have a dataset that is non-english and i want to use vader for sentiment analysis so i created a function that calculate the polarity and subjectivity of the sentence.But when i tried to apply this function on the dataset the system crash and display the below error :I know that vader is used for english but in the description they said that it can be used for non-english dataset also with request limitation for 1000 words, so i tried a test with 995 words and it still display the same error.if anyone had use this tool for non-english dataset i need to know if it works for him and where i am making the mistake in my project."
2867,can i use VaderSentiment to calculate polarity and subjectivity on language other than English?,"['python', 'nlp', 'sentiment-analysis']","i am trying to create a nlp project that calculate the polarity and subjectivity for texts that are not English so i can use 2 tools: Vader - Textblob.After i done a lot of researches i found that Vader is more efficient and accurate for social media.My question is : can i add language to vader in order to calculate socres?
 or is their a package for vader like multilanguage?For the project i read from csv file and import it to dataframe pandas than pre-process and clean the text  than analyse it to extract the sentiments. i will appreciate any help."
2868,Looping through posts in a Subreddit with PRAW (+ analyze sentiment),"['python', 'python-3.x', 'sentiment-analysis', 'reddit', 'praw']","I got the following Python code, basically I scrape information from Reddit and then analyze the sentiments for each post:Output:I wonder if I can do the same but looping trough post in a subreddit With the following features: Is there any way to structure the code so that the second time it's run, it'll loop through the next 100 newest posts (i.e. posts 101 through to 200)?I found in Reddit this PRAW functionality:""The PRAW library has functions for streaming posts or comments. 
Reddit.Subreddit.stream.comments() and Reddit.Subreddit.stream.submissions() will yield comments or submissions as they become available.""Here's example usage:StreamSubmission added:But is not working it gets stuck...no errors in the console either...Thanks for any replies! Also I'm brand new to Python and pretty new to working with APIs in general, so if anything I said didn't make sense or reflects a lack of understanding then please let me know!"
2869,Calculating average of polarity after performing sentiment analysis,"['python', 'average', 'sentiment-analysis', 'textblob']","I want to calculate the average of all the outputs i receive for polarity. So basically, the average of all values of the variable blob.sentiment.polarity My current code is shown below:I already tried adding this code to get the average average = sum(blob.sentiment.polarity) / len(blob.sentiment.polarity)But im getting this error when i try to print averageTypeError: 'float' object is not iterablePlease tell me what im doing wrong and how to correct it. "
2870,"R Stringr, how to replace part of a string, which varies in length?","['token', 'sentiment-analysis', 'stringr']","I am trying to do a sentiment analysis of some tweets, and I need to use the lexicon to ""translate"" the emoji code into an emotion.The data looks like this:The lexicon looks like this:Would it be possible to use the lexicon to replace the emoji code with the emotion?"
2871,"R: generateDictionary, Error: x should be a matrix with 2 or more column","['r', 'dictionary', 'sentiment-analysis']","I am trying to generate a dictionary with the ""SentimentAnalysis"" package. When i use the function generateDictionary i get the following error: "" Error in glmnet(x, y, weights = weights, offset = offset, lambda = lambda,  : 
  x should be a matrix with 2 or more columns) ""My code looks as such: With the ""News"" column being a string and the ""Score"" column being an integer between 1 and 3. I have tried to convert the news column into every format i could think of, but nothing has worked. I hope one of you can help me.Thanks in advance! "
2872,show_most_informative_features() errors with exception in python 3.8,"['python-3.x', 'sorting', 'typeerror', 'sentiment-analysis', 'naivebayes']","Issues:Code: accuracy prints, but show_most_informative_features error out with the above exception indicating an error with naivebayes.py. Any idea on the fix?
this is the line at which exception occurs: Since, it's not a good idea to change code in standard python library, am I missing out something while invoking the function?"
2873,Semeval twitter data download is not working,"['python', 'web-scraping', 'twitter', 'terminal', 'sentiment-analysis']",I want to download the training set of the following dataset: http://alt.qcri.org/semeval2017/task4/index.php?id=data-and-toolsI am required to use a script to download the tweets from their respective ids and the script is on this github: https://github.com/seirasto/twitter_downloadWhen I run the following command in powershell:I getWhen trying to run this command:I get the following error:What am I doing wrong?
2874,Feature extraction using flairNLP,"['python', 'nlp', 'sentiment-analysis', 'feature-extraction', 'flair']","I'm trying to use flair for sentiment analysis, but I also need to know how much each word influenced the score of a sentence.I've followed this article to predict the sentiment, but it doesn't show how to extract the features of the given sentence.
I'm assuming there is a way to do this feature extraction because of the way it's presented in that article, but I can't find it. I've tried reading the flair documentation and the code itself but didn't see a way to do so.What I'm looking for is a functionality of this sort:Result:I'm not trying to train my own model, but rather use a pre trained one like in the code example above.Note: I'm not bound to flair, if a different framework with this functionality exists I'll be happy to know about it as well. I'm trying to use flair because it out performed Textblob and nltk's VADER in accuracy when I tested it."
2875,Aspect Based Sentiment Analysis in R,"['r', 'sentiment-analysis']","I have a dataset containing reviews about a product. I am interested to find the sentiment and reasons why people think a product is expensive using the reviews. For example there might be a review like this 'Product ABC is quite easy to use but support cost is something to be aware of'Here the reviewer says that the product is easy to use but has a high support cost. Similarly there are other reviews mentioning about other themes like setup cost, license fee etc and I want to map each of the reviews to which aspect of pricing they are talking about. I did some research and found a few papers on aspect based sentiment analysis which seems to be the way forward for this but I would prefer some guidance from the experts here ;) Are there any packages or APIs which I can use in R to make my task a bit easier? Thanks in advance!"
2876,what is the accurate Twitter sentiment analysis solution with Python?,"['python', 'machine-learning', 'deep-learning', 'nlp', 'sentiment-analysis']","I have a CSV file of 20K tweets with all information such as location, username, and date which I want to assign a label positive/neutral/negative to each tweet by Python. 
I used the following Python code from textblob library for Tweets Sentiment Analysis. this code runs perfect and produces the sentimentTweets.csv file. I like the idea that for each tweet, it gives me two labels: a number between -1 and 1, and also classify tweet to negative/neutral/positive.but it is not accurate. for example for the following tweet, it assigns positive with the number:0.285714285714285.
""RT @eliyudin: ‚ÄúI‚Äôll have a Corona... hold the virus!‚Äù -a dad on vacation somewhere in Florida right now""
but as you can understand, the sentiment of the above tweet should be negative.
How can I make it accurate? and how can I find the accuracy of my output?"
2877,How to extract country-specific data using rtweet->search_fullarchive->place_country: commands?,"['r', 'twitter', 'sentiment-analysis', 'rtweet']","I need to extract Twitter data, specific to a country using the place_country: keyword in the search_fullarchive command.After execution, I get the error: 
""object 'place_country' not found."""
2878,Evaluating word-lists for sentiment analysis,"['random-forest', 'sentiment-analysis', 'shap']","I'm thinking about ways to evaluate sentiment-dimensions of word lists like LIWC and ANEW (e.g. negemo / posemo in the case of LIWC) for sentiment analysis. Those word lists assign humanly rated values to words and allow rating the valence of a text on basis of word occurrences. Now, if I have a long text corpus, in which every single sentence has been rated for valence, could I use random forest to predict the ratings of the sentences using the word-list valence values given to the sentences as input variables and then finally use permutation based feature importance like SHAP to get the most important feature, since those methods are supposed to be exact according to Lundegren et al. (2017: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)? Gomez-Ramirez et al. (2019: https://www.biorxiv.org/content/10.1101/785519v1.full.pdf) use a similar technique for seeking out important features in predicting cognitive decline and now I'm wondering if the analogon can be done in the case of word-lists. Thanks for your thoughts and excuse me if I'm making wrong inferences since my knowledge on ML is still rudimentary.Edit: also, if this may be the wrong place for this question, please redirect me."
2879,Scale between -1 and 1,"['python', 'pandas', 'dataframe', 'sentiment-analysis']","I have a data frame with positive,negative and neutral sentiment analysis percentages of a text and I am trying to scale this data into a number that is between -1(most negative) and 1(most positive). What would be the best formula to determine this score?
Dataframe example: 
Data columns (total 11 columns): New field called score needs to be added with appropriate formula .
Example score:
Downloading comments of Video Number : 49 
Positive sentiment :  39.37210499227998 
Negative sentiment :  18.57951621204323 
Neutral sentiment :  42.04837879567679 "
2880,Is there a faster way to preprocess huge amount of text data in Python?,"['python', 'gpgpu', 'sentiment-analysis', 'preprocessor', 'gpu-programming']","I'm building sentiment analyse algorithm to predict the score of IMDb reviews. I wanted to do it from scratch, so I scraped half a million reviews and created my own data set. I'm sending small review packages (consist of 50 reviews) to review_cleaner with pool. It helped me to reduce run time from 40 minutes to 11 minutes for 1000 reviews. But, I have half a million reviews, I need faster way to process them. I was thinking if it's possible to run it on my GPU (GTX1060 6GB)? I installed CUDA, but I couldn't find how to run specific function(review_cleaner) on GPU cores. Basically, what I need is, solution to run preprocess faster. I searched and tried many different things but couldn't do it. Is there any way to run it faster?I'm storing around half a million (400k) reviews in SQLite database. One column for review and one column for score of the review. In another table, I'm inserting the preprocessed reviews same way, one column for review and one column for score. I have 16 gigs of RAM, Intel i7 6700HQ, SSD and GTX1060 6GB."
2881,How to do Topic Detection in Unsupervised Aspect Based Sentiment Analysis,"['python', 'nlp', 'word2vec', 'sentiment-analysis']","I want to make an ABSA using Python where the sentiment of pre-defined aspects (e.g. delivery, quality, service) is analyzed from online reviews. I want to do it unsupervised because this will save me from manually labeling reviews and I can analyze a lot more review data (looking at around 100k reviews). Therefore, my datasets consists of only reviews and no ratings. I would like to have a model that can first detect the aspect category and then assign the sentiment polarity. E.g. when the review says ""The shipment went smoothly, but the product is broken"" I want the model to assign the word ""shipment"" to the aspect category ""delivery"" and ""smoothly"" relates to a positive sentiment. I have searched for approaches to take and I would like to know if anyone has experience with this and could guide me into a direction that could help me. It will be highly appreciated!"
2882,Constant Training Loss and Validation Loss,"['machine-learning', 'pytorch', 'recurrent-neural-network', 'sentiment-analysis']","I am running a RNN model with Pytorch library to do sentiment analysis on movie review, but somehow the training loss and validation loss remained constant throughout the training. I have looked up different online sources but still stuck. Can someone please help and take a look at my code?Some parameters are specified by the assignment:My main codeMy ModelThe training functionThe output I run on 10 testing reviews + 5 validation reviewsAppreciate if someone can point me to the right direction, I believe is something with the training code, since for most parts I follow this article:
https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
2883,Python: IF statement consisting of data frame and list,"['python', 'for-loop', 'if-statement', 'jupyter-notebook', 'sentiment-analysis']","I am very new to python and require help.
I have a list of keywords which was obtained from a data frame as follows:
key_a_list = df_key_words['words'].tolist()I have a second data frame which consists of statements: df_response['statement']
I have already corrected spelling errors, tokenised and stemmed the text in the df_response['statement'] column. 
I need to check if there are any words in the key_a_list that match words in the df_response['statement']; then I must set a counter to count the number of times a word from the key_a_list is present in the df_response['statement']. Thank you for your time and help, it is greatly appreciated :) This is the current code that I have but it gives me an error: ValueError: Lengths must match to comparethe key_a_list consists of words like: ['think', 'college', 'education', 'help', 'better', 'prepare', 'career', 'chosen', 'eventually', 'enable', 'enter', 'job', 'market', 'field', 'like', 'make', 'choice', 'social', 'orientation', 'believe', 'additional', 'year', 'improve', 'competence', 'worker', 'prove', 'capable', 'completing', 'degree', 'rich', 'succeed', 'feel', 'important', 'show', 'intelligent', 'person', 'order', 'salary', 'later', 'on', 'want', 'the', 'good', 'life', 'study', 'highschool', 'actuary', 'find', 'highpaying', 'obtain', 'prestigious']The df_response['statement'] looks as follows:where the desired output of df_response is:"
2884,What are the list of all the algorithms for Natural Language processing (NLP)?,"['machine-learning', 'deep-learning', 'nlp', 'sentiment-analysis']","I know it is a little bit wide topic but all I am looking for is if someone can help me with the list of all the NLP algorithms and when to use them, or maybe a resource which I can refer. for example - **RNN **might be a good use case for a question and answer NLP use case and a simple dense network might just work quite good for binary segregation of documents or identify sarcastic comments from useful news.I was hoping we can add to the list below from whatever anyone has an idea about will be great. Of course, the below list is not a hard and fast rule and more often than not on various use case we might have to try different things approaches but this is an effort just to have an exhaustive list for NLP algorithms.Dense layer - useful for document segregation or (sarcastic comments from useful news)
RNN(LSTM) - Good for Question and Answer API"
2885,Is it possible to do sentiment analysis of unlabelled text using word2vec model?,"['python-3.7', 'gensim', 'word2vec', 'sentiment-analysis']","I have some text data for which I need to do sentiment classification. I don't have positive or negative labels on this data (unlabelled). I want to use the Gensim word2vec model for sentiment classification.
Is it possible to do this? Because till now I couldn't find anything which does that?
Every blog and article are using some kind of labelled dataset (such as imdb dataset)to train and test the word2vec model. No one going further and predicting their own unlabelled data.Can someone tell me the possibility of this (at least theoretically)?Thanks in Advance!"
2886,Google Cloud Analyze Sentiment in JupyterLab with Python,"['python', 'google-cloud-platform', 'sentiment-analysis', 'jupyter-lab']","I am using Google Cloud / JupyterLab /PythonI'm trying to run a sample sentiment analysis, following the guide here However, on running the example, I get this error:AttributeError: 'SpeechClient' object has no attribute
  'analyze_sentiment' Below is the code I'm trying:I had no problem generating the transcript using Speech to Text but no success getting a document sentiment analysis!?"
2887,How can I perform sentiment analysis by accessing a dataframe instead of a csv in python?,"['python', 'pandas', 'dataframe', 'sentiment-analysis', 'vader']",Im trying to understand how to apply the csv logic to a dataframe output that already exists in my script.I have a dataframe which has 4 columns one of which has a transcript[353 rows x 5 columns]How do I apply my above script of sentiment analysis to this particular columns(transcript) in the test3 dataframe?
2888,How to predict unlabelled data's sentiment using Gensim word2vec model?,"['python-3.7', 'gensim', 'word2vec', 'sentiment-analysis', 'valueerror']","I trained and test the 'IMDb movie reviews dataset' using the Gensim word2vec model and I want to predict the sentiments of my own unlabelled data. I tried but got an error. I am reusing an open-source code. Below is the full code:When I run this code I got the below error:ValueError                                Traceback (most recent call
  last)  in 
  ----> 1 pred_y2 = w2v_dnn.predict_classes(new_data['Articles'])
        2 print(pred_y2)
        3 pd.DataFrame(pred_y2, columns=['Sentiments']).to_csv('abcd_sentiments.csv')~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/sequential.py
  in predict_classes(self, x, batch_size, verbose)
      266             A numpy array of class predictions.
      267         """"""
  --> 268         proba = self.predict(x, batch_size=batch_size, verbose=verbose)
      269         if proba.shape[-1] > 1:
      270             return proba.argmax(axis=-1)~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in predict(self, x, batch_size, verbose, steps, callbacks,
  max_queue_size, workers, use_multiprocessing)    1439     1440
   # Case 2: Symbolic tensors or Numpy array-like.
  -> 1441         x, _, _ = self._standardize_user_data(x)    1442         if self.stateful:    1443             if x[0].shape[0] > batch_size
  and x[0].shape[0] % batch_size != 0:~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in _standardize_user_data(self, x, y, sample_weight, class_weight,
  check_array_lengths, batch_size)
      577             feed_input_shapes,
      578             check_batch_axis=False,  # Don't enforce the batch size.
  --> 579             exception_prefix='input')
      580 
      581         if y is not None:~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training_utils.py
  in standardize_input_data(data, names, shapes, check_batch_axis,
  exception_prefix)
      143                             ': expected ' + names[i] + ' to have shape ' +
      144                             str(shape) + ' but got array with shape ' +
  --> 145                             str(data_shape))
      146     return data
      147 ValueError: Error when checking input: expected dense_1_input to have
  shape (500,) but got array with shape (1,)Can somebody suggest me how to solve this error and predict sentiments of my unlabeled data?
I am using python 3.7 and jupyter notebook from Pycharm IDE.Thanks in Advance."
2889,Confusion matrix and test accuracy for PyTorch CNN tutorial,"['python', 'sentiment-analysis', 'confusion-matrix', 'cnn']",I am interested in reporting only train and test accuracy as well as confusion matrix (say using sklearn confusionmatrix). How can I do that? The current tutorial only reports train/val accuracy and I am having hard time figuring how to incorporate the sklearn confusionmatrix code there. Link to original tutorial here: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb
2890,Get-sentiment Powershell,"['powershell', 'sentiment-analysis']",How would I go about getting the score decimal returns into just returning the scores as a Percentage?Current powershell lineReturnsAny Help is appreciated.
2891,Receiving Input Error When Loading R code in Google Colab,"['r', 'input', 'text', 'google-colaboratory', 'sentiment-analysis']","Hi everyone. I'm learning text_mining. When I learn get_sentiments(""afinn""), The console confirms that is not valid to get input_text. I make sure that I'm running these code in R kernel from Google Colab and also Rstudio.cloud accepted my code to get input_text.Could you help me to solve these problems."
2892,Adding and averaging a set of columns depending on the value of a secondary column in python,"['python', 'pandas', 'numpy', 'dataframe', 'sentiment-analysis']","I have a dataset which has the following values:Final_label would be 1 if majority of Labels (LabelA, LabelB and LabelC) would be 1 and vice-versa.I want to calculate a column called ""Polarity"" which has the following defination:For example in the above dataset, Polarity would have the following value:How do I implement this in python? Over here I have shown 3 columns, in my dataset I have 7 Label columns. "
2893,Adding column of values to pandas DataFrame,"['python', 'pandas', 'nlp', 'sentiment-analysis']","I'm doing a simple sentiment analysis and am stuck on something that I feel is very simple. I'm trying to add an new column with a set of values, in this example compound values. But after the for loop iterates it adds the same value for all the rows rather than a value for each iteration. The compound values are the last column in the DataFrame. There should be a quick fix. thanks!"
2894,how to plot a histogram using pyplot package in python on jupyter notebook,"['python', 'matplotlib', 'jupyter-notebook', 'sentiment-analysis']",I am trying to plot a histogram graph on jupyter Notebook  where the data are extract from a dataframe  for visualize the positive and negative sentiment of each record from the dataset used.The problem is that once i try to plot the result display the below error and display an empty visualization. 
2895,what is the error in this code using dataframe and matplotlib,"['python', 'pandas', 'matplotlib', 'sentiment-analysis']","I have a python that read from CSV file and converts it to dataframe using pandas then using matplotlib  it plots a histogram. the first task is correct it read and write to and from CSV file.the csv file fileds are:
date"",""user_loc"",""message"",""full_name"",""country"",""country_code"",""predictions"",""word count""BUT the task of plotting is displaying the below error.Error:Code:"
2896,Why isn't my matplotlib histogram plotting in PyCharm from Tweepy acquired data?,"['python', 'matplotlib', 'histogram', 'tweepy', 'sentiment-analysis']","I have code for a Twitter streamer that performs sentiment analysis on tweets that go into a CSV file. The sentiment analysis results are then plotted on a pie chart, and what I'd like to do is also plot these on a histogram too. I've looked at a couple of posts here (https://stackoverflow.com/questions/18736978/plotting-histogram-with-matplotlib)(https://stackoverflow.com/questions/47538718/histogram-plotting-matplotlib) and a YouTube vid on how to do it and can't understand why it's not working. I'll post the whole code so you can see everything it's doing but the code for the charts is very near the bottom. The error I receive is, ""AttributeError: 'SentimentAnalysis' object has no attribute 'plotHist'"" but it's laid out exactly the same as the pie chart and this plots fine. Can someone point out what I'm doing wrong please?"
2897,Creating vector using Sentence,"['python', 'machine-learning', 'project', 'sentiment-analysis']","I want to do sentiment analysis and created SVM Model in python, which uses points(coordinates) to plot a vector, but only works with numeric values.
I want to map a sentence to a vector.
Example: ""The food is very delicious"" converted to (a,b) co-ordinate which signifies whether a sentence is +ve or -ve.
Is there a way to convert the sentence to a vector."
2898,Is it possible to use parallel processing with the getVaderRuleBasedSentiment (VADER) sentiment analysis tool on multiple data sets at a time?,"['r', 'parallel-processing', 'sentiment-analysis', 'vader']","I am pretty new to R code and therefore struggling with the task of understanding parallel processing. The code below is an example of what I am trying to do whereby I input a PDF, split the pages for analysis and then feed it into the sentiment analysis tool. However, I have found that I have to do it one by one and considering the final data set could be comprised of thousands of individual PDF files, I was hoping for some tips on how to streamline the process as well as use more than one computer core (I have access to a computer with 16 cores). I understand this question is basic, however, I could not find an adequate answer that was related to R, mostly all the answers are for Python. 
Many thanks in advance!  "
2899,Arabic Sentiment Analysis With Google,"['python', 'sentiment-analysis', 'google-cloud-nl']","I just visited the language support page of google
https://cloud.google.com/natural-language/docs/languages
I found the Arabic (ar) is supported in sentiment analysis.When I try the tutorial they mentioned but with an Arabic sentence, it gives me an error that ar is not a supported language.Could anyone confirm the support of Arabic or not?"
2900,how to preprocess the csv file of tweets?,"['python', 'machine-learning', 'twitter', 'nlp', 'sentiment-analysis']","I have a csv file of 10K tweets that contain all information such as id, location, tweet text,...
For sentiment analysis, first I should preprocess these tweets which I use the following code and My machine is IMAC with Python 3.6:but I get the following error:how can I fix this code to correctly preprocess my csv file?"
2901,Tag of Google news title for beautiful soup,"['python', 'beautifulsoup', 'sentiment-analysis']","I am trying to extract the result of a search from Google news (vaccine for example) and provide some sentiment analysis based on the headline collected.So far, I can't seem to find the correct tag to collect the headlines.Here is my code:The result are always 0 for the sentiment and 0 for the subjectivity. I feel like the issue is with the class_=""phYMDf nDgy9d""."
2902,Calculate sentiment of a text review using custom trained dictionry that containing n-gram,"['python', 'nlp', 'sentiment-analysis']","Goal: I want to calculate sentiment for highly specialized reviews using custom trained sentiment dictionary.Steps: 
 1. Train the sentiment dictionary (Done)
 2. Use the dictionary to calculate sentiment of a review (Stuck)Background: I have a df with 10M+ rows that contains all the reviews, one per row. An average review is around 8 sentences, some could be as short as just one or two sentences. It also has a binary 'recommend' column which I used to train the dictionary. Before I predict pass or fail, I want to see a number to understand the place of the review in the result distribution. (is it top 10 percentile? is it bottom 50 percentile? etc.)Problem: the dictionary contains a lot of bi-gram and a few tri-gram. How do I get the code to calculate bi-gram as a bi-gram instead of two unigrams? Also, after the bi-gram is calculated, I don't want to re-calculate the two unigrams the bi-gram used. My knowledge of algorithm and Python can only handle an 'unigram exclusive' situation.My thoughts: Since I use logistic regression to fit the model, one way to go around the issue is to use lr.predict_proba. However, I'd love to know if there is a way to directly solve the problem. I am actively learning, any advice that could potentially help with similar problems are much appreciated! (even if it doesn't fully solve this one particular problem)Thank you for your time!"
2903,How to get tweets to upload into an SQLite database using Python/Tweepy?,"['python', 'sqlite', 'twitter', 'tweepy', 'sentiment-analysis']","Hi so I have some code that streams tweets from Twitter's API and performs sentiment analysis on them as a CSV file. What I want to do is for the tweets to upload into an SQLite database prior to performing the sentiment analysis. I've had a pretty good go at getting it work and just can't figure it out. I've used the documentation from https://docs.python.org/3/library/sqlite3.html and looked at numerous Stack posts ( tweepy stream to sqlite database - invalid synatx , Python tweepy writing to sqlite3 db , tweepy stream to sqlite database - invalid synatx , Tweepy to sqlite3 ) but I still can't get it doing what I need it to do. I've also looked at tutorials here (https://tech.marksblogg.com/sqlite3-tutorial-and-guide.html) (https://recycledrobot.co.uk/words/?sqlite) and still can't get my INSERT statement right. The CREATE TABLE works and creates the SQLite file but I can't get my tweets to store into. I also only run the CREATE TABLE statement once then comment it out. Whole code is below. Any help much appreciated!"
2904,Part-of-Speech tagging problem for pre-process in sentiment analysis,"['python', 'csv', 'nlp', 'sentiment-analysis', 'part-of-speech']","I need some help finishing the code for the part-of-speech in my pre-process for a sentiment analysis.
I managed to tokenize, remove the stop words, but in order to perform lemmatization i also need to do a part-of-speech. I created some code and I get no error, but still the words are not lemmatized. 
Can someone tell me where I made the mistake? I believe it must be some lines of the part-of-speech.I appreciate the help guys/girls"
2905,How to categorize tweets(supportive vs. unsupportive) to predict elections results,"['python', 'twitter', 'nlp', 'prediction', 'sentiment-analysis']","The ideaI am collecting tweets talking about the three major candidates for the US presidency in November. After collecting the tweets from different states, I will score these tweets, and then analyze each candidate's followers/supporters on various aspects.The problemI am sure what method I should use to classify the tweets in order to produce reasonable outcomes. More precisely, I don't know how to tell if a tweet is supporting or opposing a specific candidate. What I triedI tried to use a library called textblob. Given a tweet, it returns a tuple of the form Sentiment(polarity, subjectivity). Polarity is a float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. This method does not return reasonable results at all when applied. For example, given a tweet like ""Donald Trump is horrible! I still support him tho."", it returns a polarity of -1.0 (negative), which does not make any sense at all. Further ResearchI looked for more examples and found this. In that example, the author uses a mood vocabulary (from the internet) and later assigns a mood to each tweet. I am planning to take a close look at that article and apply the method used there. My questions"
2906,how to replace dtype=K.floatx()?,"['pandas', 'numpy', 'machine-learning', 'keras', 'sentiment-analysis']","I am very much new to python and I am trying to understand the following code snippet, but I don't know actually what is being done for train test split and also I am having a problem with Keras. can anyone make me understand what is happening in there and is there any replacement for dtype=K.floatx()? "
2907,Training a model when using Naive Bayes,"['python', 'machine-learning', 'sentiment-analysis', 'naivebayes']","I have a movie review dataset and I want to perform sentiment analysis on it.I have implemented this using logistic regression. Following are the steps that I took in the process:Now, I need to implement the same thing using Naive Bayes and I'm confused as to how to approach this problem. I assume the first 4 steps are going to be the same. But what is the training step when using Naive Bayes? What is the loss function and cost function in this case? And where do I use the Bayes' theorem to calculate the conditional probability? And how do I update the weights and biases?I've searched a lot of resources on the web and I've mostly only found implementations using sklearn with model.fit and model.predict and I'm having a hard time figuring out the math behind this and how it could be implemented using vanilla python."
2908,ValueError: dimension mismatch While Predicting New Values Sentiment Analysis,"['python', 'machine-learning', 'twitter', 'sentiment-analysis', 'naivebayes']","I am relatively new to the machine learning subject. I am trying to do sentiment analysis prediction. Type column includes the sentiment of the tweet(pos, neg or neutral as 0,1 and 2). Tweet column includes the tweets. I am trying to predict new set of tweets's sentiments as 0,1 and 2.When I wrote the code given here I got dimension mismatch error.The error I am getting is here:Would be so glad if you could help me."
2909,Using multiple conditions in NewApi with Python,"['python', 'sentiment-analysis']",I'm tryin to run the following code It works up the third row of OR conditions after which the number of articles retrieved starts to decline (which I would not expect given that it is an OR condition?) and by running the full command I get an empty list which is not the case if I run only the first two rows of OR conditions.Anyone with similar experiences?
2910,How to solve the SyntaxError: invalid syntax formal parameter name expected in PyCharm?,"['pycharm', 'sentiment-analysis', 'vader']",I want to try doing text analysed using VADER sentiment in PyCharm but I don't know why there is an error in this code.Can someone please help me to figured out why this happened. Thank you.This is what happened when I'm trying to run the codes
2911,Sentiment Dictionary for Business,"['python', 'dictionary', 'sentiment-analysis', 'lexicon']","I am looking for a sentiment dictionary/lexicon for business context to perform sentiment analysis. I have already built the analysis and calculation portion, but simply need the classes with contents.Here's an example of what I want to build on:I am looking to populate these lists further but only for words used when talking about stock performance of a company.Thanks!
Ryan"
2912,Aspect Based Sentiment Analysis Classifier - techniques on how to return unknown from a classifier?,"['decision-tree', 'sentiment-analysis']","I Created an Aspect Based Sentiment Analysis Classifier.i need to return answer with very high certenty ,
so i want to return unknownin case i am not sure about the answer
my model return for each label a percent and all 3 add up to 1
for example i try the threshold in the ROC curve point  for each label against all 3 and also the precision point of each labelfor now i am  just returning the max between all 3 . attached link to 1172 tagged exampleshttps://github.com/ntedgi/bert-sentiment/blob/master/decision_maker_weka.csvthanks"
2913,How to get tweets to store into an SQLite file using Python/Tweepy?,"['python', 'sqlite', 'twitter', 'tweepy', 'sentiment-analysis']","Hi so I have some code that streams tweets from Twitter's API and performs sentiment analysis on them as a CSV file. What I want to do is for the tweets to upload into an SQLite database prior to performing the sentiment analysis. I've had a pretty good go at getting it work and just can't figure it out. I've used the documentation from https://docs.python.org/3/library/sqlite3.html and looked at numerous Stack posts ( tweepy stream to sqlite database - invalid synatx , Python tweepy writing to sqlite3 db , tweepy stream to sqlite database - invalid synatx , Tweepy to sqlite3 ) but I still can't get it doing what I need it to do.With my code how it is I currently receive the error, ""AttributeError: 'list' object has no attribute 'db'""I've assumed that the .db is recognised by the SQLite but I guess I'm wrong, I can't find a Stack post with this specific error though so am quite lost. Any help with this would be amazing as I'm really at a loss now. The part of my code I'm receiving the error from will be in between lines of ********'s:"
2914,How can i tokenize all rows in a specific column from a csv file using Python?,"['python', 'pycharm', 'spyder', 'tokenize', 'sentiment-analysis']","I'm doing a sentiments analysis using Python (I'm still a rookie with this specific programming language). I have some Twitter data in a csv file that I need to pre-process before doing the real analysis. First of all I need to tokenize the text from a specific column, in my case the second or col B. I found some suggestions how to do the tokenization but not to pick the specific col. Anyone who has experience with this? I tried this code, which seems to work for all columns, but how can I isolate it to the second col?Any suggestions to modules and code that works for pre-processing to sentiments analysis?Thanks a lot!"
2915,Python Sentiment Analysis on Excel File,"['python', 'excel', 'sentiment-analysis']",I'm very new to python but I would like to run a sentiment analysis on customer reviews I have collected in a form of excel files. I got this code originally from a friend who was doing sentiment analysis on Tweets. Could someone guide me on which parts to cut out to exclude the Tweets aspect and what I need to add to the code in order to make it run Sentiment Analysis on the data I've collected?
2916,How to upload Tweets to an SQLite file prior to sentiment analysis?,"['python', 'sqlite', 'twitter', 'tweepy', 'sentiment-analysis']","I have code written that streams Twitter for a given hashtag and number of tweets, got that working, great. I need to upload the Tweets into an SQLite file before performing sentiment analysis on them. Currently they write straight to a CSV file and the sentiment analysis is being performed as the Tweets are stored. I have no real experience with SQL and am looking for help specifically with this part. I'll include all my code so you can see everything it's doing (the two parts where I need to write the SQL code I've put in between lines of *'s):"
2917,Sentiment is looking at only 1 character at a time,"['python', 'tweepy', 'sentiment-analysis']","I am working on a sentiment analysis problem in twitter. My goal is to collect data in a csv file, in three columns based on the sentiment. It seems like the sentiment is looking at only 1 character at a time when it is supposed to look 1 tweet at a time. Any help will be appreciated "
2918,AttributeError: module 'sst' has no attribute 'train_reader',"['python', 'nlp', 'stanford-nlp', 'sentiment-analysis', 'sst']",I am very new to sentiment analysis. Trying to use Stanford Sentiment Treebank(sst) and ran into an error.[Output]:
2919,Sentiment analysis using images,"['python-3.x', 'neural-network', 'sentiment-analysis']","I am trying sentiment analysis of images.
I have 4 classes - Hilarious , funny very funny not funny.
I tried pre trained models like VGG16/19 densenet201 but my model is overfitting getting training accuracy more than 95% and testing around 30 
Can someone give suggestions what else I can try?
Training images - 6K"
2920,Dimension Mismatch when predicting y for new set of vectorized text data using scikit learn,"['python', 'linear-regression', 'sentiment-analysis', 'dimensionality-reduction', 'tfidfvectorizer']","I have trained a 3d-order polynomial regression to predict the value of y given X, where X is a Tfidf Vectorized column of strings. For example, if one string reads ""I hate this book"" with a y=1 and another string reads ""I love this book"" with a y=2, the model should predict y=1.5 for ""I love hate this book"" (I'm obviously oversimplifying, but you see what I'm trying to do). The model works when I use train/test split with the same dataset, but if I try to predict y values for a different set of strings, I get a ""dimension mismatch"" error. My code is as follows:(Note: X_vect is a <2895x771 sparse matrix of type '' with 8673 stored elements in Compressed Sparse Row format>
t2015Q1_vect is a <605x106 sparse matrix of type ''
    with 2615 stored elements in Compressed Sparse Row format>)"
2921,To extract Entity Sentiment and Entity sentiment magnitude apart from using Google API,"['nlp', 'sentiment-analysis', 'document-classification', 'named-entity-extraction']","Any Idea on how to extract Entity Sentiment and Entity sentiment magnitude apart from using Google API.
I need to Extract the Salient Score, Entity Sentiment Score and Sentiment Magnitude Score. Just like the result from Google API.enter image description here"
2922,"How can a list of words, after inner_join() with the AFINN lexicon, still have more elements than the lexicon?","['r', 'sentiment-analysis']","For my R statistics homework I've converted the text of ""Pride and Prejudice"" into a data frame with each row containing one word. The data frame had 37,246 rows after removing stop words. Then I used inner_join() to combine the data frame with the AFINN sentiment lexicon. The resulting data frame had 6,065 rows while the AFINN lexicon only has 2,477 rows. How can this be? Shouldn't the resulting data frame have at most the same number of rows as the lexicon?I tried finding out which words are in the data frame but not the lexicon, but the code doesn't work:afinn_sentiments is the data frame with 6,065 rows, while afinn contains the afinn lexicon.This code still gives 6,065 rows, which is impossible.Can someone please help explain this?"
2923,Sentiment analysis with Lambda Expressions in Python,"['python', 'sentiment-analysis', 'generic-lambda']","I'm trying to use TextBlob to perform sentiment analysis in Power BI. I'd like to use a lamdba expression because it seems to be substantially faster than running an iterative loop in Power BI. For example, using Text Blob:creates a Power BI data column named ""Polarity Score"" that has the numerical values from TextBlob.I would like to do similar withe the TextBlob.classify() function. However, I don't know how to pass it the second argument of the classifier.Tutorials show to create and use a classifier:I've tried and Neither work and point to the way I'm passing the classifier. How would I pass the classifier parameter in a lambda expression?"
2924,How to drop data frame row if there is less than 4 character in sentence column?,"['python', 'machine-learning', 'sentiment-analysis']","let's say i have already tokenized sentence in my data frame like this :I want to clean out unnecessary data from my data frame by removing df row if there is less than 4 character is sentence column, so the end result will be like this :is there anyone who can provide the program code to solve this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention"
2925,Why does training my Naive Bayes Classifier take so much memory?,"['python', 'python-3.x', 'twitter', 'sentiment-analysis', 'naivebayes']","Recently, I have been working on a project which requires Sentiment analysis of twitter data. I am using a Naive Bayes Classifier from the Textblob library, and am trying to train it with 1.6 million tweets (which can be found here if anyone is wondering: https://www.kaggle.com/kazanova/sentiment140). Just outright passing in the 1.6 million tweets causes a Memory Error, so I decided to chunk it so only 1000 tweets get trained at a time. This has minor success as I can only get to about 10,000 tweets on my local machine until my computer freezes up, because I am using too much ram. I then tried it on Google colab, so I could run my code in the cloud. With both the TPU, and the GPU, the max I have gotten too is 28,000 tweets, before the session crashed and I had to restart the runtime. Here is my code:Some notes:Since my primary problem is how big my sentimentclassifier.pickle file gets, I have tried using gzip, but it just takes way too long to open and close the file, and this is especially bad because I need to open the file every loop since I do not want to lose any progress if the program crashes.I switched from using lists to using generators, which did improve the speed quite significantly.In google colab I tried passing in 10,000 at a time, which was sort of a last ditch effort, and unsurprisingly, it did not work out for the best.I am not sure if nltk's Naive Bayes Classifier is more efficient, but I really want that to be a last resort, as reformatting my list of tweets may take a few hours. But if it really is more efficient I will happily redo my code if it means I can get this working.Thank you and any advice will be greatly appreciated!"
2926,modifying polarity words in sentimentr package,"['r', 'sentiment-analysis', 'sentimentr']","Is there a way to modify words in the sentimentr package?
For example I want to change the word ""please"" to have a negative score rather than a positive one. 
Now I'm using the function:to evaluate sentence sentiment."
2927,"ValueError: Error when checking input: expected embedding_2_input to have shape (500,) but got array with shape (1,)","['python', 'tensorflow', 'keras', 'sentiment-analysis']","I have Trained the model using keras and When trying to predict the values with raw text data using the keras sentiment analysis
Getting the Raw Data from SQL serverHere is my CodeGetting the Error on this Line ErrorValueError: Error when checking input: expected embedding_2_input to have shape (500,) but got array with shape (1,)"
2928,Logistic Regression from scratch tfidf sparce matrix in Python,"['python', 'machine-learning', 'logistic-regression', 'sentiment-analysis', 'tf-idf']","I am trying to write a logistic regression from scratch and getting the following error. I have used sklearn's tfidfvectorizer to create a sparse tfidf matrix from tweet tokens after doing data cleaning and tokenization. Can someone please help me with this?The Code:tfidf_train.get_shapey is of shape (89988,)"
2929,Polarity and Sentiment Analysis in Power BI with Python,"['python', 'powerbi', 'sentiment-analysis']","I would like to use Python's Textblob for sentiment analysis in Power BI desktop. The code below works to create a separate dataframe that I can filter down to with the polarity scores.However, I would like to write directly to the existing data table likeReference: https://www.absentdata.com/power-bi/sentiment-analysis-in-power-bi/"
2930,Difficulties while using Tensorflow_hub and universal-sentence-encoder,"['python', 'sentiment-analysis', 'keras-layer', 'tensorflow-hub', 'tensorflow2.x']","While running the following piece of code, I get an error.
Anybody who can help me? I am using Tensorflow 2.1.0I get the following error:File ""/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 4727, in _override_gradient_function
      assert not self._gradient_function_mapAssertionError"
2931,How to generate independent(X) variable using Word2vec?,"['python', 'word2vec', 'sentiment-analysis']",I have a movie review data set which has two columns Review(Sentences) and Sentiment(1 or 0).I want to create a classification model using word2vec for the embedding and a CNN for the classification.I've looked for tutorials on youtube but all they do is create vectors for every words and show me the similar words. Like this-I already have my dependent variable(y) which is my 'Sentiment' column all I need is the independent variable(X) which I can pass on to my CNN model. Before using word2vec I used the Bag Of Words(BOW) model which generated a sparse matrix which was my independent(X) variable. How can I achieve something similar using word2vec?Kindly correct me if I'm doing something wrong. 
2932,How can I install the ibm watson module?,"['ibm-watson', 'sentiment-analysis']",I am trying to run IBM Watson's Natural Language Understanding in Python 3 but can't get the modules installed. This is what I started with:The error message was: no module named ibm_watson
2933,Naive Bayes SVM (NB-SVM),"['python-3.x', 'svm', 'stanford-nlp', 'sentiment-analysis', 'naivebayes']","I'm working on implementing the code from 'https://github.com/mesnilgr/nbsvm' for a Naive Bayes Sentiment analysis and Topic Classification that comes from the 'www.stanford.edu/~sidaw' paper from Side Wang and Christopher Manning.I also looked back at SO post and found a few that are similar so this could be an answer to past posts need for a hybrid NB tool. I'm not sure how to implement the code as it gives errors that I don't know how to fix.So i figure post my question, after its been researched a bit on StackOverflow and maybe some of you with more experienced can point out a solution or point me the in the right direction.The first two links are past SO posts that are similar to mine and the last link is the closest post to what I'm doing but that is an explanation of NB logic, theory and diagrams vs an optimized implementation of trained models with code.When I run the code from git hub it gives type error which I patch by wrapping the ngram object definition in list().then i run it again either as list() or tuple() but I get the same error and I'm stuck.I'm still an undergrad so i knew that non type needed a type, but not sure that is the solution it needs.I could post the code but there is also a .sh script that is needed and it's linked in the first sentence of the post.Let me know how I can run this code and I'll give you credit on the repo pull request when I update it.Thanks in advance!"
2934,Error in UseMethod(“sentiment”) : no applicable method for 'sentiment' applied to an object of class “factor”,"['r', 'text-mining', 'sentiment-analysis']","I am a newbie at R and im trying conduct a sentiment analysis with sentimentr.I am trying to analyse data from a csv file:However, I get this errorError in UseMethod(""sentiment"") :     no applicable method for
  'sentiment' applied to an object of class ""factor"""
2935,Is it possible to get sentiment for emojis using IBM Watson NLU?,"['ibm-watson', 'sentiment-analysis', 'watson-nlu']","I am using IBM Watson to get sentiments of social media texts. But many of these texts are just emojis. Currently, I am not able to get any sentiment for emojis. I get unsupported text language error. Is there anyway to get the sentiments for emojis using Watson NLU?"
2936,For loop is not appending data properly to csv file,"['python', 'for-loop', 'tweepy', 'sentiment-analysis', 'textblob']","I am working on a sentiment analysis problem in twitter. My goal is to collect data in a csv file, in three columns based on the sentiment.
After some attempts, the script is running (partially). It seems like the sentiment is looking at only 1 character at a timeIn addition, after looping for some time, it breaks and show an error"
2937,vanderSentiment Analysis Returning only Compound value,"['python-3.x', 'macos', 'sentiment-analysis']",I was wondering how we could return only the compound number from a vanderSentiment instead of returning pos neut neg and compound ?Thank you
2938,"I have lexicon with sentiment score, I want to find these words from a tokenised tweets and add the score [closed]","['python', 'twitter', 'nlp', 'sentiment-analysis']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 6 months ago.For each list i want to see a sum of score that matches a key word
E.gIs there any library which will allow me to find words like this? Any help in this front is appreciated "
2939,Multi-label text classification CNN,"['neural-network', 'conv-neural-network', 'lstm', 'sentiment-analysis', 'multilabel-classification']","ContextI'm building an Emotion Analysis Neural Net for my final year project, I have decided to use a CNN as there have been some impressive results with CNNs in Emotion Analysis. I wish to be able to make multi-label classifications, such as:{Joy: 0.92, Humour: 0.67} and {Anger: 0.54, Disgust: 0.65} I have researched multi-label classification and found this article: https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff    but I am still not sure how to approach.The article suggests that there are several common approaches to solving multi-label classification problems: OneVsRest, Binary Relevance, Classifier Chains, Label Powerset. The article also mentions under 'Further Improvements' at the bottom of the page that the multi-label problem can be solved using an LSTM. The Label Powerset approach seems fairly straightforward, and feasible as I will only have 6 or so labels. I would consider using an LSTM if there are any advantages to doing so, though I'm not sure whether the article is suggesting using an LSTM in place of a CNN or a combination of both. I have seen some tutorials and articles around on multi-label classification with CNNs but they are all geared toward image classification.QuestionsHow can an LSTM be used in combination with a CNN to perform multi-label classification?Are there any other ways to do multi-label classification with a CNN other than the ones mentioned above that I should consider?"
2940,Facebook sentiment analysis,"['python', 'facebook-graph-api', 'nlp', 'jupyter-notebook', 'sentiment-analysis']",when i import facebook then got error belowplease need help!
2941,Manual scoring of texts for sentiment analysis,"['machine-learning', 'text', 'sentiment-analysis', 'supervised-learning', 'scoring']",I am trying to do manual scoring of sentiment of texts so that they can be used as input for supervised learning models. Is there an objective way to consolidate and get the sentiment score done with multiple persons? I want to know if there is a standard method that is practiced in manual scoring of sentiments.
2942,How to know if an application is using Cloud Foundry?,"['.net', 'cloudfoundry', 'ibm-watson', 'sentiment-analysis']","I am working on a .NET application that uses IBM Watson for sentiment analysis. I am new to this project and don't know the proper working of the IBM Watson. Recently, sentiment Analysis stopped working and we sent an email to IBM about it and they said that it might because we have not yet migrated it from Cloud Foundry to Resource Group. I don't know whether we are using Cloud Foundary or not. How can I check it? 
People who have worked on this project before are currently not available and I am on my own now. Please help me out. "
2943,Dimensionality reduction in text data for KNN,"['machine-learning', 'text-mining', 'sentiment-analysis', 'knn', 'dimensionality-reduction']","I have text data (tweets) classified as positive, neutral and negative. I want to test if KNN will do better for reduced dimensions. I tested KNN with different CountVectorizers (different max_feature) from sklearn, but it doesnt really do anything. Is there a point in applying for example PCA or manifold? If yes, how to apply these to text data?"
2944,Sentiment Analysis With KNN Using R Shiny,"['r', 'shiny', 'sentiment-analysis', 'knn']","I'm trying to make sentiment analysis with KNN using R Shiny, but there is an errorServer.rui.rError is unused arguments (train = modeldata[train_set, ], test = modeldata[test_set, ], cl = classifier[train_set], k = 5)this error does not appear in the R script."
2945,How can I deploy my features in a Machine Learning algorithm?,"['python', 'machine-learning', 'sentiment-analysis', 'feature-selection']","I’m way new to ML so I have a really rudimentary question. I would appreciate it if one clarifies it for me.Suppose I have a set of tweets which labeled as negative and positive. I want to perform some sentiment analysis. I extracted 3 basic features: How should I use these features with SVM or other ML algorithms? In other words, how should I deploy the extracted features in SVM algorithm?
I'm working with python and already know how should I run SVM or other algorithms, but I don't have any idea about the relation between extracted features and role of them in each algorithm!Based on the responses of some experts I update my question:At first, I wanna appreciate your time and worthy explanations. I think my problem is solving… So in line with what you said, each ML algorithm may need some vectorized features and I should find a way to represent my features as vectors. I want to explain what I got from your explanation via a rudimentary example.Say I have emoticon icons (for example 3 icons) as one feature:1-Hence, I should represent this feature by a vector with 3 values. 2-The vectorized feature can initial in this way : [0,0,0] (each value represents an icon = :) and :( and :P ). 3-Next I should go through each tweet and check whether the tweet has an icon or not. For example [2,1,0] shows that the tweet has: :) 2 times, and :( 1 time, and :p no time.4-After I check all the tweets I will have a big vector with the size of n*3 (n is the total number of my tweets). 5-Stages 1-4 should be done for other features.6-Then I should merge all those features by using m models of SVM (m is the number of my features) and then classify by majority vote or some other method.
Or should create a long vector by concatenating all of the vectors, and feed it to the SVM.Could you please correct me if there is any misunderstanding? If it is not correct I will delete it otherwise I should let it stay cause It can be practical for any beginners such as me...
Thanks a bunch…"
2946,how to extract data from google review using jupyter notebook,"['web-scraping', 'jupyter-notebook', 'jupyter', 'sentiment-analysis', 'jupyterhub']","im newbie in jupyter notebook. I need to do my fyp title about sentiment analysis on tourist attraction using online reviews. my plan is to have a GUI that can user type name of the place. then, they will click search button. the process behind it.. it will extract all the visitor reviews about that place from google reviews. then from that reviews, it will identify top 3 features that visitor like and dislike about that place such as cleanliness / ticket price / staff friendliness. then, the system also will rate overall visitor satisfaction on that place either that place is good or not based on highest percentage of sentiment analysis. however... i don't know where to start. can anyone suggest me some website or tutorial that similar to my project? i already search but im still clueless. :/ Thank you."
2947,Package `sentimentr`: how remove emoticons and stopwords before `sentiment_by`,"['r', 'text-mining', 'sentiment-analysis', 'sentimentr']","Here is a basic sentiment example. The text data is splitted into sentences via the get_sentences function. With sentiment_by we approximate the sentiment (polarity) of text for an entire element of a list (mytext in this example).E.g. for the example:I obtained the following result:Before applying sentiment function, I would like to remove stop words, number, emoticons from mytext. I figured I could use, e.g:but I obtained:"
2948,How to print descriptive statistics on sentiment analyses using Python's TextBlob?,"['python', 'analytics', 'sentiment-analysis', 'textblob']","I am conducting a sentiment analysis for research purposes using TextBlob in Python 3. 
I used this code to retrieve the sentiment per line of feedback (around 4000 lines):Some examples of my output:Even though I can now analyze the sentiment of every line, what methods exist to make sense out of the data as a whole (e.g. mean, distribution etc.) and what lines of codes can I use for this?Thanks in advance!"
2949,Twitter Sentiment Analysis using Dandelion API Error,"['java', 'url', 'twitter', 'sentiment-analysis', 'dandelion']","I am trying to analyse a Tweet's sentiment using the dandelion api Sentiment Analysis. While I do have quite a few methods, the method below is my main one in Class SentimentAnalyser:And this below is my main method:However, when I run this, I get the following error message:Please enter a Twitter handle, do not include the @symbol --> rudwick2Enter the nth most recent tweet you would like analysed from rudwick2--> 3Enter your user Token --> 35a5b026870c49f69d3c688779ff26dbjava.io.IOException: Server returned HTTP response code: 400 for URL: https://api.dandelion.eu/datatxt/sent/v1/?lang=en&text= cien max Nice presentation &token=35a5b026870c49f69d3c688779ff26db
      at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1913)
      at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1509)
      at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:245)
      at SentimentAnalyser.method(TwitterPart3.java:130)
      at TwitterPart3.main(TwitterPart3.java:42)
  nullI'm quite new to this sort of project, and I'm not sure where I went wrong—what I can tell is that the response from the URL in method isn't working. Is it beceause the URL, when combined with the tweet, isn't quite right? "
2950,"How to print twitter data stream to a file whener ever i try,i am getting a unicode error","['python', 'nlp', 'data-analysis', 'text-mining', 'sentiment-analysis']","This is my python code to retrieve data from twitter.
but when I am trying to store the data to gannie.txt I am encountering the following error.Any help regarding this,i am new to this text mining and im trying to build project of sentiment analysis using natural language processing here's my code: "
2951,Creating text summary using NLP,"['python-3.x', 'nlp', 'sentiment-analysis']","I am in middle of applying NLP to the set of comments that I have received from my data. These comments are stored in one column. I have cleaned them altogether stored them in a list. There are no stop words, special characters etc. Now I want to create a summary from this text. What could be the best method to do that? I have already failed myself with heapq, so I dont want any solution around that. My clean text is stored in list named : clean_text_summary and it looks like this :I need to get most common things that people have talked about as a summary. "
2952,Welch/p-test usage,"['machine-learning', 'statistics', 'data-science', 'sentiment-analysis']",I want to prove that sentiment of text has connection with the classification problem I am addressing. Can p-test/Welch test be used for laying statistical proof?  The size of dataset is 100k records.
2953,Sentiment anaysis using Harvard IV-4 dictionary,['sentiment-analysis'],"I was trying to compute the sentiment using Harvard IV-4dictionary.
I installed the ""pysentiment"" successfully.
I run the following:and I got the following error:Could any body tell why I am getting this? Thanks."
2954,Python SentiStrength binary score outputs only 1 score,"['python', 'nlp', 'sentiment-analysis']","I am currently working on text analysis using SentiStrength python library by command result = senti.getSentiment(cs, score='binary'). Firstly, I run it in Jupyter notebook and it works well. It outputs 2 scores which are positive and negative sentiment scores such as [(2,-1)]. However, when I try to run it in anaconda prompt or spyder. It outputs only 1 values like [1] and I do not understand why. I guess it is because I run it on different environment. I would like to ask how could I run this command in anaconda prompt or other IDE so that it could output result correctly? or did I do something wrong."
2955,Problems in Unsupervised Aspect Based Sentiment Analysis,"['machine-learning', 'nlp', 'sentiment-analysis', 'unsupervised-learning', 'vader']","I'm working on unsupervised aspect based sentiment analysis. I tried using Vader for it, which gave me good result but the problem is if the topic is negative like 'food waste' then the sentiment is always coming as negative even though content is saying 'and i really hate food waste'.
Can someone help me in tackling this issue, or even suggest me a method better than Vader.
I've also tried using 'Flair' but its' results are not as promising as Vader."
2956,Google NLP API: Document Sentiment Score != average(Sentence Score),"['google-cloud-platform', 'nlp', 'sentiment-analysis']","How does the NLP calculate the overall document's sentiment score?  I see this question about the magnitude Google Cloud Natural Language API - How is document magnitude calculated? and the question mentions that the score is the mean score of sentences. That doesn't seem to be the case in my example.This is a news articletext =  ""Six people were injured Sunday when police fired rubber
  bullets to break up clashes between  South  African and Zimbabwean
  residents of an informal shack settlement near Johannesburg, police
  said.Seventy-four shacks were burned and another 124 were looted in
  the clashes, said police spokeswoman Betty Ngobeni. The conflict was
  sparked last month when a  South  African woman was killed at the
  Zandspruit informal settlement. Residents believe a Zimbabwean man was
  responsible, she said.A number of Zimbabweans have since been attacked
  by  South  Africans, who told them to return to their own country.The
  fighting intensified Sunday, and a mob of  South  Africans allegedly
  burned and looted the Zimbabweans' homes, Ngobeni said.Police were
  called in and fired rubber bullets to disperse the crowd, she said.
  Twenty people were arrested and will be charged with public
  violence.Xenophobic attacks are commonplace in  South  Africa, with
  locals accusing foreigners of crime and taking up scarce jobs.""NLP API gives me the following sentences (content, offset and the score indicated below)As you can see, the sum/count is -2.2000000029802322/4 which makes the average score -0.5500000007450581. But the same document gives me an overall score of score: -0.6000000238418579I am curious to understand how the sentiment score of the overall document is calculated. Interestingly, the magnitude is also not the absolute sum. In this example, for each sentence above, my magnitudes are 0.699999988079071, 0.20000000298023224, 0.800000011920929, 0.5, which seem to be the same as score without the sign, and the absolute sum is 2.2000000029802322 whereas the overall document score is 2.400000095367431"
2957,Is there a way to explicitly set start and end of sentences on Google Natural Language API sentiment analysis?,"['google-cloud-platform', 'sentiment-analysis', 'google-natural-language']","I am using Google Natural Language API for sentiment analysis. I have an array of string texts that I concatenate and send to google to get sentiment values for each sentiment, but Google has its own idea on where the sentences start and end, so getting scrambled sentiment results and a different count of sentiment scores, then sent sentences.If you could only set a flag like <sentence> </sentence> for each phrase you would like to treat as a separate sentence - that would be great, but docs don't have info about it.P.S.
I am using it this(sending a chunk and not doing a separate API request for each sentence) way because I have thousands of sentences and latency is important."
2958,"Naive Bayes classifier for movie reviews has very low accuracy, despite several attempts of feature selection","['machine-learning', 'scikit-learn', 'sentiment-analysis', 'naivebayes']","I am fairly new to machine learning and have been tasked with building a machine learning modelt o predict whether a review is good (1) or bad (0). I have already tried to use a RandomForestClassifier that output an accuracy of 50%. I switched to the Naive Bayes classifier but am still not getting any improvement, even after conducting a grid search.My data looks as such (I am happy to share with data with anyone):My code to preprocess then text and use TfidfVectorizer before training the classifier is as such:The results of the classification report seem to indicate that whilst one label is predicted very well, the other is extremly poor, bringing the whole thing down.I have tried to follow 8 different tutorials now on this and tried each different way of coding but I can't seem to get it above 50% which makes me think it may be a problem with my features.If anyone has any idea or suggestions, I'd greatly appreciate it.EDIT:
Okay so I have added several preprocessing steps here including, removing html tags, removing punctuation and single letter and removing multiple spaces from the code below:I believe TfidfVectorizer automatically puts everything in lower case and lemmatizes it. The end result is still only 0.5 "
2959,Automatic labelling of unstructured text to perform aspect based sentiment analysis,"['machine-learning', 'deep-learning', 'nlp', 'label', 'sentiment-analysis']","I have unstructured text extracted from online platform regarding product reviews and news. I need to label the unstructured text based on most mentioned tags to perform aspect based sentiment analysis. Any suggestions to proceed would be really helpful.
Thanks in advance!"
2960,How do I efficiently loop over this dataframe and perform a function using inbuilt numpy or pandas?,"['python', 'pandas', 'numpy', 'machine-learning', 'sentiment-analysis']","I read this article earlier and noticed that the pandas apply function, iterrows and for loop are terribly slow and efficient way of working with pandas dataframes.I am doing sentiment analysis on some text data, but using apply causes high memory usage and low speeds similar to shown in this answer.How can I implement this using either built-in numpy or pandas function?
Edit:- The column contains essay text data"
2961,What is the “DataAccessRoleArn” for Comprehend in boto 3?,"['python', 'boto3', 'sentiment-analysis', 'amazon-comprehend']",I am using boto3 for comprehend sentiment analysis on AWS. I have issue with the 'DataAccessRoleArn' parameter. What kind of value is inserted in this? I am requesting for the format of it or some sample.
2962,Get data from Twitter using Tweepy and store in csv file,"['python', 'python-2.7', 'csv', 'twitter', 'tweepy']","I'm new to Python, Twitter, and Tweepy. I managed to pull data from Twitter, but I now want to store it into a CSV file.My code is:This prints data out on CLI, but I want it to output to a CSV file. I tried a few different options, but it only outputted the first tweet and not all tweets."
2963,"Is is possible to do sentiment analysis other than just positive, negative and neutral in Python or other programing language","['python-3.x', 'nlp', 'sentiment-analysis']","I have searched the internet and there is more or less the same sentiment analysis of a sentence i.e Positive, Negative or Neutral. I want to build a sentiment analyzer that look for the following sentiments/emotions for a sentence. "
2964,How can i do sentiment analysis on only one sentence?,"['python', 'machine-learning', 'sentiment-analysis']","I am currently trying to do a sentiment analysis program on a dataset provided by tensorflow. This code works, however, i do not know how to predict if only one sentence is positive or negative ( its probability ). The testing is done on the dataset firstly and it works fine. I know there is something that i am missing, but i can't figure out what it is.This is the code ( i thought it might help):And this is what i tried adding to it in order to test the sentiment analysis only on this one sentence: Can you please help me?"
2965,error: module 'got' has no attribute 'manager',"['python', 'twitter', 'sentiment-analysis', 'twitterapi-python']","'''I had used the got library for fetching older tweets i. e past 6 months tweets of some particular hashtags but i am getting the 'error: module 'got' has no attribute 'manager'. '''
'''so after searching in net there are some solutions. so i tried one this 
1)you need to uninstall the twitter and again to install correct type of python version twitter so i install by 'pip install python-twitter' after uninstalling the wrong twitter one''''''is there any solution to overcome this error. And my python version is 3.6'''"
2966,"I need to fetch older tweets, i.e. from 01-01-2019 to 01-12-2019, by using the standard API (not premium or enterprise) using Tweepy","['python', 'tweepy', 'sentiment-analysis', 'twitterapi-python']","I am getting the data of the past 7 days only after passing the API credentials.Even if I request for 10000 records, I am getting only less than 1000 records that correspond to only 1 week of data. I want to get the data from the past 12 months using the standard API (which is not a paid version). Is this possible? If so, please provide me the solution."
2967,"How can I solve - Error in function (type, msg, asError = TRUE) : Unknown SSL protocol error in connection to api.twitter.com:443","['r', 'twitter', 'sentiment-analysis']","I am using R studio latest version for doing sentiment analysis using twitter. All the functions and packages are working fine but I am stuck on this error whenever I am trying to run I am getting this -- >  Error in function (type, msg, asError = TRUE)  : 
    Unknown SSL protocol error in connection to api.twitter.com:443 Please anybody knows solutions for this ?"
2968,I am looking for a dutch language tokenizer for technical product review,"['nlp', 'tokenize', 'sentiment-analysis']",I am trying to find out the better text cleaning method for Dutch NLP problem. I have used dutch version for pos tags and nltk for removal of stop words. But I am not getting desired results.
2969,"merging tweets by date, returning count of sentiment score","['python', 'pandas-groupby', 'sentiment-analysis']","I'm working on a sentiment analysis problem. My dataframe is as followsI want to (1)Aggregate (merge) the tweets per ""ticker"" at a day level. So I can run a sentiment analysis and get overall sentiment score per ticker on a given day.(2)""sentiment_score"" has values {0,1,2,3,4}, I want to create 5 new columns which contain the count of no.of tweets where sentiment_score is {0,1,2,3,4} for every ""ticker"" at a day level.Expected outputI tried individual groupby operations but they didn't give the required output. Appreciate the help."
2970,Python: Couldn't convert from String to Float,"['python', 'scikit-learn', 'sentiment-analysis']","I followed this tutorial to implement sentiment analysis:
https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/but I'm not a pro so I don'T understand every step in detail.
Now, I wanted to apply it to new data, using this tutorial:
https://stackabuse.com/scikit-learn-save-and-restore-models/but at the point I get the Value error: could not convert from String to float 'positive' (Positive being a label for the sentiment analysis done earlier). What surprises me is that the error happens even when using X_train and y_train (from the first tutorial), but works just fine without any errors. So I am assuming that the fit() method does something that the score() method does not, and this creates the problem. However, I have no idea how to fix it.Here is the full error message:ValueError                                Traceback (most recent call last)ValueError: could not convert string to float: 'positive'and here'S the piece of code the error occurs in:"
2971,Sentiment analysis using Vader library,"['python', 'sentiment-analysis']","Below is the Python code that I am using to do sentiment analysis using the Vader Library. I have used multiple for loops to get positive, negative, neutral & compound sentiment score. How can I get positive, negative, neutral & compound score using one for loop?"
2972,I'm getting Key error in python,"['python', 'dictionary']",In my python program I am getting this error:From this code:Can anyone please explain why this is happening?
2973,Textblob reproduces the same text only,"['sentiment-analysis', 'textblob']",I'm classifying Students sentiments based on their comments using Texblob - but the code is reproducing the same comment again under SA column. pip install textblob is downloaded & updated and all the packages on nltk.download() are up to date. I also tried the code with different dataset but still shows same error of just copying the original comment and paste it again under SA column.
2974,How to have a sentiment score for a document in Quanteda?,"['sentiment-analysis', 'quanteda']",I am new in sentiment analysis. Quanteda examples show how to output numbers of positive and negative words. I tested some documents. It output below: Case 1or below Case 2Can you let me know how to have scores for file1 .. file5 in both cases? Is that (#positive - #negative) / #all in case 1 file2， （71-98）/(71+98)=-27/169= - 0.15 ?what about case 2?Thanks a lot.A
2975,Semi-supervised sentiment analysis in Python?,"['python', 'nltk', 'sentiment-analysis']","I have been following this tutorial 
https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/
to create a sentiment analysis in python. However, here's what I don't understand: It seems to me that the data they use is already labeled? So, how do I use the training I did on the labeled data to then apply to unlabeled data?I wanna do sth like this:Assuming I have 2 dataframes:
df1 is a small one with labeled data, df2 is a big one with unlabeled data. I just finished training with df1. How do I then go about predicting the values for df2?I thought it would be as straight forward as text_classifier.predict(df2.iloc[:,1].values), but that doesn't work for me.Also, forgive me if this question may seem stupid, but I don't have a lot of experience with machine learning and nltk ...EDIT:
Here is the code I'm working on:Most of it is taken exactly from the tutorial, except for the line with astype in it, which I used to convert the unlabeled data, because I got a valueError that told me it can't convert from String to float if I don't do that first."
2976,Laravel “ErrorException (E_NOTICE) Undefined variable: class” [duplicate],"['php', 'laravel', 'eloquent', 'sentiment-analysis']",*Error in here
2977,How to predict a new text with doc2vec and LogisticRegression,"['nlp', 'logistic-regression', 'sentiment-analysis', 'doc2vec']","i'm new to python and trying to predict a new text by using Doc2vec and logisticRegressionFor example ""i am happy"" return POS , ""i am sad"" return NEGBelow is my example code download from https://github.com/nisarg64/Sentiment-Analysis-Word2Vec/blob/master/doc2vec_sentiment.py i had tried this beforebut this is the error code"
2978,TextBlob sentiment analysis: nan values,"['python', 'nlp', 'sentiment-analysis', 'textblob']","I'm new to sentiment analysis and I'm exploring with TextBlob.My data is pre-processed Twitter data. It's in a series and each tweet has been cleaned and tokenized:When I run textblob sentiment (using help from Apply textblob in for each row of a dataframe), my result is a column of nan values:"
2979,Running a Python Script on a Website (in the background),"['python', 'caching', 'sentiment-analysis', 'web-frameworks']","Firstly, apologies for the very basic question. I have looked into other answers but they haven't quite answered what I'm after. I'm confident designing a site in HTML/CSS and have very very basic knowledge of Python.I want to run a very basic Python script on my website. It analyses tweets about a specific topic, and then posts a sentiment analysis score. I want it to run this sentiment analysis every hour and cache the score.I have a working Python script which does this in Jupyter Notebook. Could you give me an overview of how I would make this script function online and cache the results? I've read into using Python web frameworks, but from my limited understanding, they seem like overkill?Thank you for your help!"
2980,Iterating through list of lists and count matches with different list,"['python', 'list', 'loops', 'frequency', 'sentiment-analysis']","I am new to python and I am working currently on sentiment analysis for my Master thesis. However, there is this problem I am currently working on where I don't really know how to solve it.I need to find a sentence in a string that contains the word BLA and then compare every word in the sentence with my POSITIVE and NEGATIVE words dictionary. If there are more negative words than positive, the counter should do +1. In the end, I would have something like: in file 1, there are 4 negative sentences that include the word BLA.So far I used regular expressions to delete all sentences that do not include the word BLA. Then I separated the words within the sentences and created a list of lists. It looks e.g. like that:[['we', 'underperform', 'because', 'of', 'BLA'], ['BLA', 'is', 'bad'], ['BLA', 'is', 'good']]Now I would like to compare every single word with the dictionaries of negative and positive words. As I need to find out if the sentence containing the word BLA is rather positive or negative, it is important that I only count this within one list in the list of lists before moving to the second one.The result should be 2 for this particular example as 2 sentences are negative and one is positive.In other cases where I only look for e.g. negative words within the text, I do it this way:So I would probably do this but within a loop that goes over the lists.If you have an idea how to approach this problem differently I am also open for that."
2981,python; how to display the selected words from ngram in sentiment analysis?,"['python', 'scikit-learn', 'sentiment-analysis', 'n-gram']","I have a dataset of twitter comments where I use TextBlog to calculate the sentiment and then I use that to calculate which words are used the most for positive and negative sentiment.
One approach I am using is to use NGram with the following code;This shows the accuracy of the test, but I want to see the top 5 mostly choosen positive words and the top 5 mostly choosen negative words.How do I do this?"
2982,Sentiment Analysis using azure error 'Resource not found',"['python', 'azure', 'sentiment-analysis', 'microsoft-cognitive', 'azure-cognitive-services']","I have created a python program that accepts a String as an input and performs sentiment analysis on it.I have created Environmental variable as stated in DOCUMENTATION and also restart the cmd as well as Visual Studio but still, I get the following error:Encountered exception. Operation returned an invalid status code 'Resource Not Found'The python program is as follows:"
2983,How do I append sentiment values from oseti to a pandas dataframe?,"['python', 'pandas', 'dataframe', 'sentiment-analysis', 'cjk']","First post here!After struggling with mecab and encodings I got oseti to work for Japanese sentiment analysis, where oseti.Analyzer() takes a string and prints a list with one value per sentence:(Sorry, not 100% sure if it prints or just resturns the list)I have a pandas dataframe with about 10,000 rows and want to create a new column (OS) with the mean value of the oseti.Analyzer output from a certain column (HD). Some of the HD cells have hundreds of sentences, rendering hundreds of sentiment values.The line below gave an error message:I am not sure what I should do. Isolate the string part of each HD observation? Convert the column to strings? Run oseti in a for loop before turning by CSV data into a Pandas dataframe?If you couldn't tell by now, I'm a newbie and just starting out with Python and pandas. I have gone through some of the usual Web courses and videos, but have not seen anything about oseti or directly applicable examples from other modules."
2984,For loop and add a new column to dataframe in R,"['r', 'for-loop', 'sentiment-analysis']","I have a dataset called tweet_new, and tweet content is in ""text"" column in this dataset.I will perform a sentiment analysis on each tweet content using get_avg_sentiment function below:I want to use a for loop to run the function on all tweets in database, and then add a new column to store the result sentiment result. How can I do that? Thanks!"
2985,Setting n-grams for sentiment analysis with Python and TextBlob,"['python', 'sentiment-analysis', 'textblob']","I want to do sentiment analysis of some sentences with Python and TextBlob lib.
I know how to use that,  but Is there any way to set n-grams to that?
Basically, I do not want to analyze word by word, but I want to analyze 2 words, 3 words, because phrases can carry much more meaning and sentiment.For example, this is what I have done (it works):But how can I apply, for example n-grams = 2, n-grams = 3 etc?
Is it possible to do that with TextBlob, or VaderSentiment lib?"
2986,How to implement the language automatically in detect sentiment in azure text analytics in a logic app?,"['azure', 'azure-logic-apps', 'sentiment-analysis', 'text-analytics-api']","I've created a logic app which detects the language of a text and then the sentiment with cognitive services. I want to change the language parameter to the actual language which is detected. I've tried the following and much other stuff but it doesn't work out for me.
Can anyone suggest me a solution?
(The text is German but when I run it it says: ""Supplied language not supported. Pass in one of: ar,da,de,el,en,es,fi,fr,it,ja,nl,no,pl,pt-PT,ru,sv,tr,zh-Hans"")
For a copy:Hallo E-Bike Team, ich habe ein Problem mit meinem E-Bike. Es ist das
Model ProRide2E. Seit heute Morgen steht auf dem Display folgender
Hinweis: „Akkuleistung beeinträchtigt. Fehlercode: XB1200AB“ Das darf
doch wohl nicht wahr sein. Ich bin echt sauer. Wieso ist das doofe
Bike immer kaputt?Ein genervter KundeName"
2987,"How to Use CountVectorizer and TfidfTransformer on large data set(Train,Dev,Test)?","['python-3.x', 'machine-learning', 'sentiment-analysis']","Implement each of the following feature templates using only
CountVectorizer​ and​ TfIdTransformer. I have train, dev, test data.Here is my sample train data:a. Represent only word occurrences with binary values (​baseline​).
b. Remove stop words (these are the most common words in the English language).
c. Represent term frequencies of words."
2988,Load data: TypeError: cannot convert the series to <class 'int'>,"['python', 'pandas', 'sentiment-analysis']","I am currently working on a sentiment analysis project. When I was trying to load data, the errorTypeError: '<=' not supported between instances of 'str' and 'int' appears. So I modified the str to an int. However, now the error TypeError: cannot convert the series to <class 'int'> appears. How can I solve this error?
The data type = dtype('O')Data:
This is the data I have got"
2989,gcp NL api sentiment analysis - How to store the results in BigQuery,"['google-bigquery', 'gcloud', 'sentiment-analysis', 'google-natural-language']","I am using gcp bigquery to store news streaming by google function and save it in bigquery.How can I run a python script that is using the data from bigquery and finally writes back the result for score and magnitude to the related dataset?I could not find anything in the google documentation about it, just how to run the sentiment analysis but not, how to get data from bigquery in and results out back to bigquery.Thanks a lot for your support."
2990,Bigram in polarity table not being properly assigned a score in sentimentr,"['r', 'nlp', 'text-mining', 'sentiment-analysis', 'sentimentr']","In the sentimentr package, there are some bigrams in the lexicon hash lexicon::hash_sentiment_jockers_rinker. However, when running sentiment on it, it is not being picked up. For example, in the lexicon::hash_sentiment_jockers_rinker table, the word ""time honored"" has a y value of 1, but when running that phrase through the algorithm, it comes out as ~0.7Is there a way to alert the sentiment function to look for bigrams in the text and in the list?"
2991,Sentiment analysis using LSTM on imbalanced citation dataset,"['nlp', 'pytorch', 'lstm', 'sentiment-analysis']",I have an extremely unbalanced dataset for sentiment classification. https://cl.awaisathar.com/citation-sentiment-corpus/Here is my network:Loss function:My accuracy is low on the small classes. How can i improve it futher?
2992,RemoveWords command not removing some weird words,"['r', 'text', 'twitter', 'sentiment-analysis', 'word']","The point is that im trying to remove some weird words (like <U+0001F399><U+FE0F>) from my text corpus to do some twitter analysis. There are many words like that that i just can't remove by using  <- tm_map(X, removeWords).i have plenty of tweets agregated in a dataset. Then i use the following code: corpus_tweets <- tm_map (corpus_tweets, removeWords, c(""<U+0001F339>"", ""<U+0001F4CD>""))

if i try changing those weird words for regular ones (like ""life"" or ""animal"") that also appear on my  dataset the regular ones get removed easily. Any idea of how to solve this?"
2993,How to predict Sentiments after training and testing the model by using NLTK NaiveBayesClassifier in Python?,"['nltk', 'python-3.7', 'sentiment-analysis', 'predict', 'naivebayes']","I am doing sentiment classification using NLTK NaiveBayesClassifier. I trained and test the model with the labeled data. Now I want to predict sentiments of the data that is not labeled. However, I run into the error.
The line that is giving error is :The error is :ValueError: not enough values to unpack (expected 2, got 1)Below is the code:I understand that the problem is arising because I have to give two parameters is the line which is giving an error but I don't know how to do this.Thanks in Advance."
2994,"I get a TypeError while doing sentiment analysis, how can I fix this issue?","['python', 'anaconda', 'jupyter', 'typeerror', 'sentiment-analysis']","I am doing Sentiment Analysis on Bitcoin News. During my coding a TypeError Problem occured. I hope you can help me and thank you very much in advance!Then this TypeError occurs:
imgur_link"
2995,How to save nested dictionary into a xlsx file in a for loop- Sentiment Analysis,"['python', 'sql', 'dictionary', 'for-loop', 'sentiment-analysis']","I need to perform a sentiment analysis (I used the API for Azure) for each of the 2 different tables present in my SQL database, and I need to store the results (a nested dictionary) in a .xlsx file. 
So far the code I did is this:I don't know where is wrong but it gives me this error:The nested dictionary are something like this:"
2996,How to do sentiment analysis of headlines with TextBlob and Python,"['python', 'pandas', 'sentiment-analysis', 'textblob']","I want to calculate the polarity and subjectivity for some headlines that I have.
My code works fine, it does not gives any error but for some rows it gives result 0.00000 for polarity and subjectivity. Do you know why?You can download the data form here:https://www.sendspace.com/file/e8w4twAm I doing something wrong?
This is the code:I mean, I know that 0 is possible result, but Im getting 0.0000 in 40%-50% of rows, thats a lot, not even 0.00001, that seams strange to me.Can you help me?"
2997,How to load unlabelled data for sentiment classification after training SVM model?,"['machine-learning', 'svm', 'python-3.7', 'sentiment-analysis', 'sklearn-pandas']","I am trying to do sentiment classification and I used sklearn SVM model. I used the labeled data to train the model and got 89% accuracy. Now I want to use the model to predict the sentiment of unlabeled data. How can I do that? and after classification of unlabeled data, how to see whether it is classified as positive or negative?I used python 3.7. Below is the code.When I run this code, I get the output:ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.   ""the number of iterations."", ConvergenceWarning)
  Accuracy :  0.8977272727272727 
  Precision :  0.8604651162790697 
  Recall :  0.925What is the meaning of ConvergenceWarning?Thanks in Advance!"
2998,FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?,"['python', 'beautifulsoup', 'jupyter-notebook', 'lxml', 'sentiment-analysis']","I try to running this code to process ""teks"". And it show 
FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?error: I already read some of this error, and it said that i should change tobut the error come up again and said NameError: name 'html' is not definedWhat should i do?"
2999,My text from csv file is being read as raw string. It contains “it\'s” instead of it's. How do I clean this?,"['python', 'regex', 'nlp', 'sentiment-analysis', 'rawstring']",Sentence: How do I remove escape characters to clean the data?
3000,Azure text analytics accuracy in german is different from the demo case,"['azure', 'azure-logic-apps', 'sentiment-analysis', 'azure-cognitive-services', 'text-analytics-api']","I've created a website which communicates with my logic app while that uses the text analytics from azure. But my applications acts different from the demo case which you can find here: https://azure.microsoft.com/de-de/services/cognitive-services/text-analytics/ ! When you post for example: Hallo E-Bike Team,ich habe ein Problem mit meinem E-Bike. Es ist das Model ProRide2E.
  Seit heute Morgen steht auf dem Display folgender Hinweis:„Akkuleistung beeinträchtigt. Fehlercode: XB1200AB“Das darf doch wohl nicht wahr sein. Ich bin echt sauer. Wieso ist das
  doofe Bike immer kaputt?Ein genervter KundeNamewhich is very negative it responses with a sentimental score of 31% in the demo but in my app it responses with 50% which is clearly wrong because it is negative and it should be below 50%. I uses the same cognitive services as the demo but my accuracy is not similar to the demo. Is there any way to improve my accuracy?ps: I'm using the free subscription. Does the accuracy change if change that?"
3001,Urdu language dataset for aspect-based sentiment analysis,"['python', 'dataset', 'sentiment-analysis', 'aspect', 'urdu']",when i run my code i get this error this error because of what>
3002,how to import dictionary into FixedDictionaryStringToWordVector in weka?,"['java', 'weka', 'sentiment-analysis']",I create tf-idf in java using weka.api.I want to save features and then use this features on other text.this is my code to save features in dictionary:After that i use FixedDictionaryStringToWordVector to fit on text:But i gave this error:I want to know what is the problem?
3003,LSTM accuracy doesn't change no matter what I do,"['python', 'keras', 'classification', 'lstm', 'sentiment-analysis']","I'm implementing my first Neural Network, it being an LSTM for binary sentiment analysis classification. I've pre-processed the data with lowering the letters, tokenizing and removing most punctuation (keeping only .,').
I'm also using GloVe's 100d pre-trained embeddings for this.The problem is: Whatever I do the accuracy is terrible and doesn't change with epocs (also doesn't change when changing the LSTM architecture)I've tried changing the optimizer and its learning rate, adding more neurons to the LSTM, changing number of epochs and batch size.Nothing seems to work"
3004,"How can i pass one feature for the whole sentence,along with words of the sentence?","['python', 'keras', 'deep-learning', 'sentiment-analysis']","I am new to Keras and I want to know How can I pass one feature for the whole sentence, along with words of the sentence?Scenario:
I have a sentence and every word of the sentence is tagged with its polarity.
Eg. W1/POS W2/NEG W3/NEU W4/POS W5/NEG W6/NEUAlso, the overall polarity of the sentence is also given.
Eg. W1/POS W2/NEG W3/NEU W4/POS W5/NEG W6/NEU   (Polarity: POS)I am passing the words as one-hot vectors, as Input1. Also, I am passing the setiments of the words as Input2. I am concatenating them to get the combined output of Input1 and Input2, as Input 3. I want to pass the overall polarity of the sentence as Input4 and consequently merge it with Input3, to get Input5. Input5 will then be fed to a LSTM layer.My question is, how can I merge Input3 and Input4?"
3005,Not getting change in validation accuracy,"['neural-network', 'lstm', 'sentiment-analysis']",I am using nn for sentiment analysis and im getting good results but my validation accuracy and loss are not changing much even though training accuracy increases or decrease
3006,"sentiment analysis call fails with cognitive service, returning “HttpOperationError”","['sentiment-analysis', 'microsoft-cognitive']","Hi, with the sample code from API manual https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python-sdk#sentiment-analysis
 I got the following error while trying to call the sentiment classifier"
3007,Tweepy Not Displaying Tweets?,"['python', 'tweepy', 'sentiment-analysis']","I am using The Tweepy library to build a Twitter Sentiment Analysis tool, however when I try to print out tweets it's not accessing them, any clue why?  Could it be related to my Twitter Application I created?"
3008,"Python: Subset strings solely of the first bracket that opens up, but contains multiple","['python', 'text', 'sentiment-analysis']","Assuming the text text = """"""{ |p{3cm}|p{3cm}|p{3cm}|  } \hline \multi{3}{|c|}{City List} \ \hline Name ... """"""I would solely like to subset the content of the first curly brackets. 
So the desired output would be: desired_output = ""p{3cm}|p{3cm}|p{3cm}"" Currently I receive the content of all curly brakets of the lines "
3009,Insert a string based on differing occurence counts of strings in python,"['python', 'text', 'sentiment-analysis']","I hope someone can help me with the following. Assuming I have the following text (more precise - a code chunk from a latex table). Next, I would like to place the string inside of the text based on two input factors, inside of the text with one specifying the row based on \ and the other the column based on &.So in case of the the pair (1,1) the string should be placed before the first occurrence of  \ and the first occurrence of &. 
(1, 1, “HERE”, text) as input should return:And for 
(2, 2, “HERE”, text)  should return  Ideally the function would also take multiple pairs with inputs, so with 1,1 and 2,2 as input: should be the outcome. My current approach does not treats & and \ different. 
and HERE does not appear in front. "
3010,how can i automate my sentiment analysis in python,"['python', 'automation', 'sentiment-analysis']",I am done predicting my sentiment analysis using real-time tweets from twitter but I need to automate it so I don't need to run it again at every interval.How can i go about thisKindly assist
3011,Model trained with Keras for new data,"['python', 'keras', 'text-mining', 'sentiment-analysis', 'imdb']","In follwing a sample sentiment analysis using Keras,  How to predict sentiment analysis using Keras imdb dataset?, I want to see how it works on new data.With ""i love this movie"", it gives:With ""i hate this movie"", it gives:Am I right that ""i love this movie"" and ""i hate this movie"" are of the same kind? (either positive and negative) If not, how to use the model to tell if a comment is positive or negative?Thank you.Full code:"
3012,Aspect extraction from product online reviews,"['text', 'sentiment-analysis', 'feature-extraction', 'apriori']","I am trying to implement aspect extraction method from Mining and Summarizing Customer Reviews article.I trying to find all frequent features by Assocation rule mining(Apriori algorithm).First i applied preprocess function(POS-Tagging,Stop words,PorterStemmer) to sentences just like article.Then i extract noun/noun-phrases from every sentences to transaction file and save them in csv format like After that i apply apriori algorithm to this transaction file and frequent item sets are achieved like that:
In article there is compactness pruning phase:This method checks features that
contain at least two words, which we call feature phrases,
and remove those that are likely to be meaningless.
In association mining, the algorithm does not consider the
position of an item (or word) in a transaction (or a
sentence). However, in a natural language sentence, words
that appear together and in a specific order are more likely
to be meaningful phrases. Therefore, some of the frequent
feature phrases generated by association mining may not
be genuine features. The idea of compactness pruning is to
prune those candidate features whose words do not appear
together. We use distances among the words in a candidate
feature phrase (itemset) to do the pruning.Definition : 
• Let f be a frequent feature phrase and f contains n
words. Assume that a sentence s contains f and the
sequence of the words in f that appear in s is: w1, w2,
…, wn. If the word distance in s between any two
adjacent words (wi and wi+1) in the above sequence is
no greater than 3, then we say f is compact in s.
• If f occurs in m sentences in the review database, and
it is compact in at least 2 of the m sentences, then we
call f a compact feature phrase. I want to know how to implement pruning phase and how i can calculate distance between words in sentences.Also i want to know is it true to consider noun phrases in itemset like what i do?"
3013,Why am I getting String where should get a dict when using pycorenlp.StanfordCoreNLP.annotate?,"['stanford-nlp', 'sentiment-analysis', 'pycorenlp']","I'm running this example using pycorenlp Stanford Core NLP python wrapper, but the annotate function returns a string instead of a dict, so, when I iterate over it to get each sentence sentiment value I get the following error: ""string indices must be integers"".What could I do to get over it? Anyone could help me? Thanks in advance.
The code is below:"
3014,How to handle a “multi-argument returns are not permitted” error,"['r', 'sentiment-analysis']","I am trying to perform sentiment analysis on a range of csv file, with data from different years in them. I want to ultimately compare the sentiment of the date range.I am trying to create a function in R to apply the year to each files sentiment.I am expecting an output of the files positive and negative sentiment, along with its file name and Year (from the file name). Instead I get
Error in return(., sentiment) : multi-argument returns are not permitted"
3015,How can I add automate my script to generate daily tweets at real-time,"['python-3.x', 'sentiment-analysis']","The script below is what i use in scrapping tweets from twitter but I want to modify it to scrape daily tweets at real-time(every seconds) as against the manual input of start date.I would love to have real-time tweets for proper analysis purpose,this will aid my automation process and also easy accessibility"
3016,I have multiple questions regarding cleaning the data and sentiment analysis,"['r', 'sentiment-analysis']","I am doing my undergraduate research for my thesis, and I am dealing with twitter data from the twitter database. The file I downloaded is in CSV format. Sentiment Analysis is a key point for my thesis, so I found R language is excellent for this analysis.There are two questions I would like to ask.First is how to do clean the sentence (highlighted part) before the sentiment analysis. I know how to import the xlsx file to the R, but I don't know how to use the R after importing the data..
because there are many sentences, I would like to know how can I clean all the sentences in each highlighted cells..Second is after cleaning the data, I am going to use the sentimentR for the sentiment analysis. It is similar to the first question, but how can I do the sentiment analysis for each sentence in one time. Thank you so much for reading it."
3017,Sentiment Analysis using VADER not returning any results,"['csv', 'python-3.7', 'sentiment-analysis', 'vader']","I am trying to get sentiment (polarity score) using Vader sentiment analyzer. But my CSV file is returned as it is without any addition of polarity score.In my CSV file, each cell has a news article and I want to calculate polarity of each article.
There is no error but when I run the code it is returning the same CSV file as it is.
I don't understand the problem.
Thank you in advance."
3018,How to treat the sentences if there are positive and negative statement in the same sentence,"['python-3.x', 'sentiment-analysis']","I wanted to know how to tackle the problem if we have both positive and negative statements in the same sentence.
For example: I have sentence like ""The staffs were good, but the shared bathroom was so disgusting"". When we use SentimentIntenseAnalyser and calculate polarity_scores it gives compound value as -0.6059. But we cannot treat this statement as negative completely.What can be done in such case. Or do we have some other package to predict these kind of sentences accurately."
3019,How to Find Sentiment to Pair Noun Phrase in Sentiment Analysis Frequency-Based?,"['python', 'sentiment-analysis']","I have a Sentiment Analysis Frequency-Based Project. I have followed these steps from my professor:The next part is finding sentiment/opinion for my noun phrase. I haven't tried anything yet, because I'm still searching on internet. But as I write this, I haven't find them yet.These are the high frequency noun phrases (I use threshold = 2):These are some of the reviews:(Note: I... actually use textblob so I can get noun phrases more easier, but I still POS tag the review just in case I need it.)"
3020,PyTorch RNN-BiLSTM sentiment analysis low accuracy,"['python', 'pytorch', 'lstm', 'recurrent-neural-network', 'sentiment-analysis']","I'm using PyTorch with a training set of movie reviews each labeled positive or negative. Every review is truncated or padded to be 60 words and I have a batch size of 32.  This 60x32 Tensor is fed to an embedding layer with an embedding dim of 100 resulting in a 60x32x100 Tensor.  Then I use the unpadded lengths of each review to pack the embedding output, and feed that to a BiLSTM layer with hidden dim = 256.I then pad it back, apply a transformation (to try to get the last hidden state for the forward and backward directions) and feed the transformation to a Linear layer which is 512x1.  Here is my module, I pass the final output through a sigmoid not shown hereI've tried 3 different transformations to get the dimensions correct for the linear layerThese all produce an output like thisinput text size  torch.Size([60, 32])embedded size  torch.Size([60,32, 100])padded lstm out  torch.Size([36, 32, 512])attempt to get last hidden  torch.Size([32, 512])after linear  torch.Size([32, 1])I would expect the padded lstm out to be [60, 32, 512] but it is always less than 60 in the first dimension.I'm training for 10 epochs with optim.SGD and nn.BCEWithLogitsLoss().  My training accuracy is always around 52% and test accuracy is always at like 50%, so the model is doing no better than randomly guessing.  I'm sure that my data is being handled correctly in my tochtext.data.Dataset.  Am I forwarding my tensors along incorrectly?I have tried using batch_first=True in my lstm, packed_seq function, and pad_packed_seq function and that breaks my transformations before feeding to the linear layer.Update
I added the init_hidden method and have tried without the pack/pad sequence methods and still get the same results"
3021,Textblob misses to sentiment few small sentences,"['python-3.x', 'sentiment-analysis', 'textblob']","I've been trying to make a review sentiment analytic. I finishes it but when my review has a multiple sentences and one of them is a very small sentence, then textblob misses to give the polarity to that sentence ?
How can I rectify this or shall I use some module other than textblob ?"
3022,Aspect based sentiment analysis using R,"['r', 'sentiment-analysis']",I want to find sentiments of aspect or entity in R. My first step is to get the dependency tree and find noun and adjective. My question is how to find sentiment score of entity?  Here i am using Udpipe library in R. Below is code i used to get dependency tree. I am not sure how to proceed further with the code:Can you please help me with R code to find sentiment score of below entities?Thank you in advance for guiding me!!!
3023,Lexicon for Terrorism and Cyber Terrorism,"['machine-learning', 'sentiment-analysis', 'lexical']","I want to work on sentiment analysis on Terrorism. I will use tweets related to terrorism. 
I am looking for some good terrorism related Lexical, which can score my statement w.r.t terrorism domain.For example, if I use online tool -> https://app.monkeylearn.com/main/classifiers/cl_pi3C7JiL%EF%BB%BF/tab/demo/It classify these two sentences as negativeHere I am looking for some good terrorism based Lexicon which can classify better, at least for such easy statements. Thanks"
3024,How to decide the best Vectorizer and their optimal parameters that suits my dataset?,"['machine-learning', 'nlp', 'sentiment-analysis']","I am performing Sentiment Analysis on Lyrics and I am trying to figure out which is the best Vectorizer to use for my dataset.I am NOT looking for a general answer as which is better (ie TfIdf vs CountVectorizer, I understand the dataset determines that), rather than trying to figure out which is the process to determine which is better, depending on each dataset.Looking at this guide https://medium.com/@annabiancajones/sentiment-analysis-on-reviews-feature-extraction-and-logistic-regression-43a29635cc81annabiancajones is determining which is the most optimal vectorizer, using each one once with no parameters. Then she proceeds to figure out the parameters, one at a time.Is this a good idea?
Or brute forcing every vectorizer with every combination of parameters better? Is there even a point in this?
For example, using her analysis, she wants to find the best score, using: Should we use 3 nested for loops to determine the best parameters?"
3025,"Error when checking input: expected dense_1_input to have shape (1500,) but got array with shape (1,)","['machine-learning', 'deep-learning', 'sentiment-analysis']","i am getting an error only in the fitting part.
is there an issue with x_train and y-train? "
3026,Difference between pairwise_cor() and findAssocs() function,"['r', 'text-mining', 'sentiment-analysis']",What is the difference between pairwise_cor() and findAssocs() function in R?
3027,How to perform Binary Classification with many features using ML.Net,"['c#', 'sentiment-analysis', 'multiclass-classification', 'ml.net']","So I’ve been messing with ML.net and practicing using their examples and was wondering how I would train a model with the following input:int, string, int, string, int, string, int, string, bool (Label)and predict the binary label provided the other 8 features.The first 3 strings are selections that could be converted to a lookup, and the fourth string is random text that needs more of a sentiment approach.Any ideas on where to start / how to approach this? I’ve implemented using multiclass classification but I don’t think it makes sense since my output is binary."
3028,Sentiment analysis using Pattern python,"['python', 'python-2.7', 'sentiment-analysis']","For a project i'm working on I want to implement sentiment analysis to determine how positive/negative a certain string of text (written in Dutch) is. After some research i've chosen for Pattern, mainly because it supports the Dutch language.I'm using python version 2.7 and installed the package using pip install patternFirst I tried the following code:the result:This worked perfectly, next I tried the analysis:the result:This is not working as expected, am I doing something wrong with the sentiment? Or is something wrong with the package itself? It seems that only the sentiment is not working."
3029,How do I plot a bar graph of the bi gram tokens I obtained from Ngram tokenizer?,"['r', 'nlp', 'analytics', 'text-mining', 'sentiment-analysis']",Please find my codeNow I want to plot a bar graph for my bi-words
3030,Twitter sentiment analysis based on selected users,"['python', 'api', 'twitter', 'nltk', 'sentiment-analysis']","I'm in need of some advice with my Twitter sentiment analysis. 
I'm trying to do a pretty common sentiment analysis, but not on random tweets from Twitter search, but on the tweets of selected users. What I've tried so far, is that I read in this csv of the users. And then iterated over this list and then, user by user conducted this tweet analysis.
I'll put my write_tweets function here, just so it could get some feedback maybe :)Output would be a few indicators of sentiment, like positivity, negativity, neutral etc.
My question is, that maybe some of you has done this kind of thing already and can give me some recommendations about it? My version of it seems very slow and not very efficient (for me, at least). Thanks in advance"
3031,How to get Googlenews links for a custom query entered,"['web-scraping', 'nlp', 'python-3.6', 'sentiment-analysis', 'google-news']","I want to write a code in Python 3 that would search Google News with a different query every time.
All the resulting links are to be stored in a list and I plan to scrape those links further for sentiment analysis.
I am using the following GoogleNews package and the following code to get the links, but I am not getting any result. I am getting an empty list. enter image description here"
3032,Adding phrases to lexicon for sentiment analysis in r,"['machine-learning', 'nlp', 'sentiment-analysis', 'sentimentr']","I want to add phrases in my lexicon so to allow them to be either negative or positive. for e.g  ""so long"" as negative phrases in negative word list in lexicon. 
I am using polarity function from qdap package in r and added ""so long"" in negative list but it is not catching that phrases. There must be some other way of adding phrases"
3033,how to build and label a non english dataset for sentiment analysis,"['deep-learning', 'dataset', 'data-mining', 'sentiment-analysis']","lately I start a new project about sentiment analysis and I should build a dataset in persian language. while building a dataset is important for accuracy of whole process ,I want to do it as good as i can.what is the best way to build and label it in a short time?"
3034,Sentiment analysis on customer chat data in r,"['r', 'nlp', 'sentiment-analysis', 'qdap', 'sentimentr']","I am doing sentiment analysis in r on chat data using qdap package.
While chatting customer says they receive product in good condition which is positive but there are lots of chats again which greets the chat agent saying good morning or good afternoon which i don't want to classify as positive. I want following chats to be classified accordingly""good"" being a positive word in lexicon in default settings in qdap my 1st objective is attained but not the 2nd. I checked the help file of polarity and then updated dictionary by removing good from positive word list and added ""good condition"" hoping to achieve the objective but its not catching the phrase ""good condition"" as positive   "
3035,Extract hashtags from entities and add them to new column,"['python', 'pandas', 'dataframe', 'data-science', 'sentiment-analysis']",I created the following dataframe from a json tweets file.The current output of the hashtags column is the following:I'm trying to create a new column that only contains the hashtags. For example:NOJUSTICE and TrumpenceI tried the following code:and I received the following error:sample csv filei want the hashtags column to only contain the hashtags without the indices and other stuffUPDATE: i used the following suggestion and got the same error
3036,Sentiment Analysis:using a dataset (IMDB reviews) to train a neural-net and using it to predict entirely different datasets (Political articles),"['machine-learning', 'neural-network', 'nlp', 'sentiment-analysis']","We need to analyse a lot of articles relevant to political instability in a given country (things like the possibility of a coalition / a snap election etc). 
The problem is that I could not find any labeled datasets which could be plugged into a neural network (CNN/LSTM in TensorFlow) so as to supervise it for real-time events (news articles, tweets etc).I believe we can't use publicly available big datasets - like IMDB film reviews - for training the models to accurately identify and predict the occurrence of such events (or can we?).Are there other ways to solve this problem?I also thought of using unsupervised learning - libraries like VADER - but that gives me a more generic sentiment-score, rather than attuned to the specific corpora relevant to the problem."
3037,Why can't i get the text value using request.form in flask to perform sentimental analysis on the text?,"['python', 'html', 'flask', 'sentiment-analysis']","I am quite inexperienced in both HTML, JS and flask but I am working on a chatbot that able to detect sentimental analysis of the sender.My HTML code:This is my python-flask code:My js that links to the onlick =Console log:It seems really simple but it is like I miss something. Thank you so much!"
3038,Integrating Flask with Matplotlib failed,"['python', 'matplotlib', 'flask', 'tweepy', 'sentiment-analysis']",I am working on my twitter sentiment analyzer project. I am using matplotlib to plot the responses. I want to create an user friendly Interface Page which would take the input (topics or tags) from users and would send it to the analyzer which would analyze them and generate the graph in matplotlib. I succeeded giving inputs from the terminal. But now after integrating with flask it is not working.my main.pyMy app.pyNow I am not getting any response at all. Can anyone tell where am I wrong?
3039,How to get the matched values from an array to another array via (NODE.JS);,"['javascript', 'twitter', 'sentiment-analysis']","I'm new to JSCRIPT and I would like to try the twitter sentiment analysis. I have an array1 named ""disectToWords"" where the sentiments are located. Then, I have another array called ""getScoredWords"" where the ""AFINN.txt"" or ""the scores of words"" is located. I want to match both of arrays, to get the matched values. I'm trying to use filter(), however, I always got a null/empty array which means, all are false.but it should return the word 'abducted' and 'aboard'.
please help"
3040,How to count frequency of specific positive/negative words from a list in a .csv file with text and date values? in R,"['r', 'list', 'csv', 'count', 'sentiment-analysis']","I'm trying to get the sentiment from a document containing messages, the specific user and date. I have cleaned both documents so that the words contained in them are of a standard format and then I tried to count them but I seem to be able on to count them separately (after defining the word) but not with the use of a list of words. the file. raw is in the format: text,user_id, date and the positive/ negative lists are in the format: id,word_cz, polarityI can count the specific words like ""Okay"" with the function )but I can't seem to find a way to automate this process with the list of words The ideal results would add a column for each positive and negative counts to each rowThanks for the help"
3041,How to use predict_proba at SVM Sentiment Analysis,"['python', 'machine-learning', 'scikit-learn', 'svm', 'sentiment-analysis']","I applied predict_proba on classfication problem. I have some experience with building classification models in R but it's my first time with Python's sklearn.So the the problem is: In sklearn after fitting I can't find a way to access probabilities. Is it possible? There is a method predict_proba(), but...as the name suggests, it is prediction. This is my code:but I got this error :and this is the SkripsiPipeline code :I am new to Python Sklearn package. Can anyone tell me what is wrong with my Python code. I've googled it, but could not understand properly."
3042,Unable to load Sentihood dataset Json file in Python,"['python', 'json', 'python-3.x', 'nlp', 'sentiment-analysis']","Sentihood Dataset is a dataset for Target Aspect-based Sentiment Analysis. Its Test and Train file are available in Json format. However, when I try loading it using the json module of python, it gives the following error-JSONDecodeError: Expecting value: line 7 column 1 (char 6)Is there some other way of loading Json files? I don't have much knowledge of Json and hence would appreciate any help.
Link for Sentihood dataset : https://github.com/uclmr/jack/tree/master/data/sentihoodMy code is simply:"
3043,how to save twitter live streamed data into a well structured csv file,"['csv', 'dictionary', 'duplicates', 'tweepy', 'sentiment-analysis']","the below code will save data to a csv file.But the result is not structured as I expected.
it looks like this'{""tweet"": ""I saw a cop go by and I was like \u201ccrap he\u2019s gonna see me on my phone\u201d....then I realized I was sitting in Starbucks\u2026 , ""id"": 1170735792022376448}\n{""tweet"": ""RT @brainwxrms: imagine being a girl and having to keep an extra pair of underwear in ur car just in case some dude revs his truck up at th\u2026"", ""id"": 1170735792416858114}\n{""tweet"": ""RT @Im_TweetinKid: This is definitely a chick\u2019s car , ""id"": 1170735793402302464}\n{""tweet"": ""RT @monicafraga: Bom dia, meu povo, menos pro estagi\u00e1rio mau car\u00e1ter do G1.\n#EuConfioEmBolsonaro "", ""id"": 1170735793528279046}\n{""tweet"": ""\ud83d\udcfa As\u00ed es el collar m\u00e1s caro del mundo...... "", ""id"": 1170735793931005953}\n{""tweet"": ""RT @lemondefr: \u00ab Le point commun des aidants est d\u2019\u00eatre invisibles, pour tout le monde, et parfois pour eux-m\u00eames. Ils aident leur proche p\u2026"", ""id"": 1170735794086207490}\n{""tweet"": ""car just drove by listening to \u201cwithout me\u201d by eminem and totally changed the mood of my day"", ""id"": 1170735794656546816}\n{""tweet"": ""RT @ShuntaeS_: nobody : \n\Why the name tweet is being repeated in every tweet?. If i understand correctly there wont be any duplicate values in dictionary. Then why this happened?. Can someone please explain what I am doing wrong here. I expect the output in this formatthanks in advance"
3044,Keras deep learning accuracy 100% problem,"['python', 'keras', 'sentiment-analysis']","I'm going through a film classification with Keras.The file is tagged [film review, sentiment(emotion)].OUTPUT = always Accuracy 100%As a basic deep learning model, 
If you enable the imdb annotation, it will output normally.I don't think the Text preprocessing process is wrong.I downloaded the imdb.csv file directly, processed it, printed it and it shows the same graphIMDB_Graph, MYData_Graph (very strange MYData graph..)For reference, my Csv fileI've classified horror movies as scary or brutal films.[cruel, astonish] I've sorted it into tags.Can we sort it out frankly? I've been doubting it.I don't know if the data itself is wrong or the code is wrong.Do you have any idea what's wrong?Why is 100% accuracy always displayed?"
3045,Need helping performing sentiment analysis on customer reviews and for a string of text,"['python', 'nlp', 'sentiment-analysis']","This is a 2 part code question.1.) Need to perform sentiment analysis on a csv file for customer reviews.2.) Need to perform sentiment analysis on a harry potter book review saved as a .txt1.) The name of this Dataframe is ""reviews"" and what I want to do is display the sentiment score for each of these 5 reviews under the ""sent"" column.  Thank you so much!!! If you can provide the code with the ""sent"" column filled  with its sentiment analysis score for each row that would be awesome!!reviews.head()For this string I just to know what the overall sentiment score is... thanks!!2.) ""Parents need to know that Harry Potter and the Sorcerer's Stone is a thrill-a-minute story, the first in J.K. Rowling's Harry Potter series. It respects kids' intelligence and motivates them to tackle its greater length and complexity, play imaginative games, and try to solve its logic puzzles. It's the lightest in the series, but it still has some scary stuff for sensitive readers: a three-headed dog, an attacking troll, a violent life-size chess board, a hooded figure over a dead and bleeding unicorn, as well as a discussion of how Harry's parents died years ago."""
3046,Sentiment analysis - flair pretrained model classifier. How to speed up,"['python', 'machine-learning', 'sentiment-analysis']","I want to classify emotions with pretrained flair english model from flair library. I have about 90 000 tweets and I want to classify everything. Problem is that flair library doing that in about 7 hours.
To compare NLP sentiment classifier or TextBlob can do this in 1 minute...My code to that problem is:"
3047,How can I extract tweets through tweet IDs?,"['python', 'pandas', 'csv', 'tweepy', 'sentiment-analysis']","I am trying to make a .txt file of certain tweets from the 2010 NFL. I want to write each tweet in a separate line. For the tweets that are unavailable/deleted, I want to have ""--"" in the corresponding line. When I tried the code below, I could only get 218 tweets written into my .txt file, all the other lines were ""--"". (I am new to coding so please excuse the bad code!)"
3048,why svm get normal results on imbalanced dataset?,"['nlp', 'svm', 'sentiment-analysis', 'imbalanced-data']","I read in many articles that svm is not perform good on imbalanced dataset.but in my case svm get better result than svm+oversample method.i want to know what is the problem?I working on binary classification sentiment analysis.number of positive sentences is 365 and number of negative sentences is 135.thus imbalance ratio approximately
is 1:3.first i run data cleaning on sentences.then i make training train set and test set with following code:Then I vectorize them with the vocabulary of extracted features:After that I pass this values to following functions:The results are here.I want to know what is going on."
3049,Python Twitter Sentiment Analysis Not Loading showing [] only,"['python', 'python-3.x', 'nlp', 'sentiment-analysis']","I am working on a sentiment analysis project using twitter. My code works by pulling the first 100 tweets when search a topic but stops working when I try to turn tweets into a dataframe to analyze. How do I turn the JSON tweets to a dataframe to analyze?I expected it to get the text of the tweets but it only outputs the bracket. 
I would like to get text in the tweets, load into a dataframe to categorize and analyze."
3050,What are the best ways to improve a sentiment analysis classifier on Bing News Data?,"['sentiment-analysis', 'bing-news-search-api']","I am solving problem of sentiment analysis and i am trying various techniques to improve my results. Previous, i applied different preprocessing steps such as change the text to lowercase, removing non ASCII chars, numbers removing, and applied stemming and stopword removal, then did some feature selection technique to generate vectors and then classified using SVM.But I only get 62% accuracy score. How to improve the accuracy score?"
3051,What is a good method to extract text spans of interest?,"['machine-learning', 'nlp', 'classification', 'sentiment-analysis']","I want to pursue aspect-based sentiment analysis with customer reviews. Normal methods such as (noun-) chunking do not help me on my data and I need to find the text spans / sequences in which a certain category is addressed.
I have annotated customer reviews. We annotated spans of words in the reviews that represent aspect classes. E.g.: ""The technician was  and ""  The brackets '<>' represent annotations of classes such as 'friendliness of the craftsman'.I tried several things like rules, POS-patterns, BOW, IO/B-tagging with a variety of models (CRF, ...). The current task is not suitable for IOB-tagging, as the borders of the spans oftentimes vary and I do not care whehter one word is missing as long as the span got covered. Having different starting points comes from human annotations, but also from the data itself, e.g. 'He was ' and 'He was ' can be two examples in which it is right to have different starting points. Another example can be that human annotators included the verb ('was') and sometimes, they just didn't. 
In general, my issue is comparable to the proposal of explosion.ai which was not yet realized.Example data (translated from German):
""Dr. Müller was very friendly [friendliness] and competent [comptence] when he treated our daughter.""
""He took a lot of time [time consumption] and listened to us [friendliness].""
In the first sentence, it may happen that the phrase to be annotated is 'very friendly', just 'friendly' or something else such as 'extremely friendly and helpful'. Such differing phrases may not be covered by rules or IOB-tags. I would love to get methods / algorithms / libraries for identifying text spans of interest. Later on , I can classify them. But for now, I would be happy to just have a way to extract spans that may represent one class."
3052,better and easy way to find who spoke top 10 anger words from conversation text,"['r', 'sentiment-analysis', 'grepl', 'tidytext', 'sentimentr']","I have a dataframe that contains variable 'AgentID', 'Type', 'Date', and 'Text' and a subset is as follows:First, I found out the top 10 anger words using the following:And the I created a vector of top 10 anger words as follows:Next what I wanted to do is to find which were the 'Agent'(s) who spoke these words frequently and rank them. But I am confused how we could do that? Should I search the words one by one and group all by agents or is there some other better way. What I am looking at as a result, something like as follows:"
3053,"This python code with regex successfully remove URL but if URL found in the beginning of tweets, all of the sentence will be remove as well","['python', 'regex', 'twitter', 'sentiment-analysis']","I need to remove any URL in the tweets review. How to only remove the URL if it is found in the beginning of tweet?I've try some code and this python code with regex successfully remove URL but if URL found in the beginning of tweets, all of the sentence will be remove as well.If URL found in the beginning of tweets, all of the sentence will be remove as well."
3054,Can a group of 3 researchers share/pool Twitter API tokens to accelerate/improve data collection on a sentiment analysis project?,"['twitter', 'sentiment-analysis', 'sttwitterapi', 'twitterapi-python']","Our group is working on a sentiment analysis research project. We are trying to use the Twitter API to collect tweets. Out aimed dataset involves a lot of query terms and filters. However, since each of us has a developer account, we were wondering if we can pool API access tokens to accelerate the data collection. For example, we will make an app that allows us to define a configuration file that contains a list of our access tokens that the app will try to use to search for a tweet. This app will be run on our local computer. Since the app uses our individual access tokens, we believe that we are not actually not bypassing or changing any Twitter limit as the record is kept for each access token. Are there any problems legal/technical that may arise from this methodology? Thank you! =D Here is a pseudocode for what we are trying to do: The question is, are we actually breaking any Twitter rules or policies by doing this? By sharing one access token per each of us three and creating an app that we name as clones of the research project, we believe that in turn we are also losing something which is the headroom for one more app that we fully control. I can't find specific rule in Twitter so far about this. Our concern is that we will publish a paper and will publish the app we will program and use for documentation and the app we plan to build. Disclaimer: Only the app's source code will be published and not the dataset because of Twitter's explicit rules about datasets."
3055,Is there a reason why the following code does not execute (print the tweets) after taking input?,"['python-3.x', 'twitter-oauth', 'tweepy', 'sentiment-analysis', 'textblob']","So basically I want to print a set number of tweets related to a topic that user enters but when I run the following code after giving in the input nothing happens, I see no output after that. I would be really grateful if you could tell me why :-)I tried regenerating the access token keys and then again copy pasting it but the problem still persiststhis is what my console is showing after execution(It does not print the tweets)"
3056,Text analytics client library for .NET throws an exception when used in an Azure Function,"['c#', 'azure-functions', 'sentiment-analysis', 'azure-sdk-.net']",I'm trying to implement the Azure Text analysis API SDK for .NET (Microsoft.Azure.CognitiveServices.Language.TextAnalytics 4.0.0) in a service bus queue triggered Azure function but throws an exception when instantiating the TextAnalyticsclient object. mscorlib: Exception while executing function: Function1. mscorlib: Attempted to access an element as a type incompatible with the array.Same code works in a console application.The code works fine in a console application.After trying to understand the source code I found out that TextAnalyticsClient tries to set a header key/value and the value is not valid http header value or something close to that as its occurring at TextAnalyticsClient.Initialize()
3057,How do you edit a function in R to process rows of Twitter data?,"['r', 'function', 'sentiment-analysis', 'vader', 'sentimentr']","I am trying to use nrguimaraes's VADER sentiment tool on R to get the sentiment scores of several tweets. 
The description of the tool and how to install it is described here:
https://rdrr.io/github/nrguimaraes/sentimentSetsR/man/getVaderRuleBasedSentiment.htmlHowever, when I try to process more than one element, it comes out with an error. I would like to edit the function so that it is able to handle several elements. The function is: So when I run The error comes out as I'm wondering how I can edit this function to be able to handle data that has several rows of texts, such as when you collect tweets using Rtweet."
3058,Working with document term matrix in xgboost,"['r', 'xgboost', 'sentiment-analysis', 'tm']","I am working on sentiment analysis in r. i've done making a model with naive bayes. but, i wanna try another one, which is xgboost. then, i got a problem when tried to make xgboost model because don't know what to do with my document term matrix in xgboost. Can anyone give me a solution?i've tried to convert the document term matrix data to data frame. but it doesn't seem to work.the code below describes how my current train & test dataand i have xgboost template with another data set :i want to use the xgboost template above to make my model using my current train & test data. what i have to do?"
3059,Using dictionaries for sentiment analysis in PySpark,"['python', 'pyspark', 'bigdata', 'pyspark-sql', 'sentiment-analysis']","At the very beginning I would like to say that I am new at programming. I spent a lot of time transforming my data set but then I stuck. The goal is to make a sentiment analysis for a time period of 2011-2019 in PySpark.What I want to do is to check whether there is a negative or positive sentiment for the statement in column Body. This data is stored in one data frame. To get proper sentiment analysis I will use the Loughran-McDonald Sentiment Word List - as the text within Body will contain some (or many) finance jargon. The dictionary with words and assigned sentiment is stored in a second data frame. Each of the data frames (one with column: 'Body' and second with the LM dictionary) contains thousands of rows (ca 80 ths. each). To do the sentiment analysis, I have to iterate through each row in the first data frame by column Body using the words from the second data frame --> looking whether particular word exist in the sentence stored in column 'Body'. Having in mind that there may be both negative and positive words in one sentence, let's assume that one 'negative' word equals -1 and one positive word in a sentence equals +1. The final result (sum of n(-1)/(+1)p words) will be stored in new column in the first data frame. For example - if a particular row in Body contains the word abandon, which is tagged negative (in the second df the number not equal 0 (in this case 2009) means that the word is assigned to particular column of sentiment - in this case: negative) the result in new column should be -1. Hope that I described my problem in an understandable way. Despite days spent on looking for a solution on SO I haven't found any answer matching my problem :( I will be grateful for any tips.Current first data frame:Second data frame (Loughran-McDonald dictionary):"
3060,How to process data from twitter crawler to artificial neural network,"['python', 'neural-network', 'nlp', 'web-crawler', 'sentiment-analysis']","I'm studying sentiment analysis in python using deep learning (Artificial Neural Network) and have difficulties, and sorry if my question is difficult to understand.So, I have a backend for sentiment analysis and a Twitter crawler, but I have a hard time processing lots of data from Twitter. I have successfully classified it, but only one sentence is classified as positive/negative. The output is as you can see at the screenshot link. https://imgur.com/krh4pfYhere's my code:ann.this is from crawler:What I need is after crawling (1000 raw data) the data is directly processed at once to get the sentiment polarity analysis (percentage) on the neural network.anyone can help?"
3061,how to tackle the problem of contrast words in sentiment analysis (lexicon-based approach),"['python', 'nlp', 'nltk', 'sentiment-analysis']","I want to build a function to tackle contrast words in sentiment analysis of an informal text. The text is in Malay language. Any idea how to tackle this issue?""the food is good and tasty but it is too expensive"" - negative sentimentHowever, the system detects the sentence to be positive since the quantity of positive words (good, tasty) is higher than negative words (expensive).so what i did is, a valence shifter for contrast words, I made a list of contrast words, then if there is contrast word in the system it will shift the polarity from positive to negative and vice versa.This is the code that I had tried.I don't know whether my concept is wrong or right, it seems too simple and accuracy is a bit low, any other way?"
3062,Google Cloud Natural Language API (with Python): Entity Sentiment Score & Magnitude always 0,"['python', 'google-cloud-platform', 'nlp', 'entity', 'sentiment-analysis']","I am trying to use google.cloud entity sentiment analysis. I have managed to set up everything; entity analysis and sentiment analysis are working. However, if I try to conduct an entity-sentiment analysis, it always returns the value 0 (for score & magnitude), regardless of the text I analyse.It currently returns:========================================I hope to change the code so that the sentiment scores and magnitudes are rendered, rather than always displaying 0s. Thanks for your help!"
3063,How to evaluate text data with only Y variable?,"['python', 'sentiment-analysis', 'ml.net-model-builder']","Team,As I am working on sentimental data where raw text is only variable that I have.Based on data, I applied classifier and vader algorithm for sentiment score. Now I want evaluate model for accuracy. But I don't have X variable to build model using test and train"
3064,Sentiment analysis on reviews using NLTK in Python,"['python', 'nlp', 'nltk', 'logistic-regression', 'sentiment-analysis']","I have a csv data file containing column 'notes' with satisfaction answers in Hebrew. I would like to use Sentiment analysis in order to assign a score for each word or bigrm in the data and receive positive/negative probability using logistic regression.My code so far:This loop isn't working.
How can I generate the data and then receive positive/negative probabilities for each bigrm?"
3065,Bag-of-words model with python,"['python', 'model', 'text-mining', 'sentiment-analysis']","I am trying to do a sentimental analysis with python on a bunch of txt documents.
I did so far the preprocessing and extracted only the important words from the text, e.g. I deleted stop-words, the punctuation. Also I created a kind of bag-of-words counting the term frequency. The next step would be to implement a corresponding model. I am not experienced in machine learning resp. text mining. I am also uncertain about the way I created the bag-of-words model. Could you please have a look at my code and tell me if I am on the right track. I would also like to know if my previous path is a good basis for a model and how do I build on that basis a good model in order to categorize my documents.This is my code:This is how my bow-model looks like:
"
3066,How do I print tweets from twitter?,"['python', 'web-scraping', 'twitter', 'sentiment-analysis']",I am trying to scrape tweets from twitter for a side project. Having difficulty with outputs.Using latest version of pycharm. I do not receive any errors at all but there no outputs for the tweets.
3067,How to seperate sentences in a dataframe based on last occurence of small letter followed by a capital one,"['python', 'pandas', 'dataframe', 'text', 'sentiment-analysis']",I have a dataframe containing sentences. The first sentence (the title) is followed by the text. They were merged without a space. I would like to slit the text into two parts (sentence 1 and sentence 2) based on the last occurence of a capital letter following a lowercase letter without a space in between (out of curiosity I would also be interested in a solution based on the first appearance). The solution is supposed to be stored in the original dataframe. I tried but could not work it out. As the split is supposed to be carried out at the last occurrence. The words RnB and BestMusic in the example given are not supposed to trigger the split. 
3068,How to test Stanford Sentiment model?,"['java', 'machine-learning', 'nlp', 'stanford-nlp', 'sentiment-analysis']","I know the following command can be used to train a Stanford sentiment modelNow I want to how to test the model with the testing dataset.
I tried using -trainPath option, it didn't seem to work. I didn't find anything neither on the official documentation nor on the web."
3069,How can I do Sentiment Analysis using Weka on a PHP web?,"['php', 'machine-learning', 'weka', 'sentiment-analysis']","I'm about to create a PHP website for Sentiment Analysis, and I wanted to use Weka for that. Is it possible? How can I do that?"
3070,How to check feature importances on text feature?,"['python', 'machine-learning', 'scikit-learn', 'classification', 'sentiment-analysis']","First of all, I'm still doing study about classifier comparison on sentiment analysis. Then, I want to know for every feature importances on each classifier.I've already tried model.feature_importances_, but because I vectorize my data train, I can't understand what word is on those feature importances.It shows "
3071,VADER polarity_scores returning output as “Neutral” in most cases,"['python', 'nlp', 'sentiment-analysis', 'vader']","In most cases, I am finding that polarity_scores returning output as ""Neutral"" whereas there should be some % of negative and positive sentiments highlighted e.g. consider the following cases, I found  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} for all the 3 cases mentioned below.Code:Output:I expect some % of negative and positive sentiments highlighted taking above examples versus getting 'neu' = 1.0 and 'compound' = 0.0.Can anyone advise how to get better results matching to the actual sentiment of the given text string? I am willing to explore other libraries or packages if they are better than Vader.Thanks for advising."
3072,How to update the sentiment scores for some words in textblob?,"['python', 'sentiment-analysis', 'textblob']","I want to change the lexicon for Textblob by adding several new words with scores, and by slightly adjusting the score of the words that are already there. What is the best way to approach this?In Vader sentiment it's done like this:Is there a similar command for Textblob?"
3073,Google Dialogflow: sentiment analysis results not present for google assistant,"['dialogflow', 'sentiment-analysis', 'google-assistant-sdk']","I am working on a dialogflow chatbot, I am on the ""essential enterprise"" plan and therefore I have access to sentiment analysis which works fine from the simulator panel on the left of dialogflow console, however when I switch to testing my bot on google assistant, dialogflow fulfillment code stops extracting sentiment score from the request sent by dialogflow. Looking into the request sent in both cases, google assistant and plain dialogflow, I see that ""sentiment analysis"" results are in fact missing from the request object once you switch to testing on google assistant. The two request objects are as the following:Here is the first dialogflow request, not using google assistant (notice queryTextSentiment node near the bottom )and here is the request dialogflow sends when I test from google assistant, and dialogflow doesn't set any sentiment analysis results in this case:Does anyone have an explanation for that?"
3074,Creating Self Learning Sentiment Dictionary,"['machine-learning', 'nlp', 'artificial-intelligence', 'sentiment-analysis']","I am planning to build a self-learning dictionary of sentiment word with their sentiment label.I am able to identify the sentiment words by using POS tags but not able to label those words as positive, negative or neutral.For example: ""The food was not good"" is the sentence, and I have extracted ""not good"" from the sentence as sentiment word by using the POS tag. Now I want to label this as negative and add it to my new dictionary for future use.my preference to do this project/task is by not using any pre-defined dictionary/word bank/any pre-defined sentiment analysis package.I am seeking your views to know the way to label it without using any pre-defined dictionary or with pre-defined dictionary.Currently, I have explored Word embedding, Skip through n-gram model for this. I have also used a pre-defined dictionary to train the model by using some supervised learning model like Xgboost, KNN,  Naive Bayes classifier. I have used some unsupervised model like k-mean to predict the label by using the words. 
Still not able to get the results.If You know any other way or some input to apply with any of above-used models to label word as positive, negative or neutral then please suggest."
3075,Iterate over DateTimeIndex to get overall sentiment per day,"['python', 'pandas', 'sentiment-analysis']","I am trying to iterate a pandas dataframe, where I have dateTimeIndex as my index, twitter text and sentiment for each tweet added (so three columns). I'm new to python and looking to find the overall sentiment per day (so there will only be one day per row, with the overall sentiment. Would I be right in using 'groupby' here. Is there an efficient way this can be done?"
3076,How do I check each word of a string with each word of other string?,"['python-3.x', 'pandas', 'nlp', 'sentiment-analysis']","
This is what I have as reference and I want to check my list with this list and score then based on the sum of POS and NEG 
I'm new to python so this might be easy. I'm calculating sentiment score of strings with reference to dataset which has list of words with +ve and -ve score. So i want to add +ve score and subtract -ve score if I find the word from  test set in reference dataset. For that I would have to access each word from test string and check with reference dataset.
This is my code:"
3077,incorrect result of fasttext model,"['pyspark', 'sentiment-analysis', 'fasttext']","I created a fasttext model to do comments sentiments analysis , I used a train_File with 51% positive comments, 47% negative and 2% neutral.
When I want to test with some given sentences always results are divided between: 0.49 .. positive, 0.47 .. negative, 0.02 neutral, even if I type a single negative word.
I have the following code : I get always a result around 0,4.. positif , 0,4... negatif , 0.0.. neutral : Can someone tell me where the mistake lies"
3078,How to fix “no package called textdata” error?,"['r', 'sentiment-analysis', 'tidytext']","I am trying to run sentiment analysis in R. I have installed tidytext and it is in the correct library with all other packages. However, when I run I get the following error: Any suggestions on how to fix?"
3079,"Calculating precision, recall, and F-measure for Logistic Regression classifier","['python-3.x', 'sentiment-analysis', 'precision-recall']","I have a labeled and clean dataset for sentiment analysis, and I used logistic regression for classification. Here is my code. when I try to calculate precision, recall, and F-measure:I got an error: Can anyone tell what's the problem here? Thanks in Advance"
3080,Error in UseMethod(“type”) : no applicable method for 'type' applied to an object of class “factor” - Sentiment Analysis,"['r', 'sentiment-analysis']","I am a total newbie when it comes to R and want to conduct a sentiment analysis for my term paper, relying on the code of my instructor. However, she used another dictionary so I have to adapt my code which is where the trouble starts.I am trying to create a variable that scores the occurrence of positive terms. When I run the loop however, I get the error: I have already searched the internet and read that my data could probably be stored in the wrong format. However, both my data sets (nss2018 is the data I want to conduct the analysis on; posterms contains the positive words of the dictionary) are stored as a list which is the same data type my instructor used.Since I usually don't work with R, I'm a little desperate and can't make any sense of this. My code for creating the variable:"
3081,Package ‘Rstem’ is not available (for R version 3.5.1),"['r', 'rstudio', 'data-science', 'text-mining', 'sentiment-analysis']","I am trying to install the Rstem package, but I am getting the message that there is no version available for the version of R 3.5.1. I'm using the macOs El Captain.The error is:I already tried the suggested options in this link issues in installing Rstem package and also downloading the package locally from the official website Rstem Package, but the result is also unsatisfactory.I'm studying how to do an sentiment analysis with Twitter data. I would like to know if there is any alternative to this package or if there is any trick to install it."
3082,Is there a way to classify Vader compound scores into emotion levels/categories?,"['python', 'sentiment-analysis', 'vader']","I have been trying to search for a scale or classification metric to assign some emotional degree to VADER sentiment analysis beyond just positive, negative or neutral. I would really appreciate if someone can share their view or a resource to help classify VADER compound score along the following lines:"
3083,Error in check_input(x) In R Sentiment Analysis,"['r', 'nlp', 'text-mining', 'sentiment-analysis']","This keeps appearing in my code after writing unnest_tokens: Error in check_input(x) : 
    Input must be a character vector of any length or a list of character
    vectors, each of which has a length of 1."
3084,Stock prediction + news sentiment with SVM in R?,"['r', 'machine-learning', 'svm', 'prediction', 'sentiment-analysis']","I would like to predict the stock prices and news sentiment score together with SVM in R, in order to see whether news have an impact on stock price and their prediction. I read that support vector machines (svm) are a good machine learning approach for this problem. I have one column that represents the date of the stock and news, one column represents the stock prices on that day and 4 columns which represent the sentiment scores based on different lexica. I would like to test first with one of that lexica and if the models works, trying on the other. The dataset is included below. I found some examples with python but couldn't found something for R. I like to use the svm() function from the e1071 packageI split the data into train and test set:And I tried already this SVM code, but my wrong prediction rate is 100What I am doing wrong here? Hope somebody can help me! Dataset"
3085,How to speed up my Python apply function across a DataFrame,"['python', 'dataframe', 'apply', 'sentiment-analysis']","I have a rather large data set and I am trying to calculate the sentiment across each document. I am using Vader to calculate the sentiment with the following code, but this process takes over 6 hours to run. I am looking for any way to speed up this process.Any thoughts would be great because looping through the rows like this is terribly inefficient.As an example, I have run my code on a mini sample of 100 observations. The results from the alternative forms of code are below. My original code is first, the suggested change to a list comprehension is second. It seems strange that there is no increase in performance between the two methods.Wall time: 4min 11sWall time: 3min 59s"
3086,How to match asset price data from a csv file to another csv file with relevant news by date,"['python', 'csv', 'sentiment-analysis', 'finance']","I am researching the impact of news article sentiment related to a financial instrument and its potenatial effect on its instruments's price. I have tried to get the timestamp of each news item, truncate it to minute data (ie remove second and microsecond components) and get the base shareprice of an instrument at that time, and at several itervals after that time, in our case t+2. However, program created twoM to the file, but does not return any calculated price changesPreviously, I used Reuters Eikon and its functions to conduct the research, described in the article below.https://developers.refinitiv.com/article/introduction-news-sentiment-analysis-eikon-data-apis-python-exampleHowever, instead of using data available from Eikon, I would like to use my own csv news file with my own price data from another csv file. I am trying to match the However, the programm is not able to return the twoM price change values"
3087,Twitter sentiment analysis on a string,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'sentiment-analysis']","I've written a program that takes a twitter data that contains tweets and labels (0 for neutral sentiment and 1 for negative sentiment) and predicts which category the tweet belongs to.
The program works well on the training and test Set. However I'm having problem in applying prediction function with  a string. I'm not sure how to do that.I have tried cleaning the string the way I cleaned the dataset before calling the predict function but the values returned are in wrong shape."
3088,How can we do sentiment analysis of remote stream in an agora app?,"['video', 'sentiment-analysis', 'agora.io']",We would like to know how we can do sentiment analysis of remote video stream. We are using the agora basic video call/ one to one video as the starting point. We searched extensively for the solution but we did not get what we are looking for.<label>Sentiment</label>The goal is to do sentiment analysis of the remote stream periodically and update the label.
3089,Add a feature using a pipeline and FeatureUnion,"['python', 'scikit-learn', 'pipeline', 'sentiment-analysis', 'feature-extraction']","In the code below I use a tweeter dataset to perform sentiment analysis. I use  a pipeline which performs the following processes:1) performs some basic text preprocessing2) vectorizes the tweet text3) adds an extra feature ( text length)4) classificationI would like to add one more feature which is the scaled number of followers. I wrote a function that takes as an input the whole dataframe (df) and returns a new dataframe with scaled number of followers. However, I am finding it challenging to add this process on the pipeline e.g. add this feature to the other features using the sklearn pipeline. Any help or advise on this problem will be much appreciated.the question and code below is inspired by Ryan's post:pipelines"
3090,Stock price prediction based on financial news in R with SVM,"['r', 'machine-learning', 'svm', 'prediction', 'sentiment-analysis']","I'm new in R and tryining to predict the S&P500 stock price based on financial news with the help of support vector machines (svm). I have 2 datasets. One is the stock market data and the other the cleaned financial news corpus data. I converted the corpus into a Document Term Matrix and also applied sentiment analysis on it (once with SentimentAnalysis Package and once with tidytext package). And now I'm desperate to get this model running. I've found different approaches on how to use svm to predict the stock price, but nowhere with financial news. Or how can I combine the two data sets to create the model? My current code and actual situation is this:That is my status quo. Now I would like to run the svm model based on my two dataset described above. But I think I need to do some classification before. I have seen they worked with (-1 / +1) or something like that. My sentiment analysis provided me terms into positive and negative classes. But I just don't know how to put both sets together to build the model. I would be very happy if somebody could help me please! Thanks so much in advance!"
3091,Error of TfidfVectorizer on cleaned text dataset,"['python', 'data-mining', 'sentiment-analysis', 'tfidfvectorizer']","I am trying to vectorize a sentiment data set. It has review text and sentimentlabel given. When I try to vectorize the data set It gives an error called 'LazyCorpusLoader' object is not iterable The reviews were cleaned as follows.After these my dataframe reviewdataset_df has following columns: then I split the data set using below code,That worked well. Then I use vectorizer using following code.This gives an error as 
return frozenset(stop)
TypeError: 'LazyCorpusLoader' object is not iterable I searched and tried on some solutions which didn't worked. How to overcome this error. I need to vectorize the data set to train for a recommendation system. note: I searched through internet and read similar question in stackoverflow but couldn't find a proper answer."
3092,How to get news feed out of Bloomberg API regarding a particular security(equity) and date range?,"['python', 'python-3.x', 'nlp', 'sentiment-analysis', 'bloomberg']","I'm working on a project that requires I source news articles from the Bloomberg API regarding a particular security ( say Netflix) within a specific date range. I want to do this in Python and get the news articles in a structure (JSON/XML) format. I believe this can be done using EDTF(Event-Driven Trading Feed) using the Bloomberg Terminal, but I want to do this using the Bloomberg API.I need these news articles to perform a sentiment-analysis on the articles.I read the answer to this question: Scrape News feed from Bloomberg Terminal I understand I do have access to EDTF feed but don't know how to get the feed out programmatically in Python as there is really little to no documentation around it. If I could use PDBLP (https://matthewgilbert.github.io/pdblp/api.html), it would be even greater!Please link some documentation, code examples as to how to go about this problem. If you've worked on a similar project on Bloomberg, it would be great if you could share some code examples. Thank you!"
3093,sentimentr 0 instead of NA,"['r', 'sentiment-analysis']","Function sentiment from 'sentimentr' package returns 0 for NAs, however, I need NAs.I'm using 'sentimentr' package for the Sentiment Analysis on a large dataset. Some values are, of course, missing. However, for NAs the function 'sentiment' returns 0. The problem is that sometimes sentiment is indeed 0, but sometimes is just NA. I need to differentiate for future analysis. I've tried to use na.rm = TRUE and also na.action=NULL, but this did not work. I still get an output with 0s.  Ideally, I would like NA to stay NA after the sentiment analysis and if the sentiment is really 0, then 0. It should be easy, but it's not working for me. I will be grateful for any help!"
3094,Extract top positive and negative features when applying dictionary in quanteda,"['r', 'dictionary', 'sentiment-analysis', 'quanteda']","I have a data frame with around 100k rows that contain textual data. Using the quanteda package, I apply sentiment analysis (Lexicoder dictionary) to eventually calculate a sentiment score. 
For an additional - more qualitative - step of analysis I would like extract the top features (i.e. negative/positive words from the dictionary that occur most frequent in my data) to examine whether the discourse is driven by particular words.However, going through the quanteda documentation, I couldn't figure out how to achieve this - is there a way? 
I'm aware of topfeatures and I did read this question, but it didn't help. "
3095,How to determine which words have high predictive power in Sentiment Analysis?,"['twitter', 'nlp', 'sentiment-analysis', 'tf-idf', 'feature-selection']","I am working on a classification problem with Tweeter data. User labeled tweets (relevant, not relevant) are used to train a machine learning classifier to predict if an unseen tweet is relevant or not to the user. I use a simple preprocessing techniques like removal of stopwords, stemming etc and a sklearn Tfidfvectorizer to convert the words into numbers before feeding them into a classifier e.g. SVM, kernel SVM , Naïve Bayes. I would like to determine which words (features) have the higher predictive power. What is the best way to do so?I have tried wordcloud but it just shows the words with highest frequency in the sample.UPDATE:The following approach along with sklearns feature_selection seem to provide the best answer so far to my problem:top features Any other suggestions?"
3096,How to categorize positive and negative features from top features,"['python-3.x', 'nlp', 'sentiment-analysis']",I have trained user reviews thru average tfidf wor2vec model and got top features. Would like to tag top features as positive & negative.Could you please suggest. 
3097,Trying to do sentiment analysis with “afinn” dictionary,"['r', 'sentiment-analysis']","I am trying to sentiment analysis in my project and want to use different dictionaries. Here I am trying to use ""afinn"" but I am getting this error. I am able to use ""bing"" dictionary but not others."
3098,TypeError: object of type 'float' has no len() for Sentimental Analysis,"['python', 'artificial-intelligence', 'sentiment-analysis']",Trying some Sentimental Analysis on some csv files and keep getting an error I'm not sure how to fixI tried changing in the loop type.I expended it to clean the questions of stop words but it gave me the error:TypeError: object of type 'float' has no len() for Sentimental Analysis
3099,How to make a loop with IMAP in Python to keep getting the latest email for sentiment analysis,"['python', 'python-3.x', 'email', 'imap', 'sentiment-analysis']","Basically I am making a code that saves emails locally as .txt files then apply sentiment analysis on it to find angry customers, my current code saves ""TODAY'S"" emails locally then do the analysis but what I want to do is just run the script once and let it loop and get the latest email, so every time a new email is received it will apply sentiment analysis on it, otherwise if the latest email is still the same it does nothing.Here is the part of the code that gets today's emails:"
3100,How to use naive bayes classifier after Extract the features using TF_IDF,"['python', 'classification', 'nltk', 'sentiment-analysis', 'naivebayes']","I'm Trying to classify features using Naive Bayes classifier, I used TF_IDF for feature extraction.The finaltfidfVector is a list of vectors, each vector represents list of numbers, 0 if the word not found, else the weight of word if it found.  And classlabels contains all class label for each vector. I'm trying to classify it with this code but it doesn't work. 26652 lines for DatasetThe output :The used reference for TF_IDF and applied on finaltfidfVector https://triton.ml/blog/tf-idf-from-scratch?fbclid=IwAR3UlCToGYFEQSmugXo3M5Q9fcld79JfXSfBaDG7wKv5a49O0ZDEft9DFNg. 
data setthis is sample about the used data set before preprocessing and TF_IDFThis is sample for the first vector for index of zero in finaltfidfVector listclasslabels contains class label for each vector 
, 1 for sarcasm 0 for not sarcasm. The class label of index 0 is 1, this 1 for the first vector in finaltfidfVector.The first item for train_set is ({(0.0, 0.0, 1.3803652294655615,.....ect): '0'}, ""former versace store clerk sues over secret 'black code' for minority shoppers"")"
3101,Sentiment Analysis in R using TDM/DTM,"['r', 'text-mining', 'data-analysis', 'sentiment-analysis', 'sentimentr']","I am trying to apply a sentiment analysis in R with the help of my DTM (document term matrix) or TDM (term document matrix). I could not find any similar topic in the forum and on google. Thus, I created a corpus and from that corpus I generated a dtm/tdm in R. My next step would be to apply the sentiment analysis which I need later for stock prediction via SVM. My give code is that:I read that it is possible through the tidytext package with the help of the get_sentiments() function. But it was not possible to apply that with a DTM/TDM. How can I run a sentiment analysis for my cleaned filter words which are already stemmed, tokenized etc.? I saw that a lot of people did the sentiment analysis for a hole sentence, but I would like to apply it for my single words in order to see if they are positive, negative, score etc. Many thanks in advance!"
3102,How to apply coreNlp(Stanford NLP) on a tweets column in python?,"['python', 'nlp', 'stanford-nlp', 'sentiment-analysis', 'pycorenlp']",my code is :-so here my data frame is DataSet_new and my tweet column is tweet_stemming i want to use nlp on this column and i want the result in Stanford score column like (sentiment value and sentiment)can anyone help me?
3103,Why do I get a TypeError when importing a textfile line by line for sentiment analysis instead of using a sentence hard-coded?,"['python', 'stanford-nlp', 'sentiment-analysis', 'pycorenlp']","I am trying to analyze the sentiment of each given sentence from a text file line by line. The code is working whenever I am using the hard coded sentences from the first question linked. When I use the text file input, I get the TypeError.This is related to the question asked here. And the line by line from text file code is coming from this question:The first one works, the second with the text-file (""I love you. I hate him. You are nice. He is dumb"") does not work. Here is the code :I get this error:line 21, in s[""index""],TypeError: list indices must be integers or slices, not str"
3104,How do I train model in Google NLP Sentiment Analysis correctly,"['google-app-engine', 'nlp', 'sentiment-analysis', 'natural-language-processing']","I need to compare to sentiment models trained with different types of content. Google supplies you with a training dataset filled with tweets in a .csv file, As expected training with this went well however, when I decided to train a model using the Stanford NLP's dataset of IMDB reviews, I manage to upload the dataset without issue but when I train it the NLP, for some reason only predicts that the sentiment value is 2, regardless of what I write.  I figured that the dataset was diluted since, while there were 800-2000 examples of sentiment 0,1,3 and 4, there were 6000 examples of sentiment 2. Although after removing 4000 of these examples, the problem persisted. I'm expecting my confusion matrix to not simply only have 100% prediction on each sentiment value. It should be distributed over the matrix w"
3105,Downloading historical Tweets via rtweet from Premium API via R,"['r', 'twitter', 'sentiment-analysis', 'rtweet']","I need to extract the Tweets with ""#bitcoin"" from 2013-04-28 until today from Twitter to do a sentiment analysis.For this I have access to Twitter's Premium API.I am using the rtweet package which offers the function search_fullarchive.My questions:
1. How can I ensure to download e.g. 50,000 Tweets from every day in the intervall from 2013-04-28 until today?Would this function download 50,000 Tweets from every day between 2013-04-28 until 2019-06-02 (note: I have to look up the environment name, df_tweets should be an empty data.frame):Is there an estimate possible how long this download will take?Thank you very much."
3106,Stopwords coming up in most influential words,"['python', 'nlp', 'nltk', 'sentiment-analysis']","I am running some NLP code, trying to find the most influential (positively or negatively) words in a survey. My problem is that, while I successfully add some extra stopwords to the NLTK stopwords file, they keep coming up as influential words later on.So, I have a dataframe, first column contains scores, second column contains comments.I add extra stopwords:I check that they are added, using the len method before and after.I create this function to remove punctuation and stopwords from my comments:I run the model (not going to include the whole code since it doesn't make a difference):...And then to get the most influential words:But, Cat and Dog are in the results.What am I doing wrong, any ideas?Thank you very much!"
3107,How to fix json.decoder.JSONDecodeError when I use googletrans API?,"['python', 'pandas', 'google-translate', 'sentiment-analysis', 'vader']","I am trying to translate a series of tweet from Italian into English. They are contained in a csv file so I extract them with pandas to compute the sentiment with Vader. Unfortunately, I get this error json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0).I have tried both to remove the emoji from the tweet and to use a vpn as indicated on some other posts but it doesn't work."
3108,Negativity score for sentences,"['nlp', 'sentiment-analysis', 'linguistics']","I am working on a dataset of airline customer complaints. Since it is ""complaints"" the general consensus is all the sentence are ""negative"" sentiment. So I am think of an approach to quantize the negativity score.For example:Less Negative review:High Negative Review:Any suggestions on existing approaches? P.S I am not looking for an exact answer, any suggestions on the direction or approaches would be great."
3109,Adding Sentiments with the same date in R,"['r', 'sentiment-analysis']","I can not find a code to add sentiments for the same dates.To explain my problem further:I have a data frame with different articles and dates. I did a Sentiment Analysis for the articles, but some of these got published on the same date.
I need the sentiment for the days and not for each article and i would like to know how to do that.
For example:I appreciate any help you can provide.Thank you in advance."
3110,Python beginner : Preprocessing a french text in python and calculate the polarity with a lexicon,"['pandas', 'nlp', 'nltk', 'sentiment-analysis', 'treetagger']","I am writing an algorithm in python which processes a column of sentences and then gives the polarity (positive or negative) of each cell of my column of sentences. The script uses a list of negative and positive word from the NRC emotion lexicon (French version) I am having a problem writing the preprocess function. I have already written the count function and the polarity function but since I have some difficulty writing the preprocess function, I am not really sure if those functions works. The positive and negative words were in the same file (lexicon) but I export positive and negztive words separately because I did not know how to use the lexicon as it was. My function count occurrence of positive and negative does not work and I do not know why it Always sends me 0. I Added positive word in each sentence so  the should appear in the dataframe:stacktrace :This my csv_data : line 44 , 45  contains positive words and line 47 more negative word but in the column of positive and negative word , it is alwaqys empty, the function does not return the number of words  and the final column is Always positive whereas the last sentence is negativeHere the full code :If you could also see if the rest of the code is good to thank you."
3111,Generating Tables and Diagrams for Sentiment Analysis Using R,"['r', 'ggplot2', 'sentiment-analysis']","Got a task in my work to simply analyse 30 reviews for 3 different products using R. So, I used the library sentimentr to do this and generate the web page with the good and bad words highlighted green and red respectively. That's fine. However, it also says on my sheet to:
""Generate Tables, Diagrams using R""
but I'm completely at a loss on what to even plot for this and how to plot for stuff like this.The CSV file I'm using simply has 2 columns.
there are 30 rows, all divided by 10 for 3 products, so there's ID (1,2,3)Don't know if this will help anything or not.I've tried googling stuff like ""ggplot for seniment analysis"" but nothing helpful came up for me."
3112,Beginner Python for sentiment analysis : AttributeError: 'list' object has no attribute,"['python', 'class', 'nlp', 'sentiment-analysis', 'self']","I am trying to create a sentiment analysis program using a sentiment lexicon. The sentences that will be analyzed are read from a CSV file, and after analyzed, it will be written again in a different CSV file. However, I got the AttributeError: 'list' object has no attribute 'lower' error. The error seems to appear from this part of the code. Here is the full code :Stacktrace looks like this the lexicon looks like this  : the csv looks like this but it is much longer : "
3113,Add validation data into training data,"['python', 'pandas', 'csv', 'dataframe', 'sentiment-analysis']","I already have split data: 60% for training, 20% for testing and 20% for validation.The validation data have 2 parts, each part have 1821 data and 1913 data.
How can I add 200 each parts into the training data?this csv training data, 
this csv validation data "
3114,TypeError: string indices must be integers (Text Data Preprocessing in CSV files for Sentiment Analysis),"['python', 'string', 'integer', 'sentiment-analysis', 'indices']","I'm kind of new to programming and NLP in general. I've found some code on this website :(https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed) to use for sentiment analysis on twitter. I have the csv files i need and so instead of building them i just defined the variables by the files.When i try to run the code it's giving me a type error when running this line:preprocessedTrainingSet = tweetProcessor.processTweets(trainingData) And traces back to the line: processedTweets.append((self._processTweet(tweet[""text""]),tweet[""label""])). I don't know how to circumvent the issue and still keep core functionality of the code intact.I expect it to start cleaning the data I've found before I can start using Naive Bayes"
3115,"micro macro and weighted average all have the same precision, recall, f1-score","['python', 'sentiment-analysis']","I've been using different machine learning classifiers to conduct a sentiment analysis based on positive, neutral and negative sentiments. when trying to see the classification metrics of a classifier whilst using Sklearns classification report, the micro macro and weighted average all have the same precision, recall, f1-score. why could this be happening?the code to print the classification report is:the results can be seen here"
3116,word mapping for 2D word embedding,"['vector', 'sentiment-analysis', 'word', 'natural-language-processing']","For my Masters Thesis, I created a Word2Vec model. I wanted to show this image to clarify the result. But how does the mapping works to display the words in this 2D space?All words are represented by a vector of 300 dim. How are they mapped on this 2D image? What are the x & y axes?Code:"
3117,how to add confusion matrix and k-fold 10 fold in sentiment analysis,"['python', 'scikit-learn', 'cross-validation', 'sentiment-analysis', 'confusion-matrix']","I want to add an evaluation model using the cross-validation and confusion matrix k-fold (k = 10) method, but I'm confused
dataset : https://github.com/fadholifh/dats/blob/master/cpas.txtUsing Pyhon 3.7the result is confusion matrix and for k-fold each fold has a percentage of F1-score, precission, and recall"
3118,Is the a way of getting the degree of positiveness or negativeness when using Logistic Regression for sentiment analysis,"['machine-learning', 'deep-learning', 'logistic-regression', 'sentiment-analysis', 'python-3.7']","I have been following an example about Sentiment Analysis using Logistic Regression, in which prediction result only gives a 1 or 0 to give positive or negative sentiment respectively.My challenge is that i want to classify a given user input into one of the four classes (very good, good, average, poor) but my prediction result every time is 1 or 0.Below is my code sample so farI want to get some values between -0 to 1, like when you use Vader SentimentIntensityAnalyzer's polarity_scores. Here is a code sample of what i want to achieve using SentimentIntensityAnalyzer's polarity_scores."
3119,How to do sentiment analysis using the textblob in python(Pandas),"['python', 'sentiment-analysis', 'textblob']","I have a data frame called Comments_Final which has one column - ""Comments""this column has different reviews like for example1.Fit good fast shipping2.Product as described and functioned perfectly.3.this product doesn't fit my Remington rm1415 it is way to long and much larger chain..... looks like it would be a pain to return to Canada to sender4.Would have given it 5 stars but it is not a sealed battery5.Was not told I needed to sign to receive item missed delivery, made contact with carrier , then received item next day!6.Quick delivery. Part as expectedso first i want a column called sentiment which will show that review is negative or positive?and second i want third column as emotion that will tell you that review defines anger, sad,joy,disgust etc. emotions in that."
3120,Anomaly in sentiment analysis done by IBM Watson Discovery,"['sentiment-analysis', 'watson-discovery']","When I give a positive comment document, Discovery is correctly categorising it as positive sentiment document.But when I give a document with both positive and negative comments, Discovery is categorising it as positive sentiment document.Again, when I give a negative comment document (Eg: not skilled), Discovery is categorising it as neutral sentiment document.Is there any settings or configuration in Discovery by which this anomaly can be resolved?"
3121,Unsupervised Machine Learning vs sentimentR,"['machine-learning', 'text', 'sentiment-analysis', 'sentimentr']","I am currently using sentimentR to label political texts (150-400 words) and so far it's not doing too well (accuracy is around 50-low 60% depending how generous I am when reading the texts). Someone recommended that I turn to unsupervised machine learning (suggesting I try GP and/or neural networks). I don't want to reinvent the wheel here so I'm wondering, conceptually,  what the differences are between using a ML approach vs sentimentR? Would one usually expect better results from the former and, if so, how much better (i.e. accuracy rating of 60-70%)?Thanks"
3122,Text sentiment scoring returning <function sentiment_value at …> instead of int score,"['python', 'sentiment-analysis', 'tweets']","I am trying to add a column to my twitter dataframe with a sentiment score.  I have tried the below code but I keep getting an output that isn't a 1, 0, or -1 int. as I am expecting Then I did a little test to see if this worked, which it seemed to: Then I tried to apply the new sentiment_value function to the column I want to scoreWhen I call the new column, I was expecting to get a sentiment score, for example: But what I actually get is: I am very new to this and appreciate any help anyone can offer!"
3123,Sentiment Analysis of twitter data using hadoop and pig,"['hadoop', 'apache-pig', 'sentiment-analysis']","Tweets from twitter are stored in hdfs in hadoop.
The tweets need to be processed for sentiment analysis. The tweets in hdfs are  in avro format so they need to be processed using Json loader But in pig scripting the tweets from hdfs are not getting read.After changing jar files the pig script is showing failed messageBy using these following jar files by pig script is getting failed.REGISTER '/home/cloudera/Desktop/elephant-bird-hadoop-compat-4.17.jar';REGISTER '/home/cloudera/Desktop/elephant-bird-pig-4.17.jar';REGISTER '/home/cloudera/Desktop/json-simple-3.1.0.jar';These are another set of jar files with which its not failing but data is also not getting read.REGISTER '/home/cloudera/Desktop/elephant-bird-hadoop-compat-4.17.jar';REGISTER '/home/cloudera/Desktop/elephant-bird-pig-4.17.jar';REGISTER '/home/cloudera/Desktop/json-simple-1.1.jar';Here is all my pig scripting commands i have used:Error on dumping above tweets command for the first set of jar files:Input(s):
  Failed to read data from ""/user/cloudera/OutputData/tweets""Output(s):
  Failed to produce result in ""hdfs://quickstart.cloudera:8020/tmp/temp-1614543351/tmp37889715""Error on dumping above tweets command for the second set of jar files:Input(s):
  Successfully read 0 records (5178477 bytes) from: ""/user/cloudera/OutputData/tweets""Output(s):
  Successfully stored 0 records in: ""hdfs://quickstart.cloudera:8020/tmp/temp-1614543351/tmp479037703""Expected output was sorted positive and neative tweets but getting errors.
Please do help. Thank you."
3124,How to detect text language of a hindi string written in english font using R?,"['r', 'sentiment-analysis', 'data-cleaning', 'data-processing']","I am cleaning the text data using R 
I have tried 'textcat' library and its giving this output.x<-""My Name is Mohit""y<-""Mera Naam Mohit Hai""textcat(x)[1] ""middle_frisian""textcat(y)[1] ""sanskrit"""
3125,How do you create Bag-of-Words feature vector after applying GloVe embedding?,"['machine-learning', 'nlp', 'sentiment-analysis', 'libsvm', 'glove']","If I have two movie reviews:And I apply GloVe embedding to them I will get two vectors, with multiple word vectors inside them that look like this:Basically each word in the review will be converted into a 300 element array of float point numbers. Since the arrays are of variable length I can't just plug them into a classifier.I thought about doing some kind of Bag-of-Words representation, but I am not sure how I would implement that now that the words have become numbers."
3126,Issues with streaming tweets using tweepy and Sentiment analysis,"['python', 'machine-learning', 'tweepy', 'sentiment-analysis', 'textblob']","I'm a beginner Python programmer I am finding it hard to figure out a simple Tweepy Streaming api.Basically I am trying to do the below.Stream tweets in Portuguese language.Show the sentiment of each tweets.I am unable to stream language tweets.
Could someone please help me in figuring out what is it that I am doing wrong.The example o/p is However it stops after few tweets and I get this errorAnd also the tweets are not in Portuguese.
How can I stream continuously and also get tweets which are in portuguese and perform a Sentiment analysisCould you folks please also guide me on how to even stream language tweets and then analyze the sentiment using textblob.Thank you"
3127,How to convert text to integer list as in imdb.pkl,"['python', 'sentiment-analysis', 'tflearn', 'imdb']","I have trained a model for sentiment analysis on imdb movie review dataset. Now I wish to test it on a custom input i.e. some string for example, ""Hello"".
But I have loaded train and test from 'imdb.pkl' file which returns already preprocessed text which is in tuple of list of list of integers format. I read about this, they say the words are assigned with integers. So my question is how do I convert my custom input (or encode the string) to that format so that I would be able to use it using model.predict(custom_input)?"
3128,how can I speed up my sentiment analysis?,"['python', 'nlp', 'classification', 'sentiment-analysis', 'textblob']",I'm doing sentiment analysis for several languages. my code runs successfully but it's extremely slow (10mn for just 11K records). Here is my code:I've checked other posts about textblob.sentiments import NaiveBayesAnalyzer being very slow but I don't think it's the same situation i'm facing here?Thank you
3129,How to loop through the dataframe after droping some rows from the dataframe?,"['python', 'pandas', 'loops', 'sentiment-analysis']",I have a dataframe from which i have deleted some rows. But the problem i have been facing is when i try to loop through dataframe on the basis of index values it gives me 'key error!' due to some indices are missing from the dataframe. How to loop through the dataframe?
3130,Newbie: Python “AttributeError: 'NoneType' object has no attribute 'text' ” when scraping Tripadvisor Reviews,"['python', 'web-scraping', 'sentiment-analysis', 'tripadvisor']","I am trying to scrape some Tripadvisor reviews as a complete newbie to this.I'm using code from Susanli2016.It worked (though, removing the attribute ""language"") for one link but it doesn't work for any more link (for example.)I'm receiving the error: I'm attaching the code here with the changes I made in case someone can help me. Thank you so much!
Silvia--I substituted the original:with With the original code I get the error
    ValueError: invalid literal for int() with base 10: '5.695'(where 5.695 is the number of reviews in the page)--Hereby the complete code:"
3131,Is there a dataset available similar to AFINN (Each word is score between -5 and +5 based on the sentiment) in French?,"['python', 'r', 'sentiment-analysis']",I am working on a sentimental analysis of a french dataset in both R and Python. I know there is a dataset for english like AFINN where each word is rated for sentiment. I am looking for something similar for French where I could see a dataset which gives you numerical score for each of the wordsBelow is a simple query in R to get the sore of AFINNPlease let me know if there is any datasource available for French words.Thanks
3132,I need twitter dataset for last 3-4 months relating to any company/ comodity for stock price prediction,"['sentiment-analysis', 'stock']","I need twitter dataset for last 3-4 months relating to any company/ commodity for performing sentiment analysis and thereby stock price prediction.
But the twitter API only goes back upto 10-12 days.The code that I've prepared works well but I need more dataset to reach a reliable conclusion.I can't wait for 3-4 months since I've to submit the project soon.
If anyone knows any link where I can find some dataset or if anyone has it,
Please let me know.
Thank you in advance"
3133,OSError tsv not found,"['python-3.x', 'sentiment-analysis']","I am taking this error:
OSError: \C:\Users\u165127\Desktop\Restaurant_Reviews.tsv not found.I am so confused!"
3134,German dataset for training text classifier,['sentiment-analysis'],"I am looking for an annotated dataset in German similar to the well-known English IMDB movie review dataset (here).
The background is that I would like to categorize German texts into multiple categories (starting with positive sentiment / negative / neutral).
I have not found German word embeddings pre-trained with sentiment analysis, neither have I found a suitable dataset to train my own word embeddings with.
Any advice would be appreciated!"
3135,How does “sentimentr” package split a paragraph or sentences into more than 1 sentences?,"['r', 'sentiment-analysis', 'sentimentr']","I am trying to run sentiment analysis in r using ""sentimentr"" package. I fed in a list of comments and in the output got element_id,    sentence_id,    word_count, sentiment. Comments with long phrases are getting converted into single sentences. I want to know the logic based on which package does that ?I have 4 main categories for my comments- Food, Atmosphere, Price and service. and I have also set bigrams for those themes, i am trying to split sentences based on themes For e.g - "" We had a large party of about 25, so some issues were understandable. But the servers seemed totally overwhelmed. There are so many issues I cannot even begin to explain. Simply stated food took over an hour to be served, it was overcooked when it arrived, my son had a steak that was charred, manager came to table said they were now out of steak, I could go on and on. We were very disappointed"" got split up into 5 sentences 1)  We had a large party of about 25, so some issues were understandable
2) But the servers seemed totally overwhelmed.
3) There are so many issues I cannot even begin to explain.
4) Simply stated food took over an hour to be served, it was overcooked when it arrived, my son had a steak that was charred, manager came to table said they were now out of steak, I could go on and on. 
5) We were very disappointedI want to know if there is any semantic logic behind the splitting or it's just based on full stops?"
3136,"TypeError: __init__() got an unexpected keyword argument 'n_folds',sentiment_analysis_with_SVM","['python-3.x', 'scikit-learn', 'svm', 'sentiment-analysis', 'sklearn-pandas']","I am trying to implement svm for sentiment analysis, i trying to implement this gitlink https://github.com/jatinwarade/Sentiment-analysis-using-SVM/blob/master/SVM.ipynb.i refered this as it says to change cross origin to model_selection since it is depricated Error: __init__() got an unexpected keyword argument 'n_splits'
so I replaced with thisThis Returns Error: Please Help me solve this error"
3137,Cleaning \u2764\ufe0f \u2026 data in file with python,"['python', 'regex', 'sentiment-analysis', 'preprocessor', 'data-cleaning']","I try to cleaning data twitter in python with regex, but i can't remove \u2764\ufe0f \u2026. twitter data is in the datas.txt file, this is the data:Berkat biznet aku bisa online terimakasih BiznetHome \u2764\ufe0f
  Gangguan hari sabtu perbaikan nanti senin  hari offline Slow respon \u2764\ufe0f Terima kasih TelkomCare masalah indihome sy sudah terselesaikan terima kasih fast responnya terus selalu tingka\u2026 TelkomCare Sudah beres fix internet dan telpon berfungsi normal thanks atas respons dan perbaikan pihak Indihom\u2026I have tried three ways :
FirstSecondthirdbut still not work"
3138,Obtaining word polarity in each review,"['keras', 'nlp', 'sentiment-analysis']","I'm working on a domain-specific sentiment analysis, and I want to get each independent word polarity in that specific corpus (not a general score like ""SentiWordNet"" or other lexicons).At first I thought using the following formula would help:but then I found some issues regarding to this solution:So basically my inputs are reviews and their polarities and I need a lexicon containing words and their polarities."
3139,KeyError: 'irrelevant' In dictionary while sentiment analysis using Naive and SVM,"['python-3.x', 'nltk', 'sentiment-analysis', 'naivebayes']","I am doing Sentiment analysis using Naive bays and SVM, 
i refered this code https://github.com/sravyaysk/MyProjects/blob/master/SentimentAnalysis.pyI converted this implementation in python 3, Here is code to implement it assigns positive as 1 and negative as 2, I added neutral as 3 but it still return's error as irrelevant My code's and dataset
https://drive.google.com/drive/folders/1NiZC7DNOkzpUFZ1X6PjD2rrn99TGXnkL?usp=sharing this is the part of code where I am getting error: SVM.pyERROR:"
3140,How to Plot Multiple Line chart Using Pandas of Sentiment analysis data stored in csv,"['python-3.x', 'pandas', 'numpy', 'matplotlib', 'sentiment-analysis']","I have data set After doing Sentiment analysis which has  1st column(date) and 2nd column(sentiment)  Here is the DataSet : https://drive.google.com/file/d/1jlmuzFi9OS3mBWjgQvQuKGdNzan708R6/view?usp=sharingI want to Plot 3 graph having positive, negative and neutral as follows on x-axis  date  and on y-axis no of positive/neg/neutral somewhat like this any suggestion would useful thanks
"
3141,Sentimental Analysis of text,"['python', 'sentiment-analysis', 'naivebayes']","I have a code for performing sentimental analysis using Naive Bayes.
I have three files where in When I try executing them,Sometimes it shows a weird output like negative o/p for positive statements and positive o/p for negative statements.
Here is my code.classifier.pypreprocess.pymain.pyI will run this code in Anaconda Spyder.
Please help me to sort out this code for exact output.
Thank you."
3142,Sentiment analysis using twitter,"['python-3.x', 'twitter', 'sentiment-analysis']","i m having some problem while fetching the data from twitter for 10 companies , i am fetching 10 thousands records from twitter  for each company and performing sentiment analysis on that but while fetching records it is getting stuck , and then it stops !i am fetching  data performing sentiment analysis and creating seperate files and then merging into one file ,but code is getting stuck while fetching data for first company itself"
3143,Balanced sample with defined n in R,"['r', 'r-caret', 'sentiment-analysis', 'downsampling']",I have an imbalanced dataset for sentiment analysis with about 65000 observations (~60000 positive and ~5000 negatives). This dataset should be balanced so that I have the same number of positive and negative observations to train my machine learning algorithms. The package caret and the function downSample help me to get ~5000 negative and ~5000 positive observations (downsampling to minority class). But I like to have exactly 2500 randomly selected positive and 2500 randomly selected negative observations. Is there anyone who knows how to do this?
3144,RNN LSTM Sentiment analysis model with low accuracy,"['keras', 'lstm', 'recurrent-neural-network', 'sentiment-analysis']","I have a dataset with 200000 samples.
I am using the train_test_split from Sklearn.I got a low accuracy = 0.39.Can I know what I am doing wrong here?"
3145,Python TextBlob translate issue,"['python', 'nltk', 'sentiment-analysis', 'textblob']","I am doing a quick sentiment analysis console application with Python, TextBlob and NLTK.Currently i am using a link to a wiki article in spanish, so i don't need to translate it and i can use the nltk spanish stopword list, but what if i wanted to make this code work for different language links?If i use the line TextFinal=TextFinal.translate(to=""es"") below textFinal=TextBlob(texto)  (code below) i get an error since it can't translate spanish into spanish. Could i prevent this just by using a try/catch? Is there a way to make the code try to translate to different languages (as well as using different stopword list) depending on the language of the links im feeding to the application?"
3146,Sentiment analysis with predefined text,"['python', 'nltk', 'sentiment-analysis']","I'm working on a sentiment analysis project in Python using  NLTK. The output of the project must show whether the given statement is positive or negative. I have succeeded in doing that, but how can I obtain an output for a neutral statement?
And is it possible to output in the form of percentages (i.e., positive %, negative %, or neutral %)?classifier.pypreprocess.py"
3147,Which API is similar to Twitter streaming API from which i can fetch the data from flume and do sentiment on that data?,"['sentiment-analysis', 'twitter-streaming-api']",tell me the  names of such API's where I can practice my sentiment skills
3148,yellowbrick visualiser.fit() raises ValueError,"['python', 'visualization', 'sentiment-analysis', 'yellowbrick']",I am trying you Visualise a dispersion plot for my twitter datadatasetThis is the code I'm trying to pass a list like this in the above code stored in ht_negative_unnestThe error raised
3149,How to skip over np.nan while iterating through a dataframe for sentiment analysis,"['python-3.x', 'pandas', 'boolean', 'nan', 'sentiment-analysis']","I have a data frame with 201279 entries, the last column is labeled ""text"" with customer reviews. The problem is that most of them are missing values, and come up as NaN. I read some interesting information from this question:
Python numpy.nan and logical functions: wrong resultsand I tried applying it to my problem:I tried experimentingby doing this:
    df['firstName'][202360]== np.nanwhich returns False but indeed that index contains an np.nan.So I looked for an answer, read through the question I linked, and saw that is a true statement. I thought, okay, I can run with this.So, here's my code so far:Then I would just convert aList with the sentiment to pd.DataFrame and join it to df1, then impute the missing values with K-nearest neighbors. My problem is that the little routine I made throws a value errorSo I'm not really sure what else to try. Thanks in advance!EDIT:  I have tried this:which correctly populates the list with NaN.But this gives me a different error:AttributeError: 'float' object has no attribute 'translate'Which doesn't make sense, since if it is not NaN, then it contains text, right? "
3150,Error when installing package ‘openNLPmodels.en’ in R / RStudio. Error: cannot open URL 'http://datacube.wu.ac.at/src/contrib/PACKAGES.rds',"['r', 'sentiment-analysis', 'opennlp']","When I try to install package ‘openNLPmodels.en’ in RStudio I get the following error messages:
I run the latest versions of RStudio an R. openNLP and openNLPdata are correctly installed. Can you help me to install the package? Thank you very much in advance."
3151,SyntaxError: Non-ASCII character '\xa3' in file when function returns '£',"['python', 'unicode', 'python-unicode']","Say I have a function:I want to print some stuff with a pound sign in front of it and it prints an error when I try to run this program, this error message is displayed:Can anyone inform me how I can include a pound sign in my return function? I'm basically using it in a class and it's within the '__str__' part that the pound sign is included."
3152,Scikit Learn ValueError: Found array with dim 3. Estimator expected <= 2,"['machine-learning', 'sentiment-analysis']","I am having a training data set of 144 student feedback with 72 positive and 72 negative feedback  respectively. The data set has two attributes namely data and target which contain the sentence and the sentiment(positive or negative) respectively. The testing data set contains 106 unlabeled feedback.
Consider the following code:I do not know what is wrong. Please help."
3153,CountVectorizer Error: ValueError: setting an array element with a sequence,"['machine-learning', 'sentiment-analysis']","I am having a data set of 144 student feedback with 72 positive and 72 negative feedback respectively. The data set has two attributes namely data and target which contain the sentence and the sentiment(positive or negative) respectively.
Consider the following code:I do not know what is wrong. Please help"
3154,Design of a Neural Network for Emotion Classification using Tweet Data,"['neural-network', 'sentiment-analysis', 'feature-extraction']","I have a dataset of four emotion labelled tweets (anger, joy, fear, sadness). For instance, I transformed tweets to a vector similar to the following input vector for anger:Mean of frequency distribution to anger tokensword2vec similarity to angerMean of anger in emotion lexiconMean of anger in hashtag lexiconIs that vector valid to train a neural network?"
3155,Sentiment Analysis- Low number of comments,"['python', 'sentiment-analysis']","I am running a Sentiment Analysis from Twitter. My code is running without any issue but there is one thing that got my attention. When I run my script for ""Stack OverFlow"", it shows only 15 comments in total. I don't think this is possible(only 15 comments related to Stack OverFlow in Twitter).
I am not sure if I have done something wrong in the logic.****My Results: Sentiment(polarity=0.125, subjectivity=0.8)
Positive: 7
Negative: 1
Neutral: 7****"
3156,Cannot update VADER lexicon,"['nlp', 'nltk', 'sentiment-analysis', 'natural-language-processing', 'vader']","print(news['title'][5])
Magnitude 7.5 quake hits Peru-Ecuador border region - The Hinduprint(analyser.polarity_scores(news['title'][5]))
{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}Positive: []
Neutral: ['Magnitude', '7.5', 'quake', 'hits', 'Peru-Ecuador', 'border', 'region', '-', 'The', 'Hindu']
Negative: []Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}Positive: []
Neutral: ['Magnitude', '7.5', 'quake', 'hits', 'Peru-Ecuador', 'border', 'region', '-', 'The', 'Hindu']
Negative: []Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
3157,R: sentiment analysis using Azure Cognitive Service Text API and HTTR package,"['r', 'azure', 'sentiment-analysis', 'microsoft-cognitive', 'httr']","I have problem with connection to the Azure Cognitive Service Text API (here is documentation:) using HTTR.How shall I implement ""text"" into the body of the API query?"
3158,How do you count a negative or positive word prior to a specific word - Sentiment Analysis in Python?,"['python', 'nlp', 'sentiment-analysis']","I'm trying to count how many times a negative word from a list appears before a specific word. For example, ""This terrible laptop."" The specified word being ""laptop"", I want the output to have ""Terrible 1"" in Python."
3159,Train multiple class in sklearn,"['python', 'scikit-learn', 'sentiment-analysis']",i have data frame like this picture.i'm looking a way to train this data set so i tried it with sklearn with this codebut i got error like thisactually i don't know whether it's okay to train my data with its 4 labels directly
3160,"ValueError: Expected 2D array, got 1D array instead. Training model","['python', 'scikit-learn', 'sentiment-analysis']","i want to train 12 models for my sentiment analysis since i have 4 aspects and 3 polarities (positive, neutral, negative). but i got error like thisi updated my code in here to make it more clear,
here is my code:is there anything wrong with the way i make the model? originally this code was made for making 8 models. so, instead of "
3161,Improve Microsoft cognitive services sentiment analysis?,"['sentiment-analysis', 'microsoft-cognitive', 'text-analytics-api']","I find sentiment analysis to be frequently off, completely off, of what the sentiment actually should be. Here are some examples,etc. many other such examples!Questions,But saying just ""count to ten"" should be not as positive.How far can I take Microsoft cognitive services sentiment analysis? Are there better/other options? Thanks!"
3162,How to create a word2vector model from imdb dataset and get it's featuremap using CNN,"['python', 'nltk', 'word2vec', 'sentiment-analysis']","I'm a newbie to python, i need to complete a project on IMDB Review sentiment analysis. I did not quiet understand how to train the imdb dataset aclImdb_v1.tar to a model.  Please show me how to train a model from this dataset.Below is the method I need to implement for my project.Text Reviews -> Embedded Layer -> Word vector generation ->
CNN -> Feature Maps -> LSTM -> Classification layerThe result I expect is to find if a review taken as input is either positive or negative.
Please help me out and pardon my unknowledgeable words, if any.."
3163,Simple sentiment analysis using ml.net and IEnumerable dataview,"['c#', 'sentiment-analysis', 'ml.net']",I was testing a simplest example to learn sentiment analysis using custom IEnumerable dataview instead traditional data load from text files. I created a list of TestData and TrainingData with some example review to easily learn by following the sample available on github and documentation. But something is missing and the model I created is not working correctly...it's just giving wrong result as positive for everything.MainTesting and TrainingModels and Training/Test data
3164,Sensitivity Analysis for Missing Data in R with MICE,"['r', 'metadata', 'sentiment-analysis']","I am working on a meta analysis and a sensitivity analysis for missing data. I want to replace censorsed data either with 0 or 1 according to a predefined probability. I have a dataset with colum x: timepoints and y: events (1 = event, 0 = censored). For the analysis I replaced some of the 0 with NAs. Z is the indicator for the treatment arm. I want to replace NAs to either 1 or 0 with a predefined probability. 
This is my code:Just an example:I would expect that NAs in y will be replaced with 0 (20% of cases) and 1 (80% of cases). But NAs are either replaced only with 0 or only with 1. I have to admit, that I am quite a beginner with R and did not have to write own little functions before.Thank you very much for your help!"
3165,Loop to retrieve sentiment analysis in pandas.core.series.Series,"['python', 'sentiment-analysis', 'textblob']","I have 47 news-articles that I want to extract the sentiment from. They are JSON format (Date, title and body of the article). All I want is to obtain a list with the sentiment using TextBlob. So far I am doing the following:Obvioulsy, I get the following error: TypeError: The text argument passed to __init__(text) must be a string, not <class 'pandas.core.series.Series'> Any idea on how to create a list with the sentiment measure?"
3166,Real-time plotting of two columns of a dynamic DataFrame,"['python', 'pandas', 'dataframe', 'matplotlib', 'sentiment-analysis']","I am trying to plot a real-time data getting loaded in dataframe. But the attempts have led to printing of multiple blank graph frames in response to dynamic data feed, instead of plotting the data in single frame of graph.I am implementing a solution to perform sentiment analysis on live twitter stream. I am able to stream the tweets, put them into a DataFrame and apply the required sentiment analysis algorithm on them one by one. I created a column in the DataFrame which holds the compound value generated by that algorithm for an individual tweet.This DataFrame is getting dynamically updated as the tweets stream and the intent is to plot this real time updated compound value against time.I have tried plotting the graph as per mentioned advises of using plt.ion(), plt.draw() instead of plt.show() functions etc. But instead of plotting one frame which gets updated with the values, the program starts printing multiple frames one after another as the data gets updated in the DataFrame.Expected Result - One frame of graph getting updated and plotting the real-time data.Actual Result - Multiple blank graphsI apologize if i have missed on any information/point. "
3167,Is there any step by step stuff to implement sentiment analysis and voice into my current Google Cloud Vision based face recognition app project?,"['android', 'google-cloud-platform', 'sentiment-analysis']","I want to get an easy to understand guide to implement sentiment analysis and voice to my current project Cloud Vision project -object detection and face- besides. I downloaded kind of a sample, but it only recognized faces, but I want to implement custom sentiment analysis and add voice feature in real time using camera. Appreciate a lot guys! I downloaded kind of a sample, but it only recognized faces, but I want to implement custom sentiment analysis and add voice feature in real time using camera"
3168,Sentiment Analysis using RNN through Stanford Core NLP,"['deep-learning', 'stanford-nlp', 'sentiment-analysis']","I'm trying to perform sentiment analysis on twitter data using standard Machine Learning algorithms ( SVM , Logistic Regresression , Naive Bayes etc) . Now i want to compare these results from Deep Learning algorithms . So first off i wanted to know if Stanford Core NLP Package provides libraries for deep learning 
(RNN specifically) .  Stanford has already written a paper on Recursive Deep Models and implemented it as well . https://nlp.stanford.edu/sentiment/
But they use an already trained model , which was trained using Sentiment Tree Bank .Is there a way to plugin my own twitter dataset and train this model and perform classification ?"
3169,Split column values to several in pandas dataframe,"['pandas', 'dataframe', 'lambda', 'sentiment-analysis']","I am trying to do sentiment analysis on tweets using sentimentIntensityAnalyzer() from nltk.sentiment.vaderThis is the resulting dataFramee :In order to do statistical analysis on the 'neg','pos','neu' and 'compound' entities in the polarity column I wanted to split the data into four different columns. To achieve this I used :The resulting dataFrame:Is there a more concise way of achieving a similar dataFrame? Using the lambda function perhaps? Thanks for the help! "
3170,How to get the score of sentiments?,"['python', 'nltk', 'sentiment-analysis', 'textblob']",I am using TextBlob i am training my classifier on a training set after that i am successfully able to get the classified out putBit how can i get the score of a particular text in terms of positive or negativity should i  put scores of sentiments in my training datahere is what i have tried Here is the outputHow can i get the score of this like pos .7 or in any other format
3171,How to do transfer learning in sentiment analysis?,"['twitter', 'deep-learning', 'sentiment-analysis', 'transfer-learning']","I want to build an app where I can enter any Twitter keywords, the backend will crawl related tweets and return sentiment analysis of the tweets in percentage of negative, neutral and positive tweets. For example, I enter the keyword 'pepsi', the app will output something like this: Tweets related to pepsi contains 10% negative sentiment, 10% neutral sentiment and 80% of positive review.So the problem is how to train an machine learning algorithm that I can use in the backend to do such sentiment analysis on various kind of topics. The main idea involved here is transfer learning, where we train one model on large amount of labeled data and use it as baseline to train other data. Transfer learning has limits in NLP mostly because knowledge learned at one task is not broad enough to downstream to other tasks. For example, I pretrained a good neural network to do sentiment analysis on airlines with a prediction accuracy of over 70%. However, when I use the same model to do sentiment analysis on pepsi, I get only around 30% prediction accuracy.I did some research and noticed that Google's universal sentence embedding is quite popular. However, I realized this is a new way of converting input text into feature vector, not a universal algorithm. I wonder anyone can point me to directions I should go? Thanks a lot in advance!"
3172,Problem implementing sentiment analysis for imdb movies reviews data,"['python', 'scikit-learn', 'svm', 'sentiment-analysis']",I was implementing sentiment analysis for imdb movie reviews dataset and got the value error when making predicitons using LinearSVC().# STOP IS FOR STOPWORDSI was expecting the model to work but got the error 
3173,Include in-text numeric values during sentiment anaylsis,"['machine-learning', 'keras', 'sentiment-analysis']","I am working on sentiment analysis-like problem, Supposedly the text contains numbers which will make difference in categorization, for example:How can these numbers be interpreted to affect the result, noting that, the text is free and no standards are used."
3174,Transfer learning in Keras with your own saved model,"['python', 'keras', 'lstm', 'sentiment-analysis', 'transfer-learning']","I have seen some examples of transfer learning where one can use pre-trained models from keras.application (Xception, VGG16, VGG19, ResNet50 e.t.c) but what I want is to transfer the learning from the model I saved using model.save('model.h5')This is my current model:Now, Instead of saying I want to load the saved model probably with load_model('model.h5') and add it as a layer to my current model."
3175,Predicting values using trained MNB Classifier,"['python', 'python-3.x', 'classification', 'sentiment-analysis']","I am trying to train a model for sentiment analysis and below is my trained Multinomial Naive Bayes Classifier returning an accuracy of 84%.I have been unable to figure out how to use the trained model to predict the sentiment of a sentence. For example, I now want to use the trained model to predict the sentiment of the phrase ""I hate you"".I am new to this area and any help is highly appreciated."
3176,I get 'Task not serializable' when I try to run the John Snow spark-nlp example in Scala,"['scala', 'apache-spark', 'sentiment-analysis', 'johnsnowlabs-spark-nlp']",I have been trying to run the John Snow Spark-NLP example from this repository: https://github.com/JohnSnowLabs/spark-nlp/blob/master/example/src/TrainViveknSentiment.scalaon my local machine. But it throws the org.apache.spark.SparkException: Task not serializable error when it arrives on val sparkPipeline = pipeline.fit(training) in the stack it also says Caused by: java.io.NotSerializableException: com.johnsnowlabs.nlp.annotators.param.AnnotatorParam$SerializableFormat$
3177,Any specific python library to perform sentiment analysis for reviews written in german or french please?,"['python', 'nlp', 'sentiment-analysis']","We have reviews written in German and French which needs to be analysed and classified either as positive, neutral or negative based on the sentiment it reflects. We tried some tools which translate the reviews to English but the accuracy wasnt that great since the meaning is lost during translation. Any specific library that can be used in such a case? Any help is highly appreciated. Thanks in advance. "
3178,How to import a lexicon in XML-LMF format for sentiment analysis in R,"['r', 'xml-parsing', 'text-mining', 'sentiment-analysis', 'quanteda']","I'm trying to import the following lexicon in R, to be used with text mining packages such as quanteda, or to export it as a list or data frame:https://github.com/opener-project/VU-sentiment-lexicon/tree/master/VUSentimentLexicon/IT-lexiconThe format is XML-LMF. I could not find any way to parse such a format with R. (see https://en.wikipedia.org/wiki/Lexical_Markup_Framework)As a workaround I tried to use the XML package, but the structure is a bit different from usual XML, and I did not manage to parse all the nodes. "
3179,Lime explainer shows prediction probabilities different to the classifier prediction - sentiment analysis,"['python', 'machine-learning', 'nlp', 'sentiment-analysis', 'lime']","I am using Lime to trace the behavior behind why the model take his decision  to predict if this sentence is (NEG, POS or NEUTRAL) and for the most of cases lime explain correctly but in case like this why i entered NEG sentence, the model predict it as NEUTRAL but Lime visualize it with NEG highest percentage, so why i got logical error like this?Model prediction vs Lime prediction"
3180,How to find location of a user in youtube from his comment,"['python', 'youtube', 'geolocation', 'sentiment-analysis']","
I'm making an application about sentiment analysis.
So I want to find the location of a user that post the comment in a Youtube video ?
Is there any approach to do that ?EDITFor example I want to know the location; I mean the emplacement ( City or Country or even geo-localisation) any thing that can refer to his emplacement.I'm using python right now, so I can fetch information of a given video, and his comments, but i don't know any approach to find the location of the comment owner. "
3181,How to display the sentiment analysis values in a pie chart using matplotlib in python 3.6?,"['python-3.x', 'matplotlib', 'nlp', 'sentiment-analysis', 'textblob']","The main objective is to display the sentiment analysis values positive, negative and neutral of any user input in a pie chart. While, the code has no error, the pie chart only displays the neutral value as 100% of the entire chart and classifies the input as neutral even after a negative or positive is fed in as the input. I have tried modifying the conditional statements and taking passing the main input variable itself in Textblob. However, the expected results have not been generated. The Expected output is to correctly classify and display the positive, negative and neutral in the pie chart. But the output, regardless of the input's context, only classifies as neutral and pie chart also only displays neutral."
3182,Sample example of Sentiment feature of Watson NLU failing with error code 400,"['ibm-cloud', 'sentiment-analysis', 'watson-nlu']",I was trying the sample examples of various features documented at https://cloud.ibm.com/apidocs/natural-language-understanding. All the features examples are working properly except the Sentiment feature while trying with Curl.
3183,Reading a large pre trained fastext word embedding file in python,"['python', 'keras', 'sentiment-analysis', 'fasttext']","I am doing sentiment analysis and I want to use pre-trained fasttext embeddings, however the file is very large(6.7 GB) and the program takes ages to compile.Is there any way to speedup the process?"
3184,How to fix object not found in the following code?,"['r', 'sentiment-analysis']","I am working on a sentiment analysis project in R and getting an error message of "" object not found"" whenever I run the code, the library used and the code are as follows (also I did not forget to put the api details in my code):the error message that shows up is on running the R script is:"
3185,How to fix 'encoding' issue in Python using vaderSentiment package,"['python', 'encoding', 'nlp', 'sentiment-analysis', 'vader']",I am working on a sentiment analysis problem and found the vaderSentiment package but cannot get it to run. It is giving me an 'encoding' error. I have tried adding 'from io import open' but that did not fix my issue. Please see code below.Here are the results I am wanting: The results I am getting: 
3186,Sentiment analysis for sentences with overall positive sentiment but have negative words,"['nlp', 'sentiment-analysis', 'natural-language-processing']","I am trying to work on a sentiment analysis tool for reviews data. I came across a few edge-cases where the overall sentiment of a sentence may be positive but contain a negative word.  
For example : I want this iPad so bad. 
  F*ck yes, it looks good!I tried 2-3 sentiment analyzing libraries and they infer such sentences with negative sentiment. I have not found any work which is looking into such solutions. 
Is there any known solution to handle sentiment in such contextual cases?I tried mostly lexicon based approaches. I used NLTK, SPACY, IBM tone analyzer, TextBlob, VADER. Currently I am averaging the summation of results from all of them. "
3187,Problem with CountVectorizer from scikit-learn package,"['python', 'scikit-learn', 'classification', 'sentiment-analysis', 'text-recognition']","I have a dataset of movie reviews. It has two columns: 'class' and 'reviews'. I have done most of the routine preprocessing stuff, such as: lowering the characters, removing stop words, removing punctuation marks. At the end of preprocessing, each original review looks like words separated by space delimiter.I want to use CountVectorizer and then  TF-IDF in order to create features of my dataset so i can do classification/text recognition with Random Forest. I looked into websites and i tried to do how they did. This is my code:But, i get this output...which doesn't make sense at all. I tackled with some parameters and commented out the parts about TF-IDF. Here's my code:and this is my output:Am i missing something? Or am i too noob to understand? All i understood and want was/is if i do transform, i will receive a new dataset with so many features (regarding the words and their frequencies) plus label column. But, what i am getting is so far from it.I repeat, all i want is to have a new dataset out of my dataset with reviews in which it has numbers, words as features, so Random Forest or other classification algorithms can do anything with it.Thanks.Btw, this is first five rows of my dataset:"
3188,Using Spark's MapReduce to call a different function and aggregate,"['java', 'apache-spark', 'mapreduce', 'sentiment-analysis', 'aws-glue']","I am woefully unfamiliar with spark but I'm pretty sure there exists a good way to do what I want much faster than I currently am doing it. Essentially I have an S3 bucket that has lots of JSON of twitter data. I want to go through all of these files, grab the text from the JSON, do sentiment analysis (currently using Stanford NLP) on the text and then upload the Tweet + Sentiment to a database (right now I'm using dynamo, but this is not make-or-break)The code I currently have isThis works and is fine. And I was able to expedite the process to only about 2 hours by running this code on certain date ranges (i.e. getKeys only gets keys for certain date ranges) and spinning up many instances of this process across different EC2s, each one acting on a different date range.However, there's gotta be a faster way to do this with a good ole map-reduce, but I just have no idea of how to even begin looking into this. Is it possible to do a Sentiment Analysis in my map and then reduce based on timestamp? Further, I was looking into using AWS Glue but I don't see a good way to use the Stanford NLP library there. Any and all help would be greatly appreciated."
3189,Is it possible to pass more than one query in the Twitter API for sentiment analysis in python,"['python', 'python-3.x', 'twitter', 'tweepy', 'sentiment-analysis']","Ive tried to look this up on google and here but I don't seem to find anything that helps. I would like to search for more than one keyword, such as ""java"" ""python"" and ""ruby"" but I'm not too sure how to go about this problem, it involves sentiment analysis. TIA I expect to get output of all tweets containing the words java, python and ruby, right now I'm only getting tweets about java. "
3190,"Universal Sentence Encoder Error: Input 0 is incompatible with layer conv1d_6: expected ndim=3, found ndim=2","['keras', 'deep-learning', 'conv-neural-network', 'sentiment-analysis', 'embedding']","I'm worked on sentiment analysis task using universal sentence encoder embed_size=512 with CNN but have an error says: Input 0 is incompatible with layer conv1d_6: expected ndim=3, found ndim=2. 
and wanna know if this is right to add universal sentence encoder with CNN in this way or not?  "
3191,"GCP Sentiment Analysis returns same score for 17 different documents, what am I doing wrong?","['python', 'google-cloud-platform', 'sentiment-analysis']","I'm running Google Cloud Platform's sentiment analysis on 17 different documents, but it gives me the same score, with different magnitudes for each. 
It's my first time using this package, but as far as I can see it should be impossible for all these to have the exact same score. The documents are pdf files of varying size, but between 15-20 pages, I exclude 3 of them as they're not relevant. I have tried the code with other documents, and it gives me different scores for shorter documents, I suspect there's a maximum length of document it can handle, but couldn't find anything in the documentation or via google. Results (score, magnitude):doc1
0.10000000149011612 - 147.5doc2
0.10000000149011612 - 118.30000305175781doc3
0.10000000149011612 - 144.0doc4
0.10000000149011612 - 147.10000610351562doc5
0.10000000149011612 - 131.39999389648438doc6
0.10000000149011612 - 116.19999694824219doc7
0.10000000149011612 - 121.0999984741211doc8
0.10000000149011612 - 131.60000610351562doc9
0.10000000149011612 - 97.69999694824219doc10
0.10000000149011612 - 174.89999389648438doc11
0.10000000149011612 - 138.8000030517578doc12
0.10000000149011612 - 141.10000610351562doc13
0.10000000149011612 - 118.5999984741211doc14
0.10000000149011612 - 135.60000610351562doc15
0.10000000149011612 - 127.0doc16
0.10000000149011612 - 97.0999984741211doc17
0.10000000149011612 - 183.5expected different results for all documents, at least small variations. 
(I think these magnitude scores are also way too high, compared to what I have found in documentation and elsewhere)"
3192,How to extract tweets posted only from local people?,"['python', 'twitter', 'web-crawler', 'sentiment-analysis', 'social-media']","I am doing a sentiment analysis project about local people's attitudes toward the transportation service in Hong Kong. I used the Twitter API to collect the tweets. However, since my research target is the local people in Hong Kong, tweets posted from, for instance, travelers should be removed. Could anyone give me some hints about how to extract tweets posted from local people given a large volume of Twitter data? My idea now is to construct a dictionary which contains traveling-related words and use these words to filter the tweets. But it may seem not to workAny hints and insights are welcomed! Thank you!"
3193,Text Mining responses with very varying answer lengths,"['text', 'nlp', 'analytics', 'sentiment-analysis', 'udpipe']","I have a dataset of responses where people were requested to answer a set of questions. There's only one column of text data to process.My challenge is; only very few respondents have actually written long texts that I found easy to process and gain insights from it. Most of the other responses are often found to be very short such as ""Somewhat"", ""Yes"", ""No"", ""Larger extent"". That too, it has been not possible to scale it ordinally because there's no logical order to it.I have been able to use the longer text responses to gain insights on Sentiments, extract keywords and phrases and apply Machine learning such as RAKE and PMI. I used UDPIPE library with R.However, for shorter ""few words"" responses, I am finding it really difficult to gain insights from these.Is there any other machine learning technique possible with the current issue I'm having ? Or do I need to try any NLP technique ?"
3194,How to make a table or DataFrame from dtype=object?,"['python', 'dataframe', 'jupyter-notebook', 'sentiment-analysis']","I would like to show how the predictions are made in a table or DataFrame.I tried to put X_test, y_test and predictions (predictions = model.predict(X_test)) into a DataFrame to show wich reviews are positive or negativ predicted.Variable ""X_test"" (Name: review, Length: 4095, dtype: object) looks like:15806    ['tire', 'gauges', 'kind', 'thing', 'makes', '...541      ['like', 'said', 'title', 'review', 'say', 'pr......Variable ""y_test"" (Name: label, Length: 4095, dtype: object) looks like:15806    positiv541      positiv...Variable ""predictions"" looks like:array(['positiv', 'positiv', 'positiv', ..., 'positiv', 'positiv',
       'positiv'], dtype=object)At the moment I got a DataFrame with all Data in the first line but I need as a table with all lines."
3195,How to add svm on top of cnn as final classifier?,"['python', 'scikit-learn', 'keras', 'svm', 'sentiment-analysis']","I work on sentiment analysis task and i want to add SVM layer on top CNN as a final classifier, how can i do that without using hing-loss?"
3196,"How to use 2 dataset, 1 for training and 1 for testing on WEKA for sentiment analysis","['svm', 'weka', 'sentiment-analysis']","So I have 3 dataset that I used for sentiment analysis and I want to use only 1 dataset for building the model and the rest of the dataset for testing purpose. The model that I will use is SVM(SMO algoritm). The datasets at start only have 2 attributes (text,label) but after preprocessing with string to wordvector it become many attributes. I was able to build a model and test it using 10-fold cross validation and now I want to test it with the other dataset. But since it has different attributes due to string to word vector I can't do it. Any solution for my problem?I already applied the same preprocess to the test set and tried using ""inputmappedclassifier"" but the result is still errorI was hoping the model can be used on datasets that it never see"
3197,How to add emoji to the Keras Tokenizer API?,"['python', 'twitter', 'keras', 'nlp', 'sentiment-analysis']","I am doing a Twitter sentiment analysis project. It has been demonstrated from some literature that the use of information from emoji and emoticon could improve the performance of a sentiment classifier on Twitter data(such as a work done by IBM Sentiment Expression via Emoticons on Social Media in 2015). Moreover, the emoji2vec project emoji2vec which could create the representation of each emoji based on emoji descriptions emoji description is really helpful for Twitter sentiment analysis.Now, I am using Keras to construct the sequential model to do this sentiment classification. But my question is since before constructing all the sequential models, you should pass your text data to the Tokenizer API first:where df is my pandas dataframe. Hence, is it possible to add emoji to the Tokenizer(since the Tokenizer API first select the top vocabulary size most frequent words and construct the word-index pair)? The emojis are apparently less frequent than the words and they are quite significant features in sentiment classification. Hence, I want to add emojis to the keras Tokenizer API and create emojis' emoji-index pair.When it comes to model, I am constructing a BiLSTM model with pre-trained embedding(such as trained by FastText). How could I combine the emoji representation and the word representation in this task? The following code shows my BiLSTM model:Any help and insights would be appreciated! Thanks! Merry Christmas!"
3198,Doc2Vec vs Avg Word Vectors : Which is better for Sentiment Analysis?,"['nlp', 'word2vec', 'sentiment-analysis', 'doc2vec']",I was performing Sentiment Analysis on the IMdb dataset on Kaggle. I used the BOW approach with bigrams and that gave me a decent accuracy of ~89%. But I dont know how to approach the same using word embeddings: Should i go for averaged word vectors or doc2vec?Someone please help. Thanks in advance.
3199,RetryError: Deadline of 600.0s exceeded while calling functools.partial by using Gcloud,"['python', 'python-3.x', 'gcloud', 'sentiment-analysis']","I'm a beginner to python and trying to apply Gcloud Sentiment Analysis to analyze some sentences. The python code is provided by official here: https://cloud.google.com/natural-language/docs/analyzing-sentimentThe error is as follows:RetryError: Deadline of 600.0s exceeded while calling functools.partial(.error_remapped_callable at 0x0000020081C0EB70>, document {
  type: PLAIN_TEXT
  content: ""yes i do!""
}
, metadata=[('x-goog-api-client', 'gl-python/3.6.5 grpc/1.17.1 gax/1.7.0 gapic/1.1.1')]), last exception: 503 channel is in state TRANSIENT_FAILUREI've tried many ways (e.g. disable firewall, switch to other laptop / wifi) to solve but failed. I'm sure environment variable is set up using Application Default Credentials, and API is authenticated.Could you have any ideas? Many thanks!Coding environment:Python 3.6Win10python package from CMD pip list -- gcloud (0.18.3), google-api-core (1.7.0), google-cloud-language (1.1.1)  "
3200,Join and run calculations on Pandas DataFrame based on text matching,"['python', 'pandas', 'sentiment-analysis']","I have two dataframes of customer reviews data.My first dataframe, 'df' contains thousands of raw customer reviews, processed/cleaned reviews data, and sentiment scores:My second dataframe, 'bigrams' contains the most frequent bigrams in the field called 'reviewClean' from my first dataframe:My goal is to take each of my topBigrams, e.g. 'like goggles' or 'strap broke', look up every 'reviewClean' that contains each bigram AND the associated sentiment to that entire review, and and calculate an average sentiment score for each topBigram.My end result would look something like this (numbers for pure illustration):From this data, I would then look for trends on each bigram to determine the drivers of positive or negative sentiment in a more succinct way.I am not even sure where to begin. Many thanks for any insight into a potential approach here."
3201,Non-ASCII character '\xc3' in sentimentPipeline.py,"['python', 'ascii', 'sentiment-analysis']","Running Spanish & English textual sentiment analysis that originally work but now causing ASCII issues.I downloaded Anaconda again and tried to run my code on python 2.7.  Looked at other answers that suggested adding things on the top. Including things like:
    # -- coding: utf-8 --
    # -- encoding: utf-8 --
    # coding: latin-1
    # coding=It should give numerical values of the sentiments and the results are added as a separate column of the input data dataframe. The resulting new dataframe is saved as a csv. The error produced is as follows:"
3202,How to extract tweets related to a particular country?,"['python-3.x', 'twitter', 'tweepy', 'sentiment-analysis', 'textblob']","I am working on a Project, in this project we are extracting tweets of some of the world leaders and then we will try to compare their relationships with other countries based on their twitter comment. So far we have extracted the tweets from Donald Trump Account  We have categorized the tweets into positive and negative but what problem I am facing is how we can separate the tweets country-wise, Is their any way by which only those tweets are extracted in which he/she has tweeted about some country and the rest of the tweets are ignored so that we can only get the tweets related to the country.    "
3203,How to detect tweets posted from official account?,"['twitter', 'nlp', 'sentiment-analysis']","I am doing a research project which wants to work out Hong Kong people's attitudes toward the public transportation system. I have collected millions of tweets using Twitter API. Since my research target is local Hong Kong people, tweets posted from official accounts or tweets contain advertisements may be useless. Hence, could anyone give some hint about how to filter out the tweets posted from the official account? I know my explanation might be a little bit abstract. Any tips would be appreciated! Thank you!"
3204,PHP- compare similarity score in foreach loop,"['php', 'foreach', 'sentiment-analysis']",This is my PHP code to show similarity score(positive/negative/neutral) for each text.How can it show the result from comparing the similarity score($similarity[$key]) and print the category($key) which is the highest score.The expected output:Similarity score(positive): 0.029764673182427Similarity score(negative): 0.020378478648481Similarity score(neutral): 0.057639041770423neutralSimilarity score(positive): 0.028088336282316Similarity score(negative): 0.019230769230769Similarity score(neutral): 0.054392829322042neutral
3205,How to do sentiment analysis of different topics/aspects from the same text sample,"['python', 'nlp', 'sentiment-analysis']","Suppose I have the following customer review and I want to know the sentiment about the hotel and the food:""The room we got was nice but the food was average""So had this been in a dataframe of reviews, the output from the analysis would have looked like:
Reviews           Hotel   Food
The room was ...   Pos     Neg I have come across multiple tutorials on Kaggle and Medium which teach sentiment analysis, but they always look for the overall sentiment. Please help me out, if you know the way, or are aware of any tutorials or know what terms to google to get around this problem. Thanks!Edit: Please refer to these sides:
http://sentic.net/sentire2011ott.pdf
They seem to be lecture notes. Does anyone know a python implementation of the same? Thanks!Edit: This question pertains to ABSA (Aspect Based Sentiment Analysis)"
3206,Unsupervised Sentiment Analysis on product reviews,"['machine-learning', 'deep-learning', 'nlp', 'sentiment-analysis', 'natural-language-processing']",I would like to perform an unsupervised sentiment analysis on the reviews posted by customers on different product web-page. Most of the online resources use supervised methods and the examples/tutorials always have a labelled training data-set.My objective is not to just deduce the polarity of the review but also do content/subjective analysis. Any online implementation or suggest high level approach for this ?
3207,Negators and modifiers with Syuzhet vs. SentimentR for sentiment analysis in R,"['r', 'sentiment-analysis', 'sentimentr']","This question is twofold. An answer to either question would be an adequate solution. Very thankful if you could show suggestion as R-code.1) The NRC lexicon in the Syuzhet packet yields the broadest range of emotions, but it doesn't seem to control for negators. After reading the documentation I'm still not sure how to overcome this. Perhaps by multiplying the positively and negatively coded words for each sentence, e.g. I(0) AM(0) NOT(-1) ANGRY(-1) = (-1*-1) = 1. However, I don't know how to write this in proper code.2) After much research and testing, I found the jockers_rinker lexicon in SentimentR handles negators and modifies better(https://github.com/trinker/sentimentr#comparing-sentimentr-syuzhet-meanr-and-stanford). I could use SentimentR to ""quality test"" the results from the Suyzhet/NRC results by comparing the binary sentiment outputs from the two packages. If they deviate too much, the NRC isn't accurate enough for that particular body of text. However, I only know how to get individual scores and not the total scores for each sentiment (sum of positive and sum of negative)You can see how my test results compare here on a concatenated string with emotions expressed with and without modifiers and negators. The first output seems not to recognize the importance of ""very"", ""not"" and ""never""."
3208,Training and evaluating accuracies come different in lstm model of keras,"['python-3.x', 'machine-learning', 'keras', 'lstm', 'sentiment-analysis']","I am training a LSTM model for sentiment analysis using keras. Training the training set gives accuracy:80+ percentage during the epochs processing, but evaluating or predicting the model with the same training set gives accuracy:53.9% every time. I cant figure out the problem; and I have been at it for so long. Also, I have commented out the data loading part in the following code, since i saved the data on disk for time efficiency. The data is textual and the labels are 0/1 for sentiment. Please help!!"
3209,Training issue in keras,"['python-3.x', 'keras', 'sentiment-analysis']","I am trying to train my lstm model for sentiment analysis but the program doesnt proceed at all after displaying the following output:The code below has some commented out since it was used to save some textual data on disk beforehand. Now, the code only trains the lstm model using that training and testing textual data. It is given below:"
3210,'list' object has no attribute 'encode': sentiment analysis,"['python', 'sentiment-analysis', 'vader']","I would like to conduct sentiment analysis of some texts using Vader (but the problem I am describing here applies to any lexicons as well, in addition to Vader).
However, after going through all the data processing including tokenizing and converting to lower case (which I have not mentioned here) I get the following error:Any idea how to process the documents so that the lexicon can read the texts? Thanks.AttributeError: 'list' object has no attribute 'encode'AttributeError: 'list' object has no attribute 'encode'"
3211,Reading a processed dataset for sentiment analysis in keras,"['python', 'pandas', 'keras', 'sentiment-analysis']",I want to use the processed dataset for the sentiment analysis from here but I am not quite sure how to read it to input it in keras model.I am used to using pandas to read the csv or json format but this one has the following format.This is the example (one review and it's sentiment):It turns out this dataset has been used in several paper and is kind of a benchmark for multi-domain sentiment analysis so I want to know how to read it in keras and use it as a input. 
3212,Using pretrained Word2Vec model for sentiment analysis,"['python', 'twitter', 'nlp', 'word2vec', 'sentiment-analysis']","I am using a pretrained Word2Vec model for tweets to create vectors for each word. https://www.fredericgodin.com/software/.  I will then compute the average of this and use a classifier to determine sentiment. My training data is very large and the pretrained Word2Vec model has been trained on millions of tweets, with dimensionality = 400. My problem is that it is taking too long to give vectors to the words in my training data. Is there a way to reduce the time taken to build the word vectors? Cheers. "
3213,Unable to store pandas data frame as a csv,"['python', 'pandas', 'sentiment-analysis']","I am following this tutorial to retrieve data from news sites. The main function is getDailyNews. It will loop on each news source, request the api, extract the data and dump it to a pandas DataFrame and then export the result into csv file.But when I ran the code, I am getting an error. Error:I know that I have to give the path name in pd.read_csv but I don't know which path I have to give here. "
3214,Real Time Analysis And Sentiment Analysis,"['java', 'python', 'twitter', 'real-time', 'sentiment-analysis']","My project is to do the prediction by doing sentiment analysis of tweets(Cricket).
I will gather tweets as stream. 
For this I need to do real time analysis of tweets and then sentiment analysis of tweets.
I'm using Kafka and Spark Streaming.How to do real time analysis of tweets using kafka? I need those tweets which is related to cricket 
After real time analysis how to do Sentiment Analysis of tweets using spark streaming? Can you please tell me which language is best to do this whole task, Python or java?"
3215,Remove single character in R,"['r', 'regex', 'gsub', 'sentiment-analysis']","i'm working on sentiment analysis with Arabic language by using R and in cleaning step i need to remove the single character. 
I used this code to remove them and it works but a had some problemfor example here is the dataas you see here ""غ"" is single characterbut when I tried to apply it on the whole data set on text column like hereits doesn't remove anything and I don't known why?hope you understand me thank you "
3216,Wordcloud from Data Table in R,"['r', 'data-visualization', 'sentiment-analysis']","I have a data table made from positive and negative word associations. I would like to create two wordclouds, one for positive words and one for negative words.Example of sentiment_words table:I am using library(wordcloud) and library(sentimentr)For example, how do I pull only the words from the ""positive"" column to create a wordcloud? I'm not sure how to address the fact that there are multiple words associated with each row (e.g., ""agree, available"" should be treated as two entries)I've made different attempts at the wordcloud() function such as
wordcloud(words = sentiment_words$positive, freq = 3, min.freq = 1, max.words = 200, random.order = FALSE, rot.per=0.35, colors=brewer.pal(8, ""Dark2"")) but this only returns a cloud with the term in the first entryEdit: I've tried the tidyverse answer below, and the result I get is:

   words                      n
   <chr>                  <int>
 1 "" \""ability\""""             3
 2 "" \""ability\"")""            1
 3 "" \""acceptable\"")""         1
 4 "" \""accomplish\""""          1
 5 "" \""accomplished\"")""       1
 6 "" \""accountability\""""      1
 7 "" \""accountability\"")""     1
 8 "" \""accountable\""""         2
 9 "" \""accountable\"")""        1I've tried multiply variants of gsub() and apply to remove the extra ) and c( but haven't found anything that works yet. The result is words that should be counted together are counted separately (e.g., ""acceptable"" and ""acceptable)"" are two different words in the wordcloud)Edit: In order to get it to work correctly, I had to first clean up my sentiment_words as suggested below.and I had to also filter out the remaining ""character(0"" strings within the count_words function. Note that it filters ""character(0"" and not ""character(0)"" because I removed the closing parenthesis aboveImplementing the above gave the cleanest wordcloud based on polarity of text"
3217,Date format using scale_x_date giving Error,"['r', 'ggplot2', 'sentiment-analysis']",Hello I need to get my ggplot with date format having this format in X axis:.But my date format has time with it.This is what I have done to get the result. Now how do I achieve it like the picture? Conversion to date doesn't help as there are instances where the tweet takes place on same day but different time and that then messes the graph.
3218,Applying sentiment analysis to tweets,"['python', 'tweepy', 'sentiment-analysis']","I am struggling to apply sentiment analysis to every tweet that is generated in an SQLite file. Here is the below code. I think I am not referring to the StreamListener appropriately like I should, but I really don't know how to properly implement it.Does my sentiment analysis need to be implement in my def on_data function?UPDATE:Currently, my code only generates an extensive list of tweets (until I stop it) into an SQLite file. Here is an example of one:"
3219,Making Predictions on single review from input text using saved CNN model,"['python', 'tensorflow', 'keras', 'nlp', 'sentiment-analysis']","I am making a classifier based on a CNN model in Keras.I will use it in an application, where the user can load the application and enter input text and the model will be loaded from the weights and make predictions.The thing is I am using GloVe embeddings as well and the CNN model uses padded text sequences as well.I used Keras tokenizer as following:I trained the model and predicted on test data, but now I want to test the same with loaded model which I loaded and working.But my problem here is If I provide a single review, it has to be passed through the tokeniser.text_to_sequences() which is returning 2D array, with a shape of (num_chars, maxlength) and hence followed by a num_chars predictions, but I need it in (1, max_length) shape.I am using the following code for prediction:Output is:I need a single prediction here array([0.29289   , 0.36136267, 0.6205081 ]) as I have a single review."
3220,change the format of time and date in R,"['r', 'sentiment-analysis']","I am working on twitter data by useing R and I have column for date and time like this ""10/19/2018 23:00"" , ""10/19/2018 23:01"", ""10/19/2018 23:02"" ,""10/19/2018 19:45"".
I want  to change the format to be without date and also the time be without minute for example any time on 23 hour I need it to be on 23 only without minute.
this is an example of the column:and I need it to be like this 
        15:00
        23:00
        16:00 hope you understand me 
can you help me to do it and thank you."
3221,How to detect numerical value of a text?,"['ruby', 'text', 'nlp', 'sentiment-analysis']","We have data for survey question (e.g. rate us between 1-5) that's supposed to be numerical. However, we find that the response also includesI'd like a way to turn the user response into a numerical value. e.g. the text above should translate into 5, 4, 5, 4, 3.5 respectively. Obviously this won't work 100% of the time so I'm looking for an optimal solution (perhaps a text analysis approach) that gets me over 80%. "
3222,Any elegant solution for finding compound noun-adjective pairs from sentence by using Spacy?,"['python', 'sentiment-analysis', 'spacy']","I am recently have known with Spacy and quite interested in this Python library. However, in my specification, I intend to extract compound noun-adjective pairs as a key phrase from the input sentence. I think Spacy provides a lot of utilities to work with NLP task, but didn't find a satisfied clue for my desired task. I looked into a very similar post in SO, related post, and solution is not very efficient and doesn't work for custom input sentence.Here is some of the input sentence:Here is the code that I tried:I am speculating that what kind of modification should be applied above helper function because it didn't return my expected compound noun-adjective pairs. Is there anyone who have good experiences with Spacy? How can I improve above sketch solution? Any better idea?Desired output:I am expecting to get compound noun-adjective pairs from each of input sentence as follow:Is there any way I could get the expected output? what kind of update will be needed for the above implementation? Any more thoughts? Thanks in advance!"
3223,Refresh a dataframe evey day in shiny R,"['r', 'shiny', 'sentiment-analysis']","I am doing a sentiment analysis app in R shiny. The problem is that I need new tweets evey day to add more information to the analysis.  Now I have the tweets from 12th November to 25th November but I would like that every day the dataframe of tweets got bigger adding the tweets from the last day automatically.  I've been trying with functions invalidateLater and reactivetimer but it doesnt work because when I run the app, it automatically begins by searching for new tweets instead of showing the historical results and then search for the most recent tweetsMy code in the server looks like this:As I said before, I've created a recentdata reactive value with invalidateLater but I haven't been able to do it properly..."
3224,Significantly lower accuracy while using class_weight for imbalanced dataset in keras,"['python', 'tensorflow', 'keras', 'sentiment-analysis']","I have rather unintuitive problem. I am doing the sentiment analysis on Amazon Book reviews and the data set is heavily imbalanced. Positive reviews are almost 10 times the negative reviews, accuracy for both training and testing are around 90% (with imbalanced dataset). However, when I try to balance the dataset with the help of class_weight = {0:10 , 1:1} both training and testing accuracy drops to around 65%. Again if I do class_weight = {0:1 , 1:10} accuracy booms again, so apparently I am setting the class_weight wrong but as I understood because the number of positive reviews(1) are 10 times the number of negative reviews(0), shouldn't the class_weight be set as {0:10 , 1:1} ?This is how I classify training and testing data:This is my model:"
3225,How can I find expected target phrase or keywords from given sentence in Python?,"['python', 'nlp', 'sentiment-analysis', 'feature-extraction', 'spacy']","I am wondering that is there any efficient way to extract expected target phrase or key phrase from given sentence. So far I tokenized the given sentence and get POS tag for each word. Now I am not sure how to extract target key phrase or keyword from given sentence. The way of doing this is not intuitive to me. Here is my input sentence list:here is the tokenized sentence:Here I used Spacy to get POS tag of words:so I may use NER (a.k.a, name entity relation) from spacy but its output is not the same thing with my pre-defined expected target phrase. Does anyone know how to accomplish this task either using Spacy or stanfordcorenlp module in python? what is an efficient solution to make this happen? Any idea? Thanks in advance :)desired output:I want to get the list of target phrase from respective sentence list as follow:so I concatenate my input sentence_list with an expected target phrase, my final desired output would be like this: How can I get my expected target phrases from a given input sentence list by using spacy? Any idea?"
3226,Any efficient way to find surrounding ADJ respect to target phrase in python?,"['python', 'parsing', 'nlp', 'sentiment-analysis']","I am doing sentiment analysis on given documents, my goal is I want to find out the closest or surrounding adjective words respect to target phrase in my sentences. I do have an idea how to extract surrounding words respect to target phrases, but How do I find out relatively close or closest adjective or NNP or VBN or other POS tag respect to target phrase.Here is the sketch idea of how I may get surrounding words to respect to my target phrase.Note that my original dataset was given as dataframe where the list of the sentence and respective target phrases were given. Here I just simulated data as follows:Here I tokenize the sentence as follow:then I try to find out surrounding words respect to my target phrases by using this loot at here. However, I want to find out relatively closer or closet adjective, or verbs or VBN respect to my target phrase. How can I make this happen? Any idea to get this done? Thanks"
3227,How do i make R show its computations (sentiment analysis),"['r', 'sentiment-analysis']","When R is computing the sentiment values (package syuzhet), can I somehow get the program to show the exact computations leading up to, say a value 2?I get that it assigns (according to our lexicon) for example +1 to the sentence-value if the word ""good"" shows up somewhere, and it subtracts 1 if ""bad"" is found. But i would like an output for each sentence reading something like this:There must(?) be a command for this, but i can't for the life of me find it."
3228,Ignore '️' while tokenizing tweets,"['python', 'twitter', 'tweepy', 'sentiment-analysis']","I'm making a twitter web crawler for sentiment analysis. 
I'm following this tutorial https://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/.In this tutorial(part 3) Marco teaches how to ignore some terms using a stop variable in his algorithm. However when I start to collect the tweets and try to match the co-occurrence matrix - which contains the number of times the term x has been seen in the same tweet as the term y - tweepy is collecting this term: '️' . I don't know what this is in Unicode and how can I ignore. I've tried the apostrophe, single right/left quote, and none of these seems to work.Any thoughts?"
3229,sentiment classification using keras,"['python-3.x', 'keras', 'sentiment-analysis']","I very new to deep learning classification. I have reviews data with the label(pos, neg) and I 'm trying to classify the data using keras. here is my code:I got an error:I have tried every solution mention related to this error but still cannot fix.Any help? thanks in Advance"
3230,Workaround for python MemoryError,"['python', 'keras', 'sentiment-analysis']","How can I change this function to make it more efficient? I keep getting MemoryErrorI call the function here:Train and Test data are IMDB dataset for sentiment analysis, i.e.EDIT: I am running this on 64 bit Ubuntu system with 4 GB RAM.Here is the Traceback:"
3231,Sentiment analysis using Hidden Markov Model,"['python', 'nlp', 'sentiment-analysis', 'hidden-markov-models']","I have a list of reviews, each element of the list is a review of IMDB data set in kaggle. there are 25000 reviews in total. I have the label of each review +1 for positive and -1 for negative.I want to train a Hidden Markov Model with these reviews and labels.1- what is the sequence that I should give to HMM? is it something like Bag of words or is it something else like probabilities which I need to calculate? what kind of feature extraction method is appropriate? I was told to use Bag of words on review's list, but when I searched a little I find out HMM cares about the order but bag of words doesn't maintain the order of words in sequences. how should I prepare this List of reviews to be able to feed it into a HMM model?2- is there a framework for this? I know hmmlearn, and I think I should use the MultinomialHMM, correct me if I'm wrong. but it is not supervised, its models do not take labels as input when i want to train it, and I get some funny errors which I don't know how to solve because of the first question I asked about the correct type of input I should give to it. seqlearn is the one I find recently, is it good or there is a better one to use?I appreciate any guidance since I have almost zero knowledge about NLP. "
3232,Unsupervised sentiment Analysis using doc2vec,"['nlp', 'gensim', 'word2vec', 'sentiment-analysis', 'doc2vec']","Folks,I have searched Google for different type of papers/blogs/tutorials etc but haven't found anything helpful. I would appreciate if anyone can help me. Please note that I am not asking for code step-by-step but rather an idea/blog/paper or some tutorial. Here's my problem statement:  Just like sentiment analysis is used for identifying positive and
  negative tone of a sentence, I want to find whether a sentence is
  forward-looking (future outlook) statement or not.I do not want to use bag of words approach to sum up the number of forward-looking words/phrases such as ""going forward"", ""in near future"" or ""In 5 years from now"" etc. I am not sure if word2vec or doc2vec can be used. Please enlighten me.  Thanks. "
3233,Python: Split text by keyword into excel rows,"['python', 'regex', 'text', 'extract', 'sentiment-analysis']","New to programming, found a lot of helpful threads already, but just not quite what I need.
I have one text file that looks like:  As output I would like JUST the body of every article in a new row (one cell per article body) in one file (I have around 5000 articles to process like this). The output would be 5000 rows and 1 column.
From what I could find it seems 're' would be the best solution. So the recurring keywords are BODY: and perhaps DOCUMENTS. How do I extract just the text between those keywords into a new row in excel for every article?or something like this?I'm a bit lost in where to look, thanks in advance!  EDIT: Thanks to ewwink I'm currently here:"
3234,Specifics on sentiment analysis,"['sentiment-analysis', 'luis']","I am working on a bot that uses sentiment analysis to analyze questions from people in the health care insurance realm.  One of the main things we do is connect people with care through a find care tool.  The basic questions people ask are in the form ""I need a new doctor"" or ""I need to see a dermatologist"".  When the sentiment is negative we want to pass the user on to customer service, but as we just discovered, any statement starting with ""I need"" is considered a negative statement by LUIS' built in sentiment analysis.  This is a big problem for us!If there is a way to tweak that, please someone let me know, though I don't hold out much hope.  But I would like to understand - why was that decision made?  Why is a simple ""I need"" question considered negative instead of neutral?"
3235,Why is stemming important for sentimental analysis,"['r', 'sentiment-analysis', 'text-analysis', 'stemming']","I am using seven lexicons to calculate sentimental scores on a data set containing forum posts. Apart from removing all noise such as whitespace, special char, digits and stopwords, why is it also important to stem the words?I am using Harvard.IV, Qdap, Henry's Financial dictionary and Loughran-McDonald Financial dictionary from SentimentAnalysis package, as well as AFINN, NRC and BING dictionaries."
3236,Classifying negative and positive words in large files?,"['nlp', 'nltk', 'sentiment-analysis', 'wordnet', 'senti-wordnet']","I am trying to get the count of positive and negative in a very large file. I only need a primitive approach(that does not take ages). I have tried sentiwordnet but keep getting a IndexError: list index out of range, which I think it's due to the words not being listed in wordnet dictionary. The text contains a lot of typos and 'non-words'.If someone could give any suggestion, I would be very grateful!"
3237,Hatespeech Detection in R plain text,"['r', 'sentiment-analysis']","I have a column in my R dataset that has been filled in through plain/free text. I'd like to be able to search this column for any sort of hate speech, I know there already exist sentiment analysis but I was wondering if there was a hate speech specific one?"
3238,Sentiment wordcloud using R's quanteda?,"['r', 'sentiment-analysis', 'word-cloud', 'quanteda']","I have a set of reviews (comment in words + rating from 0-10) and I want to create a sentiment word cloud in R, in which:I used quanteda to create a dfm of the comments. Now I think I want to use the textplot_wordcloud function and I guess I need to do the following:I'm having trouble with step 1 and 4. The rest should be doable. Can somebody explain how a dfm can be read manipulated easily?"
3239,"Sentiment Analysis, Naive Bayes Accuracy","['python', 'sentiment-analysis', 'naivebayes']","I'm trying to form a Naive Bayes Classifier script for sentiment classification of tweets. I'm pasting my whole code here, because I know I will get hell if I don't. So I basically I use NLTK's corpuses as training data, and then some tweets I scraped as test data. I pre-process them and do a bag of words extraction. The classifier is trained with no problem and when I do the followingit correctly outputs 'pos'.Now my problem is how to calculate accuracy using ntlk.util accuracy. I doand I get the following error:I also tried this and I get the same kind of errorI am 100% I am missing something extremely obvious for Machine Learners.I think t But it's been 3 days and I'm slowly losing my mind, so any help will be appreciated.Code ->"
3240,How to pass a string value to a Sentiment Analysis RNN Sequential Model and get back a prediction,"['python', 'machine-learning', 'keras', 'recurrent-neural-network', 'sentiment-analysis']","I recreated a sentiment analysis machine learning project using my own data set along with some minor modifications to improve its completion time, I can create good model, compile it, fit it and test it without issues, the problem comes however on how to pass the model a new string/ article and it in return pass a prediction on whether the string comments are positive or negative and was hoping someone could help me.I posted my code below for your review."
3241,Any pre-trained ML models or classifiers for finding a tweet's sentiment?,"['python', 'twitter', 'nltk', 'sentiment-analysis']","I wonder are there any existing/pre trained ML models or trained classifiers for finding a tweet's sentiment.I came across Twitter samples in Python NLTK library at http://www.nltk.org/nltk_data/. This dataset is useful for classifying a Tweet to be either a positive or a negative. But I'm looking for a pre-trained ML model for finding DOW/Fintech companies Twitter sentiment which can predict the TextBlob's polarity and subjectivity of tweets.IMO, a domain specific pre-trained ML model would yield better results. Please correct me if I'm wrong."
3242,Plotly - represent two lines with very different scale,"['python', 'python-3.x', 'plotly', 'sentiment-analysis']","I have to present a work about Sentiment Analysis on Social Media and I need to show the price of a crypto vs its sentiment value.I am using plotly whicj shows the plot online in the html panel; so, I'm ok with data of the same scale (sentiment vs sentiment for example), but I am not able to use different scales and plot the sentiment ([-1;1]) and the price of bitcoin
(ex. [5000, 70000]). My code is the follows:I have been using matplotlib and I was able to represent two type of scaled y axis. But I don't know yet this package and I have some difficulties with this code. Thanks advice!The sample are:
SENTIMENT dataset:BITCOIN dataset:"
3243,Is there any sentiment forum dataset for unsupervised training available?,"['sentiment-analysis', 'unsupervised-learning']","I recently finished a machine learning course and would like to make a forum sentiment analysis tool, to apply it in stock-related forums. The idea is to:Actually, I do this myself (pay attention on forums) plus my own technical analysis and the obligatory due diligence, and it has been working very well for me. I just wanted to try to automate it a little bit and maybe even allow a program to play with some of my accounts (paper trading first, and if it performs decently assign some money in a real account) This would be my first machine learning project (just as a proof-of-concept) so any comments would be very kindly appreciated.The biggest problem that I find is that I would like to make an unsupervised training, and I need a sample dataset to do the training.Question: Is there any known forum-sentiment dataset available to be used for unsupervised training? I've found several sentiment datasets (twitter, imbd, amazon reviews) but they are very specific to their niche (short messages, movies, products...) but I'm looking for something more general."
3244,Data set for Doc2Vec general sentiment analysis,"['dataset', 'artificial-intelligence', 'gensim', 'sentiment-analysis', 'doc2vec']","I am trying to build doc2vec model, using gensim + sklearn to perform sentiment analysis on short sentences, like comments, tweets, reviews etc.I downloaded amazon product review data set, twitter sentiment analysis data set and imbd movie review data set.Then combined these in 3 categories, positive, negative and neutral.Next I trinaed gensim doc2vec model on the above data so I can obtain the input vectors for the classifying neural net.And used sklearn LinearReggression model to predict on my test data, which is about 10% from each of the above three data sets.Unfortunately the results were not good as I expected. Most of the tutorials out there seem to focus only on one specific task, 'classify amazon reviews only' or 'twitter sentiments only', I couldn't manage to find anything that is more general purpose.Can some one share his/her thought on this? "
3245,ML.Net Bad value at line 7 in column Label,"['c#', 'machine-learning', 'sentiment-analysis', 'ml.net']","I am trying ML.NET for basic sentiment analysis as given in link https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/sentiment-analysis .
I have followed step by step and used same files given in the link for training. Also checked all comments and answers in similar problem by another user in this link:
ml.net sentiment analysis warning about format errors & bad values But still getting following errors. Most of training data has been resulted in errors as shown below (Processed 860 rows with 818 bad values). Ideally this should not happen as the data and code both is provided from Microsoft official site (first link given above). Code and error is pasted below.   Is there any change in microsoft site data, which is not yet updated by them?  Below is the code:"
3246,"TensorFlow ValueError: Cannot feed value of shape (32, 2) for Tensor 'InputData/X:0', which has shape '(?, 100)'","['python', 'tensorflow', 'sentiment-analysis', 'training-data', 'valueerror']","I am new to TensorFlow and machine learning. I'm trying to create a sentiment analysis NN with tensorflow.I've set up my architecture and I'm attempting to train the model but I encounter the error ValueError: Cannot feed value of shape (32, 2) for Tensor 'InputData/X:0', which has shape '(?, 100)'I think the error has to do with my input ""layer net = tflearn.input_data([None, 100])"".
The tutorial I was following suggested this input shape, batch size as None and the length to be 100 since that's the sequence length. Hence (None, 100), to my understanding this is the dimensions the training data being fed into the network needs to be, correct?Could someone explain why the suggested input shape of batch size was None and also why Tensor flow is attempting to feed the network put shaped (32,2). Where is the sequence length of 2 coming from? If my understanding anywhere in this explanation is wrong feel free to correct me, I'm still trying to learn the theory as well.Thanks in advance"
3247,Attach sentiment to each word from a dataframe,"['python', 'pandas', 'dataframe', 'sentiment-analysis']",I did a sentiment analysis of tweets but now I have to attach the sentiment to each word from the tweet text. My sentiment analysis was based on a sum of words that appeared in a dictionary. I hope this example can help you.I tried to use this function but it does not work here.Example:Wanted result:
3248,Creating a function to remove only specific word in a list (R),"['r', 'dplyr', 'filtering', 'tidyverse', 'sentiment-analysis']","I have a list with undesirable words (in spanish) which are meaningless, but they are also present inside another. I just want to remove it when they are a term, not when they are a piece of another word.For example: ""la"" is an spanish article, but if I use a function to remove it, also will break into two words a useful term like ""relacion"" (which means relationship)My first choice was creating a function to remove this terms.My second choice was with a list, and then using filter inside the dfBut always the result is removing all those words and breaking terms in two words which makes my analysis useless.
Thanks."
3249,filtering for n char inside a value in r,"['r', 'dplyr', 'analysis', 'sentiment-analysis']","i'm doing sentiment analysis, but I need to filter by n char inside every tweet. I mean:Im expecting a result like: ""most beauty"", ""ugly"", ""beauty""I've tried with $str_detect but is useless"
3250,Sentiment Analysis Of A Dataset With Multiple NewsPaper Articles,"['r', 'sentiment-analysis', 'sentimentr']",I'm trying to call get_nrc_sentiment in R but getting the following error: Error in get_nrc_sentiment(Test) : Data must be a character vector.Can anyone see what I'm doing wrong?
3251,"ValueError: Error when checking target: expected dense_2 to have shape (1,) but got array with shape (500,) [Sentiment Analysis]","['python-3.x', 'numpy', 'tensorflow', 'keras', 'sentiment-analysis']","Research done before asking this question:
Error when checking target: expected dense_2 to have shape (None, 256) but got array with shape (16210, 4096)ValueError: Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (10000, 1)Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)I have searched for a solution to this problem for days now. Please help me figure this out.The above code is my model. I will now print the summary of above model:And finally I will show you the result of np.shape():I have tried everything from Reshape() to adding input_shape to the dense layers but the result is always the same. What am I doing wrong and how to I fix this? My task is sentiment analysis.EDIT: I was told that the dimensions of output needed to be (1117228,1) and I needed sentiment scores in train_test_split for the labels. The first half of my csv is negative sentiment and the other half is positive sentiment. How would I use this?"
3252,How to build embedding layer with tensorflow for the following model?,"['tensorflow', 'keras', 'deep-learning', 'tensorboard', 'sentiment-analysis']",This is a keras model for sentiment analysis i need to convert it to tensorflow i couldn’t build embedding layer with tensorflow and using confusion matrix to evaluate this model? And I asked if tf-learn is the-same as tensorflowRead CSV FilesNormalize TextCleaned TextSplit reviews to tokens Padding Sequence to make all vectors with the same size according to MAX-length of reviews Convert target value from string to integerTrain-Test-SplitCreate the modelEvaluate model
3253,Extract emotions calculation for every row of a dataframe,"['r', 'text-mining', 'tidyr', 'sentiment-analysis']",I have a dataframe with rows of text. I would like to extract for each row of text a vector of specific emotion which will be a binary 0 is not exist this emotion or 1 is exist. Totally they are 5 emotions but I would like to have the 1 only for the emotion which seem to be the most.Example of what I have tried:Example of expected output:Any hints will be helpful for me.Example to make it for every row what is the next step? How can I call every line with the nrc lexicon analysis?
3254,How to get comments that rate the service from many Facebook pages (from restaurants) for sentiment analysis in 2018? (Twitter is not good enough),"['facebook', 'facebook-graph-api', 'twitter', 'sentiment-analysis']","I need to know If I need to give up on Facebook and just use Twitter for my purposes (if so, just tell me to use Twitter). I want to use pages I don't own. I am doing this same process in Twitter, but Twitter doesn't have pages that rate places, services or things in general with comments and a rating system (or atleast the ones I need from one city with specific places).Twitter has much more liberty to get comments from any @user_name but its not good enough for a sentiment analysis specific assignment I need to do. I am trying to get facebook comments or posts from multiple facebook pages (pages from restaurants) based on the comments used for ratings and download them to a csv or database with any program or by coding (Preferably Node.js, javavascript or php) for sentiment analysis (this is very easy with Twitter!). How can I achieve this with all the policies and restrictions Facebook has in 2018??I have some questions with reference to facebook:I tried to use Graph API from facebook for 2 different pages using GET and POST and I get some errors that I didn't have trouble with 9 months ago (this happens with every page I try to search with code in node.js, javascrip, php or the graph API from facebook):"
3255,How to remove unusual characters from JSON dump in Python?,"['json', 'python-3.x', 'sentiment-analysis']",I have been searching around for a good way to remove all unusual characters from a JSON dump of tweets that I am using to compile a dataset for sentiment analysis.characters I am trying to remove = ンボ チョボ付 最安値These characters appear in my tweet data and I am trying to remove them using regex but to no avail.I tried using re.sub() to take away these charcters but it will not work. How can I make this work?
3256,How can I concatenate the lines of dialogue while doing Natural Language Processing on a book,"['python', 'sentiment-analysis', 'natural-language-processing']",I am working on a sentiment analysis project of a book. I am using nltk.vader.sentimentintensityanalyzer to record the sentiment polarity of paragraphs in the Harry Potter series. To create paragraphs and remove the line breaks I did: This breaks the book down into paragraphs. The problem arises when it comes to dialogue.Dialogue has the same paragraph breaks in between each character's words How can I edit my breakdown methods so dialogue stays together as one element? The dialogue as a whole will then be used as a single input into the intensity analyzer.
3257,DataFrame calculus for sentiment analysis Python,"['python', 'pandas', 'dataframe', 'sentiment-analysis', 'tweets']","I have the following problem with Sentiment Analysis of tweets, using a list of words having words and values (positive or negative).
I tried the following code to create DataFrames and it works. But do you have an idea about the calculus? What I have to do is to identify if a tweet contains a word from a wordlist. If it contains some words from the wordlist, then sum the values assigned to each word and print the result (tweet | sum). Any idea??Example of the word list: abandon -2abandoned -2abandons -2abducted -2Example of a tweet text:
@Brenamae_ I WHALE SLAP YOUR FIN AND TELL YOU ..."
3258,using text2vec for multilabel classification,"['r', 'sentiment-analysis', 'text2vec']","I want to know if text2vec package can be used for multilabel classification like python's BinaryRelevance in skmultilearn.problem_transform
I'm currently referring to the pipeline documented at:
http://text2vec.org/vectorization.html"
3259,How does Google Language API split text into sentences to assign sentiment?,"['google-api', 'google-cloud-platform', 'nlp', 'sentiment-analysis', 'google-natural-language']","The question is in the title.I have joined sentences into a large text, which I then call analyze_sentiment on. The goal is to pull sentiments for the individual sentences - exactly the ones originally joined. I first clean out all punctuation, lower the characters, capitalize sentences, end them with . and join with a space. Here is an example of two sentences that Google considers to be a single sentence.She answered my questions with ease Thx. Tyler was so considerate.However, She answered my questions with ease Thx. Sam was so considerate.works correctly. You can try this yourself by going to their natural-language page and trying the API.If I know the splitting conditions, I can format my original sentences accordingly. "
3260,How to highlight negative and positive words in a Wordcloud using R,"['r', 'text', 'sentiment-analysis', 'word-cloud', 'tidytext']","I am performing a sentiment analysis using R, and I was wondering how to split the wordcloud into two parts, highlighting positive and negative words. I am quite new to R and the online solutions didn't help me. That is the code:    And this is the result I would like to achieve:Thanks for everyone will help me.EDIT:"
3261,Unable to predict sentiment of emoticons,"['python-3.x', 'machine-learning', 'utf-8', 'nltk', 'sentiment-analysis']","I am trying to predict the sentiment of facebook's comment using vader sentiment analysis tool[1] but it is not able to predict the sentiment of emoticons ,it is working in some comments while in some other it is not.The excerpt of the output is-It is running on some sentences but not on other,I am traversing from the database.Also, in some cases when I use only 1 emoticon it works but on using multiple times, it doesn't works. How to resolve this error?[1]:Vader Sentiment Analysis tool!"
3262,Finding sentiment of sentence containing not word,"['nlp', 'sentiment-analysis']","I used a code for getting sentiment (sense i.e good, bad, average) of any sentence by matching the adjective word with my predefined set of good, bad, average words, Set of bad words, set of average words in the sentence. But for negation(sentence containing ""not"") I am not able to assign exact sense(whether good or bad or average) to the sentence containing not from my code. Ex:- sentence-"" Bob is best boy in the school."" Since in this sentence there is one adjective ""best"" matching to the good set than Good sense is assigned to this sentence.But, for negation sentence-""Bob is not best boy in the school"". Since in this sentence there is only one adjective ""best"" matching to the good set than Good sense is assigned to this sentence. But here ""not"" makes the sense to bad but my code is not able to do handle ""not"" in the sentence.Help me to solve the negation problem"
3263,Why does Neural Network predict all input negative?,"['python', 'neural-network', 'keras', 'prediction', 'sentiment-analysis']","I'm working on a sentiment analysis project in python with word2vec as an embedding method. (in my NON_ENGLISH corpus I considered every negative tweet with the 0 label, positive = 1 and neutral = 2)Since I'm really new in this field I don't have any idea as to why my model predicts everything negative. I surfed the net before and read something about the number of hidden layer and batches , etc. which related to this error but I'm not sure that I understood correctly.
here is my keras model:thanks a lot!"
3264,"Object 'negative"" not found error in R - During Sentiment Analysis","['r', 'sentiment-analysis', 'mutate']","I am trying to do Sentiment Analysis in R by extracting the Tweets. Tweets are getting extracted fine. But getting this error when I try to Tokenize the tweets. Error I get is ""Error in mutate_impl(.data, dots) : 
    Evaluation error: object 'negative' not found."".This is the code I have writtenThis error is getting thrown once I include the Mutate statement. 
Appreciate any help I can get in resolving this please."
3265,UnicodeDecodeError Sentiment140 Kaggle,"['database', 'python-3.x', 'encoding', 'import', 'sentiment-analysis']","I am trying to read the Sentiment140.csv available on Kaggle: https://www.kaggle.com/kazanova/sentiment140My code is this one:And it gives me this error:UnicodeDecodeError: 'utf-8' codec can't decode bytes in position
  80-81: invalid continuation byteThe things I would like to understand are: 1) How do I solve this issue?2) Where can I see which type of encoding should I use instead of ""utf-8"", based on the error?3) Using other encoding methods will cause me other issues later on?Thanks in advanceP.s. I am using python3 on a mac"
3266,Adding multiple hidden layers keras,"['python-3.x', 'keras', 'nlp', 'deep-learning', 'sentiment-analysis']","I have a simple sentiment analyzer using keras, here is my code, in which I use the keras code on github: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.pyThe initial and working model is:The error I get is this one:ValueError: Input 0 is incompatible with layer gru_2: expected ndim=3,
  found ndim=2This happens every time I try to add a second hidden layer on the model, for instance: I believe I am missing something concerning the dimensions of the hidden layers. How should I proceed to add another hidden layer successfully?Thanks in advance,"
3267,ValueError: not enough values to unpack,"['python-3.x', 'dictionary', 'nlp', 'nltk', 'sentiment-analysis']","I am trying to learn (on Python3) how to do sentiment analysis for NLP and I am using the ""UMICH SI650 - Sentiment Classification"" Database available on Kaggle: https://www.kaggle.com/c/si650winter11At the moment I am trying to generate a vocabulary with some loops, here is the code:I keep getting this error, that I don't fully understand:in  label, sentence = line.strip().split(""\t"".encode())
  ValueError: not enough values to unpack (expected 2, got 1)I tried to add     like suggested in here: ValueError : not enough values to unpack. why?
But it didn't work for my case. How can I solve this error? Thanks a lot in advance,"
3268,How to use polarity score to give rating for review text in python,"['python', 'python-3.x', 'sentiment-analysis']","Currently, I am working in python Textblob to generate the rating for review Text 
 and by using polarity score I am able to get the polarity score (a float) between -1 to 1 for a text like below Now I want the rating from 0 to 5 (integer). I am stuck with that how to make polarity score from -1 to 1 to 0 to 5 or any other better approach for doing this to generate the rating.Thanks in advance"
3269,Existing Warmth/Competence Dictionaries for NLP?,"['nlp', 'sentiment-analysis', 'text-analysis', 'natural-language-processing']","I am doing text analysis for feedback left for a large set of employees. I am a researcher and want to assign a ""warmth"" score to each comment (warmth = comments like ""Jerry is friendly!"" ""Sarah is kind!""), and also a competence score (e.g. ""Anna is very capable."" ""Jack is very skilled at programming.""). Before I build out my own dictionaries to embody these concepts, I wanted to see if there are any existing dictionaries out there. I know there are many dictionaries for negative/positive words, but that's not what I am trying to do. I am using R to analyze the data, but any dictionary in an importable format would work. Thanks!"
3270,Sending a string to python script using PHP and get the output back to the PHP script,['sentiment-analysis'],"What i am trying to do is call a python script using PHP which is performing a sentiment analysis and return back the result of the analysis in the form of polarity either -ve or +ve.
The reason why im calling Python from PHP is that i have an android application which uses PHP and i dont know if we can call Pyhton from android studio.
Here is my Python Script:And here is my PHP scriptI dont know where im going wrong but when i only execute my script on the xampp server the following error occurs""The server encountered an internal error and was unable to complete your request.Error message:
End of script output before headers: test.py"""
3271,"Error : Not all elements of twList are of the same class, when I convert the tweets with all variables to dataframe","['r', 'sentiment-analysis', 'rtweet']","I am using rtweet package in R to extract tweets of particular hashtag which basically needs appname,api_key,api_secret,access_token,access_token_secret.
So i have created an app in Twitter to get all the above details. And then I pass the above as followsSo this will basically extract tweets with 88 variables. Now I want to write these tweets to an excel file for further analysis.And for that I am using the below code.When I run this I am getting the following error.Just want to understand what could be the potential issue here. Problem is I cannot put the exact code here as that needs my api key and all.Any help would be really appreciated.Regards,
Akash"
3272,Rapidminer more than one query text analysis,"['twitter', 'sentiment-analysis', 'rapidminer']","I am using rapidminer because I want to perform a sentiment analysis. 
The things are, I have 7 queries that I need to analyze together (companies' names that I need to analyze to obtain insights about the customer).
So my idea was then to extract the data with twitter app developer and then put in rapidminer to analyze.When I open this data in rapidminer it shows that
there are some problems with the dataset with the following:-How do I fix this?Once enter my spreadsheet data (.csv file). It shows me the error""
Cause: Unparseable number: ""FALSE""
I've searched here already for answers but none helped me to solve this error.Is it possible to analyze this data altogether or do I
have to do it separately?
I'm not sure if this is feasible, I
suppose it would interfere in the overall analysis?I'm quite new at rapidminer, so I appreciate you all's help.
Thanks in advance."
3273,How to extract an word and all his dependencies,"['python', 'parsing', 'nlp', 'data-science', 'sentiment-analysis']","I'm not data-scientist but I'm on a project where i need to do aspect based sentiments analysis, I've already done an classifier for sentiment analysis but now, I need to do the ""aspect based"" part.I have a list of aspects (4) and I need to find this aspect in a text, get all his depencies and analyse the sentiment of this group of words.The cake had good taste but the tea wasn't good at all""The cake had good taste"" = POS / ""the tea wasn't good at all"" = NEGI've already explore stanford CoreNLP depencies parser but in french (because i've to do this in french) it's not so good (maybe I need to only keep Nouns and Adjectives for the parsing).If you've any suggestions..."
3274,Request error Google Cloud NLP API with Swift,"['swift', 'google-cloud-platform', 'alamofire', 'sentiment-analysis', 'google-natural-language']","I am trying to make a request to Google Cloud NLP API to obtain sentiment analysis for a piece of text. I used Postman to design the correct request, and I was able to get a valid response using Postman. However, when I try to make the same request from Swift, it gives me an error. The error and code snippet used to make the request is shown below.Error: "
3275,How is sentiment score calculated in the R SentimentAnalysis package?,"['r', 'nlp', 'sentiment-analysis', 'sentimentr']","I'm using the General Inquirer dictionary with the SentimentAnalysis package and I can't figure out how they assign the sentiment score...For example, if I run the following code: I'll get an output like this: What's the scale being used here? -1 to 1? I don't know how to interpret these results. Thanks! "
3276,"Merging many statistical methods for Text classification, starting with SVM multiclass classifier","['python', 'machine-learning', 'scikit-learn', 'svm', 'sentiment-analysis']","Premise: I am not an expert of Machine Learning/Maths/Statistics. I am a linguist and I am entering the world of ML. Please when answering, try to be the more explicit you can.My problem: I have 3000 expressions containing some aspects (or characteristics, or features) that users usually review in online reviews. These expressions are recognized and approved by human beings and experts. Example: “they play a difficult role”The labels are: Acting (referring to the act of acting and also to actors), Direction, Script, Sound, Image. The goal: I am trying to classify these expressions according to their aspects. My system: I am using SkLearn and Python under a Jupyter environment.Technique used until now: First attempt showed 0.63 of accuracy. When I tried to create more labels from the class Script accuracy went down to 0.50. I was interested in doing that because I have some expressions that for sure describe the plot or the characters. 
I think that the problem is due to the presence of some words that are shared among these aspects. I searched for a solution to improve the model. I found something called “learning curve”. I use the official code provided by sklearn documentation http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html . The result is like the second picture (the right one). I can't understand if it is good or not. In addition to this, I would like to:How can I do this? I read that in some works researchers have used more systems... How should I handle this? From where can I retrieve the resulting numbers from the first system to use them in the second one?I would like to underline that there are some expressions, verbs, nouns, etc. that are used a lot in some contexts and not in others. There are some names that for sure are names of actors and not directors, for example.  In the future I would like to add more linguistic pieces of information to the system and trying to improve it.I hope to have expressed myself in an enough clear way and to have used an appropriate and understandable language."
3277,How do I modify my Python code to separate sentiments from tweets,"['python', 'tweepy', 'sentiment-analysis']","Problem
I am a beginner to Python coding and wrote a piece of code that performs sentiment analysis on tweets. The results are displayed as follows :
TWEETS (POLARITY_SCORE, SUBJECTIVITY_SCORE)  Now, I want to write my results to a CSV file with 3 columns where the first column should be the tweets, the second column should be polarity and the last column subjectivity score. However being new to Python I couldn't figure out how to go about it. I tried some different random techniques but I did not get the desired results. Any help (simple explanation should be enough) in this regard would be of great help.Here is the code I wrote :  What I thought was analysis.sentiment[0] was storing the polarity while analysis.sentiment[1] was storing the subjectivity, however when I tried to do the same and print the results, my answers were different from what I was expecting.OPTIONAL DOUBT (THIS QUESTION NEED NOT HAVE AN ANSWER RIGHT NOW) What would be my approach if I wanted to check if the polarity is greater than a specified value how would I do that?"
3278,Making predictions on text data using keras,"['python', 'keras', 'conv-neural-network', 'prediction', 'sentiment-analysis']","i had trained and tested a CNN for sentiment analysis. The train and test data were prepared the same way, tokenizing the sentence and giving unique integers:Then pre-trained glove model to create embedding matrix for CNN as:At this point i also used the test sentences to create this matrix which was later passed as weights into embedding layer:Now i want to reload my model and test with some new data but it would mean that embedding matrix wont include some words from new data.This makes me wonder that if even before i shouldnt have had included test data while creating embedding matrix? And if not,how does the embedding layer work for those new words?This part is similar to this question but i couldnt find answer: 
How does the Keras Embedding Layer work if word is not found?
Thanks"
3279,Stanford coreNLP sentiment analysis debug takes too much time,"['java', 'stanford-nlp', 'sentiment-analysis']",CoreNLP model load during debugging to much time.CoreNlp Demo[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger... done [566.7 sec].[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done  [390.8 sec].[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment
3280,i want to calculate what emotions are impacting sentiments how can i do this in R?,"['r', 'sentiment-analysis']","emotiofileI have 10 columns, with 8 emotion scores and 2 sentiments scores
emotions like anger sad happy...
sentiments being positive and negative
each emotion and sentiments have scores again ranging from 0 to x
how can i calculate which emotion combinations are impacting positive sentiment or negative by using R language ?"
3281,Best lexicons for sentence vs document level analysis,"['nlp', 'nltk', 'sentiment-analysis', 'lexicon']","What are the best lexicons for document-level and sentence-level analysis? I'm using Vader currently for sentence-level analysis, however I'm worried that when I move to the document level, Vader may not perform as well as others. Similar question to the post here, however more specific."
3282,How to improve the sentiment score if I am using vader in NLTK?,"['python', 'nlp', 'nltk', 'sentiment-analysis', 'text-analysis']",I am working with social media data. I am getting almost a neutral score for positive sentences too and code is not understanding the statement rather just classifying using the corpus. 
3283,Edit Vader_lexicon.txt in nltk for python to add words related to my domain,"['python', 'python-3.x', 'nlp', 'nltk', 'sentiment-analysis']","I am using vader in nltk to find sentiments of each line in a file. I have 2 questions:assaults -2.5    0.92195 [-1, -3, -3, -3, -4, -3, -1, -2, -2, -3]What does -2.5 and 0.92195 [-1, -3, -3, -3, -4, -3, -1, -2, -2, -3] represent?How should i code it for a new word? Say i have to add something like '100%' , 'A1'."
3284,"VaderSentiment bug: TypeError: a bytes-like object is required, not 'str'","['python', 'api', 'sentiment-analysis', 'vader']","Hi I have written code below to perform a sentiment analysis:However, I am receiving an error: How do I fix this?"
3285,Text mining UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1671718: character maps to <undefined>,"['python', 'api', 'sentiment-analysis', 'vader']",I have written code to create frequency table. but it is breaking at the line  ext_string = document_text.read().lower(. I even put a try and except to catch the error but it is not helping.
3286,Sentiment analysis with keras including neutral tweet,"['python', 'twitter', 'keras', 'sentiment-analysis']","I'm working on a sentiment analysis project in python with keras using CNN and word2vec as an embedding method I want to detect positive, negative and neutral tweets(in my corpus I considered every negative tweets with the 0 label, positive = 1 and neutral = 2). Since I'm new in this field I have some questions,
here is a part of my code:
***Assuming that X-train and X-test contain tweets and Y-train and Y-test contain tweet's labels.in the code above you see that I considered if a related label was 0(if labels[index] == 0 :) as negative I put [1.0, 0.0] in some specific list and if the label was 1(if labels[index] == 1 :) I put [0.0, 1.0] as positive tweets and else (if labels[index] == 2 :) as neutral i put [1.0, 1.0] so just consider that the logical part af my code that i mentioned is ok.here is my keras model: So in order to  predict a new input, I have this code:My question contains 2 parts: 
I wanna know is it true to predict in such way? AS far as I told I considered label 2 for neutral tweets and for this reason I considered if (np.argmax(sentiment) ==2) then print neutral - Is this logical or acceptable for prediction??I mean I considered to assign [0.1, 1.0] for neutral tweets in train and test set so If I consider 2 as neutral in prediction part, does it make any sense??thanks a lot****for regression is it true to change my train and test code in such way?
considering 0,1,2 as polarities in my corpusthen setting'sigmoid' for activation:and Can I predict my input tweet in the way i mentioned above??*****If I used word2vec for embedding and considering 0,1,2 as polarities in my corpus Can I set labels in such way?and then for compiling:thank you for your patience"
3287,"expected conv1d_1_input to have shape (15, 512) but got array with shape (4, 512)","['python', 'keras', 'conv-neural-network', 'prediction', 'sentiment-analysis']","I'm working on a sentiment analysis project in python with keras using CNN and word2vec as an embedding method.
according to my code, I set my input shape, 15 and 512 so when I want to predict the polarity of a new sentence say:""I am so sorry"" for example, with the length: 4 - I face this error:expected conv1d_1_input to have shape (15, 512) but got array with shape (4, 512)
  and this is a part of my code:all I can do is making new python file and load all my related models such as word2vec ... and change the input shape on it like this:I wanna know whether this method is ok or not?? any efficient solution would be really appreciated"
3288,Error while running my python program,"['python-3.x', 'sentiment-analysis']","There has been this constant error while trying to run my code and i'm unable to understand what it means and how to make it right.C:\code\sentimentanalysis\venv\sentiment analysis\Scripts\python.exe""
  C:/code/sentimentanalysis/main.py Traceback (most recent call last):
  File ""C:/code/sentimentanalysis/main.py"", line 1, in 
      import sys,tweepy,csv,re   File ""C:\Users\Vermaaka\AppData\Local\Programs\Python\Python37\lib\site-packages\tweepy__init__.py"",
  line 17, in 
      from tweepy.streaming import Stream, StreamListener   File ""C:\Users\Vermaaka\AppData\Local\Programs\Python\Python37\lib\site-packages\tweepy\streaming.py"",
  line 358
      def _start(self, async):
                           ^ SyntaxError: invalid syntaxProcess finished with exit code 1I have already installed all the packages before starting with this project. Please help. "
3289,Error in converting tweets to text format in R,"['r', 'twitter', 'sentiment-analysis']","I want to do sentiment analysis of twitter data using R. I have my dataset as dataset.txt and in R, I am doing these:I am getting error in this line:and the error is :Where am i doing wrong? Also i generated this dataset from flume, then exported this in a table in hive and then exported it as a txt file and now i want to do sentiment analysis using Rsome text from dataset.txt are:"
3290,Categorizing data using sentiment analysis,"['python-3.x', 'nlp', 'nltk', 'sentiment-analysis', 'spacy']","I have a text file which has a data as below. There are many such lines but not of the same patternI want to categorize Celina as assistant and John as M.D.I need guidance on what would be the approach to handle such problems?
Is it Sentiment Analysis?"
3291,Sentiment analysis with python,"['python', 'sentiment-analysis', 'multilabel-classification']","I am trying to do sentiment analysis with python.I have gone through various tutorials and have used libraries like nltk, textblob etc for it.But what I want is bit different and I am not able figure out any material for thatSuppose I have a statement likeThe above statement can be classified in to two classes/labels like taste and  moneyMy aim is to get sentiment of the statement with respect to these two labelsMy expected result would be positive sentiment for taste but negative sentiment for moneyHow this can be achievedWith textblobWith vader"
3292,Google Cloud Natural Language API - How is document magnitude calculated?,"['sentiment-analysis', 'google-cloud-nl', 'magnitude']",I am currently working with the Google Cloud Natural Language API and need to know how the magnitude value for a whole document (consisting of several sentences) is calculated?For the document sentiment score the average of the scores for each sentence is taken. For the document magnitude I would have assumed that it's calculated by taking the absolute sum of the individual magnitude values for each sentence. But after testing some paragraphs it's clear that it's not the correct way to calculate it. Would anyone be able to explain this to me?Any help is much appreciated!
3293,convert xml (into iob2 format for NER/ABSA),"['python', 'xml-parsing', 'nlp', 'sentiment-analysis', 'named-entity-recognition']","I have an xml like this:
< sentences > 
< sentence id=""1"">
< text > Pizza is great< / text >
< aspectTerms >
< aspectTerm to=""5"" from=""1"" polarity=""positive"" term=""pizza""/>
(SemEval ABSA dataset)and I would like to convert it like this:
pizza B
is O great Oi.e. extract the sentences from the xml and place each word in a new line. 
Achieving this would already help me. Ideally, there would also be a blank line between the last word of a sentence and the first of the next sentence. 
And in the best case, there would be a second column next to the words with an IOB2-tag (i.e. O for all words other than the xml-indicated aspectTerms, B for the aspect [or I for composed aspect terms]) the idea is to use this converted file as training data for a sequence labeling task (to extract aspect terms from sentences, somewhat similar to NER)I tried to achieve it with ElementTree but wasn't successful. Sorry for this newbie question - any tips would help :) "
3294,Searching for a word group with TFidfvectorizer,"['scikit-learn', 'text-mining', 'tf-idf', 'tfidfvectorizer', 'countvectorizer']","I'm using sklearn to receive the TF-IDF for a given keyword list. It works fine but the only thing not working is that it doesn't count word groups such as ""car manufacturers"". How could I fix this?Pfa, the first lines of code so you see which modules I used. Thanks in advance !"
3295,Error when using Lemmatization and Tf- Idf calculation on Twitter data frame in Python,"['python', 'tf-idf', 'lemmatization']","I have a data frame of tweets and I'm trying to calculate Tf-Idf on the lemmatized 'tweet' column. I have a problem with the results of the lemmatization and I'm getting an error when trying to calculate the Tf-Idf.Below is my code:This is an example of the data frame with the new column 'tweet_lemmatized':It didn't work well because there are words like 'hoping', 'beginning' in the column.My first question- How can I improve the lemmatization?Now I want to calculate the Tf- Idf for this column and produce new columns in my data frame with the top words.This is my code for the Tf-Idf:
I want to add the top words to my original data frame 'df'.This is the error I got:"
3296,Memory Error when using Cosine Similarity/Linear Kernel,"['python', 'scikit-learn', 'tf-idf', 'cosine-similarity', 'countvectorizer']","I'm building a simple item-item recommender system based only on features of the item (books in this case). The features I'm using are:I'm dividing the process into two different parts:
for the book description, I want to use TFIDF and Linear Kernel to compute similarities, and I'll use Count Vectorizer and Cosine Similarity for Author and Genre, both using sklearn.My problem is that I have a pretty big dataset:So in both cases, the resulting matrix is just too big and a Memory Error occurs. I'm getting this error when computing Cosine Similarity:And for the case of the Linear Kernel, it just killed the process and ended the python console.
Any advice on how to approach this? Should I change the library?"
3297,TF-IDF algorithm on chinese text,"['python', 'tf-idf', 'tfidfvectorizer']","I am doing TF-IDF on chinese text and searching for top 10 used words in the text.
when i getting the top 10 words i have some of the meaningless word like ""成为"", ""表示"" and other.
Is there is any ways which only get meaningful words?
I am using ""jieba"" to cut the chinese sentence to words"
3298,Get document frequencies from sklean TfidfVectorizer,"['scikit-learn', 'tf-idf', 'tfidfvectorizer']","Based on sklearn's documentation of the TfidfVectorizer, the IDF is calculated (by default) as:idf(t) = log( (1 + n) / (1 + df(t)) ) + 1Given that, if you just wanted to get the plain document frequencies for each term, does the following make sense?"
3299,R keras tfidf asking for tf-idf and tf-idf asking for tfidf,"['python', 'r', 'keras', 'tf-idf']","I am trying to create a fake news classification model for class and have been trying to do it with Keras.the problem occurs specifically on this partwhile it works with output mode ""count"",""int"", and ""binary"" when I run it with tfidf I get this errorwhen I run it with tf-idf I get this errorif anyone know a solution for this I will be very thankfull"
3300,"TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams) which should we use?","['machine-learning', 'tf-idf']",a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents.b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-gramsc. Character Level TF-IDF : Matrix representing tf-idf scores of character level
3301,TFIDF score into percentage,"['elasticsearch', 'tf-idf']","I currently have a search system that displays percentage of found words in the database. I'm thinking of moving to elastic search, but I still want the results to be displayed as percentage. Is it possible? Currently in Kibana I have an explain function that displays for example the ""value"" : 0.6931471, How can I convert it to percentage to be used in my system?Thank you for answers"
3302,Texthero TD-IDF Calculation,"['python', 'tf-idf', 'tfidfvectorizer']",What is the difference in calculating TF-IDF through Texthero:and the TD-IDF from sklearn? I would expect the results from sklearn given these example sentences.
3303,How to export TensorFlow training data to CSV,"['python', 'python-3.x', 'jupyter-notebook', 'tf-idf', 'tfidfvectorizer']",I'm running a SVM sentiment analysis project and I wanted to vectorize it using tf-idf. the code snip for the tf-idf section is below:how do I export the X_train and the y_train only data into a .csv file? I wanted to check if the data were properly split. Thank you!
3304,logistic regression .predict() keeps restarting kernel,"['python-3.x', 'crash', 'logistic-regression', 'tf-idf']","We're doing sentiment analysis in Jupyter. The code below describes the general process. We are able to train the model just fine, but when we try to merge it with our actual data there is a problem. There is a certain point where the kernel keeps restarting -- I marked it below. Obviously we are looking for a way to stop that from happening.There is a lot of data, and I tried dividing it up into chunks, but still the same problem.Define tweet-cleaning methods:Load & prepare training data:Setting up TF-IDF vectorization and Logistic Regression:Import actual data:Convert ""created_at"" to datetime format:Clean text:There is an issue with the number of features we trained the model on and the number of features we want to apply the trained model to with our actual data. The model is trained on 200,000 unique words -- the actual data has a different number. This is what we've come up with to address this problem, but we definitely think there's probably a better way. Right now, the line LR_model.predict(X_df1) causes the kernel to restart every time.Please let me know if you know what's wrong here!"
3305,Algebraic operations of tfidf and word2vec?,"['word2vec', 'tf-idf']","I am thinking of performing algebraic operations on tfidf and word2vec to produce a single representation. However, im not sure this is mathematically sound. First, they have diiferent vector lenghts. The TFIDf is the size of the vocabulary and word2vec is 300.
If for example i pad with 1 or 0 to get the same lenght and perform addtion or multiplication between these two representations, do you think it makes sense?"
3306,How to calculate tfidf score,['tf-idf'],"I applied the code below and I got the output mentioned below. Could you please let me know what those numbers represent for and how it is calculated for whole corpus.Output:(2966, 1412)"
3307,How to find the approximate match and percentage of Similarity for Strings in R using Cosine similarity/TF-IDF,"['r', 'tf-idf', 'cosine-similarity', 'text2vec', 'textmatching']","I am trying to find the approximate/ exact match between  reported terms by the doctors and the another correct strings
Here I have sample terms and code.Lung cance reported should fetch 95% percentage but its fetching 88% as shown below.Two challenges I have faced it :
Two challenges I have right now:Here is the code for this for calculating similarity using cosine function from text2vec pacakge:Could you please someone help on this and provide your valuable suggestions .
Thanks"
3308,Extracting TF-IDF features as multiple columns with pyspark,"['python', 'pandas', 'pyspark', 'tf-idf']","Usually pyspark.ml.feature.IDF returns one outputCol that contains SparseVector. All i need is having N-columns with real number values, where N is a number of features defined in IDF(to use that dataframe in catboost later).I have tried to convert column to arrayand after that use Pandas to convert to columnsI don't like that way, because i find it really slow. Is there a way to solve my problem over pyspark without pandas?"
3309,How to apply a boost to certain information for cosine similarity,"['python', 'similarity', 'tf-idf', 'cosine-similarity', 'countvectorizer']","I'm building a function to compute the similarity of a book with all the other books in the database based on the following features:I'm using CountVectorizer to compute the CosineSimilarity between the feature vectors of each book, which is composed of 'Author' and 'Genre'.I have a few doubts here and I'm searching for answers everywhere:"
3310,"LSTM with TFIDF as input, dimension error","['machine-learning', 'keras', 'deep-learning', 'lstm', 'tf-idf']","I am trying to run LSTM with TFIDF as input, but getting an error. I have TFIDF with each entry of 11915 dimensionsCode is as follows:Error is as follows
Input 0 of layer bidirectional_27 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [1, 11915]I am new to this area, any help will be highly appreciated. It will be really nice if someone writes a dummy code for running Bidirectional LSTM on such an inputMy input is tfidf of 10229*11915. I want to do fake news detection using LSTM on TFIDF as input"
3311,Elasticsearch - search wildcards (contains in strings) and tf-idf scores,"['elasticsearch', 'wildcard', 'tf-idf', 'elasticsearch-query']","how can I make a search wildcard and tf-idf scores.
example when I search like this,it returns idf and td score,
but when I search like with wildcards (contains).how can I make a search with wildcards (using contains in the string) and include the IDF-TD scores?for example, I have 3 documents
""foo"", ""foo bar"", ""foo baz""
when I search it like thatElasticsearch ResultBut I expect ""foo""  should be the first result with having the highest score because it matches %100, am I wrong?"
3312,count in list of dictionaries for IDF,"['python', 'dictionary', 'count', 'tf-idf']","i have a list of dictionaries of the formi would like to count the number of occurances of a word throughout the whole list and not just one list of values. Im confused which for-loop structure I should use.I've tried this:this gives me just zeros.Any hint how to proceed is greatly appreciated!Edit: i would like to replace the word in question with its count, 
so for the  example above that would be:
    list_of_dicts = [{id1: [2, 2, 2...]}, 
    {id1: [2, 2, 2..]},..]"
3313,how to predict a cluster for a new document without using Tfidfvectorizer,"['python', 'k-means', 'tf-idf']","Explanation:I have a dataset of DailyKos that contains words(as features) and their frequency.
I applied TfIdTransformer to compute tf-idf scores and used clusters = 7 Problem:"
3314,Search query in tfidf.matrix in R,"['r', 'matrix', 'search', 'full-text-search', 'tf-idf']","I wrote this code in r,in order to write a simple search-engine based on tf-idf calculation:On score.vector Im trying to search the term:""harry"" in tf-idf matrix and I get this error:
Error in t(query) %*% as.matrix(query) : 
  requires numeric/complex matrix/vector argumentsWhat can I do in  order to find a term in tf-idf matrix?"
3315,Best way to retrieve top tokens in TF-IDF models,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']",How may one go about getting an overview of most important tokens from a SciKit-learn pipeline with the following components:Looking for a simple snippet that visualizes/plots the top-weighted tokens overall X)
3316,Accuracy with TF-IDF and non-TF-IDF features,"['python', 'machine-learning', 'random-forest', 'tf-idf']","I run a Random Forest algorithm with TF-IDF and non-TF-IDF features.In total the features are around 130k in number (after a feature selection conducted on the TF-IDF features) and the observations of the training set are around 120k in number.Around 500 of them are the non-TF-IDF features.The issue is that the accuracy of the Random Forest on the same test set etc with- only the non-TF-IDF features is 87%- the TF-IDF and non-TF-IDF features is 76%This significant aggravation of the accuracy raises some questions in my mind.The relevant piece of code of mine with the training of the models is the following:Personally, I have not seen any bug in my code (this piece above and in general).The hypothesis which I have formulated to explain this decrease in accuracy is the following.Related to this, when I check the features' importances of the random forest after training it I see the importances of the non-TF-IDF features being very very low (although I am not sure how reliable indicator are the feature importances especially with TF-IDF features included).Can you explain differently the decrease in accuracy at my classifier?In any case, what would you suggest doing?Some other ideas of combining the TF-IDF and non-TF-IDF features are the following.One option would be to have two separate (random forest) models - one for the TF-IDF features and one for the non-TF-IDF features.
Then the results of these two models will be combined either by (weighted) voting or meta-classification."
3317,What is the difference between Tfidftransformer and Tfidfvectorizer?,"['python', 'pandas', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",I am bit confused about the use of Tfidftransformer & Tfidfvectorizer as they both look similar. One uses words to convert matrix (Tfidfvectorizer) and the other used already converted text (using CountVectorizer) to matrix.Can any one explain the difference between these two?
3318,Should my model always give 100% accuracy on Training dataset?,"['python', 'machine-learning', 'scikit-learn', 'tf-idf', 'naivebayes']",I am testing my model on the training dataset itself. It is giving me the following results:Is it acceptable to get accuracy< 100% on the training dataset?
3319,Regular expression for matching non-whitespace in Python,"['python', 'regex', 'python-2.7', 'whitespace']","I want to use re.search to extract the first set of non-whitespace characters. I have the following pseudoscript that recreates my problem:It seems to be printing the whitespace instead of STARC-1.1.1.5So far as I understand it, this regular expression is saying: 
At the start of the line, find a set of nonwhitespace characters, don't be greedyI was pretty sure this would work, the documentation says I can use /S to match whitespace in [], so i'm not sure where the issue is.Now, I know, I know this probably looks weird, why aren't I using some other function to do this? Well, there's more than one way to skin a cat and i'm still getting the hang of regular expressions in Python so I'd like to know how I can use re.search to extract this field in this fashion."
3320,How to know if your TF-IDF calculation is correct?,['tf-idf'],"Background Info: 
I'm just getting started to learn NLP, have gone through materials for basic CS course, watched some videos and read a bit...My Approach:
Use specific technique(s) learned, and to write my own code (not to use tools yet), to test against some texts, then, see if I get them right or almost right.My Challenge:
Not knowing where appropriate to post questions for my learning and get informative responses.The first technique I'm learning is TF-IDF, goal is to extract the most important and central ideas of a text.Text URL, https://news.yahoo.com/oxford-scientists-working-coronavirus-vaccine-100847254.htmlUsing TF, I get the following top ""concepts"" or ""words"" for this text (with respective scores):With a slightly different calculation formula, I get the following top ""words"":Using TF-IDF, I get the following three top ""sentences"":
22.532130774077
17.891164215124
16.190527222538Could anyone run some tests against this text and see what results you get?
By comparing my results with that of expert I would know know how I'm doing.Thanks in advance."
3321,Cosine similarity over tf idf output in spark dataframe (scala),"['scala', 'apache-spark-sql', 'tf-idf', 'cosine-similarity']","I am using Spark Scala to calculate cosine similarity between the Dataframe rows.Dataframe format is below:Sample of the dataframe is below:Code that gives me the result is below:Final dataframe looks likeI don't understand how to process ""dense_features"" to calculate cosine similarity. This article didn't work for me. Appreciate any help.Example of one row of dense_features. For simplicity the length is cuted."
3322,Manual implementation and Scikit learn's tfidf transformer show different outputs?,"['python-3.x', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I've been trying to implement the tfidf transformer from scratch, similar to the one implemented by sklearn. My IDf vectors are same as the sklearn version, but when I multiply TF and IDF and normalize them using L2 normalization, I get different output from that of sklearn.This is my function to fit the Corpus and create a dictionary with keys as Words and Values as their indicesThis is my function to transform it to Tf-idf vectorsI'm sorry if this code is too lengthy and filled with print statements, I'm new to this and needed them to see where I'm going wrong.This is the output I'm getting, for this dataMy Output vs
Sklearn's Output"
3323,Improving an elasticsearch query on human names and addresses,"['elasticsearch', 'tf-idf', 'n-gram', 'relevance']","I am searching an elasticsearch index containing human names and addresses.
The relevance ranking is good but not as good as it needs to be. It is also too slow.Our index includes a combination of ngram and edge_ngram analyzers. Our queries are boolean queries including query string and multimatch queries.The ngrams allow us to search for mispelled names more quickly than using a fuzzy search.The edge ngrams allow us to assign a higher score to terms that appear in the same order in a document. I think this only works when the terms are spelled exactly as they appear in the index.We have overridden the default similarity module to effectively turn off TFIDF, since that is mostly irrelevant for searching names, i.e. proper nouns.How can we further improve these index settings and query structure to improve relevance ranking?In particular, one issue we have with this setup is that elasticsearch undesirably boosts the score of documents in which a search term appears multiple times. For example, a search for ""Sally Smit"" assigns a higher score to Sandy Smith who lives at 10 Smith Rd than to Sally Smith who lives at 10 Plumb Rd. Of course, when a user searches ""Sally Smit"", they want to see people who have the name ""Sally Smith"" at the top of the results list.Another important angle here is that word relevance matters. For example, searching Allan Joseph should assign a higher score to Allan M Joseph than to Joseph Allan.In general, it is difficult to find best practices for searching human names in an elasticsearch index. I have searched stack overflow and the elasticsearch forums. It would be helpful if you know how to fix these index settings, mapping and/or query to improve relevance ranking for human names and addresses or if you can point me toward better information and examples than I've been able to find here on stack overflow and the elasticsearch forums.I'm pasting our index settings, an example query and result set below...Our index settings and mappingsA sample query...Here are the first two results from this query..."
3324,how parameters of TfidfVectorizer “min_df” and “max_df” work?,"['tf-idf', 'tfidfvectorizer']","I am trying to perform multi-label classification using a text dataset. And using TfidfVectorizer because I want to filter the values on the basis of a certain threshold, lets say 0.005. The values which have TF-IDF score less than threshold, should not be considered for training. Now I can see only possible way to do that is using min_df and max_df. But I dont understand how that works. How can I use these two parameters to filter my features on the basis of defined threshold?"
3325,Remove words that occur only once and with low IDF in R,"['r', 'nlp', 'data-cleaning', 'tf-idf', 'word-frequency']","I have a dataframe with a column with some text in it. I want to do three data pre-processing steps: 1) remove words that occur only once
2) remove words with low inverse document frequency (IDF) and 3) remove words that occur most frequentlyThis is an example of the data:Any help would be greatly appreciated, as I am not too familiar with R."
3326,Unsatisfactory output from Tf-Idf,"['python-3.x', 'tf-idf', 'tfidfvectorizer']","I have a document in a text file in 2 lines as shown below. I wanted to apply tf-idf to it and I get the error as shown below, I am not sure where is int object in my file? why would it throw this error?Env: Jupter notebook, python 3.7Error:file.txt:Code: "
3327,Cluster using different colours and labels,"['python', 'matplotlib', 'cluster-analysis', 'k-means', 'tf-idf']","I am working on text clustering. I would need to plot the data using different colours. 
I used kmeans method for clustering and tf-idf for similarity. Currently, my output looks like:  there are a few elements as it is a test. 
I would need to add labels (they are strings) and differentiate dots by clusters: each cluster should have its own colour to make the reader easy to analyse the chart. Could you please tell me how to change my code in order to include both labels and colours? I think any example it would be great. A sample of my dataset is (the output above was generated from a different sample):Sentences"
3328,Plot centroids in K-Means using TF-IDF,"['python', 'matplotlib', 'seaborn', 'k-means', 'tf-idf']","I'm coding to group texts using KMeans and everything is working well, but I'm not able to plot the centroids together. I don't know how to use matplotlib, only seaborn along with the vector created by tdidf.MiniBatchKMeans has the variable cluster_centers_, but I'm not able to use it in the image."
3329,Can someone check if my tf-idf weighting is done correctly?,"['tf-idf', 'information-retrieval']","I have a collection of 15 documents, where the term Smoking appears in 10 of them and where the term Health appears in 2 of them. I need to check the values of these terms after the tf-idf weighting in a Doc where the word Smoking and Health appears once each.The results that I get for smoking is 0.875 and for health, I get 0.176. Are these correct?Thank you!"
3330,Using csr_matrix as one of many columns in a dataset for NLP,"['python', 'pandas', 'nlp', 'tf-idf']","I am taking a new direction in my career and trying to explore Data Science technologies.While I am practicing with different datasets, I happen to have a particular one which has 4 columns which are: text_type, length, affiliation, text text_type: type of text, takes categorical values such as attack, policy, support, medialength: length of the textaffiliation: affiliation of text writer, takes categorical values such as republican or democratand more interestinglytext: a csr_matrix created through a pipelinedf.text.iloc[0] gives an Output of:So how would you advise me to proceed and use df.length, df.affiliation, df.text to predict text_type in X_test?I also have an access to the original texts, therefore you may also suggest a different solution to perform NLP based on three features(length, affiliation, text). Main purpose is to create a model to predict text_type of X_test based on more than df.text."
3331,Creating a new column for predicted cluster: SettingWithCopyWarning,"['python', 'pandas', 'scikit-learn', 'tf-idf']","This question will be a duplicate unfortunately, but I could not fix the issue in my code, even after looking at the other similar questions and their related answers. 
I need to split my dataset into train a test a dataset. However, it seems I am doing some error when I add a new column for predicting the cluster. 
The error that I get is: There are a few questions on this error, but probably I am doing something wrong, as I have not fixed the issue yet and I am still getting the same error as above. 
The dataset is the following: I have split the dataset into train and test sets as follows: The lines of code that cause the error are: I think these two questions should have been able to help me with code: How to add k-means predicted clusters in a column to a dataframe in PythonHow to deal with SettingWithCopyWarning in Pandas?but something is still continuing to be wrong within my code. Could you please have a look at it and help me to fix this issue before closing this question as duplicate?"
3332,"R : error inherits(x, “matrix”) || inherits(x, “Matrix”) is not TRUE when trying to calculate cosine similarity with tf-idf","['r', 'tf-idf', 'cosine-similarity', 'text2vec']","I have a corpus filled with 5 different books (all .txt files). I want to calculate the cosine similarity between these books, so I can tell how similar they are with one another. Following is my code:This is where i get :As you guys can see, so far I been relying on packages: TM and I want to use Text2vec's function to calculate the cosine similarity. Can anybody help?"
3333,How to calculate TF-IDF (using tft.tfidf function) in Tensorflow Transform,"['tensorflow2.0', 'tf-idf', 'tensorflow-transform']",While going through the docs in tensorflow transform I came across function to perform TD-IDF.As the docs in not clear in providing example of how to perform TD-IDF I tried using example_string and a vocab size of 1000.(Just random number) but the below code giving me an attribute error.AttributeError: 'list' object has no attribute 'indices'Please help me to figure this out as I am naive to Tensorflow transform ops.
3334,Why is “More Like This” in ElasticSearch not respecting TF-IDF order for a single term?,"['elasticsearch', 'lucene', 'tf-idf', 'morelikethis']","I've been trying to grok the ""More Like This"" functionality in ElasticSearch.  I've read and re-read the documentation but I'm having trouble understanding why the following behavior occurs.Basically, I insert three documents, and I try a ""More Like This Query"" with max_query_terms=1, expecting that the higher TF-IDF term is used, but that doesn't seem to be the case.""dog barks"" document""cat naps"" and ""cat fur"" documents (Also, see note about determinism below)In the documentation it mentionsSuppose we wanted to find all documents similar to a given input document. Obviously, the input document itself should be its best match for that type of query. And the reason would be mostly, according to Lucene scoring formula, due to the terms with the highest tf-idf. Therefore, the terms of the input document that have the highest tf-idf are good representatives of that document, and could be used within a disjunctive query (or OR) to retrieve similar documents. The MLT query simply extracts the text from the input document, analyzes it, usually using the same analyzer at the field, then selects the top K terms with highest tf-idf to form a disjunctive query of these terms.Since I specified max_query_terms = 1, only the term from the input document with the highest TF-IDF score should be used in the disjunctive query.  In this case, the input document has two terms.  They have the same term frequency in the input document, but cat appears twice as often in the corpus, so it has a higher document frequency.  Therefore, dog should have a higher TF-IDF score than cat, and therefore I'd expect that the disjunctive query is just ""message"":""dog"" and the returned result is the ""dog barks"" event.I'm trying to understand what's going on here.  Any help is very greatly appreciated. :)I tried rerunning this setup a few times.  When running the 4 ES commands (3 POST + MLT GET) above following a curl -XDELETE 'http://localhost:9200/samples' command, sometimes I'd get ""cat naps"" and ""cat fur"", but other times I'd get ""cat naps"",""cat fur"", and ""dog barks"", and a few times I'd even get just ""dog barks"".Earlier I handwaved and just said what the outputs were for the GET query. Let me be more precise
Actual output #1 (happens some of the time):Actual output #2 (happens some of the time):Actual output #3 (happens most rarely of the three):Maybe elasticsearch is in a weird ""processing state"" and needs a bit of time between documents.  So I gave ES some time between inserting the documents and before running the GET command.However this did not seem to affect the non-deterministic output in any meaningful way.Following this SO post about ES nondeterminism, I tried adding the dfs_query_then_fetch option,
akabut still, the results were not deterministic and they varied between the three options.I tried looking at additional debug information viabut this sometimes outputand other timesso the output wasn't even deterministic (running it back to back).Note: Tested on ElasticSearch 6.8.8, both locally and in online REPL.  Also tested by using an actual document,
e.g.but got the same ""cat naps"" and ""cat fur"" events."
3335,Combine additional data to my TFIDF array,"['python', 'pandas', 'scikit-learn', 'nlp', 'tf-idf']","I'm trying to create a text classification model using scikit-learn. At first, I was using only the text's tfidf array as a feature. The structure of my dataset can be seen below (the dataset is stored in a pandas dataframe called df):So, I split the train/test datasets and proceeded with creating the tfidf vector and transforming the data for training and testing.So far apparently the code is working ok. However, there was a need to improve the algorithm, including yet another feature. For this improvment, I want to add the id_1 column to my features (it can be an important information to our ML model). So, in addition to my tfidf matrix, I would like to add this column (id_1) with my new feature, so that I can pass it as a parameter to train the model.What I have tried:So, the shape of my structure isIn a nutshel, I want pass to my ML algorithm something like the image below - my tfidf vector and my id_1 features:I feel that I am missing something extremely basic, but even with all the research I have been able to solve my problem satisfactorily. I'm honestly lost in that part of the problem and I don't know how to evolve from here"
3336,sklearn TfidfVectorizer custom ngrams without characters from regex pattern,"['python', 'scikit-learn', 'nlp', 'tf-idf']",I would like to perform custom ngram vectorization using sklearn TfidfVectorizer. The generated ngrams should not contain any character from a given regex pattern. Unfortunately the custom tokenizer function is completely ignored when analyzer='char' (ngram mode). See the following example:What is the best way to do it?
3337,Saving TF-IDF result into CSV file,"['python', 'csv', 'tf-idf']","this is my first time asked a question and English is not my language so if I wrote something wrong please forgive me.
I just crawled the scripts from the websites and calculated the TF-IDF of the features and I want to save the result into the CSV file with all the rows and columns. Thank you for your help!Here the result:"
3338,TfidfVectorizer using my own stopwords dictionary,"['python', 'tf-idf', 'tfidfvectorizer']","I would like to ask you if I could use my own stopwords dictionary instead of the pre-existing one in TfidfVectorizer. I built a greater dictionary of stop words and I would prefer to use it. However I am having difficulties in including it in the code below (there is shown the standard one, though). but I got the following error: Let's say that my dictionary is: How could I include it?Any help would be greatly appreciated. "
3339,tf-idf for text cluster-analysis,"['python', 'cluster-analysis', 'tf-idf', 'tfidfvectorizer']","I would like to group small texts included in a column, df['Texts'], from a dataframe. 
An example of sentences to analyse are as follows: Since I know that TF-IDF is useful for clustering, I have been using the following lines of code (by following some previous questions in the community):However, since I am considering a column from a dataframe, I do not know how to apply the above function. 
Could you help me with it?"
3340,TF-IDF vectorizer with python,"['python', 'vectorization', 'tf-idf', 'tfidfvectorizer']","I have a problem with the TfidfVectorizer function in python. 
For example if I have a string like this one:
'xxx//xx. aaa.bb.ccc.d'
will be extracted these words as the key of the dictionary:
'xxx', 'xx', 'aaa', 'bb', 'ccc', 'd'
instead, I want to create these new features:
'xxx//xx.' , 'aaa.bb.ccc.d'How can I ask to TfidfVectorizer function to select words separated by the space (' ')?"
3341,GridSearchCV + StratifiedKfold in case of TFIDF,"['machine-learning', 'scikit-learn', 'cross-validation', 'tf-idf', 'grid-search']","I am working on a classification problem where I need to predict the class of textual data. I need to do hyper parameter tuning for my classification model for which I am thinking to use GridSearchCV. I need to do StratifiedKFold as well because my data is imbalanced. I am aware of the fact that GridSearchCV internally uses StratifiedKFold if we have multiclass classification. I have read here that in case of TfidfVectorizer we apply fit_transform to train data and only transform to test data.This is what I have done below using StratifiedKFold.The accuracy/f1 I am getting is not good so thought of doing hyper parameter tuning using GridSearchCV.
In GridSearchCV we doAccording to me logreg_cv.fit(X, y) would internally split the X in X_train, X_test k times and then would do predictions to give us the best estimator.In my case what should be X? If it's X which is generated after fit_transform then internally when X is split into train and test, the test data has undergone fit_transform but ideally it should undergo only transform.My concern is that in my case, inside GridSearchCV how would I be able to control that fit_transform is applied only to train data and transform is applied to test data (validation data).because if it internally applies fit_transform to entire data then it is not a good practise."
3342,Create TF-IDF list (not matrix) in Python,"['python', 'nlp', 'tf-idf', 'tfidfvectorizer']","I'm trying to create a TF-IDF list (instead of a matrix) in Python, showing the cosine similarity of all documents to just one. Suppose I have a bunch of documents:corpus= ['The first text string', 'The second text string', 'The third text string'...]I can use tfidfVectorizer, vect.fit_transform, and pairwise similarity to get the matrix. In the matrix, each document has one row and one column, right? What I want instead is just one column displaying cosine similarity to the first document, and a row for every other document.Desired OutputI'm aware that I can do this by simply selecting the desired column from the matrix, but this is an exercise I want to do with hundreds of strings. I'd love to lean the process down. I'd like to edit my code to only calculate the one column of cosine similarity I need."
3343,understanding top n tfidf features in TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I am trying to understand the TfidfVectorizer of scikit-learn a bit better. The  following code has two documents doc1 = The car is driven on the road,doc2 = The truck is driven on the highway. By calling fit_transform a vectorized matrix of tf-idf weights is generated. According to the tf-idf value matrix, shouldn't highway,truck,car be the top words instead of highway,truck,driven as highway = truck= car= 0.63 and driven = 0.44? "
3344,"When I use TF-IDF in Natural language processing, it said list is not callable.Can you help me with it?","['python', 'nlp', 'tf-idf', 'tfidfvectorizer']","I got error like this :When I run these codes in python:P.S. I also tried to convert the xtrain to list with xtrain.tolist(), but it doesn't work for me either."
3345,How to reflect the same results as from sklearn's TfidfVectorizer?,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I am trying to build the TfidfVectorizer from scratch and I have built almost the same vectorizer as sklearn's but I am not able to get the same tf-idf scores as TfidfVectorizer.Here is my code:output for the first line is (0, 0) 0.038461538461538464(0, 1) 0.038461538461538464(0, 2) 0.038461538461538464(0, 3) 0.05810867783715349(0, 4) 0.038461538461538464Whereas output from sklearn's TfIDFvectorizer is(0, 8)  0.38408524091481483(0, 6)  0.38408524091481483(0, 3)  0.38408524091481483(0, 2)  0.5802858236844359(0, 1)  0.46979138557992045can you tell me where I am getting it wrong? Thank you."
3346,How can I show percentage wise recommendation in TFIDF Algorithm?,"['python-3.x', 'machine-learning', 'tf-idf', 'recommendation-engine']",I have made one algorithm so now i want to find score of particular output that weather it is 70 % or 80 % match with output.Output is But I want this with a score likecan you help me out how i can find score and map with output.
3347,How can I count tfidf with Bigram?,"['python', 'pandas', 'dataframe', 'math', 'tf-idf']","Here I have a dataframe with the name tweetRV.csv with 3 columns (tweets, Stopword and group). And I only use the column with the name (Stopword) for TFIDF and I managed to get the result with the code below. I've managed to separate the text in Unigram then counting TFIDF like the code belowwith the word in the Stopword column is already tokenized like thisWith the TFIDF result like this as a csv filebut now I want to replace the Unigram with Bigram to do TFIDF. I want to separate the text in Bigram then count the TFIDF. Is it possible? Does anyone here can help me with the code??
thank youuu"
3348,Tensorflow: AttributeError: 'Tensor' object has no attribute 'sum',"['tensorflow', 'word2vec', 'tf-idf', 'autoencoder']","I am trying to extract my embedded matrix and normalize it to run cosine similarity.
following this github repo:
https://github.com/s4sarath/Deep-Learning-Projects/blob/master/variational_text_inference/model_evaluation.ipynbI have a function defined for this:But when I run it, I get the above error. Can someone help me out here?"
3349,How to view TF-IDF results? [duplicate],"['python', 'tf-idf']",I am looking at this examplehttps://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/exactly at the line where using TF-IDFWhen i try to view the results of xtrain_tfidf I get this messageI would like to see what does xtrain_tfidf have?how can I view it?
3350,PyTorch - sparse tensors do not have strides,"['nlp', 'pytorch', 'lstm', 'tensor', 'tf-idf']","I am building my first sentiment analysis model for a small dataset of 1000 reviews using TF-IDF approach along with LSTM using the below code. I am preparing the train data by preprocessing it and feeding to the Vectorizer as belowI am converting my csr_matrix to a pytorch tensor using the below codeAnd I am getting the training sentences tensor as thisI am creating a TensorDataSet using the below code wherein I am also converting my label data from bumpy to a torch tensorI have defined my LSTM network and calling it with the following parametersI have also defined the loss and optimizer. Now I am training my model using the below codeHowever, I am getting a major error on the line output, h = net(inputs) as RuntimeError: sparse tensors do not have stridesThe workarounds given on other websites are not understandable. I am expecting an exact code change I need to make in order to fix this issue. "
3351,How to extract keywords using TFIDF for each row in python?,"['python', 'python-3.x', 'tf-idf', 'tfidfvectorizer', 'keyword-extraction']","I have a column which has text only. I need to extract top keywords from each row using TFIDF.Example Input:Expected output: How do i get this? I tried writing the below codeI am getting the below error
Iterable over raw text documents expected, string object received."
3352,How can I compute tf-idf and save in a df only the values of the words of the query?,"['python', 'tf-idf']",I have a list of product titles (df.product_title) and a query (df.search_term) for each product title. As a result comes the following:How can I save in the df only the tfidif value from the query?
3353,Reduce Dimension of word-vectors from TFIDFVectorizer / CountVectorizer,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer', 'countvectorizer']","I want to use the TFIDFVectorizer (or CountVectorizer followed by TFIDFTransformer) to get a vector representation of my terms. That means, I want a vector for a term where the documents are the features. That's simply the transpose of a TF-IDF matrix created by the TFIDFVectorizer.However, I have 800k documents which mean my term vectors are very sparse and very large (800k dimensions). The flag max_features in the CountVectorizer would do exactly what I'm looking for. I can specify a dimension and the CountVectorizer tries to fit all information into this dimension. Unfortunately, this option is for the document vectors rather than the terms in the vocabulary. Hence, it reduces the size of my vocabulary because the terms are the features.Is there any way to do the opposite? Like, perform a transpose on the TFIDFVectorizer object before it starts cutting and normalizing everything? And if such an approach exists, how can I do that? Something like this:I was looking for such an approach for a while now but every guide, code example, whatever is talking about the document TF-IDF vectors rather than the term vectors.
Thank you so much in advance!"
3354,Trying to get an SKLearn KNN classifier to work with tf-idf,"['python', 'scikit-learn', 'tf-idf']","I'm trying to follow what's on KNN for Text Classification using TF-IDF scores using a sample (its not the best sample of documents and doesn't need to make sense at the moment)
However, I keep getting: ""cannot perform reduce with flexible type."" at the predict line. What's wrong?
Here is the code:"
3355,How to get the rank of inverse document frequency (idf value) fof all the terms of my corpus from sklearn TfidfVectorizer()?,"['python', 'pandas', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",Which is the most efficient way to obtain a dataFrame with all the terms of my corpus of documents in a column and only their IDFs values ranked in another column ?Since now i've only been able to get the dataFrame with the whole TF-IDF values in the following way:So when printing ranking i gotHow can i do it similarly to obtain the same dataFrame but with only IDF values?
3356,sentence that appear the most using tfidf in my dataframe with python,"['python', 'csv', 'dataframe', 'tokenize', 'tf-idf']","I want to look for the sentence that appear the most using tfidf in my dataframe, I did some preprocessing as tokenize and stopword, and now I have 2 columns (text & Stopword)i want to get dataframe with sentence from Stopword column which the value is the tf_idf and the columns are the words like thismaybe someone here knows the code and can help me?"
3357,adding the result of tf-idf on pandas data frame to the main data,"['python', 'pandas', 'tf-idf']","I used pandas data frame in order store dataset in Python. Then, I applied tf-idf on the dataset. Is there any way to add tf-idf result to the main pandas data frame?
My dataset has about 6000 row of data. 
I tried to add tf-idf to my main data, while I faced with following error."
3358,My nested for loops are taking so much time while calculating term-frequency,"['python', 'list', 'for-loop', 'tf-idf']","i have a list ""total_vocabulary"" with all the unique words in a collection of 56 documents. There is another list of list with words of every document ""rest_doc"". I want to calculate term frequency of each word from ""total_vocabulary"" in ""rest_doc"" so ""term_freq"" list will be a list of list of the same size of total_vocabulary and at each index of term_freq will be a list of size 56 representing the total occurrence of each word in each document. The problem is that the nested for loops are taking so much time,almost a minute to run. is there any way to do it faster?
code:here is my code."
3359,"ValueError: Error when checking target: expected dense_22 to have shape (100, 50) but got array with shape (1, 50)","['python', 'machine-learning', 'keras', 'nlp', 'tf-idf']","I'm training a neural network to predict the Document-Frequency from a set of documents.So, the main idea is to map a matrix with 100 documents and 50 tokens to the respective document-frequency array.X = (n_samples, 100, 50) -> y = (n_samples, 1, 50)My code is:But i got an error:"
3360,ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray),"['tensorflow', 'keras', 'lstm', 'tf-idf', 'embedding']","tfidf_Train and features_Train are lists of lists containing floats, i.e. [[0.14, 0.22...],[0.52,0.34]]
I tried converting the variables to np array using np.asarray(), but I still get error at the bottom below the code when fitting my model. Appreciate any help.Error:"
3361,Using ScikitLearn TfidfVectorizer in a search engine,"['python', 'scikit-learn', 'search-engine', 'tf-idf', 'tfidfvectorizer']","I'm looking at creating a search engine were I can fetch sentences (which represente a document) from a preprocessed pdf file using keywords.I'm wondering if there is a built-in function in scikit-learn to show the data similarly to a bag of word output, meaning I'd have all the words as columns (in pandas), all documents as rows, and the tf-idf values as values"
3362,LSTM using word embeddings and TFIDF vectors,"['python', 'tensorflow', 'keras', 'lstm', 'tf-idf']","I am trying to run LSTM on a dataset that has text attributes and TFIDF vectors. I word embed the text and input to LSTM layer. Next, I concatenate the LSTM output and the TFIDF vectors. However, line 2 in the code below throws the following error: ""ValueError: Layer lstm_1 was called with an input that isn't a symbolic tensor. Received type: . Full input: []. All inputs to the layer should be tensors.""The code is given below, where len(term_Index)+1 = 9891, emb_Dim=100, emb_Mat contains floats and has shape [9891,100], and sen_Len=1000:"
3363,How to compute tf-IDF using top 100 occurrences in vocabluary?,"['python', 'numpy', 'data-analysis', 'tf-idf']","I'm using a data represented as sparse matrix where column contains numerical values for ""article_Id"" ""word_id"" and ""count"" as follow:I'm representing each document by TF-IDF vectors of the top 100 coordinates. Next, I'm randomly selecting a subset of 200 documents and computing the similarity of those 200 documents.Next, the task is to store the tf-idf in a 1000 x 100 dimensional matrix. Here, 1000 is the number of documents and 100 is the top 100 most occurring words. These words will be different for each document, so we have to select top 100 words from whole vocabulary (all documents combined) and compute the tf-idf of these words only for each document.Since tf-idf is calculated separately for documents, I'm unable to get it for top 100 occurrences from whole vocabulary. Any idea?What I have tried:Here's what my code looks like right now.Here's what I'm doing to compute tf-idf.and later random sample can be easily chosen with random.sample() function."
3364,How to get best features for tf-idf classifiers?,"['python', 'scikit-learn', 'tf-idf', 'feature-selection', 'tfidfvectorizer']","I have a list of comments (text) that I have to classify with some classifiers (input).
I'm using a pipeline to do this, and I do KFold because the dataset is very small.
I would like to know the names of the best features for the classifiers with SelectKBest, but since it is in the pipeline I don't know how I can get the best feature names.comments is a list of strings .I searched on the internet and found this : But I can't do this, since I'm not calling fit on the pipeline. I use the pipeline here: How can I get the best K feature names?What I want is a simple list of the words that ""helped most"" the classifiers doing their job..."
3365,process is taking a long time to compute tf-idf of a corpus,"['python', 'csv', 'scikit-learn', 'tf-idf']","my python program is taking almost 20 sec to calculate td-idf of a corpus and saving the dataframe to csv.
my corpus has around 411 documents with avg of exact 276 words.
is this time normal if not than how can i make this process fast.first i'm tokenizing after that i'm removing stopwards and steming the words and than finally using tf-idf vectorizer.Below is the code i am using"
3366,having problem to understand the group balanced binary tree for ranked keyword search,"['binary-tree', 'binary-search-tree', 'tf-idf', 'relevance', 'binary-index-tree']","the thing i didn't understand is how to calculate the relevance score and put the value in the queue of the tree nodes here in tree picture i didn't understand is how they calculate the results for f(1,1) ,f(1,2) , f(1,3) , f(3,2) , f(3,1) , f(2,1) ]2]3"
3367,"tfidf first time, using it on a Pandas series that has a list per entry","['scikit-learn', 'tf-idf']","Data looks like this :I got this by stemming and lemmatizing the sentence and tokenizing before that. ( Hope that is right).Now I want to use:It gives me the following error :I know that I somehow cannot use it on the list, so what is my play here, trying to return the list into a string again?"
3368,TFIDF separate for each label,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","Using TFIDFvectorizor(SKlearn), how to obtain word ranking based on tfidf score for each label separately. I want the word frequency for each label (positive and negative).relevant code:"
3369,Use Tf-idf as features in CNN model,"['python', 'tensorflow', 'keras', 'conv-neural-network', 'tf-idf']","I've a CNN model that runs fine with word2vec matrix as input. Now, I'm trying to use tf-idf features as an input to my CNN model. My model has 2 convolution layers.I got the following error. Please provide any hints to resolve my problem.I've also tried to change input layer to Input(batch_shape=(None,tf_len, 1)) but got the same error.ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (1000, 5008)"
3370,How does calculating relevance scoring in Elasticsearch differ from Couchbase?,"['elasticsearch', 'lucene', 'couchbase', 'tf-idf', 'relevance']",I wonder whether relevance score in elasticsearch has differences with couchbase or not? 
3371,Sklearn Models unable to accept TFIDF vector inputs,"['python', 'scikit-learn', 'tf-idf']","I have a random forest regression model. The idea is to predict salaries as a function of TFIDF vector inputs. The tokenization process works without hiccup. I'm unsure why the following error is being triggered:Where my input looks like thisAs you can see, m0 is an instance of a user defined class; it's essentially just a data wrapper. If you need more info therein, see my related question Python NLP - ValueError: could not convert string to float: 'UKN'"
3372,Result too large when using scikit linear_kernel for TF-IDF computation,"['python', 'scikit-learn', 'data-science', 'tf-idf', 'recommendation-engine']","I am new in python and currently trying to develop a content based recommender system. I am reading a csv file with about 60,000 rows and using TfidfVectorizer.fit_transform which makes a matrix size (63098, 9081) and then apply linear_kernel that then crashes and returns the error RuntimeError: nnz of the result is too large.Logs shows the supposed result would be 20gb in size. Is there a way to chunk the data and how to apply that in my current structure? Thanks in advance."
3373,Error while compiling the program to find Tfidf of 4 document files,"['java', 'hadoop', 'mapreduce', 'tf-idf', 'filesplitting']","I have been trying to write a tfidf program that read four text files and provides the tfidf values for bigrams, i.e pairs of words. While doing so, I am facing the following errorThis is the code I'm trying to compile:Can someone help me figure this out please!"
3374,Convert each document to a vector based on TF -IDF,"['pandas', 'dataframe', 'tensorflow', 'plot', 'tf-idf']","I have written the code below that is calculating the TF-IDF score1- In the end i am able to get the vector of first document. But i ma not able to get the vector for all documents. I tried looping through and appending to dataframe but it didnt work.
2- how can I save the index of document in csv file?
I have to run it on movies plot from movie Lens dataset. This is why it is important for me to save the index of documnet."
3375,Dataframe Rows are matching with each other in TF-IDF Cosine similarity i,"['python', 'pandas', 'nlp', 'tf-idf', 'cosine-similarity']","I am trying to learn data science and found this great article online.https://bergvca.github.io/2017/10/14/super-fast-string-matching.htmlI have this database full of company names, but am finding that the results where the similarity is equal to 1, they are in fact literally the same exact row. I obviously want to catch duplicates, but I do not want the same row to match itself. On a side note, this has opened my eyes to pandas and NLP. Super fascinating field - Hopefully, somebody can help me out here.Shape: (72489, 3)Then I clean the dataThen I implement the N-Grams functionThen I get the TF-IDF MatrixThen I implement the Cosine Similarity function - Which I am still not quite sure what each parameter does.Then we actually find the matches. I am not sure what the transpose does in this case and how that finds matches.Then here is the function for extracting the matches.Then I call the function and pass in the paramsNow I only want to extract matches that are 90% similar or more.Then I sort the values by similarity So what I am finding is that the same rows are being matched with each other. I know this because the SFDC ids are exactly the same - Why is this happening? How can I avoid this in the future? I obviously do not want the row to asses itself when finding similarities."
3376,Does TF-IDF vectorization work well with small corpus?,"['machine-learning', 'data-science', 'tf-idf']","I was reading about BOW and TFIDF  and found a sentence that ""If your dataset is small and context is domain specific, BoW may work better than Word Embedding"" . I was wondering how different vecotrizers are affected because of corpus size. I know that more the data you have in Data Science, the better it is. Can someone provide the comparison between these vectorization techniques especially focusing on the size of data corpus . Comparison with other vectorization techniques like W2V , Avg W2V , Avg Tf-IDF is also appreciated. "
3377,How to make a bag of words using split method from a text file in python,"['python', 'tf-idf']",I am trying to learn TFIDF. But I coudnt bag the words from file.code: error:
3378,How to make dense matrix as input to Tfidf transformer?,"['python-3.x', 'scikit-learn', 'tf-idf']","Basically, how to make this code running with my data?The problem is those values of V or N are consec. 200k+ and 30k. There are far too beyond my memory stack and @ the line np.zeros( the memory occurs. Can I avoid this loop and produce A matrix dense as a input to the Tfidf Straightforwardly?I've read I must use PySpark. Should I really?"
3379,Why are my TF-IDF features per sample different for train and test inputs?,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']",Tf -idf is givng value error it works fine before now its throwing eror
3380,How to manually calculate TF-IDF score from SKLearn's TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I have been running the TF-IDF Vectorizer from SKLearn but am having trouble recreating the values manually (as an aid to understanding what is happening). To add some  context, i have a list of documents that I have extracted named entities from (in my actual data these go up to 5-grams but here I have restricted this to bigrams). I only want to know the TF-IDF scores for these values and thought passing these terms via the vocabulary parameter would do this.Here is some dummy data similar to what I am working with:Note:  I know stop-words are removed by default, but some of the named entities in my actual data-set include phrases such as 'the state department'. So they have been kept here. Here is where I need some help. I'm of the understanding that we calculate the TF-IDF as follows:TF: term frequency: which according to SKlearn guidelines the is ""the number of times a term occurs in a given document""IDF: inverse document frequency: the natural log of the ratio of 1+the number of documents, and 1+the number of documents containing the term. According to the same guidelines in the link, the resultant value has a 1 added to prevent division by zero.We then multiply the TF by the IDF to give the overall TF-IDF for the a given term, in a given document.ExampleLet's take the first column as an example, which has only one named entity 'Boston', and according to the above code has a TF-IDF on the first document of 1. However, when I work this out manually I get the following:Perhaps I'm missing something in the documentation that says scores are capped at 1, but I cannot work out where I've gone wrong. Furthermore, with the above calculations, there shouldn't be any difference between the score for Boston in the first column, and the second column, as the term only appears once in each document. Edit
After posting the question I thought that maybe the Term Frequency was calculated as a ratio with either the number of unigrams in the document, or the number of named entities in the document. For example, in the second document SKlearn generates a score for Boston of 0.627914. If I calculate the TF as a ratio of tokens = 'boston' (1) : all unigram tokens (4) I get a TF of 0.25, which when I apply to the TF-IDF returns a score just over  0.147. Similarly, when I use a ratio of tokens = 'boston' (1) : all NE tokens (2) and apply the TF-IDF I get a score of 0.846. So clearly I am going wrong somewhere."
3381,how tfidf value is used in k-means clustering,"['python-3.x', 'nlp', 'k-means', 'tf-idf', 'tfidfvectorizer']","I am using K-means clustering with TF-IDF using sckit-learn library. I understand that K-means uses distance to create clusters and the distance is represented in (x axis value, y axis value) but the tf-idf is a single numerical value. My question is how is this tf-idf value converted into (x,y) value by K-means clustering."
3382,Which Naive Bayes ML model should be used for classification of a dataset having exponential distribution?,"['machine-learning', 'scikit-learn', 'tf-idf', 'naivebayes']","So like I am doing a text classification and for my dataset, I used tfidf to create feature vectors and their distribution among messages are exponential. Which NB model should I use Multinomial , Complement or some other?"
3383,How to calculate the number of documents a term occurs in using python?,"['python', 'tf-idf']","I'm trying to calculate IDF values for TF-IDF vectorization. I'm trying to calculate number of documents that contain each unique word of the vocab.This is the corpus:corpus = ['this is the first document',
          'this document is the second document',
          'and this is the third one',
          'is this the first document']my code: ......Output i'm getting:
and 0
document 0
first 1
is 3
one 3
second 3
the 4
third 4
this 5
and 0
document 1
first 1
is 3
one 3
second 3
the 4
third 4
this 5
and 1
document 1
first 1
is 3
one 3
second 3
the 4
third 4
this 5
and 0
document 0
first 1
is 3
one 3
second 3
the 4
third 4
this 5The output i need:
this 4
is 4
the 4
first 2
document 3
second 1
and 1
third 1
one 1"
3384,tfidf vectorizer outputs all Zero,"['python', 'tf-idf', 'tfidfvectorizer']",I have a dataframe where one of the columns is text. I want to convert it using tfidf vectorizer. Below code where the text column is complaintWhen I print complain_features it outputs all zero. So when I convert this is pandas dataframe it is all NaNCan anyone please suggest what am I missing here?
3385,How to reduce memory usage for TfIdfVectorizer,"['scikit-learn', 'tf-idf', 'tfidfvectorizer']","This is to train a TfIDF model with my own tokenizer. So, the input of fit_transform is a list of list:Given a big file, this consumes a lot of memory, since all text files must be processed first and the results are loaded into a 'docs' list, which is then fed into the fit_transform method.Is there a way to incrementally fit_transform each doc one at a time, not all at once? If I move the fit_transform into the for loop, it won't work? which means I fit once for each incoming doc. Is this the same as fit all at once? "
3386,Best clustering method for tf-idf vector,"['machine-learning', 'cluster-analysis', 'data-mining', 'tf-idf']","I am doing research on clustering. I have generated tf-idf vector using set of documents.
I have clustered it using K-Means now I want to use other clustering methods to cluster this vector what are the other suitable clustering Methods for this?.I have tried DBSCAN method for this but i was not able to get the top keywords based on their TFIDF score for each cluster."
3387,Why does this 'Vocabulary not fitted or provided' occur in TF/IDF model save and load?,"['tf-idf', 'tfidfvectorizer']","At training time, I do this:Then at test time:But it always gives the error:"
3388,I am in need of guidance creating a program using TF-IDF,"['python', 'pandas', 'tf-idf']","I have a project I am working on using TF-IDF and the only resource I have been given is the internet. After much searching, reading and watching YouTube videos I have created this program, but it doesn't work quite right. This is the Python code written utilizing the Spyder IDE.The objective of the program given is: You will utilize your Python environment to derive structure from unstructured data. You will utilize the data set ""Airline Sentiment"" from Kaggle open data sets located at https://www.kaggle.com/welkin10/airline-sentiment.Using this data set, you will create a text analytics Python application that extracts themes from each comment using term frequency–inverse document frequency (TF–IDF) or simple word counts. For the deliverable, provide your Python file and a .csv with your results added as a column to the original data set.Any help and guidance I receive to understand what I need to do to improve my program is greatly appreciated. I don't have a lot of experience in Python yet. Thank you.I initially was receiving an error with my split command. I did some research and fixed it, but now i am receiving: AttributeError: Can only use .str accessor with string values!. I assume it has something to do with the data type of the csv file but every attempt I have made to change it or edit it to string has failed."
3389,How do i run a TF-IDF in python on a single column from a big data set (csv file)?,"['python', 'tf-idf']",I am attempting to create a python program that runs the TF-IDF of a big data set. It has multiple columns and several rows of data. My problem is I don't know how to limit it to only run on one of the columns titled Comments.
3390,How to build a TFIDF Vectorizer given a corpus and compare its results using Sklearn?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","Sklearn does few tweaks in the implementation of its version of TFIDF vectorizer, so to replicate the exact results you would need to add following things to your custom implementation of tfidf vectorizer:Now given the following corpus:Sklearn implementation:Output:I need to replicate the above result using a custom implementation i.e write code in simple python.I wrote the following code:Output:As you can see this output is not matching with the values from the sklearn’s output. I went through the logic several times, tried debugging everywhere. However, couldn’t locate why my custom implementation is not matching the output by sklearn.
Would appreciate any insights."
3391,NLP clustering documents,"['python', 'nlp', 'cluster-analysis', 'tf-idf']","I am using HDBSCAN algorithm to create clusters from the documents I have. But to create a vector matrix from the words, I am using tf-idf algorithm and want to use GloVe or Word2vec(because tf-idf based on BoW, so it can`t capture semantics). Which method can I use - GloV, Word2vec or any other methods that will be appropriated for text clusterization?
And how I can implement it?Any help will be highly appreciated! "
3392,my algorithm gives bad clusters while usingTF-IDF,"['python', 'machine-learning', 'data-science', 'tf-idf']","im getting bad clusters i would like to rewrite it in a way where i can just plug in any algorithm that i would like (e.g hierarchical, knn, k-means) etc. left as much comment as i can to help you all follow what i am going for can anyone help me improve this"
3393,sklearn oneclass svm KeyError,"['machine-learning', 'scikit-learn', 'tf-idf', 'n-gram', 'one-class-classification']","My Dataset is a set of system calls for both malware and benign, I preprocessed it and now it looks like thisNow I'm using tfidf to extract the features and then use ngram to make a sequence of themThe output is:Now for the learning section I'm trying to use sklearn OneClassSVM:this the output to fraud_pred and main_corpus_targetbut when i try to calculate TP,TN,FP,FN:I get this error:1) I know the error is because it's trying to access a key that isn’t in a dictionary, but i can't just insert some numbers in the fraud_pred to handle this issue, any suggestions??
2) Am i doing anything wrong that they don't match?
3) I want to compare the results to other one class classification algorithms, Due to my method, what are the best ones that i can use??"
3394,Should tfidf be generated with train and test sets combined?,"['scikit-learn', 'classification', 'tf-idf']","https://medium.com/machine-learning-intuition/document-classification-part-3-detection-algorithm-support-vector-machines-gradient-descent-282316b0838eIn the above example, tfidf is generated separately for train and test corpus. Shouldn't it be generated together as idf will not be the same when train and test are processed sepratedly vs when they are processed together? Thanks."
3395,Unable to save the TF-IDF vectorizer,"['python', 'machine-learning', 'pickle', 'tf-idf', 'joblib']","I'm workig on multi-label classification problem. I'm facing issue while saving the TF-IDf verctorizer and as well as model using both pickle and joblib packages. Below is the code:Error Message while saving the TF-IDF vectozier. 
Any suggestions ?
Thanks in advance."
3396,error on fitting corpus to the vectorizer,"['python', 'tf-idf', 'tfidfvectorizer']",So I am trying to make tf-idf matrix now with my corpus and my dictionary. This is how my corpus dataframe look like:and this is how my dictionary dataframe look like:and I tried to fit my corpus to the vectorizer with this code:but it gives me this error:any suggestions?
3397,How to convert tf-idf to a real number,"['python', 'tf-idf']","I want to use tf-idf as a feature in learn-to-rank algorithm. In the train file format, the value of each feature is a float number. However, the tf-idf I get for each row of data is a vector. How can I convert this vector to a unique number? In fact I need transform_to_float in the following code."
3398,R: How to calculate tf-idf for a single term after getting the tf-idf matrix?,"['r', 'nlp', 'tf-idf', 'quanteda']","In the past, I have received help with building a tf-idf for the one of my document and got an output which I wanted (please see below).But I need a little help with calculating tf-idf per singular term. Meaning, how do I accurately get the tf-idf value for each term from the matrix? I am sure, it's not like add all of tf-idf for a term from its matrix column and divide by documents where it appeared. And that would be the value for that term.I have looked at a few sources such as here, https://stats.stackexchange.com/questions/422750/how-to-calculate-tf-idf-for-a-single-term, but this fellow is asking something else entirely from what I read.I am currently weak in text-mining/analysis terminology. So I would apologize in advance for being dumb. "
3399,How to create Embedding matrix by using TFIDF?,"['python-3.x', 'nlp', 'recurrent-neural-network', 'word2vec', 'tf-idf']","I had taken a Text dataset to predict the sentiment of review is positive or negative. By using TFIDF I had converted word to vectors. Next, I had loaded Glove Embedding pre-trained file. Now how to create Embedding Matrix using TFIDF and glove word embeddings? I want to use Embedding Matrix in my Recurrent Neural Network.I am facing Index Error while creating Embedding matrix, please correct me if I did wrong anything in the coding part."
3400,Are TF-IDF and BoW techniques incompatible?,"['python', 'nlp', 'tf-idf', 'tfidfvectorizer']","I have studied the difference between TF-IDF and BoW methods but I have a big doubt about it. I thought that the two methods could be combined, I will explain better. I have a csv file (MY_DATA) with thousands of comments from a social network, I would like to use this dataset to create my BoW for the creation of a classification model of the sentiment of comments (the sentiment of comments is the other variable of MY_DATA and is of three types: positive, negative and neutral)Now that you have seen my script I would like to know if I am using the TF-IDF method correctly. How could I apply the BoW method in my case? Do the two methods inevitably remain incompatible?"
3401,TfidfVectorizer gives high weight to stop words,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","Given the following code:I would expect ""whale"" to be given a relatively high tf-idf in ""Moby Dick"", but a low score in ""Bushido: The Soul of Japan"", and ""the"" to be given a low score in both. However, I get the opposite. The results that are calculated are:Which makes no sense to me. Can anyone point to the mistake in either thinking or coding that I have made?"
3402,Get sorted in specific value,"['python', 'list', 'loops', 'sorting', 'tf-idf']","I have a list that contains text and its TFIDF score:then I want to sort the score, So I coded this:The result is the text being sorted, when in fact, the expected output is the score being sorted, like this:How do I do with the looping? Thanks anyway"
3403,Get a TF-IDF Value,"['python', 'list', 'loops', 'set', 'tf-idf']","I have a code for TF-IDF for my News Dataset :and the result is like this:I have three set, and I was doing intersection.and the intersection result is like this :Actually I expect the result of intersection is also with the TF-IDF score. How do I do with the looping?
So, the expected output will be, for example:"
3404,"R- How do I resolve dataloss & error with TermDocumentMatrix() and DocumentTermMatrix(), respectively?","['r', 'utf-8', 'emoji', 'tf-idf', 'tm']","I have a twitter data of 1000 samples. And trying to do some tf and tf-idf analysis on them to measure the importance of each emoticons in tweets. There are total of 437 unique emoticons, and 810 tweets.My current problem is that with TermDocumentMatrix, all the terms are not showing. Whereas, with DocumentTermMatrix there is a error which I can't get around. Here is a working code snippet:Furthermore, if I try to do the tf-idf, I just get the error. I have looked around, but I don't know where I should fix my mistake.If anybody can help me out, I would greatly appreciate it. It is my first time using tm package. Thank you in advance, I can give more information if you would like me to."
3405,distances greater than 1 with nearest neighbours and tdidf scikit learn,"['scikit-learn', 'tf-idf', 'nearest-neighbor', 'cosine-similarity']","If using default parameters for TfidfVectorizer and NearestNeighbors with scikit learn,
expected to have all vectors normalized to a length of 1, and hence distance == cosine distance that is 1 - cosine similarity. So if cosine similarity is in the range of [0-1], when do I get a distance which is more than 1? For an object compared with itself, I get a distance of 0, that is what I expect. For an empty entry, compared to non empty entry, I get 1 (okay, can be subject to definition). Other entries are always > 1. Need some help to understand and maybe a guidance how to do differently as to have something closer to (my) intuition. BR, Oren."
3406,Class implementation in Python (TF-IDF-CF),"['python', 'jupyter-notebook', 'tf-idf']","I want to do word weighting with the TF-IDF-CF method. I get code like this from github, but I still don't understand how to implement it in my dataframe. the data set that I have contains a collection of text with a total of 1000 lines. Here's the code:please give me an example of using that class. thank you"
3407,"KNN for text classification, but train and class have different lengths in R","['r', 'text-mining', 'knn', 'tf-idf']","Hello I am trying to classify text, here is the codeThe dimension isthe dimension isThen But error
    'train' and 'class' have different lengthsWhat should i do?
Thanks"
3408,How to calculate tf-idf when working on .txt files in python 3.7?,"['pdf', 'nlp', 'python-3.7', 'tf-idf']","I have books in pdf and I want to do NLP tasks such as preprocessing, tf-idf calculation, word2vec, etc on those books. So I converted them into .txt files and was trying to get tf-idf scores. Previously I performed tf-idf on a CSV file, so I made some changes in that code and tried to use it for .txt file. But I am unsuccessful in my attempt.Below is my code:This code is working until print ('Non Zero Count :', cvec_count.shape) and printing:Sparse Matrix Shape :  (0, 7132)Non Zero Count :  0Then it is giving error:ZeroDivisionError: division by zeroEven if I run this code with ignoring ZeroDivisionError, still it is wrong as it is not counting any frequencies.I have no idea how to work around .txt file. What is the proper way to work on .txt file for NLP tasks?Thanks in advance!"
3409,Which hashing function for text tfidf?,"['database', 'machine-learning', 'text', 'scikit-learn', 'tf-idf']","I am using tfidf to vectorize text documents from two corpus C1 and C2. The goal is to know for each document in C1 the most similar document in C2. Size of each corpus is 32k. Hence 32k x 32 k== 1 billion calculations are done. I need to deploy a hashing function or nearest neighbor algorithm to reduce the number of comparisons for each document from 32k to few dozens or less, Is there a well known technique suitable for tfidf?
I followed the steps given in this turorial which use LSH, but the accuracy I got were too low   http://ethen8181.github.io/machine-learning/recsys/content_based/lsh_text.html Note here than I am looking for reducing the matching space (number pairs to match) and not the feature dimension (tfidf matrix dimension)"
3410,Python TfidfVectorizer throwing : empty vocabulary; perhaps the documents only contain stop words,"['python', 'tf-idf']","I am using the statement sklearn.feature_extraction.text import TfidfVectorizer to import my vectorizer. I have a JSON file that is very large and I converted in smaller lists of strings based on field's name, it contains articles data. Now I am trying to perform tf-idf on those lists of strings for articles text and title; I managed to workaround a few errors but I can't seem to solve the one ""empty vocabulary; perhaps the documents only contain stop words"". I have tried the split method as I have seen in other posts but it didn't solve my problem. The weird thing as compared to other posts I have seen is that my print statement to get the features names works, only to finish off with the error. So my output is basically something like: [u'worst', u'millenial'...]
Traceback... ValueErrorThis is my code so far:I have also attached a screenshot of my terminal/output.Thanks!"
3411,How to use a TF-IDF model to find “missing” or under-represented words from a document?,"['python', 'nlp', 'data-science', 'gensim', 'tf-idf']","I'm specifically using gensim to build a TfidfModel but I believe this is more of a general TF-IDF question...Let's say I build a TF-IDF model with 10 documents. How can I use this model to detect words that are high-value in the model, but under-represented from a specific seen or unseen document?For example, if documents 1-9 all use the word ""banana"" frequently, how can I discover that document 10 (or a document not used to build the model) doesn't use it at all?I know that I could just pull a dictionary of words and values from the model and do my own kind of comparison but I'm wondering if there's a better way."
3412,Remove synonyms of TFIDF results in python,"['python', 'nlp', 'tf-idf', 'cosine-similarity']","I am currently working on a project where get the top 10 most relevant words of set of document using tfidf in python. However, there are results where are get the same word and its plurial or adverb or so. To go around this problem, I decided to use stemming, but this leads to a problem where words and their antonyms can have the same root or by reducing a word to its root does not enable to go back and find that specific word in the document if a user was to search for it. Is there a nlp that might be better in this context than nlp? Any hint or link will be useful. I working on something that is very similar to youtube."
3413,Scikit Learn TF-IDF Vectorizer : How to get top n terms with highest IDF values,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I have a task to get top IDF values.
for exampleand the result should have the top IDF values in the alphabetical order of the features/words "
3414,how to convert the word column to be the header columns and the name of the 2 articles to be the header of the rows?,"['python', 'numpy', 'machine-learning', 'anaconda', 'tf-idf']","I wrote a code that shows the data frame without the headers, only numbers insteadnow it looks like that:and I need to convert it to be like that(without the heading indexes)I wrote this code to get to the data frame:Do you have any sugestions?"
3415,how to compute TF-IDF on dataset?,"['python', 'pandas', 'numpy', 'tf-idf', 'tfidfvectorizer']","I have data set of articles and how much every word appear in those articles: 
how to calculate TF-IDF?I get this DB:so from here how to calculate TF-IDF? any suggestions? should I convert to a dictionary or there's another possibility?"
3416,TF-IDF Vectors Example (HELP),"['python', 'tensorflow', 'vector', 'tf-idf', 'tfidfvectorizer']","Hey i made 3 different approaches but i can't decide which is the right way to use TF-IDF:The first code does fit and transform to both x_train and x_test separately giving (5000, 94462)
(5000, 93007).The second code uses both train and test which i think is not right because idf is calculated based on the training documents only, giving (5000, 152800) (5000, 152800).The third code gives (5000, 94462) (5000, 94462)."
3417,Is there any inbuilt pandas operation which can find similar columns of two different dataframes?,"['python', 'pandas', 'dataframe', 'word2vec', 'tf-idf']","I have two dataframes which have similar data in the columns but different column names. I need to identify if they are similar columns or not.colName1=['movieName','movieRating','movieDirector','movieReleaseDate']colName2=['name','release_date','director']My approach tokenize colName1 and compare them using
 - levenshtein/Jaccard Distance
 - Find similarity using TFIDF score.But this works for col names having similar names for eg. movieName and Name. Suppose you have 'IMDB_Score' and 'average_rating' this approach is not going to work.Is there any way word2vec can be utilized in the above mentioned problem."
3418,How can I remove a tuple from my corpus TF-IDF?,"['python', 'tuples', 'gensim', 'tf-idf']","I have this codefrom gensim import models
import numpy as npCreate the TF-IDF model
tfidf = models.TfidfModel(corpus, smartirs= ""ntc)Show the TF-IDF weights
for doc in tfidf[corpus]:
    print([[dictionary[id], np.around(freq, decimals=7)] for id, freq in doc])Then, I have this result [['brocolli', 0.5491093], ['brother', 0.1237955], ['but', 0.1237955], ['eat', 0.568619], ['good', 0.3660728], ['like', 0.2843095], ['not', 0.1830364], ['rata', 0.1237955], ['saluut', 0.2843095]]
[['brother', 0.1647724], ['rata', 0.1647724], ['around', 0.3784174], ['basebal', 0.3784174], ['drive', 0.1647724], ['lot', 0.3784174], ['mother', 0.2436224], ['practic', 0.3784174], ['spend', 0.3784174], ['time', 0.3784174]]enter code here
[['rata', 0.1335974], ['drive', 0.1335974], ['and', 0.3068207], ['blood', 0.3068207], ['caus', 0.3068207], ['expert', 0.3068207], ['health', 0.197529], ['increas', 0.3068207], ['may', 0.3068207], ['pressur', 0.197529], ['some', 0.3068207], ['suggest', 0.3068207],['tension',0.3068207], ['that', 0.197529]]I want to remove all the tuples < 0.2 
For exaple the word ""DRIVE""= 0.1335 I want to remove this tuple. How can I do this?"
3419,How to apply Kfold with TfidfVectorizer?,"['machine-learning', 'data-science', 'tf-idf', 'tfidfvectorizer', 'k-fold']","I'm having an issue in apply K-fold cross-validation with Tfidf. it gives me this errorI have seen other questions who had the same problem but they were using train_test_split() It's a little different with K-foldActually, I found where's the problem but I can't find a way to fix it, basically, when we extract train data with cv/train indices we get a list of sparse matrices I tried to apply Tfidf on the data after splitting, but it didn't work as the number of features wasn't the same.So is there any way to split the data for K-fold without creating a list of a sparse matrix?"
3420,How to add bigram stopwords when text clustering with tf-idf and k-means,"['python', 'scikit-learn', 'tf-idf']","Summary:
I'm doing text clustering with using tf-idf and k-means clustering. I have followed this article to do so. While I haven't fully understood what's going on with the clustering side, the concept works for my purpose for now. The main purpose for me is cluster and label each document and get an idea what's a document is about without reading it.Code: I'm doing it with the code below, similar to the article.Simple output:Question: I'm generating only bigrams (ngram_range=(2, 2)). The problem is, while some bigrams are stopwords for me, the unigrams of it are not. E.g. ""water supply"" might be meaningless for my situation and I want to ignore it. But the words ""water"" and ""supply"" may be informative for the vectorizer, so I just can't add them as stopwords individually. How can I ignore/add as stopword some bigrams so they won't be a topic?Optional question: As you may have guessed, with this approach and real life data, I find many meaningless topics. How can I ""score"" these topics that I can pick only those have a high score? In other words, how can I filter the results that I can pick topics that really fits a cluster like ""yes, this document absolutely belongs this cluster""?P.S. You can simply use the documents from the article to generate minimal reproducible example.Edit 1: I have tried first fitting the vectorizer, then add some bigram stopwords to vectorizer's stopwords attribute (vectorizer.stopwords_, not vectorizer.stopwords), then tried to transform documents but that didn't work; I still see the stopwords I have added as a topic."
3421,How to create inverted index for a column in a dataframe?,"['python', 'dataframe', 'nlp', 'nltk', 'tf-idf']","I created a data frame for my scrapped data and removed punctuations, stopwords and tokenized it.
How do I create inverted index for the columns name and brand ? "
3422,Is there any specific metric or method to drop tails of TF IDF vocabulary?,"['python', 'machine-learning', 'nlp', 'data-science', 'tf-idf']","I have a TF IDF vocabulary I already get from gensim or tfidfvectorizer. Is there any specific metric or method to drop tails of TF IDF vocabulary?  I mean tails at Zipf diagram. How to visualize it?
I would like to see how accuracy changes when I drop number of words in vocabulary. For instance, I have vocabulary that has 175000 of words. "
3423,term frequency inverse document frequency and word similarity,['tf-idf'],"I am asking about tf-idf and word similarity , the question is how can use tf-idf to wight terms in word similarity model (not document similarity but word similarity) ,  in order to measure word similarity a proposed model is term-term matrix , a matrix where each column is represented as a (term)word/context and each row is a word(the target word) , term-term matrix did not give importance to ""documents"" , we no longer have documents we have word instances and their contexts ,  while the tf-idf depends on partitioning the corpus to documents .
the term-term matrix model is taken from this book :- 
(Speech and language processing, Daniel Jurafsky and James Martin)"
3424,How implement TF-IDF scoring with additional weighting for certain terms,"['python', 'search', 'information-retrieval', 'tf-idf', 'cosine-similarity']","I currently have a tf-idf system for scoring and I'm using cosine similarity for searching. I want to add additional weighting that takes into account whether or not a given term is in the title of a document. I'm unsure of how to combine the title data with the tf-idf scores, however. Does anyone know of a good approach to this problem?"
3425,How to Compare two sentences semantically?,"['android', 'nlp', 'tf-idf', 'cosine-similarity']","I am trying to build an app for college so the students can have their exams on it 
but I am having problem the article questions which I can't compare the answer of the student and the answer of the model answer to check if it was right or wrong 
so can anyone help me with that or tell me where to start ?
I have read about a lot of algorithms but I don't know where to begin.I have found codes and function but I don't know how to use them like the following link click here"
3426,Implementation TF*IDF and Cosine Similarity,"['php', 'codeigniter', 'codeigniter-3', 'tf-idf', 'cosine-similarity']","Anyone know, how to implementation TF*IDF and cosine similarity algorithm for searching document ?
I'm stuck after the explode function and string replace. I don't know how to calculation the Term Frequency ?This my script to explode and replace symbol :after that, I don't know how to implementation the algorithm.
Please, anyone help me."
3427,"convert scala dataframe to rdd[(Long,Vector)]","['scala', 'apache-spark', 'rdd', 'tf-idf']","I have a dataframe with two columns id and a tfidfvector(org.apache.spark.mllib.linlag.Vector).I want to convert this to a rdd[(id,Vector)] and then convert it to a coordinate matrix.PS: Can't share the data due to constraints.I tried df.As[(Long,Vector)] didn't work"
3428,Ranking words across multiple text files by TFIDF,"['python', 'nlp', 'rank', 'tf-idf']","I have crawled a wikipedia article and extracted several climate change related url's and saved their content with their url's as the name of file. Now i wanna find out which are the most popular words amongst all this corpus by using tfidf. This is my piece of code:My code shows issue in this portion of code:here ""file"" refers to as filename and alldocs contains list of all the climate change related text file  present."
3429,Inverse Document Frequency calculation resulting in Negative values on Jupyter notebook,"['python', 'jupyter-notebook', 'google-colaboratory', 'tf-idf']","So the code I used for calculating IDF is as follows When I call this function using in Jupyter Notebook it produces Negative values, however when I run the same code on Google colaboratory it produces Positive values for IDF.IDF values are always supposed to be Positive, kindly help me with this issue "
3430,How to customize tfidf based word count,"['python', 'jupyter-notebook', 'tf-idf']","Currently I have run into a problem where when I convert my keyword list into a dictionary, I cannot multiple the frequency in the original data set to the frequency when I convert the list to the dictionary. rowid   Keyword Frequency1   dermatology 11512   psychiatry  10683   obgyn   10174   internal medicine   8835   mental health   8656   optometry   7637   pediatrician    6788   pediatrics  622I am trying to cluster some search keywords using LDA and tfidfmodel. In my data set, I have a list of keywords along with its frequency. I am trying to cluster topics based on those keywords using the frequency #.Not sure what's the best way to prep these keywords to better cluster and find topics. Please advise."
3431,Lucene tfidf does not have square of idf?,"['lucene', 'tf-idf']","In Lucene8.2.0 source code TFIDFSimilarity.TFIDFScorer#score(float freq, long norm),
I cannot find squre of idf, but according to the documentation Lucene Practical Scoring Function, 
there should involve squre of idf when calculate score.This mismatching really confuses me a lot, does the documentation not match the source code or I've just misunderstood the source code? Could someone explain it please?  Thanks in advance"
3432,Spark - HashingTF inputCol accepts one column but I want more,"['python', 'apache-spark', 'tf-idf']","I'm trying to use HashTF in Spark but I have one major problem.
If inputCol has only one column like this But if I try to add more columns I get error message ""Cannot convert list to string"".
All I want to do is Any ideas on how to fix it?"
3433,I can't create tf-idf matrix for my test data using text2vec,"['text', 'text-mining', 'tf-idf', 'text2vec']","I'm following this tutorial and doing it as I did the training set, but it keeps saying the same thing. Someone know what's wrong with this?"
3434,TFIDF model creates NAs when reading from elastic,"['pandas', 'tf-idf', 'sklearn-pandas']","I'm modeling some text with Kmeans using a TF-IDF model but I have a weird behaviour that I cannot understand.Let me give you more details on the processWhen following that process the returning dataframe shows some rows with NAN values on the new kmeans column. However, that doesn't happen when I alter the process with these extra steps:I found out about this CSV ""magic"" trick by chance when sending my code to one of my peers.
I presume it has something to do with some sort of encoding but I don't know how to fix it. Sorry if the title wasn't descriptive enough. It was difficult to express it well with a single line.Here is my codeI don't think it has anything to do with the kmeans model because I'm also using other models and still get the same rows with NANsCould anyone help?
Thanks!!"
3435,Normalization of Term-frequency and Inverse Document Frequency of varying documents lengths to calculate cosine similarity,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'cosine-similarity']",I have been trying to find thousands of textual document's similarity against one single query. And every document size is majorly varying (from 20 words to 2000 words)I did refer the question: tf-idf documents of different lengthBut that doesn't help me because a fraction of cosine value matters too when comparing with a pool of documents to maintain order.I then came across a wonderful normalization blog: Tf-Idf and Cosine similarity. But the problem here is to tweak in the TermFreq of every document. I am using sklearn to calculate tf-idf. But now I am looking for some utility similar to sklearn's tf-idf performance. An iterative approach over all the documents to calculate TF and then modify it is not only time consuming but also not efficient.Any knowledge/suggestions are appreciated.
3436,How to implement the TF-IDF of a CSV file into a frequency counting program,"['python', 'counter', 'frequency', 'tf-idf']","Calculate and display the tf–idf of words in a CSV file. Using a csv file with three columns for categories and many rows as an example, the program should calculate and output the top 10 words in each category based on their tf–idf value their frequency in the specific category divided by their overall frequency in the whole stories list.Here is my program that calculates word frequency The output gives me the number of titles and number of words of each category. My expected output should
Output all the categories in the file and the top 10 words (words with the highest tf–idf value) in each category
note:
The frequency should be based on the number of times a word appears in titles and subtitles of stories (not just the titles)
The program doesn't know which categories are being used in the file or how many stories are in each one
A word frequency is counted regardless if it's in lowercase or uppercase so I need to use dictionaries and the lower() function"
3437,Explanation of “Dimension mismatch” after using fit_transform on testing data,"['python', 'scikit-learn', 'tf-idf']",I was reading some code about NLP and saw that X_test does not have fit_transform when assigned (last line of code below). When I tried to do it with fit_transform like the X_trainand continued to use a predictive model it returned: ValueError: dimension mismatchThis question is about that case: SciPy and scikit-learn - ValueError: Dimension mismatchWhat I would like is a simple explanation as to why it occurs be cause I don't understand it clearly.Below is the code I have:
3438,How to calculate the similarity between 2 String columns using TF IDF in R,"['r', 'pattern-matching', 'similarity', 'tf-idf', 'stringdist']","It might be similar question would have asked in this forum but I feel my requirement is peculiar .I have a data frame df1 where it consists of variable ""WrittenTerms"" with 40,000 observations and I have another data-fame df2 with variable ""SuggestedTerms"" with 17,000 observationsI need to calculate the similarity between ""written Term"" and ""suggestedterms"", I am using the Stringdist package but this approach  is taking quite a long as we have more observations.df1$WrittenTermshead painlung cancerabdminal paindf2$suggestedtermscardio attackbreast cancerabdomen painhead achelung cancerI need to get the output as followdf1$WrittenTerms   df2$suggestedterms  Similarity_percentagehead pain head ache 50%lung cancer lung cancer 100%abdminal pain abdomen pain 80%I am writing the below code to meet the requirement but its taking more time as it involves for loop and is there any way where we can find similarity using TF IDF OR any other approach which will take less time"
3439,Cosine Similarity and TS-SS similarity among documents using tf-idf - Python,"['python', 'machine-learning', 'tf-idf', 'cosine-similarity', 'tfidfvectorizer']","A common way of calculating the cosine similarity between text based documents is to calculate tf-idf and then calculating the linear kernel of the tf-idf matrix.TF-IDF matrix is calculated using TfidfVectorizer().Here article_master is a dataframe containing the text content of all the documents.
As explained by  Chris Clark here, TfidfVectorizer produces normalised vectors; hence the linear_kernel results can be used as cosine similarity.cosine_sim_content = linear_kernel(tfidf_matrix_content, tfidf_matrix_content)This is where my confusion lies. Effectively the cosine similarity between 2 vectors is:Linear kernel calculates the InnerProduct as stated here  So the questions are: Why am I not divding the inner product with the product of the magnitude of the vectors ? Why does the normalisation exempt me of this requirement ?Now if I wanted to calculate ts-ss similarity, could I still use
the normalised tf-idf matrix and the cosine values (calculated by
linear kernel only) ?"
3440,How to use sklearn TFIdfVectorizer on pandas dataframe,"['python', 'pandas', 'scikit-learn', 'tf-idf']","I am working with a tab separated file that looks like this:My goal is to produce a  dataframe that looks like this:Where ech word in the long text field of the TSV appears as a feature (column), and its value is the words TFIDF.I could try and go about this manually, but I am looking to use sklearn's TFIDFVECTORIZER to produce this. However, I need to preprocess the text in the field, to follow certain guidelines.So far, I can read in the .tsv file, create the dataframe, and preprocess the text. What I am having trouble doing is combining my text formatting functions to then pass it to the TFIDFVECTORIZERBelow is what I have:Which produces:However, I am not sure if this is the proper format to allow TFIDFVECTORIZER to work right.  When I try to use it, I used below code which ran properly:But simply gave me results like:And I don't know what I am looking at there. How can I use TFIDFVECTORIZER to accomplish my goal of creating a feature matrix of each word (after my cleaning logic has been applied) with TFIDF values?"
3441,Tokenize a corpus of 10 documents in Python,"['python', 'tokenize', 'tf-idf']","I am new to coding in Python so figuring out how to code more advanced actions has become a challenge for me.My assignment is to compute the TF-IDF of a corpus of 10 documents. But I am stuck on how to tokenize the corpus and print out the number of tokens and number of unique tokens.If anyone can help or even step step guide me in the right direction, it would be so greatly appreciated!"
3442,How to calculate TF-IDF of dataframe groups using PySpark,"['python', 'pyspark', 'tf-idf']","My question is similar to this but I am using PySpark and the question had no solution there. My dataframe df is as follows, where id_2 represents a document id and id_1 represents the corpus they belong to:How can I calculate TF-IDF of the documents for each corpus? -- for the given scenario, tf should work just fine as it is document specific but using idf like this considers all the documents belonging to a single corpus. "
3443,Calculate TF-IDF in a DataFrame with columns the examined Documents,"['python', 'tf-idf', 'tfidfvectorizer']","I need a help calculating the tf-idf of a set of strings that exist in a DataFrame. In the columns I have the Documents and in the rows I have the extracted tokens/n-grams. An n-gram may exist either in only one document or in more, but only 1 count in a specific document. 
I would like to calculate the tf-idf of all n-grams according to the DataFrame provided. I have examined TfidfVectorizer() but I keep having an error because it returns the names of columns and not the tf-idf of the words contained.And the output is:
['document1', 'document2', 'document3', 'document4', 'document5']My DataFrame has this format:"
3444,Calculate TF-IDF using sklearn for variable-n-grams in python,"['python', 'text', 'scikit-learn', 'tf-idf', 'n-gram']","Problem:
using scikit-learn to find the number of hits of variable n-grams of a particular vocabulary.Explanation.
I got examples from here.Imagine I have a corpus and I want to find how many hits (counting) has a vocabulary like the following one:What I call here window is the length of the span of words in which the words can appear. as follows:'tin tan' is hit (within 4 words)'tin dog tan' is hit (within 4 words)'tin dog cat tan is hit (within 4 words)'tin car sun eclipse tan' is NOT hit. tin and tan appear more than 4 words away from each other.I just want to count how many times (window=4, words=['tin', 'tan']) appears in a text and the same for all the other ones and then add the result to a pandas in order to calculate a tf-idf algorithm. 
I could only find something like this:where vocabulary is a simple list of strings, being single words or several words. besides from scikitlearn:The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.does not help neither.Any ideas?
Thanks."
3445,How tf-idf model handles unseen words during test-data?,"['python-3.x', 'scikit-learn', 'tf-idf']","I have read many blogs but was not satisfied with the answers, Suppose I train tf-idf model on few documents example:I use this function:So X_train_tfidf output will be matrix with tf-idf score:Is this correct?What does fit does and what does transformation does?
In sklearn it is mentioned that:fit(..) method to fit our estimator to the data and secondly the transform(..) method to transform our count-matrix to a tf-idf representation.
What does estimator to the data means?Now suppose new test document comes:How to convert this document to tf-idf? We can't convert it to tf-idf right?
How to handle word thriller which is not there in train document."
3446,N_gram frequency python NTLK,"['python', 'pandas', 'nltk', 'tf-idf', 'countvectorizer']","I want to write a function that returns the frequency of each element in the n-gram of a given text.
Help please.
I did this code fo counting frequency of 2-gramcode:"
3447,Document Vector for TF-IDF,"['information-retrieval', 'tf-idf']","I'm reading the Information Retrieval book by David Grossman and Ophir Frieder and I'm having a difficulty in understanding the document vectors.Following the example of the book, I have 3 documents, namelydl = ""Shipment of gold damaged in a fire""d2 = ""Delivery of silver arrived in a silver truck""d3 = ""Shipment of gold arrived in a truck""I've calculated the TF, IDF, and TF-IDF for the documents. For d1, my TF was calculated as:{'a': 0.14286, 'arrived': 0.0, 'damaged': 0.14286, 'delivery': 0.0, 'fire': 0.14286, 'gold': 0.14286, 'in': 0.14286, 'of': 0.14286, 'shipment': 0.14286, 'silver': 0.0, 'truck': 0.0}and my TF_IDF was {'a': 0.0, 'arrived': 0.0, 'damaged': 0.06816, 'delivery': 0.0, 'fire': 0.06816, 'gold': 0.02516, 'in': 0.0, 'of': 0.0, 'shipment': 0.02516, 'silver': 0.0, 'truck': 0.0}How the document vectors are constructed? I can't seem to find a way. Document Vectors Table (Book)"
3448,Query for TF-IDF,"['python', 'tf-idf']","So I'm reading this article to implement TF-IDF https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8.After processing the text and computing TF, IDF and TF-IDF, I'll get a dictionary, something like 
{'TFIDF_score': 0.0368605, 'doc_id': 1, 'key:' 'if'}
for every word in my text passed.My question is: what's the best way to implement and compute a query for this? Let's say my query is 'two'. Its TFIDF_score is 0.9987384. Should I create a vector for my query and compute the norm and calculate the dot product based on my query and my doc_id, (dot_prod(query, doc_id))?Thanks for the help. I appreciated it"
3449,What is `idf_` of sklearns TfidfVectorizer?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I thought that .idf_ is the inverse document frequency, meaning it would be So for the two documents[""foo bar bar ist cool und so weiter"", ""Ich habe hier nichts so gesagt""]I would have expeced for everything a value of 0.69 = math.log( 2 / 1) and for one a value of 0 = math.log(2 / 2). But sklearn seems to use it differently because I get 1.406 and 1.0.This gives"
3450,Multioutput target data is not supported with label binarization,"['python-3.x', 'scikit-learn', 'tf-idf']","I'm fit my MultinomialNB model with K-fold split.I've tried to balance data with SMOTE (imblearn.over_sampling, lib)I expect cross-validation of multi-label classification"
3451,How to get “Word” Importance in NLP (TFIDF + Logistic Regression),"['python', 'nlp', 'nltk', 'tf-idf']","I have a function to get tfidf feature like this:I applied a simple logistic regression like this: I am getting feature importance like this:But this is giving me feature importance of ""columns"" not words"
3452,Calculate cosine similarity of document relevance,"['pyspark', 'apache-spark-mllib', 'tf-idf']","I have go the normalized TF-IDF for and also the keyword RDD and now want to compute the cosine similarity to find relevance score for the document .So I tried as Now I wanted to calculate the cosine similarity between the normalizedtfidf and keyWords.So I tried using But this throw the error as TypeError: float() argument must be a string or a numberWhich means I am passing a wrong format .Any help is appreciated .Update I tried then but got TypeError: Cannot treat type  as a
  vectoras the error."
3453,How to solve “ValueError: setting an array element with a sequence”,"['python', 'dataframe', 'nlp', 'tf-idf', 'lda']","Here is an example of my data setI want to implement Latent Diritchlet allocation (LDA) for context generation for each sentence. I have separately trained my model for it and want to test on these data.To reach to LDA, I tokenize the text into sentences as I am interested to classify each sentence with a topic. After sentence tokenization, I implement TFIDF and then to LDA. While reaching upto LDA, I get this error. Following is my code.Here is where I get this error ""ValueError: setting an array element with a sequence."" While going through similar errors, ValueError: setting an array element with a sequence I found it may be because the rows have a different number of sentences resulting in different length or sequences. But this is the heterogeneity I have and I am not really sure what is the problem. Please help!!"
3454,Remove first x characters from a few column headers,"['python', 'pandas', 'dataframe', 'nlp', 'tf-idf']","I have created a sparse matrix dataframe which has taken the values in a list and set them as column headers. A number of rows contain headers for example ""000 bank"". I want to remove the ""000 "" so it is just 'bank' for example. How can I get rid of the '000 '. Not all column headers have the 000 in them as you can see in the index above. "
3455,Build Scipy Sparse Matrix for TF-IDF,"['python', 'scipy', 'sparse-matrix', 'information-retrieval', 'tf-idf']","I am building vectoreSpaceModel Tf_idf. I completed the code but I want to convert my code TO build scipy sparse matrix due to memory issue and speed issue. Our Sir told us not to use tf-idf Library for calculating TF-IDF so I have to do it manually. My code stuck when Number documents increase. I have 21000 documents. It works fine on 10000 documents but when documents increase its stuck in tf-idf calculation due to 2 for loops. I also use ThreadPoolExecutor to increase the speed of my code but getting any result. Please help me to improve this code. How I convert it sparse matrix and use the dot product for tf-idf calculation.
This is my code I am attaching my file with this link[https://drive.google.com/open?id=1D1GjN_JTGNBv9rPNcWJMeLB_viy9pCfJ]"
3456,Merging Predicted values with original data frame,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I'm using TF-idf to extract some features from text then train machine with this. after predict, i need merge predicted values into original data frame.i use train_test_split like this : and my dataset something like this: after calculating TF-idf and train_test_split actually i don't know which predict is for which column."
3457,How to compute distances and rank the vectors (TFIDF),"['document', 'similarity', 'ranking', 'tf-idf']","I am learning python, basically shifting from Matlab to Python. Lets assume, I have TFIDF matrix of size m x n denoted by D, and query vector of size 1 x n denoted by q. Clearly, m denotes the number of documents and n the length of feature or the vocabulary size.Now I want to compute the distance array dd, which stores the distances of q with all the arrays in D.To keep things running, here is the code
d1 the set of documents, and d2 is the queryNow I perfom TFIDFNow I want to compute distance of q with all rows of D and store into dd."
3458,Evaluation of TF-IDF effectiveness in Gensim. Why are the results lists incomplete?,"['python', 'keyword', 'gensim', 'tf-idf']","I would like to investigate how the TF-IDF score of particular words in document depends on the number of documents on which IDF is based. Unfortunately, the list of results that I receive vary in length and yet the number of words in the document is fixed... How to get TF-IDF results for all words in a document, regardless of the number of modeling documents?I use the Gensim library to calculate the TF-IDF ratio. Here is my approach:So far so good, but now I would like to compare these results with the results obtained for a smaller number of documents based on which the model is built. In order to do this I do the following:Can somebody explain to me why the number of results is decreasing? For example, the number of unique tokens in the first document is 9; but when the model is trained on fewer documents, the number of tokens suddenly drops to 8, 7, etc... And yet this document contains a fixed number of tokens. Why not all of them are included in the results? How to include them? Maybe I'm doing something wrong ... I'll be grateful for your help."
3459,I am getting error can't multiply sequence by non-int of type 'float',"['python', 'python-3.x', 'vectorization', 'tf-idf']","While compute average word2vec I am getting below error
I am getting error The error I am getting:"
3460,#1265 Data truncated from column 'tfidfglobal'. Whats wrong with my query?,"['java', 'mysql', 'decimal', 'tf-idf', 'truncated']","Well, I'm working on text clustering and I have a problem with saving my tf-idf and tfidfglobal value to my database. So, I'm using a query to calculate the tf-idf value of a word. The data type of tf-idf and tfidfglobal column is Decimal(65,3), I thought it was the largest length for the Decimal but still have an issue of truncated data.I have tried to change the data type from Decimal to Text and it works well, one of a row that has truncated data issue has a tifidf value of 2,606.806.But when the data type is Decimal, it shows a warning of truncated data and the value of tfidf becomes 2.000. What I need is not a Text data type but a Decimal data type, because I need to do some arithmetic operations with this value. Here is my query:What's wrong with it? I thought the value 2,606.806 isn't bigger than Decimal(65,3) but I still have a truncated data issue."
3461,TFIDF vectorizer outputs weights as one for every feature,"['python', 'tf-idf', 'tfidfvectorizer']","I can not seem to figure out what is wrong with my tfidf vectorizer. I am trying to apply it on 100,000 filings but it ouputs weights as one for all my ngrams. I have filings stored in ""/Users/lucy/Desktop/newdic"". I think the error is with my import of files. Thank you so much for your help! I am extremely grateful! "
3462,Why does scklearn's tfidf_transform calculate the way it does?,"['python', 'math', 'scikit-learn', 'nlp', 'tf-idf']","I'm currently working on an NLP Problem and Need to calculate the TF-IDF score. However, Sklearn's TfidfTransformer seems to be using a different order of operations than every source recommends. Usually tf_idf score is computed by calculating the termfrequency (Count for every word / length of sentence) and multiplying it with the inverse document frequency (log of number of documents/number of documents the word appears in).However sklearn first multiplies the raw counts with the idf and normalizes afterwards.Any insight onto why they proceed like that would be greatly appreciated."
3463,How to get the average TF-IDF value of a word in a corpus?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I am trying to get the average TF-IDF value of a word in an entire corpus. Suppose we have the word 'stack' appear 4 times in our corpus(a couple of hundred documents). It has these values 0.34, 0.45, 0.68, 0.78 in the 4 documents it was found. Hence, it's average TF-IDF value across the entire corpus is 0.5625. How can I find this for all the words in the document?I am using a scikit-learn implementation of TF-IDF. This is the code I am using to get the TF-IDF values for each document:For each iteration, this outputs a dictionary of 81 words and their TF-IDF score for that document:
{'kerry': 0.396, 'paris': 0.278, 'france': 0.252 ......}Since I am only outputting the top 81 words, I know that all the words in that document won't be covered. So, I want the average TF-IDF value of each of the top 81 words in a document (Words will be repeated).EDIT: I tried out @mijjiga's solution. Here are the results:As we can see, the word 'the' has multiple values. I apologise if my question was not indicative of this but I want one value for each word. And this value is the average TF-IDF score for that word in that corpus of documents. Any help as to how to get that working? Thanks!Here is the code used:"
3464,How can I easily change tf-idf similarity dataframe using apply,"['python-3.x', 'pandas', 'dataframe', 'apply', 'tf-idf']","I'm using Python 3.
I am doing TF_IDF, and I record more than 80% of results.
But for is too slow. because shape is 51,336 x 51,336.
How can you create dataframes faster without using for statement.
It's taking 50 minutes now.
I want to make a dataframe like this.  [column_0],[column_1],[similarity]
  index[0], column[0], value
  index[0], column[1], value
  index[0], column[2], value
  ....
  index[100], column[51334], value
  index[100], column[51335], value
  index[100], column[51336], value
  ...
  index[51336], column[51335], value
  index[51336], column[51336], value  "
3465,Implementing a TF-IDF Vectorizer from Scratch,"['python', 'machine-learning', 'nlp', 'tf-idf']","I am trying to implement a tf-idf vectorizer from scratch in Python. I computed my TDF values but the values do not match with the TDF values computed using sklearn's TfidfVectorizer().What am I doing wrong?expected output:
(output obtained using vectorizer.idf_)actual output:
(the values are the idf values of corresponding keys."
3466,How to Transform sklearn tfidf vector pandas output to a meaningful format,"['python', 'pandas', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I have used sklearn to obtain tfidf scores for my corpus but the output is not in the format I wanted. Code:What I have: word1, word2, word3 could be any words in the corpus. I mentioned them as word1 , word2, word3 for example.What I need:I tried transforming it but it transforms all the columns to rows. Is there a way to achieve this ?"
3467,TFIDF vs Word2Vec,"['python', 'machine-learning', 'data-science', 'word2vec', 'tf-idf']","I am trying to find similarity score between two documents (containing around 15000 records).I am using two methods in python:
1. TFIDF (Scikit learn) 2. Word2Vec (gensim, google pre-trained vectors)Example1Doc1- Click on ""Bills"" tabDoc2- Click on ""CHAPS"" tabFirst method gives 0.9 score.
Second method gives 1 scoreExample2Doc1- See following requirements:Doc2- See following requirementsFirst method gives 1 score.
Second method gives 0.98 scoreCan anyone tell me:why in Example1 Word2Vec is giving 1 though they are very differentand in Example2 Word2Vec is giving 0.98 though they are having difference of only "":"""
3468,How to solve value error while performing TFIDF on csv file in python 3.7?,"['csv', 'python-3.7', 'tf-idf']","I am trying to perform tfidf on csv file. However, I am getting value error saying too many values to unpack (expected 3).
I am working on Python 3.7 version.I am open to other solution also. However my data is in csv file and the ultimate goal is to get tfidf.Thanks in advance."
3469,How to apply TFIDF on test set,"['python', 'scikit-learn', 'tf-idf']","Lets assume I have two files of text. file 1 contains the training set, which is mainly used to define the vocabulary. file 2 is the user entered words. Now using sklearn we find the tfidf of d1Now for query d2 I want to compute the tfidf vector based on vocubarly learned from d1. What and how should I do it?"
3470,How to conditionaly create a new variable containing one or more observations?,"['r', 'nlp', 'tf-idf']","I'm trying to match the post (from Table dataEnArbeit) with its relevent key-word/s and its/their tf-idf value/s (from Table arbeit). How do I also copy all relevent words and their tf-idf scores in wArbeit and iArbeit respectivly?All I get is one word from (arbeit), although there are more."
3471,How can I resolve “ValueError: empty vocabulary ~”?,"['python', 'tf-idf']","This error occurred when trying to sort nouns whose tf-idf score was 0.03 or higher after morphological analysis of tweets acquired in real time.
Also, I can't remove retweets and emoticons in the tweets I get.Can you tell me what is happening inside the code and how to fix it?iOS 10.12.6, Python 3.7.3, Atom"
3472,Unable to Export (or view) Total If-Idf Results for textmining,"['r', 'rstudio', 'text-mining', 'tf-idf']","As part of my efforts to textmine research papers I am interested in looking at Tf-Idf values.So far I have had difficulty using tidytext for tf-idf due to issues with columns/objects not being detected (consistent issue on this site). Therefore I utilised TM weighting and hoped to view all my results by exporting to csv.The limited results that I have are in the right format (paper; term; tf-idf value). Only a few of the papers though are available. This is despite the fact that the object states that there are 71 documents. (One document is not readable therefore shows up with error that can be ignored.)Any help is appreciated, cheersAs stated above, four papers and ten terms appear in the resulting csv file. I am unsure why the results would be limited in this manner."
3473,How to use tft.compute_and_apply_vocabulary and tft.tfidf correctly?,"['python', 'tensorflow', 'tf-idf', 'tensorflow-transform']",I try to use tft.compute_and_apply_vocabulary and tft.tfidf to compute tfidf in my jupyter notebook. However I always get the following error:but the placeholder type is actually string. Here is my code:Version:Thanks in advance!
3474,why does tfidf object takes so much space?,"['tf-idf', 'tfidfvectorizer']","I have roughly 100,000 long articles totally about 5GB of texts, when I perform from sklearn it constructs a model with 6GB. How is that possible? Isn't that we only need to store the document frequency of that 4000 words and what that 4000 words are? I am guessing TfidfVectorizer of stores such 4000 dimension vector for every document. Is it possible somehow I have some settings wrongly set?"
3475,"verify the performance of the TFIDF, LSA and LDA methods","['tf-idf', 'lda', 'lsa']","hello I want to make a grouping of the textual content of the web pages into themes, for that I transformed them into a matrix tfidf crossing the terms to the pages then reduce the dimension of the matrix in themes by applying the two methods LSA and LDA. thereafter I want to check the performance of the methods tfidf, LSA and LDA by calculating the accuracy; The problem is that I am stuck in this step and I need a code or help to check the performance of the methods."
3476,Building a logistic regression model with PySpark MLlib to make a prediction,"['pyspark', 'logistic-regression', 'apache-spark-mllib', 'tf-idf']","I am teaching myself how to build a logistic regression model in MLlib using PySpark and following an online tutorial (https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35). My dataset contains TripAdvisor user reviews for 20 different tourist attractions in NYC. Each review has a rating (1 for positive, 0 for negative). I'd like to train my model to predict whether a user review is positive or negative without using the ratings column. Next, I plan to sum up the number of reviews labeled positive and divide it by the total number of reviews to get a positive sentiment 'score' for a particular tourist attraction then rank the 20 different tourist attractions based their sentiment 'score'. This classification is simiplistic because right now I'd like to just to build a PySpark machine learning model with MLlib (not use it for real world purposes).After data ingest, ETL, tokenizing, removing stopwords/punctuation, and lemmatizing I have a clean dataframe (myDF). Next, I split the data, apply tf-idf and train the logistic regression classifier. I am able to get the accuracy of the model, but how do I turn it into an application that can perform sentiment analysis and rank the 20 tourist attractions according to a sentiment 'score'?I've tried to look at the predictions variable (predictions.show(10)), but not seeing anything useful.This code is giving me an accuracy score (55%), but how do I turn it into an application that can label user reviews as positive or negative (1 or 0)?"
3477,User Warning: Your stop_words may be inconsistent with your preprocessing,"['vectorization', 'text-processing', 'tf-idf', 'stop-words', 'stemming']","I am following this document clustering tutorial. As an input I give a txt file which can be downloaded here. It's a combined file of 3 other txt files divided with a use of \n. After creating a tf-idf matrix I received this warning:,,UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.
  'stop_words.' % sorted(inconsistent))"". I guess it has something to do with the order of lemmatization and stop words removal, but as this is my first project in txt processing, I am a bit lost and I don't know how to fix this... "
3478,calculating semantic similarity between sets of sentences,"['nlp', 'word2vec', 'tf-idf', 'cosine-similarity', 'sentence-similarity']","I have two sets of short messages, I want to compute the similarity between these two sets and identify if they are talking about the same sub-topic based on their semantic similarity. I know how to use pairwise similarity, my problem I want to compute the overall similarity among all the sentences in the two sets not for 2 sentences. Is there a way to use tf-idf or word2vec/doc2vec with cosine similarity to calculate the overall score?"
3479,UDF worker timed out during execution when using javascript udf in BigQuery for tf idf calculation,"['javascript', 'google-bigquery', 'user-defined-functions', 'tf-idf']","I have tried to implement a query in BigQuery that finds top keywords for a doc from a larger collection of documents using tf-idf scores.Before calculating the tf-idf score of the keywords, I clean the documents (e.g. removed stop words and punctuations) and then I create 1,2,3and 4-grams out of the documents and then do stemming inside the n-grams.To perform this cleaning, n-gram creation and stemming I am using javascript libraries and js udf. Here is the example query:Here is how the example output looks like:
There were only three sample handmade rows in this example. When I try the same code with a little bit larger table with 1000 rows, it again works fine, although taking quite a bit of longer time to finish (around 6 minutes for only 1000 rows). This sample table (1MB) can be found here in json format.Now when I try the query on a larger dataset (159K rows - 155MB) the query is exhausting after around 30 mins with the following message:Errors: User-defined function: UDF worker timed out during execution.;
  Unexpected abort triggered for worker worker-109498: job_timeout
  (error code: timeout)Can I improve my udf functions or the overall query structure to make sure it runs smoothly on even larger datasets (124,783,298 rows - 244GB)?N.B. I have given proper permission to the js files in the google storage so that these javascrips are accessible by anyone to run the example queries."
3480,How can I see TF-IDF values from tfidf_vectorizer?,"['python', 'nlp', 'tf-idf', 'tfidfvectorizer']",I am using PythonI have this code that analyse Text documentsI know that TF-IDF assigns a value to each word.Is there a way that let me see what are the values of inside xtrain_tfidf ?
3481,Convert parse value into proper format in python,"['python', 'vectorization', 'pca', 'tf-idf']",I have dataframe which contains this kind of data...After converting it to vector using tf-idf I have the following result:Now type of that value is 45x5 sparse matrix of type '<class 'numpy.float64'>' with 45 stored elements in Compressed Sparse Row formatMy goal is to convert this data to vector form and then pass it into a PCA to reduce the dimensions (because it contains many more columns as well).if I pass into the PCA this kind of data it gives me the error: ValueError: setting an array element with a sequence.What is the solution for this?? PCA takes this kind of data (here is the Example)How can I do that?Help will be appreciated.
3482,NLP Combining multiple TF-IDF matrices,"['python', 'scikit-learn', 'gensim', 'tf-idf', 'tfidfvectorizer']","I have a large corpus (~100 million documents, 59GB) in a CSV. I want to create a TF-IDF vector and do some feature engineering on the data, but it's too large to load into memory all at once (I'm working on Google Colab, GPU with 12GB RAM). I imagine there is a way to process the data in chunks and then combine the TF-IDFs at the end but I'm not sure how to proceed. Here's my code so far:Then, after removing stopwords and punctuation, lemmatizing (WordNetLemmatizer()), and stemming (SnowballStemmer('english')):I can read in a few chunks at a time, but to avoid memory errors I'll probably have to save the results to disk and delete the objects from memory before processing the next set of chunks.At that point, what's the process to combine the multiple TF-IDFs?"
3483,I am using cosine similarity in a tfidf normalized column. But I am getting a memory error,"['scikit-learn', 'tf-idf', 'cosine-similarity']",Here is my dataset:Here is my code. This cosine_similarity is not working here. It gives a MemoryError. How to solve this?
3484,ValueError: setting an array element with a sequence while fitting data to svm,"['python', 'pandas', 'machine-learning', 'nlp', 'tf-idf']","I have dataset as follows:where label is target variable, text column is having string and bold and size  are having int and label is having float.Now I have convert text column to array using tf-idf vectorizer.Now for training and testing I'm using 3  and 1 column respectively:Now when I try to fit data to svm model:It's showing me error:I tried to convert my X and y to dtype=float but it's not working.I'm new to nlp and all please help me out."
3485,How to fix 'int' object is not iterable in TF-IDF freqDict_list,"['python', 'python-3.x', 'tf-idf']","I'm currently coding a TF-IDF program in python. I followed a code from this, however it's not working.The problem is 'int' object is not iterable.I haven't tried anything yet because I just followed the code in the link.I expect the output to be like this: "
3486,"Python, Keras - Binary text classifier prediction results in array of values instead of single probability","['python', 'tensorflow', 'keras', 'nlp', 'tf-idf']","I am building a very simple DNN binary model which I define as:with training like:The idea is to feed tfidf vectorized text after training and predict whenever it belongs to class 1 or 0. Sadly when I run predict against it, I get an array of predictions instead of expected 1 probability for the article belonging to class 1. The array values seem very uniform. I assume this comes from some mistake in the model. I try popping prediction like so:The training data is vectorized text itself. What might be off?"
3487,term frequency calculation using python,"['python', 'machine-learning', 'nlp', 'tf-idf']","Finding term frequency for documents in a list using pythonl=['cat sat besides dog']
I have tried finding the term frequency for each word in the corpus.
term freq=(no of times word occurred in document/total number of words in a document).
I tried doing it for one document, but I'm getting an error when there's more than one document in the list. I want to pass this list and want find tf for words in each document. But I'm getting the wrong tf values.
l=['cat sat besides dog','the dog sat on bed']"
3488,Can tf-idf values be added to find document similarity?,"['scikit-learn', 'nlp', 'tf-idf']","I am working with tf-idf and text classification to rank words in documents. I was wondering if adding tf-idf values for respective words can be used to predict the nearest match for a new document. What I mean by this is:The categories in this case are very long documents consisting of all of the documents in a certain category combined, this reduces the corpus size from thousands to just 10 in my case. It's also worth noting that I am using sublinear TF in order to reduce the effects of very frequent terms.If I had a new document that had the words ""x y"" in it, what I was thinking of is adding up the tf-idf values for those words in each category and whichever category has the greatest sum would be the nearest match to the new document. In this case, the sum for category 1 would be 1.1, 1.0 for category 2, and 0.3 for category 3, therefore the nearest match to the new document is category 1. I was also wondering if this ""algorithm"" already exists and has a name.I tried this on some test data and it predicts it accurately 86% of the time. And it seems to make more sense than using a LogisticRegression. So, is this a valid algorithm?"
3489,how to view tf-idf score against each word,"['machine-learning', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I was trying to know the tf-idf scores of each word in my document. However, it only returns values in the matrix but I see a specific type of representation of tf-idf scores against each word.I have used processed and the code works however I want to change the way it is presented:code:I get the results like this (39028,01),(1393,1672). However, I expect the results to be like"
3490,"Sentiment analysis Pipeline, problem getting the correct feature names when feature selection is used","['python', 'scikit-learn', 'pipeline', 'tf-idf', 'feature-selection']","In the following example I use a twitter dataset to perform sentiment analysis. I use sklearn pipeline to perform a sequence of transformations, add features and add a classifer. The final step is to visualise the words that have the higher predictive power. It works fine when I don't use feature selection. However, when I do use it the results that I get make no sense. I suspect that when feature selection is applied the order of the text features changes. Is there a way to work around that?The code below has been updated to include the correct answerBefore feature selection
After feature selection
To use feature selection I change the following lines of code fromto "
3491,How to normalize TF*IDF or counts in scikit-learn?,"['scikit-learn', 'nlp', 'tf-idf', 'countvectorizer']",I want to check the cosine similarity of two documents having varying length (say one is a one or two liner while other is of 100-200 lines).I need a way to normalize tfidf or count vectorizer in scikit-learn for this.
3492,"TF IDF weighed frequency scores in Test data, model trained using SVC","['python', 'machine-learning', 'svm', 'tf-idf']","I am training a predictive model on text descriptions and a label corresponding to them. I am Using SVC to train the corpus of data with the tf idf weighted word frequencies. I want to understand if there's a new set of data(test, not using the train test split here) which i want to classify, Should it be vectorized using tf idf. If yes, should it be done on the Test data separately or along with Training data?"
3493,How do I apply Tfidf_vectorizer to the whole pandas column?,"['python', 'pandas', 'machine-learning', 'nlp', 'tf-idf']","First of all I'm kinda new to NLP, so I might have understood a concept in a wrong way or somethingI am trying to find a way to vectorize the whole column as 1 text, and then after getting the results, I would like to fit the model I'm using to my target set.I am currently using a pipeline to vectorize my dataframe columns, but I believe that they are being vectorized 1 by 1, instead of joining all the columns together and then doing it.Here's an exaggerated example of my dataset:Basically, I would like to give the terms ""breast"" and ""lung"" a high tfidf score because it is unique, and I don't want my model to mistake these 2 types of data because they seem similarMy Current Code:"
3494,AttributeError: 'list' object has no attribute 'lower' in TF-IDF,"['python', 'pandas', 'tf-idf', 'countvectorizer']","I'm trying to apply a TF-IDF in a Pandas columndataI know to use the CountVectorizer, I need to turn the column into list (and that's what I tried to do).To apply TFIDF, I could not apply a list (and I tried to convert it to string).But I still have this error"
3495,"Python operator : and {} in dict and set comprehension, what do they do? [duplicate]","['python', 'python-3.x', 'tf-idf', 'dictionary-comprehension', 'set-comprehension']","I'm doing some coursework and in this excercise, I'm trying to get the inverse document frequency (idf) from a corpus. Specifically, a dictionary with each entry being {word:idf}idf is defined as: number of documents in corpus / the number of documents containing the wordI'm running into a problem with the last line of my code below. The first return statement gives a set instead of a dictionary. The second return statement, which is the correct answer, returns a dictionary.What is causing this difference? What does the colon and the curly bracket operator do here? (In an assignment statement, I thought the colon operator is supposed to just be a simple way of annotating things only?)"
3496,AttributeError: 'list' object has no attribute 'lower' from Tfidf_vect.fit,"['python', 'vector', 'nlp', 'svm', 'tf-idf']","I'm trying to apply SVM using tf-idf features .
but I got this error:This is my code:I'm using python 3.6.0, my dataset is in Arabic.Thank you,,"
3497,How to get top n terms with highest tf-idf score - Big sparse matrix,"['python', 'python-3.x', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","There is this code:coming from this answer.My question is how can I efficiently do this in the case where my sparse matrix is too big to convert at once to a dense matrix (with response.toarray())?Apparently, the general answer is by splitting the sparse matrix in chunks, doing the conversion of each chunk in a for loop and then combining the results across all chunks.But I would like to see specifically the code which does this in total."
3498,How to combine TF-IDF scores to be the equivalent of concatenating two strings,"['python', 'machine-learning', 'nlp', 'tf-idf']","I have a corpus of 5000 book titles and I am trying to perform some clustering on these. I am using the sklearn TfidfVectorizer library to generate the TF-IDF matrix for each title.However, I now combine two of the titles (so ""Book A"" and ""Book B"" becomes ""Book A Book B"") and I am wondering if there is a way of getting the TF-IDF matrix for ""Book A Book B"" by combining the matrix for ""Book A"" and the matrix for ""Book B"".I have tried recalculating the TF-IDF score again but this can take a lot of time and I would prefer if there was a quicker way of doing it since I actually need to do this several thousand times for different combinations of the titles.The code below shows what I am doing right now.I would be great if there was something like:and this gave the matrix for ""Book A Book B"" but I'm not sure if this is possible.Thanks for any help or suggestions."
3499,What dimension reduction techniques can i try on my data (0-1 features+tfidf scores as features) before feeding it into svm,"['python', 'machine-learning', 'svm', 'tf-idf', 'feature-selection']","I have about 8000 features measuring a two level response variable i.e. output can belong to class 1 or 0.
The 8000 features consist of about 3000 features with 0-1 values and about 5000 features (which are basically words from text data and their tfidf scores.I am building a linear svm model on this to predict my output variable and am getting decent results/ accuracy, recall and precision around 60-70%I am looking for help with the following:  Standardization: do the 0-1 values need to be standardized? Do tfidf scores need to be standardized even if I use sublinear tdf=true ? Dimension reduction: I have tried f_classif using SelectPercentile function of sklearn so far. Any other dimension reduction techniques that can be suggested? I have gone through the sklearn dimension reduction url which also talks about chi2 dim reduction but that isn't giving me good results. Can pca be applied if the data is a mix of 0-1 columns and tfidf score columns?Remove collinearity: How can I remove highly correlated independent variables.I am fairly new to python and machine learning, so any help would be appreciated."
3500,Why is the value of TF-IDF different from IDF_?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",Why is the value of the vectorized corpus different from the value obtained through the idf_ attribute? Should not the idf_ attribute just return the inverse document frequency (IDF) in the same way it appears in the corpus vectorized? Corpus vectorized:Vocabulary and idf_ values:Output:Vocabulary index:Output:Why is the IDF value of the word this is 0.44 in the corpus and 1.0 when obtained by idf_?
3501,Tfidf with a custom list,"['tf-idf', 'tfidfvectorizer']",I have a list of raw strings that look like this;and I want to perform TfIdf with these and a list of items I have in a list (not itself).This list I am vectorising as such;This gives me the tfidf of the things in mylist. What I would like to be able to go is to check the number of times that the ngrams from mylist appear in each item of listtocheck and then perform TfIdf based on the total number times that ngram appears in all of the strings in listtocheck
3502,“setting an array element with a sequence” exception when calling fit(),"['python', 'scikit-learn', 'tf-idf']","I am trying to perform a binary classification in which the input (features) are a sentence and some integer values. I convert the sentence to a tfidf vector before passing it into the classifier.When I call the 'fit' method, I encounter a ""ValueError: setting an array element with a sequence"" exceptionI created a sample program to demonstrate the error:I'm new to this so any help would be appreciated. Thanks!"
3503,Clustering Strings (text) with Kmeans/EM using Levenshtein Distance,"['python', 'k-means', 'tf-idf', 'levenshtein-distance', 'expectation-maximization']","I am trying to cluster strings using Kmeans/EM. I have a list of strings (about 70 strings) and I want to cluster them using Levenshtein similarity metric.So basically, I am trying to implement the clustering part in this research paper: https://ieeexplore.ieee.org/document/7765062/
After doing preprocessing. I was able to formulate the similarity matrix using Levenshtein distance and then I clustered the strings using Hierarchical Clustering as well as using Spectral Clustering but I am unable to do it using Kmeans or EM. This is because in the prior to algorithms that I was able to implement, only Similarity/Distance Matrix is sufficient for clustering. But in case of K-means and EM, I need to somehow represent the text in a Mathematically operable form as we have to find their mean(in case of K-means).I was able to find a few techniques in order to convert the text into a vector like:
1) Bag of Words
2) TF-IDF
3) doc2vec or word2vecShould I convert each string into a vector using any of the above methods and then apply Kmeans? Also is it necessary to convert the strings to vector in order to apply K-means or EM? and Lastly I have to implement everything in Python so, using Kmeans from Sklearn doesn't allow me to give a metric of my choice or a similarity matrix. What should I do?Note: I had found an implementation of K-means on the text where they had converted the text using TF-IDF. And then applied Kmeans (euclidian) but I want to use Levenshtein.Also Note: I have a list of strings and not text documents, each string is around 20-30 words"
3504,How to apply tf-idf to whole dataset (training and testing dataset) instead of only training dataset within naive bayes classifier class?,"['python', 'machine-learning', 'tf-idf', 'training-data', 'naivebayes']","I have naive bayes classifier class that classifies mails as either spam or ham with tf-idf implemented in it already. However, the tf-idf section only calculates tf-idf for the training dataset.This is the Classifier class
    class SpamClassifier(object):
        def init(self, traindata):
            self.mails, self.labels = traindata['Review'], traindata['Polarity']This is how i call the classifierHow would i apply tf-idf calculation within the classifier to the whole dataset(training and testing dataset)?"
3505,get ngrams with positional information,"['python', 'nlp', 'similarity', 'tf-idf', 'n-gram']","I'm trying to group similar short descriptions together and currently using ngrams to extract text features. Here's the ngrams function that I'm using:However, I'm experiencing some undesired results after clustering. Suppose I have the following two texts:By using ngrams(n=3), my clustering model grouped these together, which is not what I want. So I think I need to pass a new function into tfidf vectorizer instead of ngrams. I think I need to anchor the first char and create substrings as my features for tfidf, so for the first text it will be something like this:Has anyone else experienced similar problems or is there a better way to approach this? Thanks!"
3506,how to convert IPs to vector values,"['python', 'pandas', 'numpy', 'scikit-learn', 'tf-idf']",i have a datasetin machine learning we use Tf-Idf for making a vectors from Text databut i am not able to pass this value in Tf-idf here is the datahere is what i want to do i set all these values(vector Type values) manually ( so that you will understand what i want)first time asking the error so please ignore the errors (maybe there are some format error and all)help will be appreciatedThanks 
3507,What are document and corpus in tf-idf?,"['machine-learning', 'nlp', 'vectorization', 'tf-idf', 'tfidfvectorizer']","tf-idf = term frequency * inverse document frequencyterm frequency is defined as the count of a term in a document.inverse document frequency is defined as the total number of documents divided by the number of documents containing the word.The formula above may vary, but that is the big picture.
Now, supposing I have a data set containing a list of 1 million sentences:1) Is a document an entry in the data set?2) Is the entire data set the corpus?The question somehow relates to [1], but the answers did not help me understand the concept for a real data set.Thank you.[1] What does ""document"" mean in a NLP context?"
3508,NLP - How to add more features?,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'tf-idf']","I want to use a sklearn classifier to train a model to classify data entries (yes,no) using a text feature (content), a numerical feature (population) and a categorical feature (location). The model below is using only the text data to classify each entry. The text is converted with TF-IDF into a sparse matrix before being imported into the classifier.Is there a way to add/use also the other features? These features are not in sparse matrix format so not sure how to combine them with the text sparse matrix. The model is expected to return the following: accuracy, precision, recall ,f1-score"
3509,decision tree algorithm on feature set,"['python', 'random-forest', 'decision-tree', 'tf-idf', 'naivebayes']",I'm trying to predict the no.of updates('sys_mod_count')based on the text description('eng')I have predefined the 'sys_mod_count' into two classes if >=17 as 1; <17 as 0.But I want to remove this condition as this value is not available at decision time in real world.I'm thinking to do this in Decision tree/ Random forest method to train the classifier on feature set.
3510,How can I remove words with less than 3 characters?,"['python', 'machine-learning', 'tf-idf']","I am using tf-idf on text data, but am unable to remove words which are less than 3 characters. I am using stop-words to ignore a few words, but how to specify the length to restrict words less than 3 characters?My result is having features with less than 3 characters.I want to remove words like ""sid"" and include the next feature in my result, so output could be to include ""helping"" feature which is next relevant featurebasically, I want to remove features which are less than 3 characters in my features_subject."
3511,"ValueError: Iterable over raw text documents expected, string object received","['python-3.x', 'text', 'tf-idf', 'countvectorizer']","Trying to identify the weight value of a using TFIDF and count vectorizerBelow code works fine when I execute that individually for each row. It throws an error adding a loop or using a function.when I executed the above code it throws me below errorError: ""ValueError: Iterable over raw text documents expected, string object received.""have seen the same error in below link clicl here to view the example"
3512,"How to get better relevance without compromising on performance, scalability and avoid the sharding effect of Elasticsearch","['elasticsearch', 'lucene', 'sharding', 'tf-idf', 'relevance']","Let's suppose I have a big index, consists 500 million docs and by default, ES creates 5 primary shards for below reasons and I also go with the same setting.Performance:- There will be less time to search in a shard with less no of documents(100 million in my use case) than in just 1 shard with a huge number of documents(500 million). Also, allows to distribute and parallelize operations across shards.Horizontal scalability(HS) :- horizontally split/scale your content volume.But when we search by default it just goes to 1 shard and gives the result. in this case, relevance isn't accurate(as idf be majorly impacted) and also it might even not give any result if my matched document is on another shard. and its called as The Sharding Effect.Above issue is explained in details here and there are below 2 options to avoid this issue but I think both the solutions have some cons :-1. Document routing: I this case all the documents will be on the same shards which lose the whole purpose of sharding.
2. dfs_query_then_fetch search type: there is performance cost associated with it.I am interested to know below: "
3513,IDF recaculation for existing documents in index?,"['elasticsearch', 'tf-idf']","I have gone through  [Theory behind relevance scoring][1]  and have got two related questions Q1 :- As IDF formula is idf(t) = 1 + log ( numDocs / (docFreq + 1)) where numDocs is total number of documents in index. Does it mean each time new document is added in index, we need to re-calculate the IDF for each word for all existing documents in index ?Q2 :-   Link mentioned below statement. My question is there any reason why TF/IDF score is calculated against each field instead of complete document ?When we refer to documents in the preceding formulae, we are actually
  talking about a field within a document. Each field has its own
  inverted index and thus, for TF/IDF purposes, the value of the field
  is the value of the document."
3514,Show Total Retrieved Documents,"['python', 'django', 'python-3.x', 'tf-idf', 'inverted-index']","I am using TF-IDF algorithm to retrieve the relevant documents with the query that i input. I have successfully retrieve the relevant documents, and show it too. But i want to show the TOTAL documents that has been retrieved.I am using this code (in result.html)to count the documents, but it show anything.Here is the main.py :The result is like this picture below, that show NULL (after Result :)Like this picture above, it just show the documents, but not the total document.The output that I expected should be like this:I hope some one can help me fix this problem.
Thank you."
3515,How elastic is assigning weight to terms in searched query?,"['elasticsearch', 'tf-idf']","I have gone through  Theory behind relevance scoring  and have got one question but could not find answer on googleMy question is on below section in the linkImagine that we have a query for “happy hippopotamus.” A common word
like happy will have a low weight, while an uncommon term like
hippopotamus will have a high weight. Let’s assume that happy has a
weight of 2 and hippopotamus has a weight of 5.How come elastic has determined the weight for each word in query ? Does elastic has predefined weight of each word in it dictionaryUpdate:-Look at Figure 28 in the link, I see the four lines . I understand the 3 blue lines corresponding to each document. These blue lines represent
the weight of each word/terms(words in query) against each document. My question is on green line. How it is drawn(weight has been calculated here ) ?"
3516,How to map TFIDF values to original words,"['scala', 'apache-spark', 'apache-spark-sql', 'apache-spark-mllib', 'tf-idf']","I've followed this example for computing TFIDF of each word in my documents. However, my final output looks something like this (which is obviously okay since I am using HashingTF):Does there exist any API which matches word to its TFIDF value, please?"
3517,Memory Error when training large dataset on tfidf,"['python', 'tf-idf']","I am trying to train a large dataset containing 700000 rows containing 210+ million words. i have 8 gb memory . when i try to train it through tfidf it is giving me memory error. can someone guide me what exactly i am doing wrong
here is my sample training codeself.vectorizer = TfidfVectorizer(min_df=0.001, max_df=0.2, norm='l2', analyzer='word')"
3518,How does Lucene scoring work in regard to queries?,"['lucene', 'lucene.net', 'tf-idf']","I understand that the default scoring function uses TF*IDF or a similar variant thereof. However, it is unclear to me how this works on queries, e.g. BooleanQuery.Say I create a BooleanQuery with many TermQuerys with Occur.SHOULD. Is the TF of the query as a whole considered? E.g. if my BooleanQuery contains two TermQuerys with the term ""hello"", the TF for ""hello"" would be high in the query itself. Is that considered? Or is only the different TF in regard to individual documents considered?Looking at the source code for MoreLikeThis(...), it seems to consider the TF in the query itself. Are there similar classes in Lucene that does this, or do all queries do this?"
3519,When to use which base of log for tf-idf?,"['c', 'tf-idf']","I'm working on a simple search engine where I use the TF-IDF formula to score how important a search word is. I see people using different bases for the formula, but I see no explanation for when to use which. Does it matter at all, and do you have any recommendations? My current implementation uses the regular log() function of the math.h library"
3520,Calculationd for TfidfVectorizer(),"['python', 'tf-idf', 'tfidfvectorizer']","I am using TfidfVectorizer() to convert the text to a numeric vector which I can use as independent variable to train the model.I have tried to convert the text string to vector using TfidfVectorizer()Input strings are -
Description- vectoriser Output generated is - Please explain how 0.26868527618515564 value is generated for Harry word in first sentence."
3521,Pandas data frame throw > exception: no description,"['python', 'pandas', 'dataframe', 'scikit-learn', 'tf-idf']","I want print document by term matrice. Working without problems in
  small documents. For example, 10000 documents But 25000 documents
  throw error"
3522,How to solve ‘cannot index a corpus with zero features ’error,['tf-idf'],"There was a ""cannot index a corpus with zero features (you must specify either num_features or a non-empty corpus in the constructor)"" errorI think it should have gone wrong here, but I don't understand why it went wrong.The output is:"
3523,RegEx in vocabulary not working in sklearn TfidfVectorizer,"['python', 'regex', 'nlp', 'tf-idf', 'tfidfvectorizer']","I'm trying to calculate tf-idf of selected words in a corpus, but it didn't work when I use regex on selected words. Below is the example I copied from another questions in stackoverflow and made small changes to reflect my question. The code is pasted below. The code works if I write ""chocolate"" and ""chocolates"" separately but doesn't work if I write 'chocolate|chocolates'. Can someone help me understand why and suggest possible solutions to this problem? I expect the results to be:But, now it returns:"
3524,How does TfidfVectorizer compute scores on test data,"['scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","In scikit-learn TfidfVectorizer allows us to fit over training data, and later use the same vectorizer to transform over our test data.
The output of the transformation over the train data is a matrix that represents a tf-idf score for each word for a given document.However, how does the fitted vectorizer compute the score for new inputs? I have guessed that either:I have tried deducing the operation from scikit-learn's source code but could not quite figure it out. Is it one of the options I've previously mentioned or something else entirely?
Please assist."
3525,Is there a way of removing all the words in the text that are not in other text?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I have a document with many reviews. I am creating a bag-of-words BW using TfidfVectorizer. What I want to do is: I only want to use words in BW that are also in other document D.The document D is a document with positive words. I am using this positive to improve my model. What I mean is: I only want to count the words that are positive.Is there a way of doing this?Thank youI created a piece of code to do that job, as fallows:
train_x is a panda data frame with Reviews.Although I was able to accomplish my needs, I know that the code is not the best. Also, I was trying to find a standard function in python to do thatPS: Python has so many features that I always ask what it can do without using the amount of code that I used"
3526,Using sklearn how do I calculate the tf-idf cosine similarity between documents and a query?,"['python', 'scikit-learn', 'tf-idf', 'cosine-similarity']",My goal is to input 3 queries and find out which query is most similar to a set of 5 documents.So far I have calculated the tf-idf of the documents doing the following:The problem I am having is now that I have tf-idf of the documents what operations do I perform on the query so I can find the cosine similarity to the documents?
3527,how can i make my document similarity from single matrix checking faster?,"['python', 'pandas', 'machine-learning', 'tf-idf', 'cosine-similarity']","I am trying to find document similarity from a large set of articles(460 files containing 4000 rows each). But doing cosine similarity is taking a lot of times to compute.I can't use python libraries of sklearn or scipy. So I have tried to implement raw tf-idf vectorizer and cosine similarity. The vectorizer gives me a list of lists. The matrix looks like this: My code:Now, the expected results are okay but it is taking like eternity to calculate. any suggestion how to make it faster? "
3528,In NLP using tf-idf how to find the frequency of specific word from the corpus(contaning large numbers of documentation) in python,"['python', 'nlp', 'tf-idf', 'n-gram', 'countvectorizer']","How to find the frequency of an individual word from the corpus using Tf-idf. Below is my sample code, now I want to print the frequency of a word. How can I achieve this?"
3529,ExactStatsCache not working for distributed IDF,"['solr', 'similarity', 'solrcloud', 'tf-idf']","I am using ExactStatsCache in SolrCloud (7.7.1) by adding following to 
solrconfig.xml file for all collections. I restarted and indexed the 
documents of all collections after this change just to be sure. However, when I do multi-collection query for these collections, the scores do not change before 
and after adding ExactStatsCache. By scores, I mean the score as field value in the response and not in the debug output. I am aware that there is one open issue for debug output not being correct for distributed query so I am not relying on that. Do you know what I might be doing wrong? "
3530,How to find TF-IDF of each term of a web page?,"['python', 'information-retrieval', 'tf-idf']","I am working on a college project. I am implementing CANTINA based phishing detection approach. In this paper, author has calculated TF-IDF for each word in the document(Web page). How to find Idf? Basically no of documents ,term is appearing in, as no of documents over the internet is very large."
3531,How to calculate tfidf score from a column of dataframe and extract words with a minimum score threshold,"['pandas', 'tf-idf']",I have taken a column of dataset which has description in text form for each row. I am trying to find words with tf-idf greater than some value n. but the code gives a matrix of scores how do I sort and filter the scores and see the corresponding word.
3532,How to find words relevances in a single document?,"['python', 'nltk', 'word', 'tf-idf', 'tfidfvectorizer']","I want to find the relevance of some words (like economy, technology) in a single document.The document has around 30 pages, the idea is to extract all text and determine words relevances for this document.I know that TF-IDF is used in a group of document, but is it possible to use TF-IDF to solve this problem? If not, how can I do this in Python?"
3533,How to get Vocabulary with weights for tf-idf word bags in ml.net?,"['c#', 'tf-idf', 'ml.net']","The documentation of ML.NET shows how to use context.Transforms.Text.ProduceWordBags to get word bags. The method takes Transforms.Text.NgramExtractingEstimator.WeightingCriteria as one of the parameters, so it's possible to request TfIdf weights to be used. The simplest example would be:That's all fine, but how do I get the actual results out of transformed_data?I did some digging in a debugger, but I'm still quite confused on what's actually happening here.First of all, running the pipeline adds three extra columns to transformed_data:After getting a preview of the data I can see what's in these columns. To make things clearer here's what GetTopicsData returns, which is what we're running our transform on:That's exactly what I'm seeing in the very first bags column, typed as Vector<string>:Moving on to the second bags column, typed as Vector<Key<UInt32, 0-12>> (no idea what 0-12 is here btw.).This one has KeyValues annotation on it and it looks like for each row it maps the words into indexes in global Vocabulary array.The Vocabulary array is part of Annotations:So that's promissing. You'd think the last bags column, typed as Vector<Single, 13> would have the weights for each of the words! Unfortunately, that's not what I'm seeing. First of all, the same Vocabulary array is present in Annotations:And the values in rows are 1/0, which is not what TfIdf should return:So to me that looks more like ""Is word i from the Vocabulary present in current row"" and not the TfIdf frequency of it, which is what I'm trying to get."
3534,Different results for same test data with trained model,"['python', 'machine-learning', 'tf-idf', 'ensemble-learning', 'tfidfvectorizer']","We have loaded trained model using joblib in python and test set of different sizes were given as input for prediction. For eg. we named test set as S1,S2 where S1 has 100 instances and S2 has 1000 instances. The instance 'X' is part of both S1 and S2 which is predicted differently when tested with trained model. We have applied TF-IDF algorithm on dataset to obtain feature vector. The TF-IDF vectorizer vocabulary of trained model is saved as pickle file which is further used to transform the testing data. It would be of great help if anyone could suggest or give solution to the problem.Actual:
X belongs to class C1Predicted result:
X belongs to C1 w.r.t S1; 
 X belongs to C2 w.r.t S2"
3535,Tfidfvectorizer - How can I check out processed tokens?,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","How can I check the strings tokenized inside TfidfVertorizer()?  If I don't pass anything in the arguments, TfidfVertorizer() will tokenize the string with some pre-defined methods. I want to observe how it tokenizes strings so that I can more easily tune my model.I want something like this:How can I do this? "
3536,Extract top words for each cluster,"['python-3.x', 'machine-learning', 'k-means', 'tf-idf']",I have done K-means clustering for text datawhere features is the tf-idf vectorThen I added the cluster label to original datasetAnd indexed the dataframe by clustersHow do I fetch the top words for each cluster?
3537,Extracting important sub-sections and the sub set of documents associated with them from a set of documents,"['cluster-analysis', 'document', 'tf-idf']","I have a set of documents all of which come under the category ""crime"". 
Now, I want to categorize them into a number of (could be overlapping) clusters of documents where each of the clusters are formed under a sub-category like murder or kidnapping, etc.
   I want to accomplish this using some way of identifying the importance of individual words occurring in each document. I have already tried using TF-IDF but it is not giving me satisfactory results."
3538,What i have to do after i'm working with tf idf and chi square,"['text', 'tf-idf', 'chi-squared', 'mining']","In text mining, i wanna ask what i have to do after I did a tf idf and chi square for terms.Tf idf is like a different file with chi square. Right? Is it right if i did a tf idf matrix times chi square weight for every terms? "
3539,Cosine similarity of list of values with each other,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I am trying to find the cosine similarity of a list of strings. I used sklearn tfidf vector to convert the text into a numerical vector first and then used the pairwise cosine_similarity api to find the score for each string pair.The strings seem similar, but I am getting a weird answer. The first and third value in the string array are similar except the word TRENTON, but the cosine similarity is 0. Similarly, the 1st,3rd and 4th string are the same, except for a space between GREEN and CHILLI and the cosine similarity is zero. Isn't that strange?My code:Output"
3540,Is the tf-idf of scikit-learn in this example correct? The most frequent words have high score,"['tf-idf', 'tfidfvectorizer']","The word ""the"" should have a low score in the three documents"
3541,what does column represent in tfidf matrix?,"['python', 'tf-idf', 'tfidfvectorizer']","I am trying to understand the result of TF-IDF matrix. Here is the code I am using.So, I am trying to understand the tf matrix thus obtained. Following is the attached picture of the tf matrix.Could someone please help me why do I have 7 columns for a corpus of 4 words? Do the rows represent the number of words.From what I study in different resources is ""The result is a matrix of tf-idf scores with one row per document and as many columns as there are different words in the dataset."" But I am not being able to validate it from the result I obtain here."
3542,Are TF-IDF scores for a single term combined?,"['nlp', 'tf-idf', 'stop-words']","I am reading up about TF-IDF so that I can filter out common words from my corpus. It appears to me that you get a TF-IDF score for each word, document pair. Which score do you pay attention to? Do you combine the scores across all documents for a word? "
3543,Tf-idf for SO posts (where tag can only occur once ),"['python', 'nlp', 'tf-idf']","Using the stackoverflow data dump, I am analyzing SO posts that are tagged with pytorch or keras. Specifically, I count how many times each co tag occurs (ie the tags that aren't pytorch in a pytorch tagged post). I'd like to filter out the tags that are so common they've lost real meaning for my analysis (like the python tag).I am looking into Tf-idfTF reprensents the frequency of word for each document. However, each co-tag can only occur once for a given post (ie you can't tag your post 'html' five times). So the tf for most words would be 1/5, and others less (because post only has 4 tags for instance). Is it still possible to do Tf-Idf given this context?"
3544,How to compute tf-idf for a dataframe of multiple responses?,"['r', 'string', 'dataframe', 'dplyr', 'tf-idf']","I have a dataframe called Q1Dummy consisting of 2 columns: resp_id (respondent ID) and Q1 (responses they made in a string format).It looks like this:Now, for text mining purposes I would like to unnest the responses in ngrams (of 3), as I did below: The next step is calculating tf-idf. There are several functions for this, but the problem is that when creating the tokensQ1Dummy dataframe above the respondent IDs are lost. So my question is how to compute the tf-idf for the created tokens from this point on.Thanks!"
3545,Python Pipeline shows only one step,"['python', 'scikit-learn', 'svm', 'pipeline', 'tf-idf']","I try to create SVM classifier for short texts with TfIdf as first step. When I create Pipeline, fir it and get accuracy scores - everything looks right.But when I load created model and print it I get only one step - TfIdf instead of two - TfIdf and SVM.I assume that I don't understand how the Pipeline works exactly but in every example that I saw there were as much steps as it was loaded in the Pipeline at first.Thank you for the help!"
3546,How can I return accuracy rates for Top N predictions using sklearn's SGDClassifier?,"['python', 'scikit-learn', 'tf-idf']","I am trying to modify the results in this post (How to get Top 3 or Top N predictions using sklearn's SGDClassifier) to get the accuracy rate returned, however I am get an accuracy rate of zero and I can't figure out why. Any thoughts? Any thoughts/edits would be much appreciated! Thank you. "
3547,How can TF-IDF be used for programming source code plagiarism detection?,"['algorithm', 'bigdata', 'data-science', 'tf-idf', 'plagiarism-detection']","i briefly understand how TF-IDF works, for detecting plagiarism in articles, it does make sense.Now i was told to use it against programming source code, how can this work ? In article most words are natural language words say English, you can count these words. Now in source code, each person can define all kinds strange variable names, so this counting of the words doesn't make much sense to me.Even if i just want to count function name, my own function name could be strange as well, while system/library function names are useful for TF.Anyone can help to explain more ? Thanks !"
3548,sklearn pipeline: running TfidfVectorizer on full training set before applying TimeSeriesSplit inside GridSearchCV?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer', 'gridsearchcv']","I'm sure this is possible but I haven't been able to figure it out.  Give a training dataset using TimeSeriesSplit with a num_split=5, the splits look like this:Problem is for the first couple passes, the TfidfVectorizer is working with a nominal amount of vocab/features, and I would like to run that on the entire training set before splitting so that the feature size stays the same for all splits. Barring that however, does anybody know of a way to, while using TimeSeriesSplit, only pass the last two splits in the series?  So instead of all 5 splits, GridSearchCV just uses these two:This would allow a much better vectorization fit even though it won't be identical between passes-- at least it has a larger portion to work with before validation.Thanks.EDIT:The pipeline I'm using is essentially TfidfVectorizer, and then on to a classifier.  But doing some inspection on the data and features it looks like the data set is being split up before being fed to the TfidVectorizer().  Here's the broad strokes:"
3549,Taking two values from two list (Random Order) of tuples and multiplying,"['python', 'list', 'tuples', 'tf-idf']",I have two lists and they are lists of tuples. For exampleIf the items were in the same order I could use the following code to multiply the two values:But my issue is the order of one the lists outputs randomly so the code doesn't work. So essentially I need to see if the word in one list matches the word in the other and then multiply to get an output in a similar way as the list of tuples.
3550,Taking two values from two list of tuples and multiplying,"['python', 'tuples', 'tf-idf']","I am calculating the TD IDF of a list of tuples. I have calculated the TF value and it is stored in a list of tuples and I have calcualted the IDF value which is also stored in a list of tuples. For example:>>print(tf)[(('0', 'CD'), 0.0036429872495446266), (('09:00', 'CD'), 0.0018214936247723133)Then >>print(idf[(('0', 'CD'), 2.4385423487861106), (('09:00', 'CD'), 2.739572344450092)Now i want to multiple the value of 0.00364 by 2.43854 and then 0.001821 by 2.739 so that the output is in any format in a way that it goes:
word, td*idfOfc these are just 2 values of many but I'm not sure how do i iterate through both lists whilst keeping the ""calculated value"" to the word."
3551,scikit learn implementation of tfidf differs from manual implementation,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer', 'python-textprocessing']",I tried to manually calculate tfidf values using the formula but the result I got is different from the result I got when using scikit-learn implementation.I tried to manually calculate tfidf for document but result is different from TfidfVectorizer.fit_transform. What I should have got is 
3552,what is the difference between tfidf vectorizer and tfidf transformer,"['python', 'scikit-learn', 'nltk', 'tf-idf', 'tfidfvectorizer']",I know that the formula for tfidf vectorizer is I saw there's tfidf transformer in the scikit learn and I just wanted to difference between them. I could't find anything that's helpful.
3553,tfidf oucomes are different for the exact same word,"['python', 'pandas', 'gensim', 'tf-idf', 'corpus']","I'm running tfidf model in python.And it returns the output which gives some patterns of values to the exact same word.
For example, I chose the word ""AAA"".Why do they have every different value even though they are exact same."
3554,tf-idf vectorizer for multi-label classification problem,"['python', 'nlp', 'tf-idf', 'multilabel-classification', 'tfidfvectorizer']","I have a multi-label classification project for a large number of texts. 
I used the tf-Idf vectorizer on the texts (train_v['doc_text']) as follows:now, I need to use the same vectorizer on a set of features (test_v['doc_text'])to predict the labels.
however, when I use the following I get an error message any idea on how to deal with this?Thanks. "
3555,nlp multilabel classification tf vs tfidf,"['python', 'nlp', 'tf-idf', 'multilabel-classification', 'tfidfvectorizer']","I am trying to solve an NLP multilabel classification problem. I have a huge amount of documents that should be classified into 29 categories. My approach to the problem was, after cleaning up the text, stop word removal, tokenizing etc., is to do the following:To create the features matrix I looked at the frequency distribution of the terms of each document, I then created a table of these terms (where duplicate terms are removed), I then calculated the term frequency for each word in its corresponding text (tf). So, eventually I ended up with around a 1000 terms and their respected frequency in each document. I then used selectKbest to narrow them down to around 490. and after scaling them I used OneVsRestClassifier(SVC) to do the classification. I am getting an F1 score around 0.58 but it is not improving at all and I need to get 0.62. Am I handling the problem correctly? Do I need to use tfidf vectorizer instead of tf, and how? I am very new to NLP and I am not sure at all what to do next and how to improve the score. Any help in this subject is priceless. Thanks"
3556,How to make a inverted list of elements in Python,"['python', 'list', 'dictionary', 'count', 'tf-idf']","I have a master list of all terms and many child lists (All of them are stored in file and I am reading them from file), these list in files looks like how I have written them below. I want to find the inverted list i.e. no of occurrence of element in different document.From the above question I want an output like:   But when I am doing it I am getting wrong valueNote:  In the above code the directoryList is not working but in orignal one I am loading pagesYou may find all code and input outout file here."
3557,"Using featuretools for text data (word count, tfidf)","['nlp', 'feature-extraction', 'tf-idf', 'featuretools']",Featuretools is best for relational categorical and numerical data. Regarding text it seems that it only counts text length and some other very basic stats. What would be the best pipeline for preparing textual data for featuretools?Should it be done with make_trans_primitive or it would be better to prepare data some other way?
3558,Movie Ratings prediction using TF-IDF,"['scikit-learn', 'tf-idf', 'python-textprocessing']","I have a dataset having the format-Movie_Name, TomatoCritics, Target_VariableHere, TomatoCritics attribute has free text from different users for different movies. And Target_Variable is a binary value (0 or 1) telling whether this movie should be watched or not.I am using TF-IDF to process this and my code is as follows-The last line using mnb.predict() gives the error-ValueError: dimension mismatchWhat's going wrong?Thanks!"
3559,On what basis my source is vectorizing & clustering data?,"['python', 'cluster-analysis', 'feature-extraction', 'tf-idf']","I am taking an input from a text wanted to build a semantic vocabulary, however without vocabulary I am just passing a token list of words. But I am not able to figure out on what basis vectorization & clustering is happening when vocabulary is not set? In the documentation it is mentioned that ""If not given, a vocabulary is determined from the input documents."" However, I am only taking   one txt file for my input.I have tried to create vocabulary out of the wordnet synonym set but not able to reach anywhere."
3560,Storing TfIdf model and then loading it to test the new dataset,"['python', 'tf-idf', 'joblib', 'tfidfvectorizer']","I m trying to store the TfIdf vectorizer/model(Don't know whether it is a right word or not) obtained after training the dataset and then loading the stored model to fit the new dataset.
Model is stored and loaded using pickleI have stored the vocabulary of TfIdf obtained during training phase. Then, I load the stored the vocabulary to vectorizer to fit the test dataI m getting an error""sklearn.exceptions.NotFittedError: idf vector is not fitted""As far as I got to know, it is trying to save the whole 'X' separately using  idf_ and vocabulary_. But I just want to store the model/vectorizer(Don't know) so that when next time it load the model/vectorizer, I just need to call vectorizer.fit() for the test data, no need to use the training data to   call fit_transform().
Is there any way to do that?"
3561,How to use tfidf in text classification?,"['nlp', 'tf-idf']","I have a dataset which has 300000 lines, each line of which is an article title, I want to find features like tf or tfidf of this dataset.
I am able to count the words(tf) in this dataset, such as:
WORD FREQUENCE
must 10000
amazing 9999or word percentage:
must 0.2
amazing 0.19but how to caculate idf, I mean I need to find some features to discriminate this dataset from the others? or HOW DOES tfidf used in text classification?"
3562,TF-IDF how to takes only a list of words,"['python', 'tf-idf', 'stop-words', 'tfidfvectorizer']","I know that we can use a list of stopwords in tf-idf, but is there a way to take only a list of words and neglect the others ?
For example, here a I declare a list of stopwords:
vectorizer = TfidfVectorizer(stop_words=""english"")
If I want to just take cat and dog into account in the sentences, I want something like:
vectorizer = TfidfVectorizer(keep_words=[""cat"", ""dog""])
My goal is to do text clustering taking into account only specific words ?
Is there a solution ? "
3563,Scikit-learn tfidf vectorizer in minibatches?,"['scikit-learn', 'tf-idf']","I've been trying to perform tf-idf heuristic on a large corpus.Can I iteratively read the documents, and call the In each iteration? Does this take into account only the current iteration, or does it remember the previous ones?Thanks!"
3564,Get the top term per document - scikit tf-idf,"['python', 'scikit-learn', 'tf-idf']","Afte vectorizing multiple documents with scikit's tf-idf vectorizer, is there a way to get the most 'influential' term per document?I have only found ways of getting the most 'influential' terms for the entire corpus, not for each document, though."
3565,What does it mean “IDF is just dependent on the term”?,"['information-retrieval', 'tf-idf', 'data-retrieval']","it possible someone explain ""Tf is dependent on term and document"" and  ""IDF is just dependent on the term"" with an example ?"
3566,"Quora Question Pairs challenge, predict if two questions ask the same thing using binary cross entropy loss to evaluate the predicition","['machine-learning', 'nlp', 'information-retrieval', 'tf-idf', 'cosine-similarity']","I have a csv file containing pairs of questions from the Quora Question Pairs Challenge. For each pair there is a corresponding label that specifies whether the questions are the same or not. I want to create a method so that if we have unknown pairs of questions I can answer if they ask the same thing or not. The accuracy of the result should be determined with the use of binary cross entropy loss.This is a project that I have to do about a course of Information Retrieval. The problem is that all the solutions that I have found so far include Machine Learning (e.g. Neural Networks) and we haven't been taught how to use any Machine Learning models in this course. How can I solve this problem without using any Machine Learning?I thought about cleaning the data (e.g. stop word reomval and punctuation removal) calculating the tf-idf and then applying cosine similarity between the two pairs. Like this I can find how similar two questions that are already given are, without using the labels. However, how can I use the labels to my advantage and predict the similarity between two unknown pairs of questions with no Machine Learning, is there a simple way that I am missing?"
3567,Tfidfvectorizer from sklearn - how to get matrix,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",I would like to get matrix out of Tfidfvectorizer object from sklearn. Here is my code:Here is what I tried and got back errors:another attempt
3568,Plotting tf-idf matrix on a 2-dimensional space,"['python', 'matplotlib', 'scatter-plot', 'tf-idf']","Good morning,
I have used a tf idf matrix to do a k-means clustering, in order to find the recurring topics in songs text.
In the end I got 4 clusters. I used the following code for plotting:The plotting seems to be correct but I can't add a legend where each of the colors used is assigned to a specific cluster.
How can i do?"
3569,calculating tf_idf for fvt table,"['python', 'pandas', 'numpy', 'nlp', 'tf-idf']","I have a frequency value table like-and I want to calculate the tf_idf.My code-Explanation-
First I am iterating through each column where I am finding the non zero rows in that column in var m and storing the particular value of that row in column as tf and then calculating the tf_idf and replacing the values in table with tf_idf weights.expected output-for column g first row we have tf=3 idf=log(5/4) therefore tf_idf=idf*tf"
3570,“Division by zero”/ NULL values TF-IDF,"['php', 'laravel', 'machine-learning', 'tf-idf', 'php-ml']","I'm trying to test tf-idf package from PHP-ML, I tried using their documentation code but it keeps giving me ""Division by zero"" when I try to use different samples(strings).And when I try using transform method from the example provide in their docs it gives me NULL.Tfdftransformer.php :}"
3571,How to get keys from pyspark SparseVector,"['pyspark', 'tf-idf']","I conducted a tf-idf transform and now I want to get the keys and values from the result.I am using the following udf code to get values:So if the sparsevector looks like:
features=SparseVector(123241, {20672: 4.4233, 37393: 0.0, 109847: 3.7096, 118474: 5.4042}))extracted_keys in my extract will look like:
[4.4233, 0.0, 3.7096, 5.4042]My question is, how can I get the keys in the SparseVector dictionary? Such as keys = [20672, 37393, 109847, 118474] ? I am trying the following code but it won't workThe result it gave me is: [null,null,null,null]Can someone help?
Many thanks in advance!"
3572,AttributeError: 'int' object has no attribute 'lower' in TFIDF and CountVectorizer,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I tried to predict different classes of the entry messages and I worked on the Persian language. I used Tfidf and Naive-Bayes to classify my input data. Here is my code:But when I run the above code it throws the following exception while I expect to give me ""ads"" class in the output:Traceback (most recent call last):   File "".../multiclass-main.py"",
  line 27, in 
      X_train_counts=cv.fit_transform(X_train)   File ""...\sklearn\feature_extraction\text.py"", line 1012, in fit_transform
      self.fixed_vocabulary_)   File ""...sklearn\feature_extraction\text.py"", line 922, in _count_vocab
      for feature in analyze(doc):   File ""...sklearn\feature_extraction\text.py"", line 308, in 
      tokenize(preprocess(self.decode(doc))), stop_words)   File ""...sklearn\feature_extraction\text.py"", line 256, in 
      return lambda x: strip_accents(x.lower())        AttributeError: 'int' object has no attribute 'lower'how can I use Tfidf and CountVectorizer in this project?"
3573,Quanteda tf-idf transform function in R,"['r', 'tf-idf', 'quanteda']","I have used quanteda package and get two huge dfm train and Valid.train and valid column are same.I known use dfm_tfidf can get tfidf weight very fast on tain, 
but my problem is how to get valid tfidf on base on tain idf.I try to use pblapply and for loop, but the run time really slow.Here is my code, how to transform tfidf base on other idf in quanteda, or
any other way to speed up."
3574,Updating TF-IDF using Gensim,"['python', 'gensim', 'similarity', 'tf-idf']",Hi I’m using Gensim to find similarity between documents to do so I make TF-IDF of documents and calculate cosine similarity. when I have new document I can calculate similarity of this document with previous documents using index[tfidf[vec]] but in this way TF-IDF doesn’t update and new words does not consider in similarity calculation is there any solution to update TF-IDF quickly without recalculating whole matrix or what is the best solution for my problem?
3575,How to count word of sentence from database with PHP,"['php', 'information-retrieval', 'tf-idf', 'cosine-similarity']",I have a table in database How to count every word in that table (or this is a TF-IDF Raw method)?Anybody help me please with PHP code?
3576,Why is this TF-IDF sentiment analysis classifier performing so well?,"['scikit-learn', 'nlp', 'logistic-regression', 'tf-idf']","Jupter NotebookThe last confusion matrix is for the test set. Is this a case of overfitting with logistic regression? Because even when not pre-processing the text much (including emoticons, punctuation) the accuracy is still very good. Good anyone give some help/advice?"
3577,A conceptual question about tf-idf using pyspark,"['pyspark', 'tf-idf']","In the official documentation of pyspark, they have an example of tf-idf.I'm also ready in other sources a code similar to that. Question is: Why the name of that dataframe is tfidf? Does the result equals to tf * idf or will it stores only the idf? If so, how to calculate the tf*idf?"
3578,"String Matching Using TF-IDF, NGrams and Cosine Similarity in Python","['python', 'tf-idf', 'n-gram', 'cosine-similarity']","I am working on my first major data science project. I am attempting to match names between a large list of data from one source, to a cleansed dictionary in another. I am using this string matching blog as a guide.I am attempting to use two different data sets. Unfortunately, I can't seem to get good results and I think I am not applying this appropriately.Code:The expected result is as follows:"
3579,Cosine Similarity between keywords,"['python', 'nlp', 'keyword', 'tf-idf', 'cosine-similarity']","I'm new to document similarity in python and I'm confused about how to go about working with some data. Basically, I want to get the cosine similarity between dicts containing keywords. I have dicts like so, which I am getting straight from a database:I query the database and I get back data in this format. These are each lists of keywords and their respective tf-idf scores/weights. All I want to do is get the cosine similarity between these two dicts, weighted by the tfidf score. Looking online, I was pretty overwhelmed by all the different python libraries/modules when it comes to document similarity. I have no idea if there is some built-in function out there that I can just pass these sorts of json objects to, if I should be writing my own function that uses the weights, or what. Any help is appreciated!Thank you!"
3580,tfidf from word counts,"['pandas', 'scikit-learn', 'tf-idf']","I have a categorical variable with large cardinality (+1000). Each of these values can occur repeatedly in each train/test instance. Although this is not really text data it seems to have similar properties and I would like to treat this as a text classification problem.My starting point is a dataframe listing the number of occurrences of each ""word"" in each ""document"", e.g.I would like to apply tfidf transformation to these ""word"" counts. How can I do that? sklearn.feature_extraction.text.TfidfVectorizer seems to expect a sequence of strings or a file as an input which it preprocesses and tokenizes. None of this is necessary in this case as I already have the ""word"" counts.So how to get the tfidf transformation of these counts?"
3581,"I've computed TF AND IDF, but how to get TF-IDF?","['python', 'dictionary', 'nlp', 'tuples', 'tf-idf']","From my code below:So I've managed to finally get this far, though I can't seem to find information on what I have to do next in terms of Python, sure the maths is there but I don't feel it necessary to spend hours trying to understand what it means. Just a quick reassurance this is what I get from tf_med:And here is what I get from idf_med:Though now I don't know how to compute these two together to get me my TF-IDF and from there my average cosine similarities. I understand they need to be multiplied but how on earth do I go about doing that!"
3582,How does one calculate the mean of tfidf-vectors by the condition if the index is in one of 3 external lists?,"['python', 'pandas', 'tf-idf']","I am trying to achieve to groupy tfidf-vectors (rows of a Pandas DataFrame) by if an index is in one of 3 lists and calculate the mean of the groupedby rows. 
Situation:I cannot use the pandas.DataFrame.groupby() function and now I am kinda lost."
3583,"Vector Space Model - query vector [0, 0.707, 0.707] calculated","['stanford-nlp', 'information-retrieval', 'tf-idf']","I'm reading the book ""Introduction to Information Retrieval ""(Christopher Manning) and I'm stuck on the chapter 6 when it introduces the query ""jealous gossip"" for which it indicated that the vector unit associated is [0, 0.707, 0.707] ( https://nlp.stanford.edu/IR-book/html/htmledition/queries-as-vectors-1.html ) considering the terms affect, jealous and gossip.
I tried to calculate it by computing the tfidf assuming that:
- Tf is equal to 1 for jealous and gossip
- Idf is always equal to 0 if we calculate it as log(N/df) with N=1(I have only 1 query and it is my document), df=1 for jealous and gossip => log(1)=0
Since the idf is 0, it turns out that the tfidf is 0.
So I decided to compute every weight of the query vector with the raw tf divided by the euclidean length. In this case the Euclidean length is sqrt(1+1)=1. 
I can't obtain the formula by which it decided that [0, 0.707, 0.707] is the query vector. 
Can someone help me? "
3584,Combining tf-idf with target/mean encoding for multi-class classification,"['python-3.x', 'tf-idf', 'categorical-data', 'feature-selection', 'multiclass-classification']","I have a dataset on all the software installed by a large group of users. I would have to classify the users into one of 4 categories based on which software they installed (each user can install up to 30 pieces of software). ""Software"" is a feature which has high cardinality (above 1000) so using naive one-hot encoding does not seem appropriate.I realise that the above problem is very similar to text classification. In this case each user is represented by a list of strings each of which denotes a software program he/she installed. Some strings occur multiple times as multiple versions of the same software can be installed. So this looks very much like a short text with some words occurring more often. In text classification it is usual to transform the raw string/token counts into tf-idf weights. This is, essentially, an un-supervised technique as it does not take into account the correlation between the features and the target variable. The latter can be captured by target/mean encoding.So is there a straight-forward way to combine tf-idf with target/mean encoding? I would also be interested how to normalise/standartise such a combination."
3585,Naive Bayes category keywords,"['nlp', 'tf-idf', 'naivebayes']","I'm using multinomial Naive Bayes to sort documents into three categories. I would like to find the most important 'keywords' of each category, akin to using tf-idf to find the keywords of a document. I have attempted to use tf-idf for each document in the corpus and getting some overall keyword lists for each category based on the keywords of documents within those categories, but I can't find a good way to combine all these keyword lists. I have also thought about treating each category itself as a single, massive 'document' and getting the keywords using tf-idf, but this doesn't give very great results when you only have three such 'documents'. Is there a known/common method for getting category keywords?"
3586,How to transform the data and calculate the TFIDF value?,"['python-3.x', 'scikit-learn', 'nlp', 'tf-idf']","My data format is：
datas = {[1,2,4,6,7],[2,3],[5,6,8,3,5],[2],[93,23,4,5,11,3,5,2],...}
Each element in datas is a sentence ,and each number is a word.I want to get the TFIDF value for each number. How to do it with sklearn or other ways?My code:My code doesn't work.Error:"
3587,using idf with denominator+1 when all documents has the specific word,"['python', 'tf-idf']",According to some references I see that most of the time 1 is added to the denominator of idf equation to avoid the log it becomes infinity if a word does not exist in any documents.But what if a word exists in all documents? Then idf would be negative. How should we deal with this situation?
3588,How to get TF-IDF scores for the words?,"['python', 'nlp', 'tf-idf', 'tfidfvectorizer']",I have a large corpus (around 400k unique sentences). I just want to get TF-IDF score for each word. I tried to calculate the score for each word by scanning each word and calculating the frequency but it's taking too long.I used :from sklearn but it directly gives back the vector representation of the sentence. Is there any way I can get the TF-IDF scores for each word in the corpus?
3589,Regarding Python's tfidf and word-cloud,"['python', 'data-visualization', 'tf-idf', 'word-cloud']","I recently was working with word-cloud and I saw that the words in the image are bigger or smaller with respect to their importance in the document.TFIDF is somewhat similar to it.But I could not understand underlying working philosophy of WordCloud how does it work, how does it calculate the importance of a word.If we have wordCloud why do we need TFIDF and vice-versaAlso if we have calculated result from TFIDF how to visualize it.Please help me I feel this things would be very helpful to build the foundation
Thank you in advance"
3590,Compute TF-IDF word score with relevant and random corpus,"['python-3.x', 'scikit-learn', 'nlp', 'tf-idf', 'natural-language-processing']","Given a corpus of relevant documents (CORPUS) and a corpus of random documents (ran_CORPUS) I want to compute TF-IDF scores for all words in CORPUS, using ran_CORPUS as a base line. In my project, the ran_CORPUS has approximately 10 times as many documents as CORPUS.My plan is to normalize the documents, make all documents in CORPUS to one document (CORPUS being now a list with one long string element). To CORPUS I append all ran_CORPUS documents. Using sklearn's TfidfTransformer I then would compute the TF-IDF matrix for the corpus (consisting now of CORPUS and ran_CORPUS). And finally select the first row of that CORPUS to get the TF-IDF scores for my initial relevant CORPUS.Does anybody know whether this approach could work and if there is a simple way to code it?"
3591,Sklearn how to get the 10 words from each topic,"['python', 'matrix', 'tf-idf']","I want to get the top 10 frequency of words from each topic, and after I use TfidfTransformer, I get: and the type is scipy.sparse.csr.csr_matrixBut I don't know how to get the highest ten from each list, in the data, (0, ****) means the 0 list, until (5170, *****) means the 5170 list.I've tried to convert it into numpy, but it fails."
3592,Tfidf empty vocabulary; perhaps the documents only contain stop words,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'tf-idf']",curernttly I am working on a project and using Tfidf to transform X_train data which contain the text data. When I am using count_vectorizer.fit_transform(X_train) I get this error:I read other stackoverflow questions like this Link But i cannot able to understand how to split the data of X_trainHere's my Train.py fileI followed all the solutions but still didnt solved the issue. Is i am doing the wrong apprach to transform the data if i am doing right then why i am getting this error.Thanks in Advance
3593,Extract text features from dataframe,"['python', 'dataframe', 'nlp', 'feature-extraction', 'tf-idf']","I have dataframe with two text fields and other features like this format : Now my goal is to predict the score, when trying to transform this dataframe into a feature matrix to feed it into my machine learning model, here is what I have did :Now when printing the shape of my X matrix I got 2x13, but when I check the X_columsn like this : I don't get all the words in the corpus, it bring me just the words existing in df.text and other features attribute without words in df.message .How can I make X contain all my dataframe features !!"
3594,Customize Apache Spark implementation of TF-IDF,"['apache-spark', 'tf-idf']","In one hand I want to use spark capability to compute TF-IDF for a collection of documents, on the other hand, the typical definition of TF-IDF (that Spark implementation is based on that) is not fit in my case. I want the TF to be term frequency among all documents, but in the typical TF-IDF, it's for each pair of (word, document). The IDF definition is the same as the typical definition.I implemented my customized TF-IDF using Spark RDDs, but I was wondering if there any way to customize the source of the Spark TF-IDF so that I can use the capability of that, like Hashing.Actually, I need something like :Thanks"
3595,Information Retrieval: How to calculate tf-idf for multiple search terms?,"['search', 'full-text-search', 'information-retrieval', 'tf-idf']","I have a corpus of the following 4 documents:<1> This is the first document.
  <2> And this is the second document.
  <3> The third document is longer than the first and second one.
  <4> This is the last document.And use the search queue ""first OR last"", how am I supposed to calculate the tf-idf?Currently I'm using this:tf(x, D) = raw frequency of term x in document D / raw frequency of most occurring term in Didf(x) = log(1 + total number of documents / number of documents containing x)So for queue I get
<1> = (1 / 1) * log(1 + 4/3)
<3> = (1 / 2) * log(1 + 4/3)
<4> = (1 / 1) * log(1 + 4/3) Is this correct? How do you do this properly? Do I calculate the value for all search terms separately and then add? multiply?"
3596,K-Means VS K-Modes? (text clustering),"['algorithm', 'cluster-analysis', 'k-means', 'tf-idf', 'unsupervised-learning']","I understand that K-Means can be used to cluster documents by vectorizing and finding their TF-IDF values. When/how do we decide which one (K-Means or K-modes) might yield better results, apart from the categorical/continuous variables definition? Does one really give better results or is it case-by-case basis?I have carried out KMeans clustering using tf-idf and they seem to give decent results, but I can't find any material comparing the two to venture out into K-Modes. Also there is so much on the internet on k-means+tf-idf for text clustering, not much on k-modes. Any help is appreciated!"
3597,Text similarity score using a single query on a single document in Gensim,"['python', 'gensim', 'tf-idf']","What I am trying to do is get a score of the likelihood of a search/query term in a single document/text/paragraph.The score should tell how much the text is talking about the query term.Here is what I have tried but failed:I only need 1 value of the likelihood of the search term in the text data, between 0 to 1. What am I doing incorrectly ?"
3598,scikit-learn TfidfVectorizer ignoring certain words,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I'm trying TfidfVectorizer on a sentence taken from wikipedia page about the History of Portugal. However i noticed that the TfidfVec.fit_transform method is ignoring certain words. Here's the sentence i tried with:output of the dataframe:Essentially, it is ignoring the words ""Aroeira"" and ""Almonda"".But i don't want it to ignore those words so what should i do? I can't find anywhere on the documentation where they talk about this.Another question is why is the word ""the"" repeated? should the algorithm consider just one ""the"" and compute its tf-idf?"
3599,Relevance and Similarity Computation in Apache Lucene 7.5.x?,"['lucene', 'ranking', 'tf-idf', 'relevance']","What is the difference between TFIDFSimilarity, DefaultSimilarity, and SweetSpotSimilarity in Lucene 7.5.1? How can we implement BM25F in Lucene?"
3600,Pure pandas implementation of TF-IDF,"['python', 'pandas', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I'm wondering why my Pandas implementation of TF-IDF shows slightly different results than the sklearn implementation.Here's my implementation:However if I use sklearn:Or if we subtract them:Edit:I found out the sklearn idf is not exactly the same as my idf, but we can attribute this to floating point precision I think:And even if I use the sklearn idf, I still get different results.Furthermore, if I don't normalize and use the sklearn idf values only the TF-IDF for dd of the second document differs:This means either two things:
1. The problem is my TF. However this is easily checkable and doesn't seems to be the case. Or,
2. sklearn doesn't just do TF * IDF, but does something more, which I have to look into."
3601,Python: Using a list with TF-IDF,"['python', 'pandas', 'text', 'tf-idf', 'tfidfvectorizer']",I have the following piece of code that currently compares all the words in the 'Tokens' with each respective document in the 'df'. Is there any way I would be able to compare a predefined list of words with the documents instead of the 'Tokens'. Any help is appreciated. Thank you!
3602,How to change parameters for tf-idf fit method in python?,"['python', 'pipeline', 'tf-idf', 'tfidfvectorizer']","I am working on tf-idf using pandas pipelines. I am using the lnc.ltc weighing scheme from SMART notation.For document vectors, I want to set use_idf=False which is done. But how do I set use_idf=True for query vector?
I tried directly changing the use_idf parameter but it does not work."
3603,Ignoring TF-IDF in Elastic Search,"['elasticsearch', 'tf-idf']","I have a use case of resume screening candidates based on job description keywords. Since I cannot afford change in score each time a new candidate profile is added to the content list (I assume IDF will change), I want to omit TF_IDF. The indexed document isAs per the documentation here, I created following queryI am getting following errorAll I want is to score 1 for 1-or-n existence of a term and 0 if does not exist(eventually skip tf-idf). Any help is appreciated.ES version: 6.4.2"
3604,How to return a dataframe from sklearn TFIDF Vectorizer within pipeline?,"['python', 'pandas', 'scikit-learn', 'tf-idf']","How can I get TFIDF Vectorizer to return a pandas dataframe with respective column names, inside an sklearn pipeline used for cross-validation?I have an Sklearn Pipeline, where one of the steps is a TFIDF Vectorizer:I have created the class InspectPipeline in order to later inspect what were the features passed to the classifier (by running pipeline.best_estimator_.named_steps['debug'].df). However, TfidfVectorizer returns a sparse matrix which is what I get when I do pipeline.best_estimator_.named_steps['debug'].df . Instead of getting a sparse matrix, I would like to get the TFIDF vector as a pandas dataframe, where the column names are the respective tfidf tokens.I know that tfidf_vectorizer.get_feature_names() could help know the column names. But how can I include this + transforming the sparse matrix to a dataframe, within the pipeline?"
3605,How to get the word corresponding to highest tf-idf using PySpark?,"['python', 'pyspark', 'tf-idf']","I've seen similar posts but with no complete answer, hence posting here.I am using TF-IDF in Spark to get the word within a document which has the maximum tf-idf value. I use the following piece of code.I get results like whereHow can I get the array extracted from ""feature""? Ideally, I would like to get the word corresponding to the highest tfidf, like belowThanks in advance!"
3606,Getting an error while giving input to Conv1D layer in a Keras model,"['python', 'machine-learning', 'keras', 'conv-neural-network', 'tf-idf']","I am using tf-idf vector data as an input for my Keras model. tf-idf vectors has the following shape:Code:Error:ValueError: Input 0 is incompatible with layer conv1d_25: expected ndim=3, found ndim=2When I am converting the input to Input(None,X_train.shape[1],) then I am getting an error while fitting because the input dimension has been changed to 3."
3607,Python code taking more than 15 minutes to generate output,"['python', 'performance', 'optimization', 'data-mining', 'tf-idf']","I have 30 txt files and i have developed a program to find the idf and normalized tf-idf vectors. Im getting the correct values but the function getweight takes more than 15 minutes to generate the output. Can anyone suggest me a few methods for optimization. 
I donot want to use any other non-standard Python package.          "
3608,How I can create a matrix template for checking sentences similarity?,"['python', 'scikit-learn', 'text-mining', 'tf-idf', 'tfidfvectorizer']","I'm new to text mining and python and I'm trying to do a simple task.
I want to create TF matrix from sentences:
['This is the first sentence','This is the second sentence', 'This is the third sentence']And in loop (or somehow) compare new sentences with this matrix.On stackoverflow I've found good example wich works fine, but in my case it would calculate TF matrix for sample sentences and new sentences each time. It will work a bit slow on large datasets.I want to know how to do it in other more accurate way, thanks."
3609,Lucene idf calculation - is it number of documents in the index from all fields or just from the field in the query,"['lucene', 'tf-idf']","Confused how IDF is calculated in Lucene while reading Elastic docs. There are two parts of it for IDF calculation:https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.htmlHow often does the term appear in all documents in the collection? The more often, the lower the weight. Common terms like and or the contribute little to relevance, as they appear in most documents, while uncommon terms like elastic or hippopotamus help us zoom in on the most interesting documents. The inverse document frequency is calculated as follows:The inverse document frequency (idf) of term t is the logarithm of the number of documents in the index, divided by the number of documents that contain the term.https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html
Inverse document frequency
How many times did the term honeymoon appear in the tweet field of all documents in the index?Here is my question:
Lets say, I have indexed 3 documents with 5 fields where f1,f2,f3 are in doc1, f2,f3,f4 are in doc2 and f3,f4,f5 are in doc5. My query is against f2 field.numDocs - is it all documents (#-3) or just documents where field is present (#-2)?
docFreq - this seems to be clear. I think it is number of documents with f2 field where this term matches. It may be 1 or 2. Hope they include the text per field when they all documents. Thanks"
3610,MXNet - Dot Product of Sparse Matrices,"['python-3.x', 'numpy', 'recommendation-engine', 'tf-idf', 'mxnet']","I'm in the process of building a content recommendation model using MXNet. Despite being ~10K rows, out of memory issues are thrown with CPU and GPU contexts in MXNet. The current code is below.``````I'm aware that the dot product is a sparse matrix multiplied by a dense matrix, which may be part of the issue. This said, would the dot product have to be calculated across multiple GPU's, in order to prevent out of memory issues?"
3611,"When using trigrams in tf-idf, should I include unigrams and bigrams?","['nlp', 'nltk', 'tf-idf', 'n-gram']","When I used bigrams, I appended the list of bigrams to the unigrams and used that as my corpus. With trigrams, I added trigrams to unigrams but left out bigrams. Is this the correct approach, or would it be better to include bigrams as well if I want to incorporate trigrams? Should the process instead be: unigrams -> unigrams + bigrams -> unigrams + bigrams + trigrams?"
3612,ValueError: X has 1709 features per sample; expecting 2444,"['python-3.x', 'tf-idf']",I am using this code:Using TFIDF vectorizationLoading data filesDoing preprocessingGetting the above mentioned error while predicting. Does anyone know what I am doing wrong?
3613,calculate term frequency in Mysql,"['mysql', 'tf-idf']","I have Mysql table something like:Now, all I want to create a new table where columns are the unique words in all documents and rows as value corresponding to each DocumentID which is equals to (number of times word appears in sentence) / (number of words in sentence). Something like:-I have tried alot but not getting expected results."
3614,n-gram vectorization using TfidfVectorizer,"['scikit-learn', 'tf-idf']","I am using TfidfVectorizer 
with following parameters: I am vectorizing following text: ""red sun, pink candy. Green flower.""Here is output of get_feature_names():Since ""candy"" and ""green"" are part of the separate sentences, why is ""candy green"" n-gram created?Is there a way to prevent creation of n-grams spawning multiple sentences?  "
3615,how to convert sparse matrix array to json python,"['python', 'json', 'matrix', 'sparse-matrix', 'tf-idf']","I'm trying to convert TF-IDF sparse matrix to json format.
Converting it to pandas datafram (toarray() or todense()) causes memory error.
So I would like to avoid those approaches. Is there other way to convert it to json ?Below is my appraoach to get sparse matrix, and my preferred json outcomeThanks for helping me out ... !TF-IDF matrixreturn matrix  Expecting OutcomeThanks for helping me out ... !"
3616,Python: Make a dictionary of word count in nested dictionary,"['python', 'dictionary', 'tf-idf']","I have hard time making a function which count the number keys each keyword appears.As an output, I want a dictionary of each genre like:So, the value is the number of group which contained the key(genre).
For example, 'Fantasy' appeared in all groups(in total 6), and 'Western' appeared only once, so it is 1."
3617,Does the TfidfVectorizer implicitly threshold its fitted output for large datasets?,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'tf-idf']","I'm trying to use sklearn's TfidfVectorizer to output tf-idf scores for a list of inputs, comprised of both unigrams and bigrams. Here's the essence of what I'm doing:This snippet outputs the following: The length of the dictionary mapping input elements to positional indices emitted by the TfidfVectorizer is shorter than the number of unique inputs it's fed. This doesn't seem to be a problem for smaller datasets (on the order of ~50 elements) - the size of the dictionary the TfidfVectorizer produces once it has been fitted equals the size of the input. What am I missing?"
3618,Difference of More Like This (MLT) and normal select query in Solr,"['select', 'text', 'solr', 'tf-idf']","Can someone explain the exact difference of MLT and normal select query in Solr ? I know that Solr uses an advanced form of TF.IDF to score documents based on a select query for a textual field, but how does the scoring algorithm differ when MLT is being used ? "
3619,Does scikit's cross_val_predict calculates tfidf anew for each fold?,"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'tf-idf']",I have trained a classifier like that: Since I am using a pipeline object with a tfidf-statistics I wanted to know if the tfidf values get calculated anew for each fold or they are calculated on the whole dataset and the same values are used for each fold.
3620,Sklearn NotFittedError for CountVectorizer in pipeline,"['machine-learning', 'scikit-learn', 'nlp', 'tf-idf', 'countvectorizer']","I am trying to learn how to work with text data through sklearn and am running into an issue that I cannot solve.The tutorial I'm following is: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.htmlThe input is a pandas df with two columns. One with text, one with a binary class.Code:When I try to run the last line:Any suggestions on how to fix this error? I am properly transforming the CV model on the test data. I even checked if the vocabulary list was empty and it isn't (count_vect.vocabulary_)Thank you!"
3621,How to Select Top 1000 words using TF-IDF Vector?,"['python-3.x', 'scikit-learn', 'tf-idf', 'sklearn-pandas', 'tfidfvectorizer']","I have a Documents with 5000 reviews. I applied tf-idf on that document. Here sample_data contains 5000 reviews. I am applying tf-idf vectorizer on the sample_data with one gram range. Now I want to get the top 1000 words
from the sample_data which have highest tf-idf values. Could anyone tell me how to get the top words?"
3622,Calculate Tf-Idf Scores in pandas?,"['python', 'python-3.x', 'pandas', 'tf-idf', 'tfidfvectorizer']","I want to calculate tf and idf seperately from the documents below. I'm using python and pandas.I want to calculate using formula for Tf-Idf not using Sklearn library.After tokenization,I have used this for TF calculation:but this giving me a count but I want ratio of (count/total number of words).For Idf:
df[df['sent'] > 0] / (1 + len(df['sent'])but it doesn't seems to work.
I want both Tf and Idf as pandas series format.for tokenization I used df['sent'] = df['sent'].apply(word_tokenize)
I got idf scores as :How I can get tf scores seperately?"
3623,Store Tf-idf matrix and update existing matrix on new articles in pandas,"['python', 'python-3.x', 'pandas', 'tf-idf', 'tfidfvectorizer']","I have a pandas dataframe with column text consists of news articles. Given as:-I have calculated the Tf-IDF values for articles as:-As my dataframe is kept updating from time to time. So, let's say after calculating of-if as matrix_1 my dataframe got updated with more articles. Something like:As I have millions of articles and all I want to store a tf-IDF matrix of the previous article and updating the same with tf-IDF scores of the new article. Running the of-IDF code for all articles, again and again, would be memory consuming. Is there any way I can perform this?"
3624,tfidf w2v giving NaN values,"['python', 'word2vec', 'tf-idf']","While using TFIDF Word2Vec giving NaN values on the review of Amazon fine foods dataset after sampling it to 100k data points I am getting NaN values in the sentence vector...
I tried almost everything with the code but am not getting real values..After printing the sentence array this is the output I am getting -The code is as follows -Any kind of help will be appreciated !!"
3625,Unbalanced clusters Spectral Coclustering sklearn,"['python', 'scikit-learn', 'tf-idf', 'svd']","I'm following the Spectral Coclustering of documents and words tutorial on sklearn (http://scikit-learn.org/stable/auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py) and my clusters are very unbalanced. I have the output :I don't know why its different from the tutorial, but is there a solution to have more balanced clusters ?I even code my ""own"" Spectral coclustering following this link and using the SVD from scipy.sparse.linalg and the Kmeans from sklearn, but I have the same problem...Thank you !"
3626,Using TFIDF with Naive bayes in MATLAB,"['matlab', 'tf-idf', 'naivebayes']","I'm building a sentiment classification model using TFIDF and naive bayes. But the model keeps misclassifying the second class.Although I have used TFIDf with other models such as SVM and random forest and it was working fine. Below I will describe my data and steps used: I have 2000 comments (1000 positive, 1000 negative). I did the following steps:
 1) data preprocessingThen I used TFIDFThen I passed the results to my naive bayes modelBut the confusion matrix will give the following results :It seems it is classifying almost all the 2000 observations to one class only. Please advice, I have tried changing the K fold value.Also I have tried diferent types of distrebution such as Multivariate multinomial distribution, Multinomial distribution, kernel, and Normal (Gaussian) distribution.Also I tried normalising the data. But all me attempts had no effect on enhancing the model. 
Using Naive bayes without TFIDF will give 0.7536 F-measure which is somehow is considered good.
My concern is if I'm using TF-IDF with naive bayes in the correct way.Because it is misclassified all observations of the second class."
3627,RavenDB — More Like This — Need a (similarity) metric; not just rank-orders,"['ravendb', 'similarity', 'tf-idf', 'morelikethis']","I have a RavenDB / 'More Like This' example running (C#) as per 
Creating more like this in RavenDBHowever, in addition to receiving similar documents back, I really need some measure of similarity back for those documents. I am assuming (correctly?) that the order in which I get the similar documents back represents the rank-order scores of the documents' similarities (first one back has the highest similarity, second one back has the second highest similarity, etc.).However, rather than rank orders I need the metric similarity results. This assumes (of course) that the rank orders are computed from a more continuous metric; e.g., tf-idf. If that is true, can I get a hold of those metric scores?  "
3628,sklearn feature union,"['python-3.x', 'scikit-learn', 'classification', 'pipeline', 'tf-idf']","The objective is to run a multi-label classifier using three inputs. Each input is an excerpt from a larger document. The pipeline has a preliminary step which vectorizes each excerpt using tfidfx is a list of strings, each an excerpt.The code below works but seems to ignore the second and third elements of the list..I tried without successI also tried FeatureUnionwhere TFIDFX1 is constructed as followsI omit classes TFIDFX2 and TFIDFX3 for brevity which look to mylist[1] and mylist[2] respectively but are otherwise identicalThis fails with the following traceback:I would appreciate any help from the SO community"
3629,Spark Hashing TF power of two feature dimension recommendation reasoning,"['apache-spark', 'tf-idf']","According to https://spark.apache.org/docs/2.3.0/ml-features.html#tf-idf:""HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3.""
...
""Since a simple modulo on the hashed value is used to determine the vector index, it is advisable to use a power of two as the feature dimension, otherwise the features will not be mapped evenly to the vector indices."" I tried to understand why using a power of two as the feature dimension will map words evenly and tried find some helpful documentation on the internet to understand it, but both attempts were not successful.Does somebody know or have useful sources on why using the power two maps words evenly to vector indices?"
3630,"Error predicting: X has n features per sample, expecting m","['python', 'python-3.x', 'scikit-learn', 'tf-idf']","I got the following code, where I transform a text to tf:Then, I train a model and save it using pickle.
The problem comes when, in another program, I try to predict new data.
Basically, I got:When I execute this code, the output is ValueError: X has 8933 features per sample; expecting 7488I know this is a problem with the TfIdf representation, and I hear that I need to use the same tf_transformer and vectorizer to get the expected input shape, but I don't know how to achieve this.
I can store the others transformers and vectorizers, but I have tried using different combinations and I got nothing."
3631,Py4JJavaError: An error occurred while calling o288.fit,"['apache-spark', 'pyspark', 'tf-idf']","While running a sample Tfidf code from spark 2.2.0 documentation, here is the link : https://spark.apache.org/docs/2.2.0/ml-features.htmlthis is the code: While doing this, I get this following Error:I am new to pyspark and I followed this tutorial to install it.
Can anyone tell me what am I missing here ? Any help is appreciated."
3632,Dealing with different scores from Solr replicas,"['solr', 'solrcloud', 'tf-idf']","In Solr's default scoring implementation, deleted documents are still considered when calculating scores. The number of deleted documents on replicas of a shard are not guaranteed to be the same, and hence the different replicas may give slightly different scores.We are currently using a setup with 8 shards each having 2 replicas. For paging search results it is vital that the sort order is deterministic to avoid missing documents/skipping documents when paging.Possible ways of dealing with this that come to mind are to use a different scoring algorithm or to try to require tools using the system to send queries to the same replicas that served their first query (this would obviously go badly wrong if that replica was later down!). As the issue is different scoring between replicas, rather than different scoring between shards, I don't think Solr's distributed IDF settings help."
3633,"Using known python packages for implementing N-Gram, TF-IDF and Cosine similarity","['python', 'machine-learning', 'tf-idf', 'n-gram', 'cosine-similarity']","I'm trying to implement a similarity function usingExample
Concept:I searched the web for a simple and safe implementation but I couldn't find one that was using known python packages as sklearn, nltk, scipy etc.
most of them using ""self made"" calculations.I'm trying to avoid coding every step by hand, and I'm guessing there is an easy fix for all of 'that pipeline'.any help(and code) would be appreciated. tnx :)"
3634,getting same tfidf values for different words,"['python-3.x', 'sentiment-analysis', 'tf-idf', 'tfidfvectorizer']","So i am doing a project regarding twitter sentiment analysis, wherein i happen to need to use TFIDF on the collected tweets.
So i converted the list of tweets into a single string and fed that to the object, the problem is that i am getting identical values for most of the words with some different values but they are also very frequent. Why is this happening ? Is it cause i am using a single string as input?here is the code https://trinket.io/python/9c2daed912Here is the screenshot, as you can see many have same TFIDF values"
3635,how to cal tfidf in different way,"['python', 'nlp', 'tf-idf']","I have two type document, one is labeled, and other is not. 
I want use the labeled document to calculate tf, and use other unlabeled document to calculate idf.This unannotated is unlabeled data, one line is one document.
Based on this data, I got a dict:I use next piece of code to calculate tfidf, and it gives all same answer:0.9999999999999946"
3636,"IDF similarity across shards does not work as expected, uses only local shard info","['solr', 'similarity', 'solrcloud', 'tf-idf']","I am using solr(7.3) for my groceries products data. I found strange results due to idf on data across multiple shards(3 shards). My search keyword was ""milk""Milk is not really rare keyword in my collection. But, in one of the shards, there are very few documents(1-2 docs among 9000) containing keyword milk. So in that shard(shard1) the idf score is very high, almost 3 times the score from other shards. Which is affecting my result. I am not expecting that specific document from shard1 to be as top result.Is there any way to control idf scoring as we can do for tf in BM25 with k1 and b parameters? Or do we have BM25 without idf similarity? I can create my own similarity and use it but our solr services does not allow customising solr.Or is there any other way to solve this?"
3637,Identify words that appear in less than 1% of the corpus documents,"['python', 'nlp', 'nltk', 'counter', 'tf-idf']","I have a corpus of customer reviews and want to identify rare words, which for me are words that appear in less than 1% of the corpus documents.I already have a working solution, but it is far too slow for my script:Does anyone know a faster implementation for this? It seems to be very time-consuming to iterate for each individual words through each individual review.Thanks in advance and best wishes,
Marcus"
3638,Add tf-idf values as columns in a matrix,"['python', 'pandas', 'scikit-learn', 'tf-idf']","I was trying to add the tfidf scores as features. Is it the correct way?item1 is of shape (400k) and same is the shape of item2.
The shape of tfidf_sp is (800k, 100k)."
3639,How to perform kmean clustering from Gensim TFIDF values,"['numpy', 'k-means', 'gensim', 'tf-idf', 'corpus']",I am using Gensim for vector space model. after creating a dictionary and corpus from Gensim I calculated the (Term frequency*Inverse document  Frequency)TFIDF  using the following lineThe corpus_tfidf contain list of the list having Terms ids and corresponding TFIDF. then I separated the TFIDF from ids using following lines:now I want to use k-means clustering so I want to perform cosine similarities of tfidf matrix the problem is Gensim does not produce square matrix so when I run following line it generates an error. I wonder how can I get the square matrix from Gensim to calculate the similarities of all the documents in vector space model. Also how to convert tfidf matrix (which in this case is a list of lists) into 2D NumPy array. any comments are much appreciated.dumydist = 1 - cosine_similarity(tfidfmtx)
3640,"Given cluster of documents, compute similarity between corpus and the cluster","['python', 'pandas', 'numpy', 'nltk', 'tf-idf']","I am doing a similarity ranking job by computing the distance between each document in the corpus and the cluster. The cluster is also given as list of documents. What I am in trouble with is that I cannot come up with a proper way of computing the centroid of the cluster so that I can compute the similarity. I tried to use the average value of tfidf matrix of cluster while it gives poor result.For example: my cluster is:And my courpus contains following 3 documents:I want to compute the similarity between every document and the cluster, which might produce result like:Do you have any suggestion? I tried represent both cluster and corpus documents as tfidf matrices, but it seems hard to give desire result by computing similarity between two matrix. And I tried LSI, but it is the corpus I want to rank not the cluster documents which forces me to find the centroid representative of the cluster."
3641,Confused with the return result of TfidfVectorizer.fit_transform,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I wanted to learn more about NLP. I came across this piece of code. But I was confused about the outcome of TfidfVectorizer.fit_transform when the result is printed. I am familiar with what tfidf is but I could not understand what the numbers mean.And the output is:(0, 630)  0.37172623140154337   (0, 160)  0.36805562944957004   (0,
38)   0.3613966215413548   (0, 545)   0.2561101665717327   (0,
326)  0.2645280991765623   (0, 967)   0.3277447602873963   (0,
421)  0.3896274380321477   (0, 227)   0.28102915589024796   (0,
323)  0.22032541100275282   (0, 922)  0.2709848154866997   (1,
577)  0.4007895093299793   (1, 425)   0.5970064521899725   (1,
943)  0.6310763941180291   (1, 878)   0.29102173465492637   (2,
282)  0.1771481430848552   (2, 243)   0.5517018054305785   (2,
955)  0.2920174942032025   (2, 138)   0.30143666813167863   (2,
946)  0.2269933441326121   (2, 165)   0.3051095293405041   (2,
268)  0.2820392223588522   (2, 780)   0.24119626642264894   (2,
823)  0.1890454397278538   (2, 674)   0.256251970757827   (2,
874)  0.19343834015314287   : :   (5569, 648) 0.24171652492226922
(5569, 123)   0.23011909339432202   (5569, 957)   0.24817919217662862
(5569, 549)   0.28583789844730134   (5569, 863)   0.3026729783085827
(5569, 844)   0.20228305447951195   (5569, 146)   0.2514415602877767
(5569, 595)   0.2463259875380789   (5569, 511)    0.3091904754885042
(5569, 230)   0.2872728684768659   (5569, 638)    0.34151390143548765
(5569, 83)    0.3464271621701711   (5570, 370)    0.4199910200421362
(5570, 46)    0.48234172093857797   (5570, 317)   0.4171646676697801
(5570, 281)   0.6456993475093024   (5572, 282)    0.25540827228532487
(5572, 385)   0.36945842040023935   (5572, 448)   0.25540827228532487
(5572, 931)   0.3031800542518209   (5572, 192)    0.29866989620926737
(5572, 303)   0.43990016711221736   (5572, 87)    0.45211284173737176
(5572, 332)   0.3924202767503492   (5573, 866)    1.0I would be more than happy if someone can explain about the output."
3642,clustering twitter users with python,"['python', 'cluster-analysis', 'k-means', 'tf-idf', 'tfidfvectorizer']","i found this code on git-hub (https://github.com/bonzanini/Book-SocialMediaMiningPython/blob/master/Chap02-03/twitter_cluster_users.py) to clustering user profiles according to their bios on twitter with k-means. i have a .jsonl file with lines in this format :but i need to know user names with bios while print clustering.How can i do that? 
And i want to add location information in this clustering. adding location as a second column to matrix and calculating tfidf with this two features. How can i do that too?"
3643,How to use GloVe to generate vector matrix?,"['python', 'vectorization', 'tf-idf', 'hdbscan']","I am using HDBSCAN algorithm to create clusters from the documents I have. But to create a vector matrix from the words, I am using tf-idf algorithm and want to use GloVe. I have searched posts but could not understand how to use this algorithm.  I also read about Gensim but did not understand how could I use this to implement GloVe. Here is what I am doing:As you could see in the above implementation, I have used HDBSCAN along with tf-idf for text clustering. How could I use GloVe in place of tf-idf?"
3644,Inputting document-term frequency matrix in TfidfVectorizer()?,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I am coming up with a made-up example of bag of words from three documents (I am demonstrating how tf-idf works given a document-term frequency matrix), and I want to transform my bow matrix into a tf-idf matrix. I don't actually have text data, just the number I made up in my example? How can I use that to produce tf-idf output? I am getting the error message ""'numpy.ndarray' object has no attribute 'lower'"" on the last line (and I am assuming it is because fit_transform is expecting text data. Is it possible to specify or override this somehow?"
3645,Is there a way to set min_df and max_df in gensim's tfidf model?,"['gensim', 'tf-idf']","I am using gensim's tdidf model like so:Now I'd like to apply thresholds to remove terms that appear too frequently (max_df) and too infrequently (min_df).  I know that scikit's CountVectorizer allows you to do this, but I can't seem to find how to set these thresholds in gensim's tfidf.  Could someone please help? "
3646,tf-idf calculation using the scikit-learn feature extraction module,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","Please read the post completely before flagging. I have looked everywhere on  the internet to try to figure this out.I am simply trying to work through this machine learning tutorial and am having trouble replicating the results. Furthermore I can't mathematically replicate the results I am producing. Everything is clear until I try to generate the tfidf as seen in the comments in the code below. To be specific, am I correctly generating the tfidf below?
If so, how can I replicate mathematically as it's my understanding it should simply be tf * idf where both are simple calculations as seen in the comments below.Thanks in advance! "
3647,How do similar documents transformed into TFIDF valued vector look in vector space,"['vector', 'machine-learning', 'scikit-learn', 'tf-idf']","This might be a strange question, but I cant help it wonder. If I lets say have three documents:And if i transform all these 3 documents into TFIDF valued vectors, in vector space, will the documents d1 and d2 be closer to each other then documents d2 and d3 for example? Sorry if it is a stupid question, but I would really like to visualize somehow this in order to better understand it. Thank you in advance!"
3648,How do scoring and indexing work together in Information Retrieval Systems,"['machine-learning', 'solr', 'lucene', 'information-retrieval', 'tf-idf']","I have a brief understanding of indexing ( inverse indexing ) and scoring ( like tf-idf ) in IR . Generally , if there is no indexing , a tf-idf matrix is pre-calculated , and a corresponding tf-idf vector is made for the query and then scores calculated for each document . What is this flow like when there is indexing done for the documents , specifically how does a library like apache lucene or terrier process a query to evaluate scores for documents ."
3649,Why is TF-IDF calculation taking so much time?,"['python', 'dictionary', 'nlp', 'tf-idf']","I use the TF-IDF code from here in my corpus of documents, which is 3 PDF documents, each about 270 pages long.The problem is, it just keeps running, without displaying anything beyond Top words in document 1. Why is it taking so long to calculate the scores? I've kept it running for an hour now, and the code hasn't terminated. Earlier I tried out the code for 50 odd txt files which were much shorter in length (like, 2-3 paragraphs on average), and there it was able to show the TF-IDF scores instantaneously. What's wrong with 3 docs of 270 pages each?"
3650,How to improve performance of LDA (latent dirichlet allocation) in sci-kit learn?,"['python', 'scikit-learn', 'tf-idf', 'lda', 'topic-modeling']","I am running LDA on health-related data. Specifically I have ~500 documents that contain interviews that last around 5-7 pages. While I cannot really go into the details of the data or results due to preserving data integrity/confidentiality, I will describe the results and go through the procedure to give a better idea of what I am doing and where I can improve.For the results, I chose 20 topics and outputted 10 words per topic. Although 20 was somewhat arbitrary and I did not have a clear idea of a good amount of topics, that seemed like a good amount given the size of the data and that they are all health-specific. However, the results highlighted two issues: 1) it is unclear what the topics were since the words within each topic did not necessarily go together or tell a story and 2) many of the words among the various topics overlapped, and there were a few words that showed up in most topics.In terms of what I did, I first preprocessed the text. I converted everything to lowercase, removed punctuation, removed unnecessary codings specific to the set of documents at hand. I then tokenized the documents, lemmatized the words, and performed tf-idf. I used sklearn's tf-idf capabilities and within tf-idf initialization, I specified a customized list of stopwords to be removed (which added to nltk's set of stopwords). I also set max_df to 0.9 (unclear what a good number is, I just played around with different values), min_df to 2, and max_features to 5000. I tried both tf-idf and bag of words (count vectorizer), but I found tf-idf to give slightly clearer and more distinct topics while analyzing the LDA output. After this was done, I then ran an LDA model. I set the number of topics to be 20 and the number of iterations to 5.From my understanding, each decision I made above may have contributed to the LDA model's ability to identify clear, meaningful topics. I know that text processing plays a huge role in LDA performance, and the better job I do there, the more insightful the LDA will be. I appreciate all insights and input. I am completely new to the area of topic modeling and while I have read some articles, I have a lot to learn! Thank you!"
3651,Does BM25 use query coordinator?,"['elasticsearch', 'solr', 'lucene', 'information-retrieval', 'tf-idf']","In Lucene's practical scoring function there is a query coordinator which punishes documents that fail to match all the query terms. does Okapi BM25 use the same trick?The reason I'm curious about it is that I'm using Elasticsearch with BM25 similarity module and sometimes I feel this algorithm does not favor documents with more matches. There are cases that a document contains one or two terms a lot, outscores a document containing all query terms."
3652,Pairwise cosine similarity Python,"['python', 'tf-idf', 'cosine-similarity', 'pairwise', 'tfidfvectorizer']","I have to compute pairwise cosine similarity after computing tf-idf matrix but I am getting a memory error! found similar questions but the answers didn't solve my case. Currently my code is ,Promising answer I found is this, But I am not sure of the what is vector in that code?"
3653,calculate nearest document using fasttext or word2vec,"['word2vec', 'knn', 'tf-idf', 'nearest-neighbor', 'fasttext']","i have a small system of about 1000 documents.
For each document I would like to show links to the X ""most similar"" documents.However, the documents are not labeled in any way, so this would be some kind of unsupervised method.It feels like fasttext would be a good candidate, but I cant wrap my head around how to do it when its not labeled data.I can calculate the word vectors, although what I really need is a vector for the whole document."
3654,How can using more n-gram orders decrease accuracy for Multinomial NaiveBayes classifier?,"['scikit-learn', 'nlp', 'nltk', 'tf-idf', 'tfidfvectorizer']","I'm building a model for text classification with nltk, and sklearn, and training it on the 20newsgroups dataset from sklearn (each document is approximately 130 words).My preprocessing includes removing stopwords and lemmatizing tokens. Next, in my pipeline I pass it to the tfidfVectorizer() and want to manipulate some of the input parameters of the vectorizer to improve accuracy. I've read that n-grams (generally, with n less than improves accuracy, but when I classify the vectorizer outputs with the multinomialNB() classifier, using ngram_range=(1,2) and ngram_range=(1,3) in tfidf, it worsens the accuracy. Can someone help explain why?EDIT:
Here's a sample datum as requested, with the code I used to fetch it and strip the header:This is my pipeline, the code run to train the model, and print accuracy:"
3655,How Do I Apply TF-IDF When I Only Have a Subset of the Total Documents?,"['database', 'elasticsearch', 'search', 'tf-idf']","Practical application:I have several databases that need to be queried from a single search box. Some of them I have direct access to (they're SQL Server / MySQL), others I can only search via an API. In an ideal world I would inject all of this data into Elasticsearch and use it to determine relevance. Unfortunately I don't have the resources locally to make that run efficiently. Elastic is taking over 400mb of RAM just while idling without adding any actual data or running queries. It looks like most people using Elasticsearch in production are running machines with 32GB - 64GB of RAM. My organization doesn't have access to anything near that powerful available for this project.So my next idea is to query all the databases and connect to the API's when the user makes a search. Then I need to analyze the results, determine relevance, and return them to the user. I recognize that this is probably a terrible plan in terms of performance. I'm hoping to use memcached to make things more tolerable.In my research for finding algorithms to determine relevance, I came across tf-idf. I'm looking to apply this to the results I get back from all the databases. The actual questionMy understanding of tf-idf is that after tokenizing every document in the corpus, you perform a term frequency analysis and then multiply it against the inverse document frequency for the words. The inverse document frequency is calculated by dividing the total document count by the the total number of documents with the term. The problem with this is that if I'm pulling documents from an API, I don't know the true total number of documents in the corpus. I'm only ever pulling a subset, and based on the way those documents are being pulled they're naturally going to all of the terms in them. Can I still apply tf-idf to this by treating the pool of documents returned by these various sources as a single corpus? What's the best way to go about this? Bonus questionIf you have a suggestion for how to accomplish this without hacking together my own search solution or using Elasticsearch I'm all ears... "
3656,What is the default smartirs for gensim TfidfModel?,"['python', 'nlp', 'gensim', 'information-retrieval', 'tf-idf']","Using gensim:[out]:If we do the TF-IDF computation manually, [out]:If we use math.log2 instead of math.log:It looks like gensim:Looking at https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel , the smart scheme difference would have output different values but it's not clear in the docs what is the default value.What is the default smartirs for gensim TfidfModel?What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?"
3657,How to set the values of Tfidf Model in gensim manually,"['python', 'gensim', 'tf-idf']","In the Python code:I want to find a way to fill the values of the corpus_tfidf manually as I already have a list of list of tfidfs for each document in the corpus, calculated using specific equations.So, how to use them to fill the corpus_tfidf instead of recalculating them using gensim calculations.I want to use my values to be passed for the gensim LSI and LDA models.Thanks."
3658,my java tf-idf program ain't reading the file it's supposed to,"['java', 'filereader', 'tf-idf']","the program is supposed to read a file, wich the filename will be arquivo.txt, and should read its content and calculate the tf-idf of a term that the user will insert, but it says that couldnt locate the arquivo.txt file, even thought its on the src file of the program.tried programing the code on a different style, the result it wont show the run option, like the intellij wont find the runneable part."
3659,How to plot the text classification using tf-idf svm sklearn in python,"['python', 'graph', 'scikit-learn', 'svm', 'tf-idf']","I have implemented the text classification using tf-idf and SVM by following the tutorial from this tutorialThe classification is working properly.
Now I want to plot the tf-idf values (i.e. features) and also see how the final hyperplane generated that classifies the data into two classes.The code implemented is as follows:I have read how to plot the graphs, but couldn't find any tutorial related to plot the features of tf-idf and also the hyperplane generated by SVM."
3660,python LightGBM text classicication with Tfidf,"['python', 'tf-idf', 'text-classification', 'lightgbm']","I'm trying to introduce LightGBM for text multiclassification.
2 columns in pandas dataframe, where 'category' and 'contents' are set as follows.Dataframe: Hereby I'm trying to classify text into 3 categories as follows.  Codes:Then I got an error as: I also convert 'category' column ['a', 'b', 'c'] to int as [0, 1, 2] but got an error as What's wrong with my code?
Any advice / suggestions will be greatly appreciated.
Thanks in advance."
3661,"Trying to replicate TFIDF example, multiplication returns wrong number","['python', 'python-3.x', 'tf-idf']","I am trying to replicate a TFIDF example from this video: Using TF-IDF to convert unstructured text to useful featuresAs far as I can tell, the code is the same as in the example, except for me using .items (python 3) instead of .iteritems (python 2):The resulting table should look something like this, where the common words(on, my, sat, the) all have a score of 0:But instead my resulting dataframe looks like this, with all words having the same score, except those just occuring in on on of the documents (bed\dog,cat\face):if I print(idfs) I get Here, the words that are included in both docs have the value 0, which will then be used to weigh down their importance, as they are common to all docs. Before the computeTFIDF function is used, the data looks like this:Since the function will multiply the two numbers, ""my"" (with an idfs of 0) should be 0, and ""dog"" (with a idfs of 0.6931) should be (0,6931*0,1666 = 0,11), as per the example. Instead, I get the number 0.02083 for all but the words not present in the doc. Is there something other than the syntax for iter\iteritems between python 2 and 3 that is messing up my code?"
3662,TfIdf vectorizer returning positive values for absent words,"['pandas', 'scikit-learn', 'tf-idf']","I'm vectorizing a corpus using the TfIdf vectorizer in sklearn. The corpus is large, but the data more or less looks like this:I want to find how the TfIdf values for words break down by speaker for the first three speakers. So I have:After vectorizing the corpus, I created a dataframe with the TfIdf values whose columns are the vocabulary:This gives a dataframe that looks like:The problem is that speakers who never use certain terms are getting positive TfIdf values for those terms. For instance, if I look at the words for Jane, I get:This seems to happen for all speakers, and words are positive that never appear in their row. The positive values differ, but they remain positive.In general, is there a reason the vectorizer would return positive values for words not in the same speaker rows?"
3663,match two sets of files to find closest relevance between them,"['nlp', 'data-science', 'tf-idf']","I have two sets of files take it A and B, both sets have around 10000 files. I want to find the files in set B which are closest match for each file in set A.Actually the files in set A and set B are actually directories but for simplicity I am referring them as files.Currently I have written few parser to extract important attributes from both sets and storing them in database saying it as characteristics for each file in both types. Now I would like to find the closest match for files in set A with files in set B.Let me know your suggestions on the best approach to counter this problem."
3664,Compare document pairs within corpus using TF-IDF - Python,"['python', 'pandas', 'tf-idf']","I managed to calculate the TF-IDF and matrix using the following code:However, I would now like to compare the similarity of different paragraphs, my end result should look like this:How can I use the TF-IDF matrix to compare the different paragraph pairs?"
3665,Using spacy and textacy. Need to find tf-idf score across corpus of original tweets but cant import textacy vectorizer,"['python-3.x', 'tf-idf', 'spacy', 'textacy']",I'm new to these frameworks as well as NLP. I am following an example which gives me the following code snippet to  calculate the tf-idf score of all the tokens in the tweets. However I keep getting either import errors or Vectorizer undefined. Code:Errors Recieved:My EnviromentWhat is the correct import statement to access the textacy vectorizer class?
3666,MySql Query Multiple Records,"['mysql', 'sql', 'tf-idf']","i have following query for tf/idf measurement . My aim is to get top 10  for each date like top 10 for 20160401, top 10 for 20160402 etc.But according to my query i have more than 10 for each date.As it can see i could not get ordered, there should be only one 1 , one 2 one 3 , i mean max 10 for each date.  The following is my query for tf idf measurement.  in newsdetails   table i have  columns and values like The output is : But i want to get  only top 10 word for each date with their ratio. "
3667,NLP Sentiment Analysis using TF-IDF Vector Size [closed],"['python', 'machine-learning', 'nlp', 'sentiment-analysis', 'tf-idf']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I am relatively new to NLP & Sentiment analysis, but I am enrolled in a Machine Learning class and am creating a Sentiment Analysis NLP that will read a financial article and determine whether or not the overall sentiment is good or bad. Currently, I have a dataset of about 2000 articles. I know that I need to implement the TF-IDF vector method to cast all the instances in the dataset to the same vector space. Also, I know that TF-IDF requires a ""Vocabulary"" and the size of this ""Vocabulary"" is the length of the vector, each vector representing an article. My question is, how do I determine this vocabulary? One method I have found is to implement pre-processing (get rid of stop words, noisy words, punctuation, etc.) and then use ALL words in EVERY article in the training set. From here you can remove the words that have very few instances (unimportant words) and remove the words that have too many instances (non-distinguishing words). However, in my opinion, the ""Vocabulary"" is still going to be quite large, hence, the vector size is going to be very large.Overall, this approach seems logical, but processing heavy. I feel that initially creating a ""Vocabulary"" containing all words in every article is going to be HUGE. And then iterating through every article to see how many times the words in the ""Vocabulary"" have occurred is going to require a lot of processing power. If I am using NLTK and scikit-learn, do I have anything to worry about? If so, is there a better way to create the vocabulary?"
3668,Sklearn Tfidf Vectorizer norm=None norm-l2,"['python', 'scikit-learn', 'normalization', 'tf-idf', 'tfidfvectorizer']","Hi I'm trying to understand how scikit-learn works out the TFIDF score in the matrix: document 1, feature 6, ""wine"":I was using the answer to a very similar question to calculate it for myself:
How areTF-IDF calculated by the scikit-learn TfidfVectorizer However in their TFIDFVectorizer, norm=None.As I'm using the default setting of norm=l2, how does this differ to norm=None and how can I calculate it for myself?"
3669,TFIDF using Apache Mahout have “Job failed!” when app run,"['java', 'eclipse', 'machine-learning', 'mahout', 'tf-idf']","I want to run simple app using Apache Mahout from http://technobium.com/tfidf-explained-using-apache-mahout/. But! When app running, i have some exception:But, I don't know how to fix it. Maybe you know how to fix this problem in Apache Mahout lib? Please, give me some idea!Configuration:
Windows 7 x64, Eclipse Oxigen 1a, Maven, Mahout-core-0.9 "
3670,Counting number of document in which each term appears,"['python', 'pandas', 'binary-search-tree', 'tf-idf']","I have a list of document of 5 documents stored at each node in a tree. In each list there are a number of words apprearing which might repeat several times in same document as well as other documents. I want to take a count of number of documents where each word is apprearing.
For example: A is parent node and B,C are child nodes. A,B,C has 5 documents.I want result in the form of {'a':3,'b':1,'c':2,'d':2,'e':1}Below is the code I am using but it is not taking count of each document but it is taking count of number of times each word is appearing in the document. Kindly help how this can be done."
3671,sklearn TfidfVectorizer : Generate Custom NGrams by not removing stopword in them,"['machine-learning', 'scikit-learn', 'statistics', 'tf-idf']","Following is my code:It generates tri gram by removing all the stopwords.What I want it to allow those TRIGRAM what have stopword in their middle ( not in start and end)Is there processor needs to be written for this.
Need suggestions."
3672,TF-IDF in Golang,"['go', 'tf-idf']","Is there any golang library similar to sklearn that can be used to find tf-idf? I can't seem to find anything well documented. I am looking to find the tf-idf given a bunch of text file, similar to the python version mentioned here"
3673,How to increase weight of a word for CountVectorizer,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I have a document that I tokenized, and then I take another document and I compare the two by calculating their cosine similarity.However, before I calculate their similarity, I want to increase the weight of one of the words beforehand. I'm thinking of doing this by doubling the count of that word, but I do not know how to do that.Suppose I have the following...Next I define the stop words and call CountVectorizer for both sets of documents.In the next part I calculate the cosine similarity...However, before I calculate the similarity, how can I increase the weight of one of the words. Suppose in this example I want to increase the weight of something, how can I do that? I think you do this by increasing the word count but I do not know how to increase that."
3674,Merging different dictionary in python without updating the value stored,"['python', 'pandas', 'function', 'binary-search-tree', 'tf-idf']","I have a tree structure where at every node idf values are stored for a large number of words. Dictionary has two fields i.e. word and idf.I want to store all the idf values in a dictionary. I want all the value of idf which are stored in the tree to get stored in the dictionary, but when I am doing so it is storing only one value of each word.For example: A has two childs B and C. A,B,C all has idf values stored at them. I want to make a dictionary which will combine all the idf values and store it together.A = {'a':10, 'b': 11} B = {'a':5, 'c': 8} C = {'b':21, 'd': 20}, I want to store it as dic = {'a':10,'a':5,'b':11,'b':21,'c':8,'d':20}Below is the code that I am using:Kindly help how this can be done.Latest code:The above latest code is storing multiple values for a key but it is repeating the same value again and again. Where I am going wrong. Please help."
3675,tf-idf to users preferences vector,"['tf-idf', 'tfidfvectorizer', 'content-based-retrieval']","I'm fairly new here and I'm thanking in advance everybody who will take the time to read this question.We're building a recommender system using tf-idf to generate normalised vectors of documents. Based on the interactions of the users with the documents (like, don't like, spend time on it etc...), we want to generate users profiles that follows the same structure than the documents themselves.While there is a lot of literature about recommender systems and content based filtering on the 'product' side, there is very little about the structuring of the users preferences themselves. I'm not exactly asking a 'solution' but rather to please point us in the right direction (or simply, a direction). We might work out something ourselves, but no need to reinvent the wheel if there's already quite developed solutions.Many thanks all!
Daniel"
3676,CountVectorizer in Python,"['python', 'tf-idf', 'text-classification', 'countvectorizer', 'tfidfvectorizer']","I am working on a problem in which I have to predict whether a sent email from a company is opened or not and if it is opened, I have to predict whether the recipient clicked on the given link or not.I have a data set with the following features:For the email body and subject I can use a CountVectorizer but how can I include my other feature to that sparse matrix created by said CountVectorizer."
3677,How can I add synonymous recognition in tfidfvectorizer using scikit-learn?,"['scikit-learn', 'token', 'tf-idf', 'tfidfvectorizer']","I am using tfidfvectorizer from sklearn.
I wan't to add synonymous vocabulary to the object, to counting the same term finding ""house"" and ""houses"" for example. How can I do it?"
3678,"Tf-idf match list vs list, instead of one list","['python', 'string-matching', 'tf-idf']",I'm experimenting with tf-idf matching. I followed the tutorial from this article. I would like to know if I can match an input list versus another list of already processed data and then get this script to return the output as potential matches from the existing second list for every item of the input list.I hope one of you guys can put me in the right direction! Thanks!!
3679,"SMOTE initialisation expects n_neighbors <= n_samples, but n_samples < n_neighbors","['scikit-learn', 'knn', 'tf-idf', 'oversampling', 'imblearn']","I have already pre-cleaned the data, and below shows the format of the top 4 rows:I have called train_test_split() as follows:I have then vectorized the X training and testing data using the following TfidfVectorizer and fit/transform procedures:I'm now at the stage where I would normally apply a classifier, etc (if this were a balanced set of data). However, I initialize imblearn's SMOTE() class (to perform over-sampling)...... but this results in:I've attempted to whittle down the number of n_neighbors but to no avail, any tips or advice would be much appreciated. Thanks for reading.------------------------------------------------------------------------------------------------------------------------------------EDIT:Full TracebackThe dataset/dataframe (df) contains 2380 rows across two columns, as shown in df.head() above. X_train contains 1785 of these rows in the format of a list of strings (df['cleaned']) and y_train also contains 1785 rows in the format of strings (df['Year']). Post-vectorization using TfidfVectorizer(): X_train and X_test are converted from pandas.core.series.Series of shape '(1785,)' and '(595,)' respectively, to scipy.sparse.csr.csr_matrix of shape '(1785, 126459)' and '(595, 126459)' respectively.As for the number of classes: using Counter(), I've calculated that there are 199 classes (Years), each instance of a class is attached to one element of aforementioned df['cleaned'] data which contains a list of strings extracted from a textual corpus.The objective of this process is to automatically determine/guess the year, decade or century (any degree of classification will do!) of input textual data based on vocabularly present."
3680,Stored Tfidf-Vectorizer ValueError when loaded again,"['python', 'scikit-learn', 'tf-idf', 'text-classification', 'tfidfvectorizer']","I trained a Tfidf-Vectorizer for a PassiveAgressive Classifier and tested it, everything works just fine. Then I saved the fitted vectorizer and the trained classifier for later use. When I loaded the vectorizer again, I transformed a new dataset on it (as I want to classify unseen data with the classifier) and tried to predict the new dataset. When I run the code, I get:This is the code I used to get the vectorizer and classifier:And this is how I open it again and use it:I also checked again in the file where I trained the classifier and looked at the length of the vocabulary and got  So yet another number... I really do not know what is going wrong here. I processed the texts for the train/test datasets and the texts that need to be classified after loading everything again exactly the same way, so I do not think this is related to the input I am giving to the vectorizer. I cannot fit the vectorizer again or train the classifier again as I am using both in an app - so it needs to work somehow with the saved versions."
3681,TF/IDF Measurement with using MySQL,"['mysql', 'tf-idf', 'tfidfvectorizer']","i have following datas in my mysql weightallofwordsintopic table.Topic Name         Word   WordCount
20160401-20160405   ahlak   954
20160401-20160405   çocuk   825
20160401-20160405   kadın   764
20160401-20160405   tecavüz 710
20160401-20160405   güzel   701
20160401-20160405   hayat   670
20160401-20160405   bakan   661
20160401-20160405   zaman   585
20160401-20160405   adam    494
20160401-20160405   çalış   453
20160406-20160407   kandil  4927
20160406-20160407   mübarek 2906
20160406-20160407   hayır   2342
20160406-20160407   çocuk   1893
20160406-20160407   güzel   1835
20160406-20160407   regaip  1574
20160406-20160407   allah   1536
20160406-20160407   tecavüz 1457
20160406-20160407   kadın   1442
20160406-20160407   hayat   1436
20160408-20160409   güzel   2385
20160408-20160409   hayat   2187
20160408-20160409   hayır   1972
20160408-20160409   zaman   1902
20160408-20160409   cuma    1589
20160408-20160409   allah   1550
20160408-20160409   gece    1233
20160408-20160409   adam    1198
20160408-20160409   saat    1153
20160408-20160409   dünya   1130
20160410-20160411   stat    1993
20160410-20160411   güzel   1854
20160410-20160411   hayat   1579
20160410-20160411   şampiyon 1464
20160410-20160411   taraftar 1426
20160410-20160411   zaman   1380
20160410-20160411   adam    1336
20160410-20160411   çalış   1297
20160410-20160411   saat    1283
20160410-20160411   başkan  1112
i would like to measure the tf/idf frequency for each word in each topic. Suppose a topic with same name as one document so i need to measure tf/idf frequency for all words seperately. I need mysql query for this.WordCount is the occurency of that words in that topic. My table is too large i just wrote a sample to explain my problem. I need a query to do this work. Thank you very much."
3682,How can I create a tf-idf matrix with character n-gram features?,"['tf-idf', 'text2vec']",How can I use the text2vec package to create a tdf-idf matrix with character n-gram features? 
3683,Associate Class With the Vectorizing Words,"['python', 'python-3.x', 'tensorflow', 'tf-idf']","I'm doing Tensorflow Classification on a list of tweets and their Classes, the problem is after splitting the tweet into words then vectorizing it using TF-IDF, the length of words are bigger than the length of the classes. (DataFrame ""example"" imported from CSV):(converting words to TF-IDF code):If I print(tweet_tfidf) the output:Classify the output:( Tweet ID, Word ID )  Word Weightthe type(tweet_tfidf) is: In the tensorflow, you should have training text and training class .. i have the training text and i don't have the training class. 
I want to have a DataFrame with the Word Weight associated with the correct class, like this for example: ( Tweet ID, Word ID ) ... Word Weight ... Class"
3684,finding the number of clusters in a vectorized text document with sklearn tf-idf,"['python', 'optimization', 'scikit-learn', 'cluster-analysis', 'tf-idf']","I'm trying to cluster dialogs using sklearn tf-idf and k-means. I calculated the optimal number of clusters using a silhouette score, but it increases almost linearly. So, are there other ways or maybe I'm doing something wrong?Code:"
3685,Attempting to 'Fit_Transform()' DataFrame results in “Input Variables with Inconsistent Numbers of Samples” Error,"['python', 'pandas', 'scikit-learn', 'vectorization', 'tf-idf']","I'm reading two columns of a .csv file into a Pandas dataframe using pandas.read_csv(). The head of the Dataframe is shown below:Following this, I call df.dropna(inplace=True)(thanks to Brad Solomon) to allow the coming fit/transform calls to proceed without producing a 'MemoryError' as shown in my previous question here.Now that I have a memory-friendly form of Dataframe, I use SKLearn's train_test_split() to create four sets of data that I intend to use for fitting/transforming on to a Pipeline.The shape of these variables is:So, I have my data split into appropriate subsections for testing and training. I then create my Pipeline, which makes use of TfidfVectorizer, SelectKBest and LinearSVC as shown below:Finally, we come across the error mentioned in the title when I attempt to call fit_transform() on the aforementioned X and y training data...which in turn produces the error:ValueError: Found input variables with inconsistent numbers of samples: [2, 1785]The full Traceback can be viewed here."
3686,CSV file with label,"['python', 'csv', 'export-to-csv', 'tf-idf', 'sklearn-pandas']","As suggested here Python Tf idf algorithm I use this code to get the frequency of words over a set of documents.With the last line, I create a csv file where are listed all words and their frequency. Is there a way to put a label to them, to see if a word belong only to the third document, or to all?
My goal is to delete from the csv file all the words that appear only in the 3rd document (book3)"
3687,Understanding TF-IDF scores,"['python-3.x', 'scikit-learn', 'feature-extraction', 'tf-idf', 'tfidfvectorizer']","I am trying to understand how tf and idf scores are calculated when we vectorize a text document usign TfidfVectorizer. I am understanding how tf-idf ranks in 2 ways, which I am writing below. Could someone please explain if any of my understanding is correct? And if not please correct what is wrong in my understanding."
3688,How to convert an RDD (that read in a directory of text files) into dataFrame in Apache Spark in Scala?,"['feature-extraction', 'tf-idf']","I'm developing a Scala feature extracting app using Apache Spark TF-IDF. I need to read in from a directory of text files. I'm trying to convert an RDD to a dataframe but I'm getting the error ""value toDF() is not a member of org.apache.spark.rdd.RDD[streamedRDD]"". This is what I have right now ...I have spark-2.2.1 & Scala 2.1.11. Thanks in advance.Code:Data: (Text files like these in a folder is what I need read in) 
      https://www.dropbox.com/s/cw3okhaosu7i1md/cars.txt?dl=0
https://www.dropbox.com/s/29tgqg7ifpxzwwz/Italy.txt?dl=0"
3689,Memory Error when attempting to apply 'fit_transform()' on TFidfVectorizer containing Pandas Dataframe column (containing strings),"['python', 'pandas', 'csv', 'scikit-learn', 'tf-idf']","I'm attempting a similar operation as shown here.
I begin with reading in two columns from a CSV file that contains 2405 rows in the format of: Year e.g. ""1995"" AND cleaned e.g. [""this"", ""is, ""exemplar"", ""document"", ""contents""], both columns utilise strings as data types.I have already pre-cleaned the data, and below shows the format of the top 4 rows:Then I initialise the TfidfVectorizer:Following this, calling upon the below line results in:I overcame this using the solution in the aforementioned thread:however, this resulted in a Memory Error (Full Traceback).I've attempted to look up storage using Pickle to circumvent mass-memory usage, but I'm not sure how to filter it in in this scenario. Any tips would be much appreciated, and thanks for reading.[UPDATE]@pittsburgh137 posted a solution to a similar problem involving fitting data here, in which the training data is generated using pandas.get_dummies(). What I've done with this is:I thought I should update any readers while I see what I can do with this development. If there are any predicted pitfalls with this method, I'd love to hear them."
3690,Cosine similarity for special vectors (only one component),"['python', 'python-3.x', 'tf-idf', 'cosine-similarity']","I'm trying to implement cosine similarity for two vectors, but I ran into a special case where the two vectors only have one component, like this:Here is my implementation for the cosine similarity:The cosine similarity for any two vectors that only have one component is always 1 then. Can someone guide me through how to handle this special case? Thank you."
3691,How to leverage affinity propagation using sklearn for large dataset,"['python', 'machine-learning', 'scikit-learn', 'cluster-analysis', 'tf-idf']","I used to affinity propagation for creating clusters for my dataset, my dataset is pretty huge. I converted my dataset which is text to vectors using tfidf and then fit into affinity propagation. For the smaller dataset affinity works seamlessly, however, for the large dataset it starts consuming a hell lot of RAM and finally, my OS kills the process. I did a lot of research and found few StackOverflow, it said using np.medien or np.mean etc.. but didn't work for my problem. I also principle analyse component and try to reduce the matrix, still consumed a lot of RAM. 
I came across leveraging affinity propagation - This addresses large data sets by doing randomly selected points for similarity rather than an NXN, but couldn't find in sklearn or python. 
I saw it R language.
enter link description hereHow do we do it in python where I am using affinity propagation from sklearn. Here is my piece of code of affinity what I am trying.Please let me know if I am doing something worng... and help out figure out how to achive for large dataset."
3692,TF-IDF vs XGBoost vs CNN,"['machine-learning', 'deep-learning', 'conv-neural-network', 'tf-idf']","I have a Natural Language Dataset where I am performing classification.The tf-idf model always performs better than a Convolutional Neural Network model.I have performed a lot of hyperparameter tuning but the performance of the tf-idf model remains better.In fact, the tf-idf model is performing better than a XGBoost model as well.Can we safely say that in certain scenarios basic and primitive Machine Learning Models can perform better than Deep Learning models?"
3693,C# Accord.net. text classfication,"['c#', 'machine-learning', 'tf-idf', 'text-classification', 'accord.net']","I have unknown number of columns in my TFIDF vector.
my clasificaton code is:i don't know how to pass unknown no of columns for input."
3694,How to let TF-IDF learn a part of a document with higher priority?,"['nlp', 'tf-idf']","I use TfIdf from sklearn.I want to learn similarity between documents. However, these documents contain a title that brings more information than other parts of the documents.Is it possible to tell TF-IDF that, for instance, if there is a word appears in the title it should be more important than the same word elsewhere?Thanks"
3695,How do you rank tf idf values to use,"['python', 'rank', 'tf-idf', 'nested-queries']","Say that you have generated a tf idf value for each term in a documentBut when it comes to queries of multiple terms for example ""hey well done"" It'll return multiple tf idf values for each document, for example....How would I rank this or which tf idf value would I use? Appreciate any responses, thank you!"
3696,NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted python,"['python-3.x', 'machine-learning', 'svm', 'tf-idf', 'predict']","Goal: Predict labels on my original dataBackground: I constructed an SVM classifier I am using the following code: 0) Import modules1) X_list and y2) convert X_list from string to float, use tfidf3) X shape4) 10 fold validation and SVM5) loop through 10 folds6) Predict on original dataset X_original7) Convert X_original from string to float But when I do so I get the following ErrorSO has a similar question but it seems different from my issue NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted8) How do I fix this error?"
3697,Scikit - TF-IDF empty vocabulary,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I have to calculate the distance/similarity of two or more texts. Some texts are genuinely really small or do not form proper english words etc, ""A1024515"". This means that it should accept every single word in the list. As a test case, I have used the following list as a corpus of words. However, I get the following error How can I ensure that the list is accepted as possible words and ensure stop words are not removed from the corpus?"
3698,"TF-IDF + KNN for text classification, what method should i use to numerize the test text?","['algorithm', 'knn', 'tf-idf']","So i wanted to use TF-IDF for review classification, i'm doing it by doing the algorithm on a bunch of reviews that i myself already categorize into 6 category of review. As usual, i do TF on each document and IDF or every document, different category included.So if i get a document to test, where i know the content but not the category, how can i numerize that one document so i can compare it to my training corpus?I know i can't use IDF because the test file only consist of 1 document, should i use only TF? Or there's any different way to solve this?Also do note that i can't change the algorithm i use here (requested personally), so i need to numerize that one single test document. Thanks before!"
3699,After vectorize the text the output shown is not full Python,"['python', 'python-3.x', 'vectorization', 'tf-idf']",I'm dealing with almost 7000 strings and when I vectorize it correctly the output shown the head and the tail of the output  the output is how can I print the full output without this ( : : ) thing in the middle?
3700,TfidfVectorizer results in memory error,"['python', 'python-3.x', 'out-of-memory', 'tf-idf', 'tfidfvectorizer']","I've got a pandas dataframe (""vector"") with one column and 178885 rows holding strings with up to 600 words each.I'm doing feature extraction (unigrams) using the TfidfVectorizer:Unfortunately I'm receiving a Memory Error. I'm using the 64bit version of python 3.6 with 16GB RAM on my windows 10 machine. I've red alot about python generators etc. but I can't figure out how to solve this problem without limiting the number of features (which is not really an option). Any ideas how to solve this? Could I somehow split my dataframe before?Thanks!Edit:"
3701,Retrieve TF-IDF Values for Textual Documents in CSV File,"['python', 'pandas', 'csv', 'scikit-learn', 'tf-idf']","I have a CSV file with two columns (no header), held in a variable known as 'dataset': I'm attempting to construct a Bag of Words model using Scikit-learn and gather the weightings using TF-IDF. However, I'm having difficulty with obtaining actual results, as the output of the code below returns 2480 rows (correct) * 346862 columns (corrected by @Jarad). I would appreciate someone helping me decipher these results, and point me in the right direction as to their formatting (to provide clarity) or correction (to provide validity) so that I can progress towards the later stages of a Bag of Words model implementation.Python Code:Output:Should I tokenize the document prior to storing it in the CSV file? I decided against it due to the fact that I would hope to analyse sentence structure at a later stage as well."
3702,Tf Idf over a set of documents to find words relevance,"['java', 'python', 'tf-idf', 'words', 'tfidfvectorizer']","I have 2 books in txt format (6000+ lines). I would like to associate, using Python,  to each word its relevance (using td idf algorithm) and order them in descending order.
I tried this code that I found here https://stevenloria.com/tf-idf/ with some changes, but it takes a lot of time and after some minutes, it crashes saying TypeError: coercing to Unicode: need string or buffer, float found.
Why?I also tried to call this Java program through python https://github.com/mccurdyc/tf-idf/. The program works, but the output is incorrect: there are a lot of words that should have a high relevance level that are instead categorized with 0 relevance.Is there a way to fix that Python code?
Or, can you suggest me another tf-idf implementation that does correctly what I want?"
3703,How to find Top n topics for a document,"['information-retrieval', 'tf-idf', 'lda', 'topic-modeling', 'top-n']","I am using the tf - IDF to rank terms in a document. When terms are arranged in descending order of the tf - IDF, top 'n' terms are most relevant to that document.
When we choose a document, top 'n' terms of that document has to be displayed.
My question is how to decide the value of 'n'?
For example: for a document terms arranged in descending order of the tf - IDF is as follows:Document 1Now when I want to show topics for document 1, I need only top 5 terms, since all others are not relevant or not topics for the document.
How do I decide this breaking point of terms in a document?
Thanks in advance"
3704,Does gensim.model.TfidfModel have the term frequency saved?,"['python', 'nlp', 'counter', 'gensim', 'tf-idf']","Does gensim.model.TfidfModel have the term frequency saved?From the docs, they use the formula:And when I prob the attributes of the dir(model) (TfidfModel object) with the following code:I'm getting:But I can't seem to find where are the term frequencies stored. If the term frequencies are not saved, is there a reason why? Since it's already stored to compute the weights anyways. Is there a way to retrieve the term frequencies somehow during the fitting process?"
3705,Text Classification with scikit-learn: how to get a new document's representation from a pickle model,"['machine-learning', 'scikit-learn', 'logistic-regression', 'tf-idf', 'document-classification']","I have a document binomial classifier that uses a tf-idf representation of a training set of documents and applies Logistic Regression to it:I save the model in pickle and used it to classify new documents:How can I get the representation (features + frequencies) used by the model for this new document without explicitly computing it?EDIT: I am trying to explain better what I want to get.
Wen I use predict_proba, I guess that the new document is represented as a vector of term frequencies (according to the rules used in the model stored) and those frequencies are multiplied by the coefficients learnt by the logistic regression model to predict the class. Am I right? If yes, how can I get the terms and term frequencies of this new document, as used by predict_proba?I am using sklearn v 0.19"
3706,TfidfVectorizer vs. definition of tf-idf,"['python', 'scikit-learn', 'tf-idf']","For a tutorial, I want to implement manually what the TfidfVectorizer is doing, just to show what's going on in the background. In this Stack Overflow article I found how the TfidfVectorizer works. With this, it was straightforward to implement it in a naive manner, and with the correct parameter settings for the vectorizer, the output is indeed the same. All good.However, now I'm a bit confused: The TfidfVectorizer calculates the term frequency tf using the CountVevtorizer. That means tf is just an integer representing the number of occurrences of a term in a document. But usually the term frequency tf(t,d) of term t in a document d is defined as:So the term frequency is a value between 0 and 1.How does this fit together? Why is using the TfidfVectorizer the term count and not the (normalized) frequency according to the definition. I assume it's no a big deal but I would to understand it."
3707,BM25 similarity with binary term frequencies in Elasticsearch,"['elasticsearch', 'tf-idf']","Does anybody tried to customize BM25 similarity used in Elasticsearch in a following way?
This is a common BM25 score. I want term frequencies to be binary (0 if a term is not presented in the document and 1 if term frequency in the document if greater 0). So in the pic below I want tf(q_i, d) to be {0, 1}.
Any ideas what is the easiest way to achieve this in Elasticsearch?"
3708,Is there a way to get only the IDF values of words using scikit or any other python package?,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","I have a text column in my dataset and using that column I want to have a IDF calculated for all the words that are present. TFID implementations in scikit, like tfidf vectorize, are giving me TFIDF values directly as against just word IDFs. Is there a way to get word IDFs give a set of documents?"
3709,"Appending 2 dimensional list (dense output of tfidf result) into pandas dataframe rows, each index","['python', 'pandas', 'dataframe', 'tf-idf', 'tfidfvectorizer']","I have the below output after tfidf vectorizer. I want to parse dense output into a pandas dataframe column but I couldn't directly apply toarray or todense function to sparse tfidf output and pass it to pandas dataframe column. So I received the dense output of tfidf results into a list. Now the list is of shape (6,20) and I want to parse each row of list iteratively into  rows in a pandas dataframe column as length of dataframe column is also 6. I tried converting list into pandas series and pass it to dataframe but didn't work on 2 dimensional list."
3710,How to convert Data Frame to Term Document Matrix in R?,"['r', 'dataframe', 'type-conversion', 'tf-idf']",I have a table (Data frame) myTable with a single column as follows:I want to convert it to a Term Document Matrix in R. I did this:but I got this error:I googled and couldn't find anything. How can I do this conversion?
3711,TF-IDF for data filtering,"['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']","I've a list of raw document, already filtered and removed english stopwords:and I've used But I got aand I cant interpret the result. So, am I using the right tool or have I to change the way?My goal is to get the relevant word in each document, in order to perform a cosine similarity with other words in a query document.Thank you in advance."
3712,How TF-IDF handles missing values?,"['python-3.x', 'tf-idf', 'text-classification']","I am working on a classification problem in which I have to classify product category based on the information of the product like title, description and other attributes.
It is working for different categories but getting biased in closed categories like mobile and mobile accessories.Let's say I have a csv in which I have the following columns:Now, If I have two categories say mobile and fashion. For mobile, Suitable For column would be NA and for fashion RAM, Processor would be NA. So, I wanted to know how TF-IDF works on those columns and also If there is a way to do TF-IDF on individual columns for different categories and later merge them. I found few articles, but none related to my specific problem.Any related articles would do. I just want to understand the working so that I can implement it.Any help would be appreciated. Thanks..!"
3713,What is the expected return type for tokenizer that is passed as parameter into Tfidfvectorizer,"['scikit-learn', 'vectorization', 'tf-idf', 'tfidfvectorizer']","I am looking at: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.htmlIt just says:tokenizer : callable or None (default) Override the string
  tokenization step while preserving the preprocessing and n-grams
  generation steps. Only applies if analyzer == 'word'.What should the function return ? A list of words ? Is there an example somewhere showing the data structure that must be returned by this function ?EDIT: For example, if I am writing my own custom tokenizer function, what should it return."
3714,TfIDf Vectorizer weights,"['python', 'nlp', 'nltk', 'data-analysis', 'tf-idf']",Hi I have a lemmatized text in the format as shown by lemma. I want to get TfIdf score for each word this is the function that I wrote:The output I am getting by:isThe result of tf_dataframeShouldn't both approaches lead to the same result of top features? I just want to calculate the tfidf scores and get the top 5 features/weight. What am i doing wrong?
3715,Ranking tf-idf scores in an array in R,"['r', 'tf-idf']","I wrote the following the function to determine the tf-idf of a document:Determine tfDetermine idf valueAnd finally - determining the tf-idfI can now use this to determine the tf-idf value of fe these documents:And find the tf-idf value of ""films"" in document1However what I would like to now is loop over all words in all the documents to determine what are the highest rating words of the document.So for document 1 a little like:But then the values should be stored in an array and ranked. A little like this in python:Any thoughts on how this could be done most efficiently in R?"
3716,Using self. in a class with Gensim TfIDF,"['python', 'class', 'gensim', 'tf-idf']","I have a class TfidfRecommendations with several methods and inputs. Some of the inputs are the trained model objects of Gensims TfIDF model (the function train_tfidf below):Where data is a pandas dataframe of text documents (one document per row).I use the output from train_tfidf above, as input into TfidfRecommendations:the problem with the class TfidfRecommendations is that it returns a list of tuples for sims which is incorrect:Whereas it should return a numpy array of length len(data) with each entry being a cosine similarity between query_text and each row in data. This works fine if get_sims is its own independent function outside of the class TfidfRecommendationsWhat is going wrong here? Why can't gensim model objects be used with self. inside a class? Any help would be much appreciated."
3717,Counting matrix pairs using a threshold,"['python-2.7', 'nltk', 'tf-idf', 'sklearn-pandas']","I have a folder with hundreds of txt files I need to analyse for similarity. Below is an example of a script I use to run similarity analysis. In the end I get an array or a matrix I can plot etc.I would like to see how many pairs there are with cos_similarity > 0.5 (or any other threshold I decide to use), removing cos_similarity == 1 when I compare the same files, of course. Secondly, I need a list of these pairs based on file names. So the output for the example below would look like:1and[""doc1"", ""doc4""]Will really appreciate your help as I feel a bit lost not knowing which direction to go.This is an example of my script to get the matrix:Out:"
3718,TF-IDF extracting keywords,"['machine-learning', 'scikit-learn', 'nlp', 'tf-idf']","Working on function somewhat like this:Which return the list of keywords in the passed text.
Is there any way to get the keywords in the same case as it is provided?
Like passed string Input:Instead of getting keyword list as:I like to get:"
3719,Computing TF-IDF on the whole dataset or only on training data?,"['machine-learning', 'tensorflow', 'scikit-learn', 'nlp', 'tf-idf']","In the chapter seven of this book ""TensorFlow Machine Learning Cookbook"" the author in pre-processing data uses fit_transform function of scikit-learn to get the tfidf features of text for training. The author gives all text data to the function before separating it into train and test. Is it a true action or we must separate data first and then perform fit_transform on train and transform on test?"
3720,sklearn tfidfvectorizer: how to intersect a tfidf frame on a column?,"['python-3.x', 'scikit-learn', 'tf-idf', 'sklearn-pandas']","In R, I can extract rows (documents) which contain a particular term, say 'toyota' by intersecting a document term matrix (dtm) with required column name like so:The problem is that I can't find an equivalent method in Python sklearn package. so I go about it in a roundabout way:MVP here:rows_to_keep=tfidf_df[tfidf_df.toyota.notnull()].index
data=my_df.loc[rows_to_keep,:]
print(data.shape)This works. Problem is how do I pass an iterator to this statement?Then for zentity in car_make:rows_to_keep=tfidf_df[tfidf_df.zentity.notnull()].indexdoes not work.AttributeError: 'SparseDataFrame' object has no attribute 'zentity'I purposefully chose zentity to avoid equivalence with any column name in the tfidf.Is there a clean way to make the intersection and extract only rows where column is not null (NaN)? Any help will be appreciated."
3721,Lucene calculate average term frequency,"['solr', 'lucene', 'similarity', 'information-retrieval', 'tf-idf']","I am currently implementing a modification of Lucene's standard BM25 similarity, based on the following paper. The implementation of the actual formula is straightforward, but I am struggling with the computation of the necessary statistics.I need the following two statistics:I found out, that I can calculate the per-document average term frequency at indexing time by overriding the computeNorm method of my Similarity implementation. I can store the value alongside the norm value by bit-operations (not exceptionally pretty, but so far it works). At query-time I can then extract the document's average term frequency and length.However, this does not help finding the mean average term frequency. It is obviously a collection-wide value and should therefore be computed in Similarity.computeWeight as far as I understand, but I don't see how this could be done given the arguments of the function.Which would be the ideal place for calculating these statistics?I am new to Lucene, so it may be that there is an obvious solution which I did not yet see. I am grateful for any input."
3722,Use features based on tf idf score for text classification using naive bayes (sklearn),"['scikit-learn', 'tf-idf', 'text-classification', 'feature-selection', 'naivebayes']","I am learning to implement text classification (into two classes)  using tfidf and naive bayes by referring to this blog and sklearn tfidfbelow is the code snippet:The above code is working as expected.I have read the documents, but I am still confuse about min_df and max_df.How to use the features for the classification based on the tf-idf score, i.e. filter the features based on tf-idf score 
eg. "
3723,Which formula of tf-idf does the LSA model of gensim use?,"['gensim', 'tf-idf', 'latent-semantic-indexing', 'latent-semantic-analysis']","There are many different ways in which tf and idf can be calculated. I want to know which formula is used by gensim in its LSA model. I have been going through its source code lsimodel.py, but it is not obvious to me where the document-term matrix is created (probably because of memory optimizations).In one LSA paper, I read that each cell of the document-term matrix is the log-frequency of that word in that document, divided by the entropy of that word:However, this seems to be a very unusual formulation of tf-idf. A more familiar form of tf-idf is:I also notice that there is a question on how the TfIdfModel itself is implemented in gensim. However, I didn't see lsimodel.py importing TfIdfModel, and therefore can only assume that lsimodel.py has its own implementation of tf-idf."
3724,Efficient string matching in Apache Spark,"['python', 'apache-spark', 'pyspark', 'string-matching', 'fuzzy-search']","Using an OCR tool I extracted texts from screenshots (about 1-5 sentences each). However, when manually verifying the extracted text, I noticed several errors that occur from time to time.Given the text ""Hello there 😊! I really like Spark ❤️!"", I noticed that:1) Letters like ""I"", ""!"", and ""l"" get replaced by ""|"".2) Emojis are not correctly extracted and replaced by other characters or are left out.3) Blank spaces are removed from time to time.As a result, I might end up with a string like this: ""Hello there 7l | real|y like Spark!""Since I am trying to match these string against a dataset including the correct text (in thise case ""Hello there 😊! I really like Spark ❤️!""), I am looking for an efficient way how to match the string in Spark.Can anyone suggest an efficient algorithm for Spark which allows me to compare the extract texts (~100.000) against my dataset (~100 million)?"
3725,Python TF-IDF product,"['python', 'python-3.x', 'tf-idf']","I'm trying to create the TF-IDF from my TF_norm matrix and IDF vector. I know that they don't have the same dimensions, so I'm lost at how I can multiply the two together. Do I need to add reduce something with the TF_norm matrix or convert the IDF vector? Completely lost from here.    "
3726,tensorflow cosine_similarity using sparse_placeholder?,"['python', 'tensorflow', 'scikit-learn', 'tf-idf', 'cosine-similarity']","I am using tfidf in python to vectorize two large corpus of text and then compute the cosine similarity.........but it is very slow.I would like to do cosine similarity using tensorflow in an effort to speed up the cosine similarity computation. TFIDF creates numpy sparse matrices so I understand I must load these using a sparse_placeholder:but I get:when running s = tf.losses.cosine_distance(tf.nn.l2_normalize(a, 0), tf.nn.l2_normalize(b, 0), dim=0) i.e. the normalize function is complainingCan I load sparse_placeholders into the normalize function?"
3727,Scikit learn Custom Transformer dimension mismatch,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I'm coming from R, so scikit API still very confusing to me. I was following this tutorial http://michelleful.github.io/code-blog/2015/06/20/pipelines/ to learn about Pipelines. So let's create a fake dataset just for reference:My goal is very simple: train a linear regression on y, using separate tfidf matrices from x1 and x2, plus some custom features from both x1 and x2 (ie, word length, etc).Let's start with the simpler task of using only tfidf from x1. Here's the full code:I'm getting the error ValueError: dimension mismatch, probably because some terms will not appear in both train/validation folds. What's the proper way of doing this? Thank you!"
3728,ValueError: setting an array element with a sequence - after making TF_IDF vectorization,"['pandas', 'scikit-learn', 'nlp', 'tf-idf']","I'm new to data science and NLP. I want to perform TF_IDF vectorization on some text documents and after use the results to train different machine learning models. But when I try to train SVC model I obtain the ValueError: setting an array element with a sequence. Here is my code. And I got an error on the line model.fit(X_train, y_train)I have already searched other similar questions and I found one where they advise using .toarray() method to transform sparse matrix into np.array. But this didn't help me. "
3729,Get the most important words in the corpus using tf-idf (Gensim),"['python', 'gensim', 'tf-idf']","I am calculating tf-idf as follows.However, now I want to identify the most 3 important words in my corpus using the words that has the highest idf values. Please let me know how to do it?"
3730,Error of using tfidf as the input of method 'model.fit()' in package lda,"['python', 'tf-idf', 'lda']","I was using package LDA in python to analysis the topics in documents, it's ok when I use term frequency as the input of method 
 'model.fit()',however, there is always a error, saying that 'TypeError: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'******',using Tfidf as the input of method 'model.fix()' in the package LDA.
The code is as follows:(Every line in papers.txt is specific terms as the representation of each document)It seems that package LDA's method 'model.fit()' can only receive a matrix with the type of integer, not float? I have checked the documentation of package lda(https://pypi.python.org/pypi/lda), unfortunately, there is no specific discussion about the  detaills. Does anyone had met the same problem? How do you fix these? Thanks in advance!"
3731,Issues in calculating tf-idf in gensim,"['python', 'gensim', 'tf-idf']","I am using Gensim to calculate tf-idf scores for my corpus mentioned below.My current code is as follows.However, I get the error RecursionError: maximum recursion depth exceeded while calling a Python object (PS: if my code is wrong I am happy to have a different code). Please help me to calculate tf-idf values for my current corpus. Moreover, I want to get the 3 terms that has the highest tf-idf score in my corpus.Please help me!"
3732,scikit-learn: Is it possible to force the matrix index values start at 1 and not at 0?,"['python', 'scikit-learn', 'tf-idf']","I'm using scikit-learn to create tf-idf. So, here's an excerpt of what I'm doing: def create_tm(content):Here's my question -- Is it possible to force the matrix index values start at 1 and not at 0?"
3733,TF_IDF calculation sending errors,"['python', 'python-3.x', 'python-2.7', 'tf-idf']","I am currently working on a small project and have hit a complete mind blank, i have the following code to calculate term frequency:
    from Bag import *I also have the following code for inverse document frequency:and the error i get is carDoc not definedThese are fine and work as i intended them too, but when it comes to implementing the tfidf function i keep getting errors. Any help in resolving the tfidf for this example will be appreciated"
3734,How to transform new data using sklearn.pipeline,"['python-2.7', 'scikit-learn', 'tf-idf', 'multilabel-classification']","I have created a pipeline with TfIdfVectorizer transformer and OnevsRestClassifier estimator and trained it on training data as followsNow, I want to use the trained model to predict classes for new unlabeled data.
I tried this and got an errorThis is the error. What Am I doing wrong?"
3735,spark similarities between text sentences,"['apache-spark-mllib', 'similarity', 'tf-idf', 'sentence-similarity']","I'm trying to find similarity between text messages (about 1 million text message), in my implementation each line represents an entry.In order to calculate similarity between those texts we adopt tfidf and columnSimilaritiesBelow is the code:The problem is when the number of similar messages increases in file, the similarity decreases.e.g.let's assume that we have the file below:The output of the former command is:%cat output/part-000*Each line in the output represents the similarity between two lines as follows:
""lineX -1"", ""lineY -1"", ""similarity""The output showing the similarity between the last 2 lines is 5.0,6.0,0.7373482646933146, which is fine.The two lines areand similarity is 0.7373482646933146while when the file input is:the output is: the output between the same lines tested in the first example is: 7.0,8.0,0.5733398760974173the similarity had decreased from 0.7373482646933146 to 0.5733398760974173 for the sames linethe two lines are:and similarity is 0.5733398760974173i.e the input above, contains multiple sentences like:same for the sentences like: could they be grouped based on similarities output?"
3736,"TypeError: list indices must be integers or slices, not str when using nested dictionaries","['python', 'python-3.x', 'tf-idf']",I'm creating an inverted index of text files stored locally using nested dictionaries. An abstract structure of the inverted index is below (the values are integer numbers). In any word value of key '0' is the idf and value of key '1' is the tf.And this is the code:but I get this error in the second last line:Can you please help me fix this bug. Thank you
3737,Error: 'list' object has no attribute 'lower',"['python', 'csv', 'error-handling', 'tf-idf', 'cosine-similarity']","So I have created this code to calculate the cosine similarity between two columns in two different csv files, both columns includes lines of jobs descriptions. So when I run the code I get this error, I included the whole traceback in a picture to be clear code tracebackcan anyone please help me with this ?"
3738,Fitting TfidfVectorizer - AttributeError / TypeError,"['python', 'python-3.x', 'vectorization', 'tf-idf']",I'm still growing in my knowledge of Python and am stuck with the TfidfVectorizer.  I've looked at some of the other questions but have not found anything which helped me so far.I'm trying to create a tfidf_matrix for a list of product descriptions but I am failing.Here is my code:I've tried doing the fit_transform with(tokens) I get the following error:With fit_transform with(tokens_line) I get:With fit_transform with(tokens_line_lists) I get:
3739,Is smooth_idf redundant?,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","The scikit-learn documentation saysIf smooth_idf=True (the default), the constant “1” is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.
However, why would df(d, t) = 0? If a term doesn't occur in any text, the dictionary wouldn't have the term in the first place, would it?"
3740,Wordcloud: Bigger font for lower tf-idf values,"['python', 'pandas', 'matplotlib', 'tf-idf', 'word-cloud']","I am trying to form a word cloud for the tf-idf values:Below are the tf-idf values in the dataframe. When I try to form a word cloud, the values with higher values, in this case ""seat - 2.57"" is shown in largest font. But I need the vice-versa. ""Nice - 2.088"" to have a bigger font as it is of higher importance. Below is the code:"
3741,Create a tf-idf matrix in MATLAB,"['matlab', 'matrix', 'tf-idf']","I'm trying to read some text files and store the term frequency in a matrix where each row should be every document and thus each column in each row is the weights for every term. Then each column in the matrix should correspond to a specific term when the matrix is completed. I've managed so far to do this, however my issue is that this only works when the dimensions of the documents are the same, i.e. the same amount of words in each document. Code as follows:What can I do to expand the matrix while still retaining the ""term structure"" when documents have different dimensions which of course is the real world scenario.I do not have access to the text analytics toolbox which has a built in function for doing this."
3742,implementing tf-idf in java with various applications,"['java', 'tf-idf', 'multifile-uploader']","i have multiple tasks in mind regarding which i guess aim all at one thing i.e finding TF-IDF. I shall give N no. of documents and get all the words/names etc with highest relevance.(i know how to upload documents, i want to know how to give multiple documents as input to analyse them and not save them somewhere)I know how to analyse one document with stanford NLP and i get all the names,dates, locations etc. i want to compare the results with other documents to see if those strings of locations,dates,names etc repeat in other documents as well. if they do, then what is the frequency.All i need is a bit of guidance through the code. If only someone can solve any one of the given points."
3743,Memory Eror : Multiplying two matrix,"['python', 'matrix', 'sparse-matrix', 'tf-idf']",I am stuck with the following issue: I have a very large sparse matrix which contains 13M+ nnz elements and a vector which size is approx. 99k+. The thing is when I iteratively multiply and save it in a crs matrix it works but takes 3 hours to complete. Thus I convert the vector into a diagonal matrix so I can directly multiply the two.However I am taking memory error all the time (8 GB ram with 64 bit operating system). Here is my code : Any advice about the issue?
3744,Understanding the matrix output of Tfidfvectorizer in Sklearn,"['python', 'matrix', 'scikit-learn', 'tf-idf']","I'm having trouble interpreting the matrix output for the Tfidf vectorizer. Given If I were to look at the output of X_train_tfidf, am I looking at a matrix that is structured like:Column 1 corresponds to document 1 where its elements are tfidf scores of the 10000 features, Column 2 corresponds to document 2... and so on?"
3745,Rapidminer- TF-IDF from csv dataset,"['machine-learning', 'tf-idf', 'rapidminer', 'text-analysis']","I have to calculate tf-idf of two columns of csv file..
Should i have to convert the rows into text files? or is there any method to calculate tf-idf from csv.how can i calculate tfidf of columns of csv file."
3746,Sklearn: using TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf']","I have data class 'scipy.sparse.csr.csr_matrix' looks likeI need to transform it.
First I try to usebut it doesn't improve value of roc_auc
And next I try to useBut it returns an errorHow can I fix that? "
3747,Transform RDD to valid input for kmeans,"['apache-spark', 'pyspark', 'k-means', 'apache-spark-mllib', 'tf-idf']","I am calculating TF and IDF using spark mllib algorithm of a directory that contains csv files with the following code:UsingI get this output:I have also tested the KMeans mllib algorithm :with this sample test caseand it works fine.Now I want to apply the rdd output from tfidf above in the KMeans algorithm but I don't know how is it possible to transform the rdd like the sample text above, or how to split properly the rdd in the KMeans algorithm to work properly.I really need some help with this one.UPDATEMy real question is how can i read the input to apply it to KMeans mllib from a text file like this UPDATE2I am not sure at all but i think i need to go from above vectors to the below array so as to apply it directly to KMeans mllib algorithm"
3748,Tf-Idf using cosine similarity for document similarity of almost similar sentence,"['tf-idf', 'cosine-similarity', 'sentence-similarity']","I am Using tf-idf with cosine similarity to calculate description(sentence) similarityInput string:Below are the sentences among which i need to find sentence similar to input stringAs the sentences are almost similar, I am using tf-idf approach which give low score to words that appear in all document( Idf ) and give more score to unique words which helps to find the similar document easier. is there any approach that works better than this?   "
3749,classifier predict_proba returns 1 and 0 only,"['machine-learning', 'scikit-learn', 'tf-idf']","I am trying to build a MultiLabelClassification model for a dataset which includes text fields (description) as well.
For the text fields I am using TfIdfVectorizer and creating a column for each word. Even after stemming and using stop words, I have thousands of rows (for each textfield, I create a new TfIdfVectorizer) 
As a result of having so many columns, with DecisionTreeClassifier model I have the predict_proba function returns only 1's and 0's as probability scores. 
The part of my code which I build the vectors is as followslater on, I append nameData and textData to the actual data frameI couldn't think of a way to reduce column numbers, any suggestion on handling text fields is welcome
Thanks in advance"
3750,Python: compare items within two different tfidf matrices of different dimensions,"['python', 'scikit-learn', 'tf-idf', 'cosine-similarity']","I want to use TfidfVectorizer() on a file that contains many lines, each a phrase. I then want to take a test file with a small subset of phrases, do TfidfVectorizer() and then take the cosine similarity between the original and the test file so that for a given phrase in the test file, I retrieve the top N matches within the original file. Here is my attempt:However I get:I wouldn't mind combining both files and then transforming, but I want to b sure I do not compare any of the phrases from the test file with in of the other phrases within the test file.Any pointers?"
3751,MultinomialNB() predicting the same category for all test documents,"['python', 'scikit-learn', 'tf-idf']",I have a bunch of documents grouped into about 350 classes. I'm trying to build a TF-IDF multinomial model to predict the class of a new document. Everything seems to be working fine EXCEPT that the test prediction takes on only one value (even if I run the test on thousands of documents). What am I missing? Here's the relevant code:
3752,getting top words from the tf-idf sparse matrix (highest tf-idf value),"['python', 'feature-extraction', 'tf-idf', 'sklearn-pandas']","I have a list of size 208 (208 arrays of sentences), that looks like:I want to get the words with the highest tf-idf values.
I created a tf-idf matrix:Now I don't know how to get the words with the highest tf-idf values.Each column of the dense_tfidf  represents a word/2-words. (the matrix is 208x5481)When I summed each column, it didn't really help - got the same result of a simple top words (I guess because it's the same as a simple word count).How can I get the words with the highest tf-idf value? Or how can I normalize it wisely? "
3753,Does gensim.corpora.Dictionary have term frequency saved?,"['python', 'dictionary', 'frequency', 'gensim', 'tf-idf']","Does gensim.corpora.Dictionary have term frequency saved? From gensim.corpora.Dictionary, it's possible to get the document frequency of the words (i.e. how many document did a particular word occur in):[out]:And there is the filter_n_most_frequent(remove_n) function that can remove the n-th most frequent tokens: filter_n_most_frequent(remove_n)
  Filter out the ‘remove_n’ most frequent tokens that appear in the documents.After the pruning, shrink resulting gaps in word ids.Note: Due to the gap shrinking, the same word may have a different word id before and after the call to this function!Is the filter_n_most_frequent function removing the n-th most frequent based on the document frequency or term frequency? If it's the latter, is there some way to access the term frequency of the words in the gensim.corpora.Dictionary object?"
3754,Calculate tf-idf in Gensim for my vocabulary,"['python', 'gensim', 'tf-idf']","I have a set of words (n-grams) where I need to calculate tf-idf values. These words are;My corpus looks as follows.I am currently getting tf-idf values for my n-grams in myvocabulary using sklearn as follows.However, I am interested in doing the same in Gensim. Forall the examples I came across in Gensim;Hence, please help me to find out how to do the above two things in Gensim."
3755,BigQuery / DataPrep: Efficient way to extract word counts; to convert HTML to plaintext,"['google-bigquery', 'google-cloud-dataflow', 'tf-idf', 'google-cloud-dataprep']","I have a table of ~4.7M documents stored in BigQuery. Some are plaintext, some HTML. They're around 2k tokens   per, with wide variation. I'm mainly using DataPrep to do my processing.I want to extract those tokens and calculate TF-IDF values.One of the more time-intensive steps is taking this:And turning it into this:One way to do it is this:However, steps 2 & 3 are very slow, especially with large documents.Ideally, I'd be able to have a function that converts [""foo"", ""bar"", ""foo"", ""baz""] into {""foo"":2, ""bar"":1, ""baz"":1}. That wouldn't require the flatten-then-group operation to extract the count, and the subsequent flatten would be smaller (since it's operating on unique terms rather than each term).I've not figured out any way to do that in DataPrep, however. :-/What's a more efficient way to do this?My source data is a combination of plaintext and html. Only about 800k of the 3.7M documents have plaintext available.I'd like to convert the html to plaintext in some reasonable way (e.g. the equivalent of Nokogiri #content) that would work at this scale, so that I can then do token extraction on the result.I could spin up a cluster that does bq query, ingests the html, processes it with nokogiri, and outputs it to a processed table. But that's kinda complicated and requires a lot of i/o. Is there an easier / more efficient way to do this?"
3756,Keyword Extraction & Computation from multiple URLs,"['r', 'tf-idf', 'text-extraction']","I have a project to extract keywords from URLs generated from a Search Query using R. Then, identify the most frequent keywords, compute for the TF-IDF, etc. of these extracted keywords.Being new to R, I have tried the following approach. I used two different links then I:STEP 1: I did keyword extraction using this code: Web Scraping and Text Mining in R. I ran this code twice because I am extracting 2 URLs by changing the links in the getURL() in the code. RESULT: I have 1 dtm for each URL extracted.STEP 2: To compute for the tf-idf, I analyzed and used Chapter 3 in this document: http://tidytextmining.com/tfidf.html. I patterned my data based on the document by: The objective is to extract keywords from the URLs generated from a Search Query. I have already generated URLs by using the following code: How to get google search results. (see below for the snippet of code)Once extracted, identify the number of occurrence of these keywords, the most frequent keywords used, then compute for the TF-IDF of these keywords. As a beginner, this is the best I could come up with (i did really try though), but I definitely think that there's a better approach in doing this rather than doing Step 1 and Step 2 for every URLs.Your help and/or feedback on this is greatly appreciated."
3757,How to print tf-idf scores matrix in sklearn in python,"['python', 'scikit-learn', 'tf-idf']","I using sklearn to obtain tf-idf values as follows.Now I want to view my calculated tf-idf scores in a matrix as follows.
I tried to do it as follows.However, then I get the output as follows.Please help me."
3758,Check the tf-idf scores of sklearn in python,"['python', 'scikit-learn', 'tf-idf']","I am following the example here to calculate the TF-IDF values using sklearn.My code is as follows.I want to calculate the tf-idf values for the two words life and learning for the 3 documents in corpus.According to the article I am referring (see Table below) I should get the following values for my example.
However, the values I get from my code is completely different. Please help me find what is wrong in my code and how to fix it."
3759,Calculate TF-IDF using sklearn for n-grams in python,"['python', 'scikit-learn', 'nlp', 'tf-idf']","I have a vocabulary list that include n-grams as follows. I want to use these words to calculate TF-IDF values.I also have a dictionary of corpus as follows (key = recipe number, value = recipe).I am currently using the following code.Now I am printing tokens or n-grams of the recipe 1 in corpus along with the tF-IDF value as follows.The results I get is chocolates 1.0. However, my code does not detect n-grams (bigrams) such as biscuit pudding when calculating TF-IDF values. Please let me know where I make the code wrong.I want to get the TD-IDF matrix for myvocabulary terms by using the recipe documents in the corpus. In other words, the rows of the matrix represents myvocabulary and the columns of the matrix represents the recipe documents of my corpus. Please help me."
3760,Calculating similarity between Tfidf matrix and predicted vector causes memory overflow,"['python', 'scikit-learn', 'gensim', 'tf-idf', 'csr']","I am have generated a tf-idf model on ~20,000,000 documents using the following code, which works well. The problem is when I try to calculate similarity scores when using linear_kernel the memory usage blows up:Seems like this shouldn't take up much memory, doing a comparison of a 1-row-CSR to a 20mil-row-CSR should output a 1x20mil ndarray.Justy FYI: X is a CSR matrix ~12 GB in memory (my computer only has 16). I have tried looking into gensim to replace this but I can't find a great example.Any thoughts on what I am missing?"
3761,Cosine Similarity score in scikit learn for two different vectorization technique is same,"['python-3.x', 'scikit-learn', 'tf-idf', 'cosine-similarity']","I am recently working on an assignment where the task is to use 20_newgroups dataset and use 3 different vectorization technique (Bag of words, TF, TFIDF) to represent documents in vector format and then trying to analyze the difference between average cosine similarity between each class in 20_Newsgroups data set. So here is what I am trying to do in python. I am reading data and passing it to sklearn.feature_extraction.text.CountVectorizer class's fit() and transform() function for Bag of Words technique and TfidfVectorizer for TFIDF technique.Both should technically give different similarity_matrix but they are yeilding the same. More precisiosly tf_vectorizer should create similarity_matrix which have values more closed to 1.The problem here is, Vector created by both technique for the same document of the same class for example (alt.atheism) is different and it should be. but when I calculating a similarity score between documents of one class and another class, Cosine similarity scorer giving me same value. If we understand theoretically then TFIDF is representing a document in a more finer sense in vector space so cosine value should be more near to 1 then what I get from BAG OF WORD technique right? But it is giving same similarity score. I tried by printing values of matrices created by BOW & TFIDF technique. It would a great help if somebody can give me a good reason to resolve this issue or strong argument in support what is happening?
I am new to this platform so please ignore any mistakes and let me know if you need more info.Thanks & Regards,
Darshan Sonagara"
3762,TFIDIF Model Creation TypeError in Gensim,"['python', 'nlp', 'gensim', 'tf-idf', 'language-features']","TypeError: 'TfidfModel' object is not callableWhy can I not compute the TFIDF Matrix for each Doc after initializing?I started with 999 documents: 999 paragraphs with about 5-15 sentences each.
After spaCy tokenizing everything, I created the dictionary (~16k unique tokens) and corpus (a list of lists of tuples)Now I'm ready to create the tfidf matrix (and later LDA and w2V matricies) for some ML; however, after initializing the tfidf model with my corpus (for calculation of the 'IDF')
tfidf = models.TfidfModel(corpus) I get the following error message when trying to see the tfidf of each doc tfidf(corpus[5])
TypeError: 'TfidfModel' object is not callableI am able to create this model using a differnt corpus where i have four docs each comprised of only a sentence.
There I can confirm that the expected corpus fomat is a list of lists of tuples: 
[doc1[(word1, count),(word2, count),...], doc2[(word3, count),(word4,count),...]...]"
3763,"Python, Matching list elements with a dictionary containing list of tuples","['python', 'list', 'dictionary', 'tuples', 'tf-idf']",I have a dictionary containing dates and a sequence of term frequency shown below:Out of all these terms I have a list of keywords for which I want to summarize all data:I want my output to look like:This basically extracts the keywords from the dictionary with the list of tuples.My code for extracting looks like this:But it returns an empty dict. Could someone please help me out?
3764,How to apply SVD on TF-IDF Dataframe in pyspark,"['python', 'apache-spark', 'pyspark', 'tf-idf', 'svd']",I have applied the pyspark tf-idf functions and get back the following results.So a dataframe having 1 column (features) which contains SparseVectors as rows.Now i want to build the IndexRowMatrix from this dataframe so that i can run the svd function which is described over here https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=svd#pyspark.mllib.linalg.distributed.IndexedRowMatrix.computeSVDI have tried the following but didn't work:I used RowMatrix because to construct it i don't have to provide tuple but i can't even build RowMatrix. IndexedRowMatrix will be more difficult for me.So how to run the IndexedRowMatrix on the out put of tf-idf dataframe in pyspark ?
3765,SOLR IDF Max docs configuration,"['search', 'solr', 'tf-idf']",I'm using SOLR for storing the documents used by search in my application. The SOLR is shared by multiple applications and the data is grouped based on the application id which is unique for each application.For calculating the score based on TF-IDF the SOLR uses the total documents available in it. How do I change that configuration to check the IDF only based on the total documents available for the application id rather than counting all the documents across applications.
3766,Do I use the same Tfidf vocabulary in k-fold cross_validation,"['python', 'scikit-learn', 'cross-validation', 'tf-idf']","I am doing text classification based on TF-IDF Vector Space Model.I have only no more than 3000 samples.For the fair evaluation, I'm evaluating the classifier using  5-fold cross-validation.But what confuses me is that whether it is necessary to rebuild the TF-IDF Vector Space Model in each fold cross-validation. Namely, would I need to rebuild the vocabulary and recalculate the IDF value in vocabulary in each fold cross-validation?Currently I'm doing TF-IDF tranforming based on scikit-learn toolkit, and training my classifier using SVM. My method is as follows: firstly,I'm dividing the sample in hand by the ratio of 3:1, 75 percent of them are applied to fit the parameter of the TF-IDF Vector Space Model.Herein, the parameter is the size of vocabulary and the terms that contained in it, also the IDF value of each term in vocabulary.Then I'm transforming the remainder in this TF-IDF SVM and using these vectors to make 5-fold cross-validation (Notably, I don't use the previous 75 percent samples for transforming).My code is as follows:My confusion is that whether my method doing TF-IDF transforming and making 5-fold cross-validation is correct, or whether it's necessary to rebuild the TF-IDF Vector Model Space using train data and then transform into TF-IDF vectors with both train and test data? Just as follows:"
3767,SKLearn TF-IDF to drop numbers?,"['scikit-learn', 'tf-idf']","I'm doing text analysis, and I want to disregard 'words' that are just numbers. Eg. from the text ""This is 000 Sparta!"" only the words 'this', 'is' and 'Sparta' should be used. Is there a way to do this? How?"
3768,Append tfidf to pandas dataframe,"['python', 'dataframe', 'tf-idf', 'sklearn-pandas']","I have the following pandas structure:I'd like to vectorise it using a tfidf vectoriser. This, however, returns a parse matrix, which I can actually turn into a dense matrix via mysparsematrix).toarray(). However, how can I add this info with labels to my original df? So the target would look like:UPDATE:Solution makes the concatenation wrong even when renaming original columns:

Dropping columns with at least one NaN results in only 7 rows left, even though I use fillna(0) before starting to work with it."
3769,Filtering cosine similarity scores into a pandas dataframe,"['python', 'pandas', 'dataframe', 'tf-idf', 'cosine-similarity']","I'm trying to calculate cosine similarity scores between all possible combinations of text documents from a corpus. I'm using scikit-learn's cosine_similarity function to do this. Since my corpus is huge (30 million documents), the number of possible combinations between the documents in the corpus is just too many to store as a dataframe. So, I'd like to filter the similarity scores using a threshold, as they're being created, before storing them in a dataframe for future use. While I do that, I also want to assign the corresponding IDs of each of these documents to the index and column names of the dataframe. So, for a data value in the dataframe, each value should have index(row) and column names which are the document IDs for which the value is a cosine similarity score.This piece of code works well without the filtering part. IDs is a list variable that has all document IDs sorted corresponding to the tfidf matrix.This modification helps with the filtering but the similarity scores are turned into boolean (True/False) values. How can I keep the actual cosine similarity scores here instead of the boolean True/False values. "
3770,Multi-dimensional documents with Gensim,"['python', 'machine-learning', 'tensorflow', 'gensim', 'tf-idf']","I'm working on a document comparison engine / search engine. I'm currently using it as follows...And then comparing the results.What I would like to do (in the simplest terms possible) is have multi-dimensional documents (a document that has multiple dimensions, rather than just the ""document"")... for example..And also be able to weight the results (for example, title is 0.6, body is 0.4, etc).My question is... is there a way to do this within Gensim, or do I need to create a separate document for each meta item of the document (for example, comparing to each meta item (title, body, tags) as a separate document, and then combining weights after the fact using the document key/id? I'm not sure i'm doing a good job of explaining this, but please let me know if I can improve my question.Thank you."
3771,Train Model fails because 'list' object has no attribute 'lower',"['python', 'scikit-learn', 'tf-idf', 'training-data']","I am training a classifier over tweets for sentiment analysis purposes.The code is the following:After normalization, the dataframe looks like:It fails with the following error:"
3772,TfidfVectorizer Returning 0 for Ngrams in Pandas DF with Duplicate IDs,"['python', 'python-3.x', 'pandas', 'scikit-learn', 'tf-idf']","I have a grouped df:I am extracting bigrams, frequencies, and tfidf scores with this function:This results in the following df:The tfidf scores are contrived. So, the function is correctly finding the frequencies. It is then finding tfidf scores for the first row of the grouped df (including for the bigram that appears in multiple rows. Finally, it is not finding the tfidf scores for bigrams unique to the second and third rows.In addition, while the tfidf scores are contrived, it is true that they are identical for any bigram that has the same frequency within a particular document. So any bigram with a frequency of 1 in the first row will have the .3 tfidf score. Any bigram with a frequency of 1 in another row might have a tfidf score of .24. This is weird because the term frequency of each bigram is certainly different.Two Questions:Thank you for any insight you all might have!"
3773,Extract most important keywords from a set of documents,"['nlp', 'rake', 'feature-extraction', 'word2vec', 'tf-idf']","I have a set of 3000 text documents and I want to extract top 300 keywords (could be single word or multiple words).I have tried the below approaches - RAKE: It is a Python based keyword extraction library and it failed miserably.Tf-Idf: It has given me good keywords per document, but it is not able to aggregate them and find keywords that represent the whole group of documents.
Also, just selecting top k words from each document based on Tf-Idf score won't help, right?Word2vec: I was able to do some cool stuff like find similar words but not sure how to find important keywords using it.Can you please suggest some good approach (or elaborate on how to improve any of the above 3) to solve this problem? Thanks :)"
3774,Extracing tf-idf values and features from TfidfVectorizer and making them into pandas Series,"['python', 'sorting', 'vectorization', 'tf-idf']","I was extracting tf-idf values for each feature name from a text document (in .csv format where each row entry represents a text message (dtype=str)) using TfidfVectorizer with default parameters. This is what I did:I also used the last two lines to extract feature names and tf-idf values. However, I was also asked to (1) sort features by their tf-idf values in both ascending and descending orders, then by alphabetical order (if multiple features' tf-idf values are tied) and (2) make the output into a pandas Series object using feature name as index, such that the output looks like this (this one in descending order):Seems that I can simply achieve this by matching 'feature_names' and 'tfidf' and sort them, but I am not sure if their sequences actually match as 'feature_names' is a list object while 'tfidf' is a numpy array and given that I don't really know what sklearn is doing under the hood.If I want to compile a sorted Series in descending (and ascending) order with the exact feature name as index (sorted in alphabetical oder), what should I proceed from the last line of my code? If will be really appreciated if someone could enlighten me on this.Thank you."
3775,Remove single occurrences of words in vocabulary TF-IDF,"['python', 'scikit-learn', 'tf-idf']",I am attempting to remove words that occur once in my vocabulary to reduce my vocabulary size. I am using the sklearn TfidfVectorizer() and then the fit_transform function on my data frame.My first thought is the preprocessor field in the tfidf vectorizer or using the preprocessing package before machine learning. Any tips or links to further implementation?
3776,TfidfVectorizer to respect hyphenated compounds (words that are joined with a hyphen),"['python', 'regex', 'scikit-learn', 'token', 'tf-idf']","I have a list of strings that look like this:df_train = ['Hello John-Smith it is nine o'clock','This is a completely random-sequence']I would like sklearn TfidfVectorizer to treat words joined with a hyphen as a whole word. When I apply the following code, the words separated by hyphen (or other punctuation) are treated as separate words:I have changed the parameter token_pattern but with no success. Any idea of how I could solve this issue? In addition, is it possible to treat as a single entity words that are separated by any punctuation? (e.g. 'Hi.there How_are you:doing')"
3777,Sorting TfidfVectorizer output by tf-idf (lowest to highest and vice versa),"['python', 'scikit-learn', 'ranking', 'tf-idf']","I'm using TfidfVectorizer() from sklearn on part of my text data to get a sense of term-frequency for each feature (word). My current code is the followingIf I want to sort the tf-idf values of each term in 'X_traintfidf' from the lowest to highest (and vice versa), say, top10, and make these sorted tf-idf value rankings into two Series objects, how should I proceed from the last line of my code?Thank you.I was reading a similar thread but couldn't figure out how to do it. Maybe someone will be able to connect the tips shown in that thread to my question here."
3778,Crawl ~50 websites looking for key words (climate) using TF-IDF,"['nlp', 'web-crawler', 'tf-idf']","""Fighting climate change - with words?""I come from the linguistics + stats side and not from the computer science/programming side of things, so please be patient with me and also thank you!I'm working on a research project that currently involves expending a lot of time and energy looking at ~ 50 different websites 2-3 times a week to find out about new developments in the energy sector/climate change, so that we don't miss any news (before they get changed or deleted) and want to save and not miss any files of interest.For now there only is a laughable set-up of bookmarks. But I'd like to make the work easier, if possible, by crawling these websites (every day would be best) looking for changes and in particular by looking for keywords either on (the relevant sections of) the website or within posted documents themselves.In regards to the documents themselves we are going to employ algorithms (or simple variations) like TF-IDF (Term Frequency - Inverse Document Frequency) and DF-ICF (Document Frequency - Inverse Corpus Frequency) and compare the language used (comparative analysis of corpora) over time and ""seasons""  (e.g. political changes).TLDR: Need help simplifying the gathering of data from ~50 websites looking for keywords by e.g. crawling.Thank You!"
3779,Why do we calculate cosine similarities using tf-idf weightings?,"['text', 'nlp', 'information-retrieval', 'tf-idf', 'cosine-similarity']","Suppose we are trying to measure similarity between two very similar documents.This corresponds to a term-frequency matrix where the cosine similarity on the raw vectors is the dot product of the two vectors A and B, divided by the product of their magnitudes:3/4 = (1*1 + 1*1 + 1*1 + 1*0 + 1*0) / (sqrt(4) * sqrt(4)).But when we apply an inverse document frequency transformation by multiplying each term in the matrix by (log(N / df_i), where N is the number of documents in the matrix, 2, and df_i is the number of documents in which a term is present, we get a tf-idf matrix ofSince ""a"" appears in both documents, it has an inverse-document-frequency value of 0. This is the same for ""b"" and ""c"". Meanwhile, ""d"" is in document A, but not in document B, so it is multiplied by log(2/1). ""e"" is in document B, but not in document A, so it is also multiplied by log(2/1).The cosine similarity between these two vectors is 0, suggesting the two are totally different documents. Obviously, this is incorrect. For these two documents to be considered similar to each other using tf-idf weightings, we would need a third document C in the matrix which is vastly different from documents A and B. Thus, I am wondering whether and/or why we would use tf-idf weightings in combination with a cosine similarity metric to compare highly similar documents. None of the tutorials or StackOverflow questions I've read have been able to answer this question.This post discusses similar failings with tf-idf weights using cosine similarities, but offers no guidance on what to do about them.EDIT: as it turns out, the guidance I was looking for was in the comments of that blog post. It recommends using the formula1 + log ( N / ni + 1)as the inverse document frequency transformation instead. This would keep the weights of terms which are in every document close to their original weights, while inflating the weights of terms which are not present in a lot of documents by a greater degree. Interesting that this formula is not more prominently found in posts about tf-idf."
3780,Can we use tf-idf and cosine similarity for document recommendation system?,"['python', 'nlp', 'recommendation-engine', 'tf-idf', 'cosine-similarity']","The document dataset is having 9000 documents and its growing day by day, most of the documents having average 1-3 paragraphs. The idea is to make a recommendation with the help of user's past usage of documents. Lets say the dataset is having 10 documents (main corpus) in total and out of them 4 related to sports, 2 related to Fashion, 4 related to technology. Now if say user x has read 2 sports and 1 technology related documents then to get the tf-idf of the user x we will combine these 3 documents and assume it as user x corpus. Now get the tf-idf of the 10 documents (main corpus) which will get us the individual tf idf vector of all those 10 documents. Now compare the cosine similarity of user x corpus with the main corpus (10 document) Result should be 2 sports document and 1 technology document which user x has not yet read. Let me know if this idea makes sense. "
3781,Should TF-IDF Score be Identical for Every Ngram with the Same Frequency in a Document,"['python', 'python-3.x', 'scikit-learn', 'tf-idf']","I am using sklearn to find frequencies and tf-idf scores for bigrams from a set of 50 documents. Let's say one of the cleaned documents is ""run fast slow"".The output is:The ngrams in the output for that one document are found in other documents. Let's say ""run fast"" is found 20 times in the document collection and ""fast slow"" is found 30 times. Why are the tfidf scores the same for ngrams within a document that have the same frequency? This doesn't intuitively seem like the correct output since the frequency across the document collection varies.This is the code I am using to extract the features. It takes a grouped df and a text column from that df: "
3782,"K-means cluster given a CSV with (tf-idf cosine similarity, doc_id1, doc_id2)?","['python', 'csv', 'cluster-analysis', 'tf-idf', 'cosine-similarity']","I have a CSV with the following dataset:Where ""similarity"" refers to a value from tf-idf cosine similarity computations and the doc_ids refer to documents. So, the closer similarity is to 1, the more similar the two documents are. I want to cluster the documents based on this information, but I'm not entirely sure how to do so. I've been reading a lot about spherical K-means clustering, but in terms of implementing it I'm having a hard time wrapping my head around it. Is there a library that might be useful? Is K-means the right way to go at all?EDIT: 
This CSV is all I have, so even though I wish I had word frequency based vectors, I don't. If K-means won't work given that all I have are similarities, are there other algorithms that would suit this data?"
3783,Scikit-Learn TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf']","I am working on a text classification problem, parsing news stories from RSS feeds, and I suspect many HTML elements and gibberish are being counted as tokens.  I know Beautiful Soup provides methods to clean up HTML, but I wanted to try to pass in a dictionary to have more control over what tokens were counted.This seemed simple enough in concept, but I am getting results I don't understand.  The output of the program is as follows;The output of the third print goes on and on, so I intentionally cut it short,  What is weird though is that it starts mid-word, exactly as I show it above.  The results of the first two print statements make sense to me;However, the features showing in the third print are not part of my corpus, why are they showing up?"
3784,idf vs relative frequency to extract important words for a corpus,"['python', 'r', 'text-mining', 'tf-idf', 'trend']","I am new to text mining. I am trying to extract important words out of a given corpus of documents. I am debating on whether I should use the idf score or relative frequency to detect important words for a corpus. By important words, I mean the words that best represents the corpus. As far as I understand, idf score would be useful to detect rare words and not important words. Therefore, I am leaning towards the relative frequency of the word to determine it's importance in the corpus but I am not sure. I am curious to know in more detail the difference in usage of these two scores. Or is there a better way to extract out important words out of corpus? "
3785,Vectorizer the combination of words in Python,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'countvectorizer']","I have a dataset with medical text data and I apply tf-idf vectorizer on them and calculate tf idf score for the words just like this:So basically my question is following-while I'm applying TfidfVectorizer it splits the text in distinct words for example: ""pain"", ""headache"", ""nausea"" and so on. How can I get the words combination in the output of TfidfVectorizer for example: ""severe pain"", ""cluster headache"", ""nausea vomiting"". Thanks"
3786,Is this correct tfidf?,"['python', 'scikit-learn', 'tf-idf']","I am trying to get tfidf from a document. But I dont think it is giving me correct values or I may be doing some thing wrong. Please suggest. Code and output below:Update 1: (As suggested by juanpa.arrivillaga)Output:Output after update 1:As per my understanding tfidf is = tf * idf. And the way I manually calculate it as example: document 1: ""Hello there this is first book to be read by wordcount script.""
document 2: ""This is second book to be read by wordcount script. It has some additionl information.""
document 3: ""just third book.""Tfidf for Hello: which is different from below (hello - 0.354084405732).Manual calculation after update 1:(not same as code output ""hello - 0.383055542114"" after idf smoothing) Any help to understand whats going on is highly appreciated.."
3787,Identical Clusters after Text Clustering in Python,"['python', 'scikit-learn', 'nlp', 'nltk', 'tf-idf']","I'm performing text clustering on a set of textdata in Python. Basically, I use tf idf score and then apply the result matrix into the kmeans algorithm just like that:Then after performing the following, I get 15 identical clusters (with almost fully identical word terms in it). I also tried the normalization using LSA method but it gives almost the same.What am I doing wrong and how it can be fixed?"
3788,Does Spark MLlib IDF shuffle data?,"['apache-spark', 'apache-spark-mllib', 'tf-idf']","In the following code, does spark have to shuffle data while computing IDF and TF-IDF vectors?"
3789,Right approach to find similar products solely based on content and not on user history using machine learning algorithms,"['machine-learning', 'similarity', 'tf-idf', 'svd', 'predictionio']","I have around 2-3 million products. Each product follows this structure The problem statement is Find the similar products based on content or product data only. So when the e-commerce user will click on a product ( SKU ) , I will show the similar products to that SKU or product in the recommendation. For example if the user clicks on apple iphone 6s silver , I will show these products in ""Similar Products Recommendation"" 1) apple iphone 6s gold or other color2) apple iphone 6s plus options3) apple iphone 6s options with other configurations4) other apple iphones5) other smart-phones in that price rangeWhat I have tried so far A) I have tried to use 'user view event ' to recommend the similar product but we do not that good data. It results fine results but only with few products. So this template is not suitable for my use case.B) One hot encoder + Singular Value Decomposition ( SVD ) + Cosine Similarity I have trained my model for around 250 thousand products with dimension = 500 with modification of this prediction io template. It is giving good result. I have not included long description of product in the training.But I have some questions here 1) Is using One Hot Encoder and SVD is right approach in my use case?2) Is there any way or trick to give extra weight the title and brand attribute in the training.3) Do you think it is scalable. I am trying to increase the product size to 1 million and dimension = 800-1000 but it is talking a lot of time and system hangs/stall or goes out of memory. ( I am using apache prediction io ) 4) What should be my dimension value when I want to train for 2 million products.5) How much memory I would need to deploy the SVD trained model to find in-memory cosine similarity for 2 million products.What should I use in my use-case so that I can also give some weight to my important attributes and I will get good results with reasonable resources. What should be the best machine learning algorithm I should use in this case."
3790,Why are TF-IDF vocabulary words represented as axes/dimensions?,"['nlp', 'tf-idf']","I want an intuitive way for understanding why each word in a TF-IDF vocabulary are represented as separate dimensions. Why can't I just add the TF-IDF values of all the words together and use that as a representation of the document?I have a basic understanding of why we do this. Apples =/= Oranges.
But apparently I don't know it well enough to convince someone else!"
3791,Solr- Find “Significant Terms” on Subset of Documents,"['solr', 'tf-idf']","I'm trying to get ""significant terms"" for a subset of documents in Solr. This may or may not be the best way, but I'm currently attempting to use Solr's TF-IDF functionality since we have the data stored in Solr and it's lightning fast. I want to restrict the ""DF"" count to a subset of my documents, through a search or a filter. I tried this, where I'm searching for ""apple"" in the name field:http://localhost:8983/solr/techproducts/tvrh?q=name:apple&tv.tf=true&tv.df=true&tv.tf_idf=true&indent=on&wt=json&rows=1000and that of course, only gives me documents that have ""apple"" in the name, but my document frequency gives the counts from the entire dataset, which doesn't seem like what I want. I would think Solr can do this, but maybe not. I'm open to suggestions.Thanks,
Adrian"
3792,"Is it possible to obtain, alter and replace the tfidf document representations in Lucene?","['lucene', 'full-text-search', 'tf-idf']","Hej guys,I'm working on some ranking related research. I would like to index a collection of documents with Lucene, take the tfidf representations (of each document) it generates, alter them, put them back into place and observe how the ranking over a fixed set of queries changes accordingly.Is there any non-hacky way to do this?"
3793,error TypeError: unorderable types: int() < str(),"['python-3.x', 'spark-dataframe', 'apache-spark-mllib', 'tf-idf', 'lda']","I am getting this error when I am running the following code. I should explain that the error happen in this line:I reviewd these cases:enter link description hereenter link description herebut they are about the conversion int and string, specially reading Input.
but here I dont have input,
Explanations abt the code:
this code is doing tfidf+lda using DataframeMay please give me your idea,when I delete the str in the error prone line:, It throws a new error
 TypeError: col() missing 1 required positional argument: 'strg'Update
My main goal is to run this code :tfidf then lda"
3794,Obtain tf-idf weights of words with sklearn,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'tf-idf']","I have a set of texts of wikipedia.
Using tf-idf, I can define the weight of each word.
Below is the code:The goal is to see the weights like shown in the tf-idf column:The file 'people_wiki.csv' is here:https://ufile.io/udg1y"
3795,How to find TF-IDF of a term in respect of a document using scikit,"['python', 'scikit-learn', 'tf-idf']","I'm trying to use scikit applied to Natural Language Processing and I'm starting by reading some tutorials. I've found this one http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/ which explains how to get tfidf scores from a set of documents.But I have a question, TF-IDF is supposed to depend from a term, the document of that term and the collection of all documents to be analyzed.So, for example. In a collection of two documents, A and B, the term 'horse' should get a different TF-IDF score if we compute TF-IDF using document A than the same term but by analyzing term frequency from document B.How can I compute TF-IDF of a term in respect of a specific document using scikit?"
3796,Decoding tracebacks using machine learning,"['python', 'machine-learning', 'data-science', 'tf-idf', 'traceback']",I am trying to solve a problem where I have files which contain decoded- tracebacks( Stack call trace) whenever there is a Crash (in Linux world) and I have a unique ID to track the Crash occuring each time. I want to build a classfier which will learn from the previous decoded-tracebacks and predict if there is an already existing ID for current traceback seen.This is my first machine learning project . I used machine learning and did a trial using CountVectorizer and TF-IDF approach in python.I want to know which features to consider for classification and suitable algorithm for text-classification to solve this problem.
3797,Scala Convert [Seq[string] to [String]? (TF-IDF after lemmatization),"['scala', 'tf-idf', 'lemmatization', 'lsa']","I try to learn scala and specificaly text minning (lemmatization ,TF-IDF matrix and LSA).I have some texts i want to lemmatize and make a classification (LSA). I use spark on cloudera.So i used the stanfordCore NLP fonction:After that, i try to make an TF-IDF matrix but here is my problem:
The Stanford fonction make an RDD in [Seq[string] form.
But, i have an error.
I need to use a RDD in [String] form (not the [Seq[string]] form).Someone know how convert a [Seq[string]] to [String]?Or i need to change one of my request?.Thanks for the help.
Sorry if it's a dumb question and for the english.Bye"
3798,Pipeline using CountVectorizer (max_df) before tfidf,"['machine-learning', 'scikit-learn', 'nlp', 'tf-idf', 'countvectorizer']","Currently i am not sure if this quation is for stackoverflow or another more theoretical statistical QA. But im confused about the following. I am doing a binairy tekst classification task. For this task i use a pipeline, one of the example codes is below: So nothing really strange about this,  but then i started playing with the parameter options/settings and noted that the code below (so the steps and parameters in the code) had the highest accuracy score (f1 score):So im pleased to sort of find out with which parameter settings and methods i get the highest score, but i cant figure out the exact meaning. As with the 'vectorizor'-step the settings for max_df (ignoring terms that appear in more 20% of the documents) seems to be strange to apply before tfidf (or somehow double) Furthermore it also uses max_features of 10.000. What step is used before the max_df or the max_features? and how do i interpret max_features setting this parameter and doing tfidf afterwards. Does it then perform a tfidf over the 10.000 features? For me it seems rather strange to do a tfidf after using parameters such as max_df and max_features? Am i correct? and why? or should i just do what gives the highest outcome.. I hope someone can help me in the right direction, thanks a lot in advance. "
3799,Inverse transform word count vector to original document,"['tensorflow', 'scikit-learn', 'nlp', 'tf-idf', 'countvectorizer']",I am training a simple model for text classification (currently with scikit-learn). To transform my document samples into word count vectors using a vocabulary I use CountVectorizer(vocabulary=myDictionaryWords).fit_transform(myDocumentsAsArrays)from sklearn.feature_extraction.text.This works great and I can subsequently train my classifier on this word count vectors as feature vectors. But what I don't know is how to inverse transform these word count vectors to the original documents. CountVectorizer indeed has a function inverse_transform(X) but this only gives you back the unique non-zero tokens.As far as I know CountVectorizer doesn't have any implementation of a mapping back to the original documents.Anyone know how I can restore the original sequences of tokens from their count-vectorized representation? Is there maybe a Tensorflow or any other module for this?
3800,Calculating IDF from Pandas DataFrame,"['python', 'pandas', 'machine-learning', 'tf-idf']","I have a DataFrame with term frequencies (tf). The columns are words and the rows are documents. The rows sum up to 1.What is the best / easiest way to weight these values with idf (inverse document frequencies)?The thing is, tfidf of sklearn doesn't expect term frequencies, but word counts..."
3801,Tfidf Vectorizer not working,"['python-3.x', 'nlp', 'tf-idf']","I have a corpus(Hotel Reviews) and I want to do some NLP process including Tfidf. My problem is when I Applied Tfidf and print 100 features it doesn't appear as a single word but the entire sentence.
Here is my code:Note: clean_doc is a function return my corpus cleaning from stopwords, stemming, and etcit returns something like this:any idea why? Thanks in Advance"
3802,What is the relation between numFeatures in HashingTF in Spark MLlib and actual number of terms in a document?,"['apache-spark', 'machine-learning', 'apache-spark-mllib', 'tf-idf']","Is there any relation between numFeatures in HashingTF in Spark MLlib and the actual number of terms in a document(sentence)?As mentioned in the documentation of Spark Mllib, HashingTF converts each sentence into a feature vector of having numFeatures as length. 
What will happen if each document here, in this case, sentence contains thousands of terms? What should be the value of numFeatures? How to calculate that value?"
3803,SciKit-Learn: Trouble with TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf']","I'm trying to use TFIDF to get features from titles of text articles.  I'm doing the following:However, I get an error at this line:The error is:I'm not sure how it's possible to get this error.  I checked the docs and it looks like TfidfVectorizer uses UTF-8 as its default encoding.  Any idea how to fix?Thanks!"
3804,How to calculate the tf-idf score for a phrase with a set of documents,"['scikit-learn', 'information-retrieval', 'tf-idf']","I need to calculate the tf-idf of a phrase eg:""judgment in developing"" with a set off documents instead of calculating tf-idf score for individual terms in python"
3805,New data with R and tm when tf-idf is used,"['r', 'tm', 'tf-idf', 'text-classification']","Using R and tm, I have loaded and cleaned up a bunch of text documents, and made them into a Corpus. After that, I built their DTM using tf-idf, and that I can use for all kind of classification-clustering algorithms. So far, so good.Now, let's suppose that I have a new document, and try to compute its distance to the documents in the Corpus. Of course, I need to apply to it all the transformations I applied to the original set. But I do not understand how to compute the tf-idf vector of the new document, because tf-idf dependes on the whole set, not on this single document. Adding the new document to the Corpus and recomputing its tf-idf DTM does not work properly: not only it is inefficient, but also every new document would chance the values of the documents already seen, which would in change the results of any clustering-classification algorithm I might have run with them as traininig set.So, my question is, how can I, using R and tm, compute the tf-idf vector of a new document and use it to compute the new document's distance to every document in the training Corpus without recomputing tf-idf for all documents?Thanks in advance."
3806,TD-IDF Find Cosine Similarity Between New Document and Dataset,"['python', 'machine-learning', 'scikit-learn', 'tf-idf']","I have a TF-IDF matrix of a dataset of products:where words is a list of descriptions. This produces a 69258x22024 matrix.Now I want to find cosine similarities between a new product and the ones in the matrix, as I need to find the 10 most similar products to it. I vectorize it using the same method above.However, I cannot multiply the matrices because their sizes are different (the new one would be like 6 words, so a 1x6 matrix), so I need to make a TFIDFVectorizer with the number of columns as the original one.How do I do it?"
3807,Tf-idf with large or small corpus size,['tf-idf'],"""An essence of using Tf-Idf method with large corpuses is, the larger size of corpuses used are, the more unique weights terms have. This is because of the increasing of documents size in corpus or documents length gives a lower probability of duplicating a weight value for two terms in corpus. That is, the weights in Tf-Idf scheme can present a fingerprint for weights. Where in low size corpus, Tf-Idf can’t make that difference since there is huge potential of finding two terms having the same weights since they share the same source documents with the same frequency in each document. This feature can be an adversary and supporter by using Tf-Idf weighting scheme in plagiarism detection field, depending on the corpus size.""This is what I have deduced from tf-idf technique .. is it true? Are there any link or documents can prove my conclusion؟"
3808,Assign document to a category using document similarity,"['nlp', 'tf-idf', 'cosine-similarity']","I'm developing a NLP project in python.I'm getting ""conversation"" from social networks. A conversation is made up of post_text + comment_text + reply_text (with comment_text and reply_text as optional).I've also a list of categories, arguments, and I want to ""connect"" conversation to an argument (or get a weight for each argument).For each category, I get the summary on Wikipedia, using wikipedia python package. So, they represent my training documents (right?).Now, I've writed down some steps to follow, but maybe I'm wrong.What do you think about the steps? Is there any guide/how to/book I've to read to understand better this problem?"
3809,Testing and Training sets with different number of features using TF-IDF,"['scikit-learn', 'classification', 'tf-idf', 'training-data']","I am doing a simple binary classification and I give you an example of the problem I have: Lets say we have n documents (Doc 1, Doc 2,..., Doc n.) We are going to use TF-IDF as feature values to train a binary classifier using bag-of-words. We have m features for our training files (m technically is the number of unique tokens that we have in all of these n documents after cleaning and pre-processing). Now, lets say we have a trained model and we are going to predict the label of a new document. We should first pre-process the testing document the same way we did for our training documents. And, then we should use TF-IDF to build a feature vector for our test document. There are two problems here:So now I am just trying to figure out how exactly we can label a new document using a model that we trained with bag-of-words model and TF-IDF values. In particular, I am looking for a reasonable answer to the two specific problems that I have mentioned above.We can calculate the accuracy of the model (for example using cross validation) but I do not know what we should do for labeling a new document.P.S. I am using scikit-learn and python.UPDATE: I could find the answer to my question. In such cases, we can simply use the same TfidfVectorizer that we used to train our classifier. So now each time that I train a new classifier and build my feature vectors using tfidfVectorizer, I save my vectorizer in a file using pickle and I use this vectorizer at the time of creating testing set feature vectors."
3810,tf-idf : should I do normalization of documents length,"['python', 'normalization', 'word', 'tf-idf']","When using TF-IDF to compare Document A, B
I know that length of document is not important.
But compared to A-B, A-C 
in this case, I think the length of document B, C should be the same length.for example
Log : 100 words
Document A : 20 words
Document B : 30 wordsLog - A 's TF-IDF score : 0.xx
Log - B 's TF-IDF score : 0.xxShould I do normalization of document A,B?
(If the comparison target is different, it seems to be a problem or wrong result)"
3811,Tf-idf calculation using gensim,"['python', 'tf-idf', 'gensim']",I have one tf-idf example from an ISI paper. I’m trying to validate my code by this example. But I get different result from my code.I don’t know what the reason is!Term-document matrix from paper:  Tf-idf matrix from paper: My tf-idf matrix:My code:I’ve tried another code like this:But I didn’t get appropriate answer
3812,tf-idf in python TfidfVectorizer,"['python', 'machine-learning', 'scikit-learn', 'classification', 'tf-idf']","I am trying to implement tf-idf in python using sklearn.Here's what I got so far:Now, when I change my corpus to my original dataset, which is like this:and code to this:It won't work.So basically, I have multiple documents in 2D List. And originally, I had a 1D list with documents.Further after calculating tf-idf, I will apply classification on it.How should I get my tf-idf working?"
3813,What method should I use to convert words into features for Machine Learning applications? [closed],"['python', 'nlp', 'tf-idf', 'word2vec']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 3 years ago.I am planning on building a gender classifier. I know the two popular models are tf-idf and word2vec. 
While tf-idf focuses on the importance of a word in a document and similarity of documents, word2vec focuses more on the relationship between words and similarity between them. However none of theme seem to be perfect for building vector features to be used for gender classification. Is there any other alternative vectorization model that might suit this task? "
3814,how to find which word has maximum tfidf in a tfidf matrix for a single document?,"['python-2.7', 'pandas', 'tf-idf']","I'm curently using the following code. I have stored the tfidf matrix for all the documents, now I need the top n words for a particular document?
I am confused on how to get it?This is the code I used till now. I need to find the words now from each document with highest tfidf"
3815,SKLearn Naive Bayes: add feature after tfidf vectorization,"['python', 'machine-learning', 'scikit-learn', 'tf-idf', 'naivebayes']","So I have been tasked with training a model on phone call transcripts. The following code does this. A little background info:
- x is a list of strings, each ith element is an entire transcript
- y is a list of booleans, stating the outcome of a call being positive or negative.The following code works, but here is my issue. I want to include call duration as a feature to train on. I'd assume after the TFIDF transformer that vectorizes the transcripts, I would just concatenate the call duration feature to the TFIDF output right? Maybe this is easier than I think, but I have the transcripts and the durations all in the pandas data frame you see at the beginning of the code. So if I have that data frame column (numpy array) of durations, what do I need to do to add that feature into my model?Additional Questions:Code:"
3816,TF-IDF Weighting after NLTK pre-processing,"['python', 'preprocessor', 'tf-idf']","I am doing some textual preprocessing prior to machine learning. I have two features (Panda series) - abstract and title - and use the following function to preprocess the data (giving a numpy array, where each row contains the features for one training example):I now need to use TF-IDF to weight the features - how can I do this?"
3817,scoring documents with lucene,"['lucene', 'information-retrieval', 'tf-idf']","i am going to change the tf-idf algorithm in lucene, so i create a new instance of IndexSearcher and call setSimilarity function.how can i implement real tf-idf algorithm with these functions and then customize them into my own algorithm?"
3818,How to deactivate the default stop words feature for sklearn TfidfVectorizer,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'tf-idf']","I am trying to get the tf-idf values for Japanese words.
The problem I am having is that sklearn TfidfVectorizer removes some Japanese characters, which I want to keep, as stop words. The following is the example:The output is:['痛い']However, I want to keep all those three characters in the list.
I believe TfidfVectorizer removes characters with length of 1 as stop words.
How could I deactivate the default stop words feature and keep all characters?"
3819,TFIDF in Python,"['python', 'python-2.7', 'tf-idf']",Hi below is my function to create tfidf matrix in pythonand i am getting following error :  Traceback (most recent call last):  Please help why i am getting this Error?
3820,Using TfidfVectorizer on a Dictionary of Lists,"['python', 'dictionary', 'machine-learning', 'scikit-learn', 'tf-idf']","I have a large corpus stored as a dict of 25 lists that I'd like to analyze with SKLearn's TfidfVectorizer.  Each list contains many strings.  Now, I care both about the overall term frequency (tf) throughout the whole corpus and the most unique terms within each list of the 25 strings (idf). The problem is, I haven't found a way to pass this kind of object to the TfidfVectorizer.  Passing the dict just vectorizes the keys, passing the values yields an AttributeError: 'list' object has no attribute 'lower' (I guess it expects a string.)   Thanks in advance.Update: Now including my preprocessing step, which used a dict of area, ID pairs called buckets"
3821,Elasticsearch 5.x match query: calculate score using term frequency and ignore inverse document frequency,"['elasticsearch', 'tf-idf', 'elasticsearch-5']","I'm trying to query a field that contains a lot of words in it, while each word is already multiplied by it's dominance in the document.
Therefore, term frequency is what I need here, while idf really changes the documents scoring.For example, I have two documents that contain a 'words' field, the first doc has the word 'fashion' 1000 times, and the second doc has the word 'baby' 1000 times as well.
Score evaluation by term frequency alone, would return the same score (approx.) for both, but the idf additional evaluation changes the results, which I would like to avoid.The mapping I have is pretty straight forward:}And the query is a simple match query:The explain plan looks like this:The results scores are pretty important, so constant_score query is unfortunately not relevant in this case.
Is there any way to make the query use only the term frequency weight?Thanks a lot in advance!"
3822,How to combine tfidf features with selfmade features,"['python', 'pandas', 'scikit-learn', 'nlp', 'tf-idf']","For a simple web page classification system I am trying to combine some selfmade features (frequency of HTML tags, frequency of certain word collocations) with the features obtained after applying tfidf. I am facing the following problem, however, and I don't really know how to proceed from here.Right now I am trying to put all of these together in one dataframe, mainly by following the code from the following link :But this doesn't return the index (from 0 to 2464) I had in my original dataframe with the other features, neither does it seem to produce readable column names and instead of using the different words as titles, it uses numbers.Furthermore I am not sure if this is the right way to combine features as this will result in an extremely high-dimensional dataframe which will probably not benefit the classifiers."
3823,How to use nltk stopwords and stemming in pandas,"['python', 'pandas', 'scikit-learn', 'nltk', 'tf-idf']","For a simple web classifier I am trying to remove stop words from my text and apply stemming before using tfidf. I know that tfidf in scikit offers the possibility to remove stop words, but I thought I could as well do it beforehand using nltk.After cleaning the raw html code with beautifulsoup and removing \n characters with regex, I ended up with what seemed to me a fairly clean text. When I try to apply stopwords, however, I end up with complete gibberish:[u' ', u' ', u' ', u' ', u'n', u' ', u'n', u' ', u'c', u'p', u'u',
  u'n', u' ', u'n', u' ', u'p', u'g', u'e', u' ', u' ', u' ', u' ',
  u'n', u' ', u'n', u' ', u'c', u'p', u'u', u'n', u' ', u'n', u' ',
  u'p', u'g', u'e', u' ', u'c', u'6']and it continues like that for a long time in every cell.The code I have been using (in Jupyter notebook) is the following: Furthermore, it would be nice to have this text excl. stopwords as a new column in the dataframe instead of replacing the raw_text with it. I tried this withbut that resulted in AttributeError: 'unicode' object has no attribute 'apply'Any help would be greatly appreciated!EDITI tried to use .split(' ') as follows:which resulted in a slightly better, more expected output:[u'', u'', u'', u'', u'mind', u'computation', u'main', u'page', u'',
  u'', u'', u'mind', u'computation', u'main', u'page', u'c690:',
  u'mind', u'computation,', u'1995-6', u'', u'', u'', u'mind',
  u'computation', u'seminar', u'features', u'talks', u'iu', u'graduate',
  u'students', u'faculty', u'occasional'] (etc)although this is still not yet as would be expected... Output before stopwords with nltk
Output after stopwords with nltk"
3824,How to apply TFIDF to find important words in csv file using pycharm,"['python', 'csv', 'nltk', 'tf-idf']","I have a file that includes some data, An example of the data I have I need to find the most frequent words in the file 
Any ideas on how can this be applied? pieces of codes would be appreciated as an example  "
3825,R: weightTf and weightTfIdf yield the same frequent word list?,"['r', 'tf-idf']","I recognised today, that tf and/or tfidf seems to be broken in R. See my example below. It uses the data from the manual i.e. crude. I would expect, that the resulting frequent term lists are not equal. But they are equal. This should never happen, right?Do I have any mistake in the example code? I copied it from the manual of the underlying tm package and added an tf case as well as the comparison.Thanks for any advice.Best regards
ThorstenEdit #1:
Okay, thanks lukeA for the answer. That helps a lot. Therefore, the ""right"" way to get the frequent terms is:Now, both lists are differently."
3826,difference between predicted and learned tfidf weights tfidfVectorizer sklearn,"['python-2.7', 'scikit-learn', 'tf-idf']","Can someone explain the difference between TFIDF (sklearn) weights obtained by two methods, given an unseen document:To get the weights from the vocabulary_ attribute of the fitted model:To get the weights by using the transform() method of the fitted vectorizer:I can see also the magnitudes are very different and probably they are showing completely different patterns giving completely different information about the unseen document. Thanks for the feedback on this concern also."
3827,Sklearn and gensim's TF-IDF implementation,"['scikit-learn', 'tf-idf', 'gensim']","I've been trying to determine the similarity between a set of documents, and one of the methods I'm using is the cosine similarity with the results of the TF-IDF.I tried to use both sklearn and gensim's implementations, which give me similar results, but my own implementation results in a different matrix.After analyzing, I noticed that the their implementations are different from the ones I've studied and came across:Sklearn and gensim use raw counts as the TF, and apply L2 norm
on the resulting vectors.On the other side, the implementations I found will normalize the term count,
likeMy question is, what is the difference with their implementations? Do they give better results in the end, for clustering or other purposes?EDIT(So the question is clearer):
What is the difference between normalizing the end result vs normalizing the term count at the beggining?"
3828,how i calculate cosine similarity by using jama,"['java', 'tf-idf', 'svd', 'cosine-similarity', 'lsa']","could anyone help me with detecting the problem? I need to calculate the similarity between the query and a collection of documents, and I've been used the program : https://github.com/aliabbasrizvi/LatentSemanticIndexing][1]. In this program, the similarity was calculated by ""dot product"" and I change it to ""cosine similarity"" but I got a high similarity score while I should get it between 0 and 1.
can anyone tell me what is the wrong? is TF_IDF have any effect? or something else?those results that I got it""Document: b4, Relevance score: 2.185808108221954Document: h286, Relevance score: 1.1350011283882473E-16This is the code I was used it for similarity"""
3829,how to choose parameters in TfidfVectorizer in sklearn during unsupervised clustering,"['python', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']","TfidfVectorizer provides an easy way to encode & transform texts into vectors.My question is how to choose the proper values for parameters such as min_df, max_features, smooth_idf, sublinear_tf?update:Maybe I should have put more details on the question:What if I am doing unsupervised clustering with bunch of texts. and I don't have any labels for the texts & I don't know how many clusters there might be (which is actually what I am trying to figure out)"
3830,TfidfVectorizer results in adding null rows and incorrect score assignment,"['python', 'pandas', 'scikit-learn', 'tf-idf']","Question: why does sklearn's TfidfVectorizer deliver scores attached to values that don't exist (i.e. the vectorizer creates null rows)? In addition, why are the scores not matching up to the appropriate attributes?Pipeline: Bring in text data from a SQL DB, split text into bigrams and calculate the frequency per document and the tf-idf per bigram per document, load the results back into the SQL DB.Current State:Two columns of data are brought in (number, text). text is cleaned to produce a third column cleanText:Drop rows with only one word:Group, then perform feature extraction:Output:Problems: Why is the TfidfVectorizer adding these rows and incorrectly assigning scores to numbers? Is it something to do with indexing? Any and all insight would be greatly appreciated! Thank you!"
3831,python - tsne reduce memory consumption,"['python', 'scikit-learn', 'tf-idf', 'sklearn-pandas']","I have a very large csv file that I am trying to do a tf-idf analysis with. I am using the tfidvectorizer for sklearn and then I want to reduce the dimensionality using tsne so I can plot the results. However, whenever I use tsne, I get a memory error. Is there a way I can prevent this from happening? I read you can use svdTruncate before implementing tsne. How would I go about doing this (or are there any other suggestions)? Here is my code:Thanks in advance!"
3832,python TfidfVectorizer gives typeError: expected string or bytes-like object on csv file,"['python', 'csv', 'scikit-learn', 'tf-idf', 'sklearn-pandas']","I am analyzing a very large csv file and trying to extract tf-idf information from it using scikit. Unfortunately, I never finish processing the data since it throws this typeError. Is there a way to programmatically alter the csv file to eliminate this error? Here is my code:The error is raised from the last line.
Thank you in advance!"
3833,Make less sparsity when selecting features in text mining,"['r', 'text-mining', 'tf-idf']",I try to create a sentiment analysis of customer reviews using R .Here the R code : My question here is : is there anyway to elliminate the stop words or to make less sparse the tf-idf matrix?Thank you in advance
3834,tf-idf document clustering with K-means in Apache Spark putting points into one cluster,"['python', 'apache-spark', 'k-means', 'tf-idf']","I am trying to do the classic job of clustering text documents by pre-processing, generating tf-idf matrix, and then applying K-means. However, testing this workflow on the classic 20NewsGroup dataset results in most documents being clustered into one cluster. (I have initially tried to cluster all documents from 6 of the 20 groups - so expecting to cluster into 6 clusters).I am implementing this in Apache Spark as my purpose is to utilise this technique on millions of documents. Here is the code written in Pyspark on Databricks:As you can see most of my data points get clustered into cluster 0, and I cannot figure out what I am doing wrong as all the tutorials and code I have come across online point to using this method. In addition I have also tried normalizing the tf-idf matrix before K-means but that also produces the same result. I know cosine distance is a better measure to use, but I expected using standard K-means in Apache Spark would provide meaningful results.Can anyone help with regards to whether I have a bug in my code, or if something is missing in my data clustering pipeline?Thank you in advance!Here is the implementation in python which does not group all documents together even with a high number of max features:I would have thought that we could replicate something similar in pyspark using KMeans with Euclidean distance before trying cosine or Jaccard distances in KMeans. Any solutions or comments?"
3835,pyLucene - How to use BM25 similarity instead of tf-idf,"['lucene', 'tf-idf', 'pylucene']","As I understand pyLucene now offers BM25 similarity also. I am using pyLucene - 4.10.1, but can't find any example as to how to use BM25 instead of tf-idf. Please guide."
3836,Average of word embeddings with TF-IDF score,"['python', 'machine-learning', 'nlp', 'tf-idf', 'word2vec']","I have been developing a python script to classify if an article is related to a body text or not. For that I have been using ML (SVM classifier) with some features, including average of word embeddings. The code for calculating the average of word embeddings between the list of articles and bodies is the following:It seems like the average word2vec is being calculated correctly. However, it has worse scores than the TF-IDF cosine alone. Therefore, my idea was to group these 2 features, by that means, multiplying the TF-IDF score of each word to the word2vec.Here is my code to do that:My problem is that this method is getting awful results and I don't know know if there's some logic that explains that (because in theory it should have better results) or if I'm doing something wrong in my code.Can anyone help me figure this out? Also, I am open to new solutions to solve this problem.Note: There are some functions used there which I didn't post the code since I thought they were not necessary. If there is something you don't understand I am here to explain it better."
3837,String similarity TF-IDF Bag of words or Word2vec,"['python', 'nlp', 'tf-idf', 'word2vec', 'sentence-similarity']","I am trying to create an application that computes the similarity between 2 strings.
The strings are not long. 3 Sentences long at maximum.
I did some research and I came across some possible solution paths.First one use bag of words: count words and compare the 2 produced vectors ( cosine similarity)The second use TF-IDF and compare produced vectors.The third is use word2vec and compare vectors.Now for the questions.Performance wise is word2vec performance better that TF-IDF for short sentences?What is the best way to train word2vec model? Should I use a large amount of text ( wikipedia dump for example) or train it using just the sentences that are being compared.How to get sentence similarity from word2vec. should I average the words in each sentence or is there a better solution?"
3838,Add document to scikit-learn's CountVectorizer?,"['scikit-learn', 'tf-idf', 'countvectorizer']",I want to add a document to a pre-generated matrix using CountVectorizer. Now I want to add another string 'third string' to words_matrix. Extending the matrix - something like this:But I can't get it to work without fit_transforming it all together. 
3839,Cosine similarity for already known pairs of duplicates,"['python', 'nlp', 'tf-idf', 'cosine-similarity', 'spacy']","I have a list of duplicate document pairs saved in a csv file. Each ID from column 1 is a duplicate to the corresponding ID in column 2.
The file goes something like this: Each Document ID is associated with text that is saved somewhere else. I pulled this text and saved each column of IDs and associated texts into two lsm databases.
So I have db1 which has all the IDs from Document_ID1 as keys and their corresponding texts as the values for the respective keys. Therefore, like a dictionary. Similarly, db2 for all the IDs from Document_ID2.
So, when I say db1[12345], I get the text associated with the ID 12345.Now, I want to get the cosine similarity scores between each of these pairs to determine their duplicate-ness. Until now I ran a tfidf model to do the same. I created a tfidf matrix with all the documents in db1 as the corpus, and I measured the cosine similarity of each of the tfidf vectors from db2 against the tfidf matrix. For security reasons, I cannot provide the complete code. Code goes like this: But this gives me cosine similarity scores against all keys in db1 for  each key in db2. Example: If there are 5 keys in db1 and 5 keys in db2, I get 25 rows as result with this code. 
What I want is to get the cosine similarity scores for just corresponding key from db1 for the key in db2. Which means if there are 5 keys each in db1 and db2, I should have only 5 rows as a result - the cosine similarity score for each pair of duplicates only.How should I tweak my code to get that?"
3840,tf-idf results analysis with python,"['python-3.x', 'scikit-learn', 'tf-idf']",I am trying to produce tf-idf on plain corpus of about 200k tokens. I produced vector counter at first that term frequency. Then I produced tf-idf matrix and got following results. My code is ResultsWhile I want to get results in following form
3841,Sci-Kit Learn and tf-idf scores for individual words?,"['python', 'scikit-learn', 'tf-idf']","I have a pandas dataframe which consists of two strings and one keyword per entry. It looks like this:What I'm trying to do is using sci-kit learn get the tf-idf of each word in the second string and compare it to a corpus of general words. But I'm not really sure how to do that. If I use tfidfVectorize() I end up with something that looks like this:But this output isn't for every word individually and it's a comparison between words in the dictionary not a general corpus... I'm not sure how to do what I'm looking for, and I was hoping someone might have some advice as the Sci-Kit Learn documentation isn't very clear. "
3842,ElasticSearch scoring / number of doc,"['elasticsearch', 'tf-idf']","I have small (max 50 char) keywords stored in text field in ElasticSearch index. I noticed that if I clear the index and add only 1 document, let's say ""samsung galaxy"", the score when I match the document is like 0.95.But when I add 500k other docs and I make the same query, the score is like 20. I would like to set a min_score for this query because I need a certain level of relevancy. But as the score is depending of the doc count. I can't set a min_score as the number of docs in the index will constantly evolve.I already looked for solutions like constant_score but I need the power of Elastic to give me a score (and not 1 or 0).1) Does this behavior come from the IDF method or not only from it?2) Is there a way to keep the current search algorythm (or just without the term frequency) and have always the same score for a query without doc count dependency ? This would allow me to set a min_score "
3843,similarity measures in vector space model,"['tf-idf', 'cosine-similarity', 'vector-space']","If I have two vectors represent two sentences contains Tf-IDF weights as following:v1 = [0.23,0.44,0.95,0.13]v2 = [0.73,0.04,0.85,0.13]The traditional (scientific) measure is cosine similarity to measure similarity between them, are there other similarity measures can be used instead?"
3844,fast way to tokenize data in Python like TfidfVectorizer does,"['python', 'python-2.7', 'nlp', 'tf-idf', 'stringtokenizer']","i used TfidfVectoriser to create tf-idf matrixNow I want to transform each element of my sentences list to list of tokens, which were used in tfidfvectoriser. I tried to extract it directly from tfidf object via this functionWhere id is index of each sentence. In this function i tried to extract given row from tf-idf object, find non-zero elements and extract corresponding elements from transformer_name.get_feature_names() . Looks too complex =). Also this solution works very slow =/Is there any way to get tokens using tfidfvectorizer's preprocessing and tokenization functions?"
3845,Is there a Python Library that calculates TFIDF from a text file?,"['python', 'python-2.7', 'python-module', 'tf-idf']",Is there a Python library or module that can calculate td/idf from links in a text file? I have heard of scikit but I am not sure how it would work with a text file. 
3846,How to compare a search query to SVD resulting 'w' matrix,"['python', 'vector', 'vectorization', 'tf-idf', 'svd']","I am working on developing a search algorithm and I am struggling to understand how to actually use the results of a singular value decomposition ( u,w,vt = svd(a) ) reduction on a term-document matrix. For example, say I have an M x N matrix as follows where each column represents a document vector (number of terms in each document)Now, I could run a tf-idf function on this matrix to generate a score for each term/document value, but for the sake of clarity, I will ignore that. SVD ResultsUpon running SVD on this matrix, I end up with the following diagonal vector for 'w'I understand more or less what this represents (thanks to a lot of reading and particularly this article https://simonpaarlberg.com/post/latent-semantic-analyses/), however I can't figure out how to relate this resulting 'approximation' matrix back to the original documents? What do these weights represent? How can I use this result in my code to find documents related to a term query?Basically... How do I use this? "
3847,Cosine similarity alternative for tf-idf (triangle inequality),"['nlp', 'cluster-analysis', 'information-retrieval', 'tf-idf', 'cosine-similarity']","I am trying to use tf-idf to cluster similar documents. One of the major drawback of my system is that it uses cosine similarity to decide which vectors should be group together. The problem is that cosine similarity does not satisfy triangle inequality. Because in my case I cannot have the same vector in multiple clusters, I have to merge every cluster with an element in common, which can cause two documents to be grouped together even if they're not similar to each other.Is there another way of measure the similarity of two documents so that:"
3848,Most representative document in a list of documents,"['python', 'nlp', 'document', 'tf-idf']","Hi I am trying to find out what the most representative document in a list of documents might be. I am wondering if there are any resources or documentation on being able to do that. I have put together some simple statistics that help me do this:So the idea is that the higher the DF is, the more representative it is of the corpus. If TF scoring is optimized for the average, so documents that overuse or underuse a high DF word are punished.It's pretty hacky but wondering if there is something better out there that people have encountered."
3849,TfIdfVectorizer with grouped tokens,"['python', 'scikit-learn', 'tf-idf']","I've vectorized ~1million of textual documents with TfIdfVectorizer.Now I need to group some (semantically similar) tokens and repeat vectorization. Replacing all tokens in the raw data with the new ones seems tedious and time consuming for me, since I intend to repeat the process for multiple grouping strategies. I've tried ValueError: Vocabulary contains repeated indices.Is there an efficient way to accomplish it without touching the input data? "
3850,Get full text from TfidfVectorizer,"['python', 'scikit-learn', 'tf-idf']","I am plotting a set of text documents in 2D and I noticed some outliers, I would like to be able to find out what these outliers are. I am using raw text and then using the TfidfVectorizer built into SKLearn.To reduce to 2D I am using TruncatedSVD.If I wanted to find what text document had the highest 2nd principle component (y axis) how would I do this?"
3851,how to set target feature dimension in Spark MLLIb's HashingTF() function?,"['hash', 'hashtable', 'apache-spark-mllib', 'tf-idf']","Apache Spark MLLIB has HashingTF() function which takes tokenized words as input and converts those sets 
into fixed-length feature vectors.As mentioned in documentation link
spark mlib documentationit is advisable to use power of two as the feature dimension.The question is whether the exponent value is the number of terms in the inputIf yes, Suppose If I consider more than 1000 text document as input which has more than 5000 terms , then the feature dimension become 2^5000  Whether my assumption is correct or is there any other way to find exponent value"
3852,How can we use TFIDF vectors with multinomial naive bayes?,"['scikit-learn', 'tf-idf', 'naivebayes']","Say we have used the TFIDF transform to encode documents into continuous-valued features. How would we now use this as input to a Naive Bayes classifier? Bernoulli naive-bayes is out, because our features aren't binary anymore.
Seems like we can't use Multinomial naive-bayes either, because the values are continuous rather than categorical.  As an alternative, would it be appropriate to use gaussian naive bayes instead? Are TFIDF vectors likely to hold up well under the gaussian-distribution assumption?  The sci-kit learn documentation for MultionomialNB suggests the following:The multinomial Naive Bayes classifier is suitable for classification
  with discrete features (e.g., word counts for text classification).
  The multinomial distribution normally requires integer feature counts.
  However, in practice, fractional counts such as tf-idf may also work.Isn't it fundamentally impossible to use fractional values for MultinomialNB?
As I understand it, the likelihood function itself assumes that we are dealing with discrete-counts (since it deals with counting/factorials)How would TFIDF values even work with this formula?"
3853,Use gensim Random Projection in sklearn SVM,"['scikit-learn', 'tf-idf', 'gensim']","Is it possible to use a gensim Random Projection to train a SVM in sklearn?
I need to use gensim's tfidf implementation because it's better at dealing with large inputs and then want to put that into a random projection on which I will train my SVM. I'd also be happy to just pass the tfidf model generated by gensim to sklearn and use their random projection, if that makes things easier.
But so far I haven't found a way to get either model out of gensim into sklearn.  I have tried using gensim.matutils.corpus2cscbut of course that doesn't work: neither TfidfModel nor RpModel are corpi, so now I'm clueless at what to try next."
3854,How to make an elbow curve from term document(tf_idf) matrix?,"['python', 'jupyter-notebook', 'k-means', 'tf-idf']","I tried the following piece of code to get elbow curve from tfidf_matrix (Which is a term document frequency matrix). But I'm getting the error as shown.
[![enter image description here][1]][1]
What can be done to resolve this?tfidf_matrix is the term document frequency matrix we got from our documents.
This is the error"
3855,How to know specific TF-IDF value of a word?,"['python', 'scikit-learn', 'nlp', 'tf-idf']","How can I know the value of a specific word using the TfidfVectorizer function?
For example, my code is:Now, how can I know the TF-IDF value of ""sentence"" in the sentence 2 (docs[1])?"
3856,Pyspark term document matrix -> term in line and document in column for term clustering,"['pyspark', 'k-means', 'tf-idf']","I am a last year french student and I am new at pyspark... I have a problem with the term-document matrix (I worked 3 days in this but without result...)
I have lot of sentence (more than 4 million) and I did the term-document matrix using pyspark.But with this code I obtain document in line and a column (and only one column). In each line of the column there is a fixed-length feature vectors of the value of the tf-idf. The problem is that: with this structure I can not transform my dataframe to matrix and so I can not do the transpose of the matrix in order to have term in line and document in column...I tried to construct the tf-idf matrix by my own (using loop) but with 4 million sentence the performance of my code was really really bad! I want a tf-idf matrix with term representing line in order to clusterize (using kmeans) line and not document... Do you have any idea in order to obtain this matrix? with good performance? Thank you for all your response,  "
3857,tfidfvectorizer prints results based on all words,"['python', 'scikit-learn', 'nlp', 'tf-idf']","Though there are six different words. There are only 5 words printed in the result. How to get result based on all words  (6 columns vector)?[[ 0.          0.          0.50154891  0.70490949  0.50154891]  [
  0.57615236  0.57615236  0.40993715  0.          0.40993715]]Also how to print the column details (features (words)) and row (document) ?"
3858,Error while training RN with GridSearch: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse,"['python', 'keras', 'scikit-learn', 'data-science', 'tensorflow2.0']","I am training a data set with keras and scikit learn using a KerasClassifier and then GridSearch for training the model. All correctly work but when I update Nvidia CUDA and some libraries to be compatible with it, it stop working.
I am using Nvidia GPU GeForce GTX 960M.
Tensorflow-Gpu version: 2.1.0
Keras: 2.4.0
Scikit-Learn: 0.23.1
The error is the following:"
3859,"207 classes ( many with one object) , Movie","['python', 'tensorflow', 'scikit-learn', 'cross-validation']","error :UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5."
3860,Using Keras' CNN model with Sklearn's SVM for binary classification (Boosting or Voting approach)?,"['tensorflow', 'machine-learning', 'keras', 'scikit-learn', 'deep-learning']",I am trying to apply a Ensemble approach for my  Binary Spam Classification model. Spam images are any image in the world which is not a picture of either handwritten or text book clicked question. I have my  non-spam images something this:So these images follow some interesting patternsSo I am trying to use these 3 features in my model. I am using this first time so I have no idea how should I be using the Ensemble.  Let us Suppose I have a trained CNN model with 1 Dense layer of 256 neurons and last layer as Sigmoid with 1 neuron. (or Softmax with 2)So my 2 candidates outputs after training would beWhch One is supposed to produce better results? Could someone please provide a dummy code for that too?
3861,"Value:Input contains NaN, infinity or a value too large for dtype('float64') [closed]",['scikit-learn'],"
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 15 hours ago.arch = pd.read_spss('data/AusentismoPres2011.sav')
arch.describe()arch_numpy = arch.to_numpy()
print(arch_numpy)distrito = arch_numpy[:,4]
distritoX = arch_numpy[:,[10,11,12,14,15]]
#X = arch_numpy[:,['porc_hogares sin_medios','alfabetismo','porc_2_nbi','IDH','GINI']]print(X)from sklearn.decomposition import PCApca = PCA(n_components = 2)X_pca = pca.fit_transform(X)Dio como resultado esto:ValueError: Input contains NaN, infinity or a value too large for dtype('float64')."
3862,Convertion of sklearn libraries to C++/Using of pkl files of trained models to predict features on C++,"['python', 'c++', 'machine-learning', 'scikit-learn']","I have a polynomial ridge model written in Python. This model saves the training models in pkl files for later predictions. Now I want to convert this script from Python to C++. In this model I use StandardScaler, PolynomialFeatures and RidgeCV to predict. How can I use the pkl models on C++ or how to convert those libraries in C++.
Python code:Then I save scaler, transformer and regr in pkl files. I want to use those models to do the predictions on C++."
3863,"How to get most important feature coefficients when i used pipeline to preprocess, train and test the linear svc?","['python', 'machine-learning', 'scikit-learn', 'classification', 'svm']","I am using a LinearSVC, i pre-processed the numeric and categorical data using column transformer,then used pipeline. I used GridSearchCV to get the best parameters for the model which i later put into the pipeline as you can see.
I fit,tested and got the score as well but i want to know the most important feature coefficients.So far, i have tried "" clf.coef_ "" as the classifier step is named as clf in the pipeline but i get a message saying clf not defined.
I also tried gridf.coef_,pipefinal.steps[1].coef_ but nothing worked.
So any help in this regard will be highly appreciated. Thanks."
3864,How to fit a stacked model on the predicted results,"['python', 'machine-learning', 'scikit-learn']","I am palning to stack the results of several models using the sklearn.ensemble.StackingRegressor. This class can take several models as base estimators, to stack them together, and a final estimator, which is a regressor that combines the base estimators. This class trains the base estimators and predict the results for each of them and then stack all of them together using the final estimator.My problem is that it is not possible for me to use the base estimators directly in this class because of some reason. I only have the predicted results for the target for models (I mean I have y_pred for 4 different models). I do not want to retrain the base estimators again because of computational time. Is there any way to use the final results (the predicted target) rather than the base estimator directly using Sklearn package?"
3865,"MultiNomialNB: ValueError: Found input variables with inconsistent numbers of samples: [7, 88394]","['python', 'pandas', 'scikit-learn', 'sklearn-pandas', 'naivebayes']","can you help me to solve this problem?
I'm trying to write naive bayes code use sklearn library. I used a dataset consist of 9 columns and about 110493 rows. I tried to classified the dataset using multinomial naive bayes. But I got this problem: ValueError: Found input variables with inconsistent numbers of samples: [7, 88394]Here's my code:The error I got:Can someone help me? I had no idea. Thank you so much."
3866,Do I need to convert sklearn predict_proba() return value from logit to probability like I do with glm?,"['scikit-learn', 'logistic-regression', 'prediction']","Do I need to convert sklearn predict_proba()return value from logit to probability like I do with glm? This question was prompted by an extremely narrow distribution of probabilities around the .5 threshold on a simple logistic regression output.The question was posed: are these true probabilities because the dispersion appears too narrow.  I have subsequently applied CalibratedClassifierCV which has increased the 'spread' of the probability outcomes, and this may help solve the problem, but the issue is whether the predict_proba needs to be modified like logit:The docs suggest there's some softmax() applied, and the results of applying conversion suggest that such conversion isn't necessary, but I wanted to ask."
3867,"Python returning Unknown label type: (array([0., 0., 0., …, 0., 0., 0.]),)","['python', 'machine-learning', 'scikit-learn', 'data-science']","i have being a trouble running the following code below, which he was supposed to fit both training predictors and training class. Idk what is happening right now, i tried debug but i get nothing in return, seems the problem comes from this slice of my code:PROBLEM I THINK IT'SHERE ARE ALL MY CODE:"
3868,explain the meaning of the [N].T in the code X_train_poly = poly.fit_transform(X_train[None].T),"['python-3.x', 'machine-learning', 'scikit-learn', 'data-science']","The full code is here plz explain the meaning of the term [N].T.its a one dimensional feature so instead of reshaping with (-1,1) how does this achieve the same thing."
3869,How to deal with dataset that contains both discrete and continuous data,"['python', 'machine-learning', 'scikit-learn', 'deep-learning', 'classification']","I was training a model that contains 8 features that allows us to predict the probability of a room been sold.Region: The region the room belongs to (an integer, taking value between 1 and 10)Date:The date of stay (an integer between 1‐365, here we consider only one‐day
request)Weekday: Day of week (an integer between 1‐7)Apartment: Whether the room is a whole apartment (1) or just a room (0)#beds:The number of beds in the room (an integer between 1‐4)Review: Average review of the seller (a continuous variable between 1 and 5)Pic Quality: Quality of the picture of the room (a continuous variable between 0 and 1)Price: he historic posted price of the room (a continuous variable)Accept:Whether this post gets accepted (someone took it, 1) or not (0) in the endColumn Accept is the ""y"". Hence, this is a binary classification.We have plot the data and some of the data were skewed so we applied power transform.
We tried a neural network, ExtraTrees, XGBoost, Gradient boost, Random forest. They all gave about 0.77 AUC. However, when we tried them on the test set, the AUC dropped to 0.55 with a precision of 27%.I am not sure where when wrong but my thinking was that the reason may due to the mixing of discrete and continuous data. Especially some of them are either 0 or 1.
Can anyone help?Here is an attachment of a screenshot of the data.
Sample data"
3870,file.py runs but file.exe doesen't,"['python', 'scikit-learn', 'exe']","I created a script that needs to be called by another program and to do it I converted it to a .exe file, however, even if the file.py runs perfectly, the .exe file keeps crashing with this message:Here's the code:Full traceback:"
3871,Fix KMeans cluster position,"['python', 'machine-learning', 'scikit-learn', 'cluster-analysis', 'k-means']","I'm trying to use KMeans for clustering RGB colors and automatically count how many pixels of each group is present on an image.
For that, I'm setting the initial position of centroids at positions I would like to categorize and running KMeans from sklearn.The problem is, depending on the image, the algorithm output changes the order of the initial centroid vector, so when I count the number of elements, it goes to the wrong color.This usually happens when I dont have one or more colors that are in initial centroids on the image. In this case, I would like it to count 0 instead.Does anyone knows how to fix the order of initial centroids on the output of KMeans prediction?Code bellow:The problem is:when I check 'pixels' variable, 0 not always correspond to black, 1 not always correspond to Col1, etc."
3872,Use same Label Encoder for train and test dataframes,"['python', 'machine-learning', 'scikit-learn', 'label-encoding']","I have 2 different csv which has a train data and test data. I created two different dataframes from these train_features_df and test_features_df. Note that the test and train data have multiple categorical columns, so i need to apply labelEncoder on them as it is suitable as for my dataset. So i had separately applied label encoder on train and test data. When i print the new encoded value of train and test dataset i see for the same categorical value of same feature the output from new encoded data is different. Does that mean i have to merge the train and test data. Then apply label encoding and then seperate them back again ?Output of above is below:-If we see in train dataframe after lebel encoding the az value in first column got transformed to value 20 while in test dataframe after lebel encoding the az value in first column got transformed to value 21."
3873,TypeError: __init__() got an unexpected keyword argument 'sparse',"['python', 'scikit-learn']","I want to one-hot-encoding the columns of an array using the OneHotEncoder in sklearn, but I encontered the following error:TypeError: __init__() got an unexpected keyword argument 'sparse' Here is my code :sklearn version is 0.23.1.How could I resolve the above error and achieve my goal ?"
3874,AWS SageMaker: Use S3 pickled models instead of hosting on sagemaker,"['python', 'amazon-web-services', 'scikit-learn', 'xgboost', 'amazon-sagemaker']","I am working on a use-case for which I have to use Amazon SageMaker notebook instances. Amazon SM resources are filled with material that works well for single model i.e. you do your thing locally on NB Instance and then deploy the model as an endpoint. My use-case on the other hand has multiple models for multiple customers and this needs to be automated. i.e. once a customer uploads a file, a model needs to be automatically created and stored.Current approach is to automate SageMaker instances through lambda for picking up the train data, training the data and saving the model back to S3 before closing the instance.My question is, is this the right approach? Or should I create an endpoint for each model for each customer? Somehow since the data size is going to be small and I am working with SageMaker for the first time, I am more comfortable with saving the models in S3 than deploying many many endpoints."
3875,apply label encoder for multiple columns in train and test dataset [duplicate],"['python', 'dataframe', 'machine-learning', 'scikit-learn', 'label-encoding']","I have a dataset which contains multiple columns which has values in string format.Now i need to convert these text column to numeric values using labelEncoder. In below e,g y is target of my tain dataset and and A0 to A13 are different features . There are 50 more features but i have provided a subset here. Now how do i apply labelencoder on for dataset from A0 to A8 together and create a new encoded dataframe for creating the model ?
I know we can do something like below, but this would say encode only one column. I want to encoder to be applied for all column from A0 to A8 and then feed the data to the model. How can i do that ?Sample data below"
3876,"Scikit-learn CountVectorizer : customizing preprocessor, tokenizer and analyzer",['scikit-learn'],"I was trying to better understand how the CountVectorizer class works.
I'm quite confused about the differences between the preprocessor, tokenizer and analyzer parameters.
In the documentation it is stated that all of this parameters can be callable, my guess is that you can produce your own function to customize the various processes.That said, I'm not sure why they are mutually exclusive (i.e. preprocessor can be callable if and only if analyzer is None, similarly tokenizer can be a callable if and only if analyzer='word' - from the doc).I'd much appreciate if someone could elucidate over the different usage of the parameters and what the relative step is supposed to accomplish.Thanks in advance, let me know if the question is not problem specific enough for stackoverflow!"
3877,Confidence Interval for Predictions Made by Sklearn LassoCV Regression Model,"['python', 'scikit-learn', 'linear-regression', 'statsmodels', 'confidence-interval']","I have a data set ""X"" with ""n"" samples, which was used for training a lasso regression model.
There were ""p"" predictors in the dataset from which the LassoCV selected ""k"" of them in the final trained model. I will use this model to make predictions.
I want to know the Confidence Interval for my predictions done by this model.Assuming ""y"" are the true lables for my data, and ""y^"" are the predicted values by my model, and 90% of confidence to be of interest, the following formula was used:I was wondering if this formula is correct. If not, I appreciate it if you could provide me with some link or tutorial for that.Thanks in advance for your opinions and comments"
3878,Best way to write an inplace option in a python method,"['python', 'pandas', 'scikit-learn']",I'm writing a extension for sklearn preprocessing. I have problem to write a 'inplace' version of the method you would typically see in many pandas methods. So my methods can either return a new df or change the input df directly.Basically I'm inputting a X to the encode method. I want to be able to change input X inplace or just return a new copy. I changed X copy new_data for many times. But unable to change X inplace. I know my way is wrong. But how to change it? Thank you.
3879,Is it possible to get prediction from SVC classifier using probabilities?,"['machine-learning', 'scikit-learn', 'svm']","I am using SVC classifier from sklearn.  So far, in my data I have two features A and B, and for each feature the SVC classifier can classify a row of data as either 0 or 1 using SVC.predict().  I have stored the probability scores from SVC.predict_proba() for both features.I want to perform something called score-level fusion.  This would mean for each row of data, we average of the probability scores for both features, and use this new probability to classify the row as either 0 or 1.  Since I stored the probability scores for both features, I can easily get averages.Is there a way to feed the sklearn SVC classifier these averaged probabilities and get new predictions from SVC.predict()?"
3880,_joint_log_likelihood give me wrong values,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'naivebayes']","I have code like thisthat contain 3 Data training 1 class each class(-1),(0),(1)
and 1 data testso now we know that we have word c and f in our data test
i fit the data to MultinomialNBNow the problem is when i try to calculate the Class Maximum log Posterior of the test data its should be
P(c) + P(w|c) in sklearn known by _joint_log_likelihoodso we can manual calculating that by predicting word [c f]but when i try to output it by the system the output was not matchwhat is wrong in the MultinomialNB?
of _joint_log_likelihood?
on naive_bayes.py documentaion of MultinomialNB
said the codeMaybe you can do review and tell me this is the data
Data
HOPE you guys can answer it"
3881,New to ML: ValueError Found input variables with inconsistent numbers of samples,"['python', 'scikit-learn']","Can anyone tell me what am I doing wrong? I have just finished the first ML course in Kaggle and it's my first time doing a solo with a football dataset. I got stuck with this error:ValueError: Found input variables with inconsistent numbers of samples: [66, 23]This is my code (apologies if code is a bit awkward - I am very new)Here's a sample of the data set:"
3882,GridSearchCV for learning rate,"['scikit-learn', 'grid-search', 'learning-rate']","I'm trying to use GridSearchCV to find the best parameters in a CNN but when I try to find the best combination for learning rate and batch size the code doesn't work (it works if I use instead of learning rate, epochs). Any idea why it is not working?The error I get is 'ValueError: learn_rate is not a legal parameter' but I did it just as in an example I found that worked for the epochs but not working for learning rate."
3883,Trying to use sklearn-crfsuite to ensemble models gets awful results---what am I doing wrong?,"['python', 'scikit-learn']","So my brilliant thought was that I could use sklearn-crfsuite to ensemble NER model predictions from multiple models; which would utilize transition probabilities as well as posterior probability ""percentiles"" from each of the models to build a better result than any of the models can.The idea was to put in features for the predicted label for each position and surrounding position --- as well as tags that would indicate discrete ""confidences"" for each of the possible tags, for each model.However, when I try to ensemble BERT and Flair models, the results on ensemble on Kaggle-NER data is significantly worse than either model alone (for the WNUT task, it is only slightly worse than either result alone).So I thought I would simplify features as much as possible --- essentially, just one feature --- predicted value for each tag from the Flair model (along with BOS and EOS features for beginning and end of sentence, which is really just additional transition probabilities).  For example, this is my feature list for the first sentence in the training set for Kaggle-NER:
''''''When I train the CRF, on both training and test set I still get substantially worse results than just giving the same prediction as the Flair model would give--- Even on training set, there are 530 ""positive"" differences and 755 ""negative"" differences.I have tried varying the c1 and c2 parameters for crfsuite training, but only get slightly better or worse results.  What am I missing, here, and is there a better way of doing this?By the way, when I look at feature weights, the highest weights are always the flr predicted tag, but other features can also have positive weights --- for example, flr_0: I-nlpOrganization has a +.27 weight for tag=O (compared to +4.947 weight for flr:0: O)"
3884,Missing values Imputation with five fold cross validation using python,"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'imputation']",I have a dataset of 165 instances and 49 features with target 1 and 0. This dataset has missing values so i am trying KNNimputer with the five fold cross validation. Here is the code:But the problem here is I don't need a score. I want the dataset (in five folds or whole) after filling the missing values in the folds because I need to do feature selection using the five folds after the imputation and then classification. So how can i get the dataset after imputation?
3885,What is the impurity of a sklearn regressor?,"['scikit-learn', 'regression', 'decision-tree']","Some regressors such as DecisionTreeRegressor have the parameter min_impurity_decrease, but I cannot find any documentation as to what is meant with impurity. Is the impurity the same as the criterion parameter? So when I set criterion='mae', is the mean average error also used in the impurity formula?Thanks!"
3886,"Statsmodel logit producing param nans, large std err, warnings, but model performance is fine","['python', 'scikit-learn', 'logistic-regression', 'statsmodels']",I keep getting warnings such asand my model summary is full of nans and very large standard errors. The model performance is near identical with what I get when I train with sklearn so it works fine for predictions. But why am I seeing so many weird numbers? I've seen answers about perfect separation causing similar issues - but that is not the case here? I've seen with real data but I get the same issues with generated data as well.Code to reproduceOutput:
3887,ModuleNotFoundError when running randomforest in flask as a docker image,"['docker', 'flask', 'scikit-learn']","I built a random forest regressor on iris dataset, I dumped the pickle file, loaded it up and everything is running perfectly but when I run it as a docker image it gives me ModuleNotFoundError: No module named 'sklearn.ensemble._forest.
I guess it boils down to my DOCKERFILE which looks like thisCan you please help me out here?"
3888,Sklearn OneHotEncoder equals,"['python', 'python-3.x', 'scikit-learn']",I made some tests on sklearn OneHotEncoders.I'm looking for an effective way to compare if my 2 encoders do the exact same thing. I already compared it on a random dataframe :But the fact it works the same in one df doesn't prove they'll always work the same. Is there a way to be sure of that ?
3889,Single image feature reduction at inference time,"['python', 'scikit-learn', 'pca', 'feature-extraction']","I am trying to train a SVM classifier using scikit-learn.. At training time I want to reduce the feature vector dimension.
I have used PCA to reduce the dimension.PCA requires m x n dataset to determine the variance. but at the time of inference I have only single image and corresponding 1d feature vector.. I am wondering how to reduce feature vector at inference time in order to match the training dimension."
3890,DecisionTreeClassifier: associating names to labels,"['python', 'scikit-learn', 'decision-tree']","I am using sklearn's DecisionTreeClassifier; it is said that the class labels can be of type str.When I plot the decision tree using sklearn.tree.plot_tree, I can specify the class_names, which the docs says should be ""Names of each of the target classes in ascending numerical order."" But what about for str classes/labels? Is it the lexicographical order then?"
3891,Scikit-learn: find records with maximum residuals,"['python', 'scikit-learn', 'regression']","I am modelling a linear equation with scikit-learn,Train set size is 35k records, test set size is 9k recordsI get coefficients which are very much inline with what I am expecting: R2 is higher than 0.98, MAE is low (around 13-14, for predicted values of 5000-10000; this was expected, due to measuring tool precision).However, I took a look at max_error and it is huge: over 1500; and this I cannot explain right away.Is there a way to find the records which have the residual equal to max_error, or greater than a certain value ? Probably I did not do a proper data cleaning before running the model, but I'd like to take a look at those records, to see if data cleaning is the problem, or if it's something else."
3892,calculating prices in the load_boston dataset in python,"['python', 'pandas', 'scikit-learn']",i am new to machine learning and need to build a house price prediction model for my project anyone who can help or guide me through i have no experience at all and i am just reading things online that seem to be way beyond my understandingi am using the tutorial from this https://www.ritchieng.com/machine-learning-project-boston-home-prices/ but i get a very different value for pricesbelow is my output which is far from what is in the tutorial and doesn't make sense how a house will be $5
3893,Partial Dependence Plot for a Keras Neural Network,"['python', 'tensorflow', 'keras', 'scikit-learn', 'neural-network']","I have a densely connected neural network that was built using the Keras Sequential API. I'm trying to create some partial dependence plots (PDP's) to use for a bit a sensitivity analysis. I am attempting to use the scikit-learn plot_partial_dependence function in order to do this. I've been getting the following error: ValueError: 'estimator' must be a fitted regressor or classifier.. When it first happened, I added the use of KerasClassifier. I've used it successfully in the past to use my Keras model in scikit-learn GridSearchCV. I'm still getting the same error. I've also tried KerasRegressor.Can anyone tell me what's wrong and how I could fix it? Do I absolutely need to use scikit-learn's decision tree based functions to be able to use the PDP function? If yes, what's the biggest implementation difference between Keras neural networks and decision trees? (I've never used decision trees. My machine learning experience is limited to Keras.)My relevant code is below and I'm running python on google colab's GPU. I'm sure there are several issues in that last line but I can't get past this one to figure them out."
3894,ROC curve sklearn probabilities vs. scores,"['python', 'scikit-learn', 'roc']","I don't understand how to interpret the output of sklearn.metrics.roc_curve(x, y) when my y are prediction labels and not probability scores.Here's the simplified code example:I don't understand how to interpret this output for tpr, fpr and threshold. I mean I understand what the output is but I don't understand how to interpret three values for each False Positive Rate, True Positive Rate and Threshold.Also when I use probability scores:Here I get different values in the output, and this time it has a better resolution. Can anyone explain how to interpret values in both the scenario?My main goal is to adjust threshold to obtain high AUC score, and so I need to set a class prediction as Positive or Negative depending upon threshold selection, but that implies I will be using class labels for y and so I will only have three values for each FPR, TPR and Threshold."
3895,"How do I create a linear regression graph using Matplotlib, pandas, and sklearn?","['python', 'pandas', 'matplotlib', 'scikit-learn', 'linear-regression']","I am trying to create linear regression graph using Matplotlib, pandas, and sklearn. However, it does not seem to be working and I am not sure why."
3896,CalibratedClassifierCV doesn't work with Pandas DataFrame for Pipeline?,"['python', 'pandas', 'scikit-learn', 'pipeline']","I built a classification model using sklearn Pipeline. Now I want to run CalibratedClassifierCV to calibrate the probability prediction.Pretrained model was loaded from a file, it basically composed of a scaler and a gradient boost classifier. If I print out the model, it looks like this:For some reason I got the following error while executing clf_iso.fit(train[features], train[['trouble']]).train[features] is clearly a Pandas Dataframe (confirmed by type(train[features])). Why does it still see it as numpy array. If I just retrain the same data again with loaded model model.fit(train[features], train[['trouble']]), it works without any issue. Why doesn't it work with CalibratedClassifierCV?EDITLooks like a known issue link"
3897,Increase the accuracy of LogisticRegression,"['python', 'machine-learning', 'scikit-learn', 'logistic-regression', 'training-data']","Here is my code:Here i tried to include all data row of Annulus_Pressure and Receive_Pressure to the training, but it seems like it only fitting for the first 5 data. Also, the accuracy from this training is really low.This is what i got:Total Data that i would like to train
Fitting only 5 Data
The accuracy is really low
Please help, thank you."
3898,SimpleImputer TypeError: '<' not supported between instances of 'str' and 'int',"['python', 'pandas', 'numpy', 'scikit-learn']","I am having a problem when I try to impute this data set. When I try to gather object class items to impute them, it throws this error.TypeError: '<' not supported between instances of 'str' and 'int'"
3899,MaxAbsScaler - AssertionError: assert shape[0] == shape[1],"['python', 'scikit-learn']","I work with scikit-learn-0.23.1.I am using sklearn.preprocessing.MaxAbsScaler which I have fitted on some data.Then I call it to transform some new data called X_all.Specifically, X_all have shape (1, 1790196) but I receive then this error:However, if I reduce the number of columns to (1, 888282) then I do not receive anymore this error.Why is this happening?This bug is also mentioned here but for slightly different reason: https://github.com/scikit-learn/scikit-learn/issues/5433.Another peculiar thing is that when I did fit_transform on data of (494800, 1790196) shape then I received no error again.Also, just for experimentation, I duplicated my (1, 1790196) data and made them (2, 1790196) and again I was receiving an error"
3900,"KeyError: “[''] not in index” while normalizing multiple specific columns, using StandardScaler()","['python', 'scikit-learn', 'normalization']","Trying to normalize only specific numeric columns, using StandartScaler()Checked several SO questions, but failed to resolve.All columns are floats or integersColumn A is float64.Current DF"
3901,Extract feature importance of ngrams in tfidfvectorizer in SVC(kernel='linear') model,"['python-3.x', 'scikit-learn', 'jupyter-notebook', 'svm', 'tfidfvectorizer']","I'm wondering what causes difference in output when it should be the same. It's like my program is ignoring the sorted function and feature_names. The sorting of the coef_ is quite crucial for me to find out which features are actually helping the predictions the most. I get the individual words from vectorizer.get_feature_names but not when it is in a loop or function definition. Does anybody have any idea what could be happening, or if anybody has other methods of extracting ngram feature weights and their names for an SVC with kernel='linear'.My code:My output:What the output format is supposed to be:Linked to second reply on How to get most informative features for scikit-learn classifier for different class?
Also linked to this is the exact question posted as the last reply on the second reply to this post's question:Amazing @alvas I tried the above function but the output looks like this:POS aaeguno móvil   (0, 60)  -0.0375375709849   (0, 300) -0.0375375709849   (0, 3279)    -0.0375375709849 instead of returning the class, followed by the word and the float. Any idea of why this is happening?. Thanks! – newWithPython Mar 15 '15 at 0:45But no one has replied to this, and since I have a very low reputation, I cannot request more information there either.It's taken a week out of my schedule and I really cannot afford to spend much longer on this. It's the last piece of the puzzle that is my thesis, which is not going to be perfect but I just need to get it done and graduate. So any help would be greatly appreciated!
Also let me know what I could add to make this question clearer, it's maybe my third or second one on this platform."
3902,Display images whose URL is given in a Dataframe in JupyterNotebook,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'ipython']",This is the code I am using to print the image whose url is stored in 'medium_image_url' column of the dataframe by the name of data. Please tell me what to do to print the image also .
3903,Difference between Mclust in R and Gaussian Mixture in sklearn,"['r', 'scikit-learn', 'cluster-analysis', 'mclust']","I was performing Gaussian Mixture modeling to find clusters within my dataset. First I used sklearn's Gaussian Mixture class and learned that 3 clusters was the optimal number according to BIC.Then I wanted to double check this in R as follows:However, in R, mclust said 13 clusters with VEE model. Where did the difference arise? I tried to set GMM min_covar=0.0000001 as stated in this answer, but that documentation is no longer supported. My thought is that it had to do with either some minor scaling differences or the covariance_type. Is it recommended to tune the convariance_type in sklearn? Is covariance_type='full' more similar to VVV model in mclust?"
3904,"Found input variables with inconsistent numbers of samples: [14559, 1455900]","['python', 'scikit-learn', 'regression', 'prediction']","I am facing some problems when I try to fit the model. This happens when I try to use LogisticRegression, Naive bayes or svm models. But I get results when I use random forest regression or decision tree.The error says:ValueError: y should be a 1d array, got an array of shape (20799, 100)
instead.The solution is to use y_train.ravel() when I fit the model. But then again, the below error appears:Found input variables with inconsistent numbers of samples: [14559,
1455900]Here's my code:I have been struggling with this for around 80 hours or so. Please help."
3905,Iterating in Dataframe's Columns using column names as a List and then looping through the list in Python,"['python', 'pandas', 'scikit-learn', 'label-encoding']","Im trying to LabelEncode particular columns of a Dataframe. I have stored those column names in a list(cat_features).
Now i want to use a For loop to iterate through this list's elements (which are strings) and use those elements to access dataframe's column. but it saysSince Im accessing the element of the list which is already a string. so i dont understand why it throw that error.
Please help me understand why it doesn't work and what can I do to make it work."
3906,"In python, do you have to create dummy variables to train a model such as RandomForests?","['python', 'r', 'machine-learning', 'scikit-learn']",Can you not just use categorical variables like in R where you can feed factor variables into the model?
3907,ValueError: Invalid parameter when fiting gridsearchcv,"['python', 'machine-learning', 'scikit-learn', 'pipeline']",This is my codeThis is the error:I dont know whats wrong. I copied the parameter and model names from the named_steps of the pipeline. If I run it without a parameter grid it works so the problem is most likely there.
3908,cross_val_score default scoring not consistent?,"['python', 'scikit-learn', 'cross-validation', 'decision-tree']","According to the docs,for the cross_val_score's scoring parameter:
If None, the estimator’s default scorer (if available) is used.For a DecisionTreeRegressor, the default criterion is mse. So why am I getting different results here?If I had to guess, it seems the default scoring is R2 instead of mse. Is my understanding of default scorer correct or is this a bug?"
3909,Having trouble creating a scatter plot for my kmeans clustering data,"['python', 'pandas', 'matplotlib', 'scikit-learn']","I am trying to use kmeans clustering to perform some anomaly detection on a simple dataset.I have some data in two variables.  x and x_ax.
Here is an example of the x dataThe x_ax data are timestamp values...The idea is that the first value in the x data is related to the first element in the x_ax data.I.e. 2018-01-01  --> 44360.125I instantiated a Kmeans cluster instance:I then calculated the distance from the center for each point in x, sorted it and then extracted the top 5 greatest distances from the center (i.e., my potential anomalies).I then attempted to plot this data as a scatter plot where dots marked in red were potential anomalies.Unfortunately, I got a plot that looks like this:The y-axis ticks seem to be correct, but why is the x-axis tick range going from 0 to 4000 in increments of 2000 for the first value, and then by 400 after that?Also, why has my plot got all of the values in the upper right as a straight line except for one red dot in the lower left?Any help appreciated."
3910,Missing weather data [closed],"['python', 'machine-learning', 'scikit-learn', 'data-science']","
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                Closed 3 days ago.I have weather data from February to July in which the data between the first week of march is missing. I want to impute and generate values from historical data of the same dataset. Which method should I use to generate this weather information? I have temperature pressure humidity data."
3911,ValueError: feature_names mismatch: when using XGB within RandomizedSearchCV but not in XGB alone,"['scikit-learn', 'xgboost', 'gridsearchcv']","I am using RandomizedSearchCV on an xgboost with early stopping within an imblearn pipeline. If I run the code using RandomizedSearchCV, I receive the following error:Here is the codeIf I alter the code to use pipeline.fit without the RandomizedSearchCV, the code executes without error. All github issues and Stackoverflow posts seem to point to an xgboost issue with DMatrix, but since it runs outside of RandomizedSearchCV, this seems odd. The reason I chose to use the parameter error_score='raise' is because I noticed the cv_results_ showed NaNs for all test_score values.Versions:I tested this out in two dev environments.Python 3.6.10 (same issue in 3.7.7)sklearn 0.23.1xgboost 1.1.1 (same issue in 0.90)imblearn 0.6.2 (same issue in 0.7.0)"
3912,How to deal with OverflowError: value too large to convert to npy_int32 in scikit-learns LinearSVC?,"['python', 'dataframe', 'scikit-learn', 'large-data', 'feature-selection']","I have a large matrix (150000 by 32000) I am trying to run a linear SVC using scikit-learn to perform a lasso regressionlsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(df, y)I am receiving the following error text:My current script loads the table into a pandas dataframe object then does the following:I am getting the following error: OverflowError: value too large to convert to npy_int32Any thoughts how to overcome this error would be appreciated. I have thought about converting the table to npy_int64, but am not sure how (already importing the dataframe using high float_precision and low_memory=False)."
3913,How is the MSE of each node in the DecisionTreeRegressor of scikit-learn calculated?,"['python', 'scikit-learn', 'decision-tree', 'mse']","I'm trying to figure out this calculation by hand. But I do not understand all the steps to how regression trees are split.My target is drug effectiveness and my feature is dosage.
The tree it produces is below. How can I calculate mse by hand to get the same outcome as sklearn?"
3914,Criteria used to create and select leaf nodes in sklearn,"['python', 'scikit-learn']","I just want to know the details of what (and how) is the criteria used by sklearn.tree.DecisionTreeClassifier to create leaf nodes. I know that the parameters criterion{“gini”, “entropy”}, default=”gini” and splitter{“best”, “random”}, default=”best” are used to split nodes. However, I could not find more information about the threshold used for spliting.There are some methods involved in the creation of leaf nodes: post-pruning (cutting back the tree after a tree has been built) and pre-pruning (preventing overfitting by trying and stopping the tree-building process early).  It would be very useful to know more details about the criteria used for splitting to have a better understanding and be able to customize these models even more."
3915,Random forest gives different results on different platforms,"['python', 'pandas', 'numpy', 'scikit-learn', 'random-forest']","I am running Random forest algorithm with scikit-learn v0.22.1 on two different platforms (my desktop and a cloud platform).The results I get are different on both, even though all parameters, seed etc are same in both cases. Any idea why this might be happening.I have also checked the ordering of the columns - that matches as well."
3916,Printing confusion matrix for multiclass random forest in pythom,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'random-forest']","I am working with sklearn.RandomForestClassifier and I have 11 classes. My data is in the dataframe, all variables are hot-encoded.  The classes are strings such as 'Potato', 'Tomato','Straberry', etc.When I'm trying to print confusion matrix, I get the following:When I am passing an index:What would be the best way to address this?"
3917,Difference Between NearestNeighbors and KNN Classifier?,"['python', 'scikit-learn', 'knn']",I just wondering what is the difference between NearestNeighbors from sklearn.neighbors and KNN Classifier from sklearn.neighbors ?
3918,How to combine two outputs of KernelDensity from sklearn (Python),"['python', 'scikit-learn', 'kernel-density']",I want to multiply two Kernel Density Estimates to get a composite likelihood ... The multiplication operator doesn't exist for sklearn.I have KDR for data from 3 independent sources and I want multiply KDE from each sources. The below code is from https://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py and should run easily.Although I made some small modification to get get two KDEs and then multiply them which fails.Someone can help. Below code should run without error except the multiplication part.
3919,Convert pipelined XGBoost 0.90 model to XGBoost 1.0+,"['python', 'scikit-learn', 'xgboost']","Suppose I have an XGBoost 0.90 model set up like so, using scikit-learn pipelines to do some preprocessing. I want to upgrade to XGBoost 1.0+ and still be able to use this model, without having to refit it.If I pickle the pipeline when I have XGBoost 0.90 installed, I can't load it when I subsequently have XGBoost 1.0+ installed. The XGBoost docs suggest that I should convert the 0.90 pickle using a script that they've provided:https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#loading-pickled-file-from-different-version-of-xgboostThe script is available here:https://github.com/dmlc/xgboost/blob/master/doc/python/convert_090to100.pyHowever, this script only works for XGBoost Booster objects. So I've tried the following:Now I'm a bit stuck though. How I can reassemble my pipeline using the mapper from my original 0.90 pickle and the booster loaded from the exported file?Here's a bit more detail about how I saved and reloaded the booster:First, I pickled the final estimator from the pipeline:Next, I ran the conversion script on the pickle:Then I imported the file produced by the script:Then I produced a pipeline using this reloaded model and tried to produce predictions with it:Further update: this worked after all."
3920,AttributeError: 'numpy.ndarray' object has no attribute '_iter_test_masks' [closed],"['numpy', 'scikit-learn', 'cross-validation', 'kernel-density', 'gridsearchcv']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 4 days ago.I am trying to use sklearn GridSearchCV to perform K-fold cross-validation to select a bandwidth for KernelDensity estimation.When I implement grid.fit(data), I receive the error:Here is my code:"
3921,Impossible Accuracy score needs to be roasted [closed],"['python', 'scikit-learn', 'data-science', 'naivebayes']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 4 days ago.i trained a model based on raw client's data.
Data is about 400,000 rows and 8 colunms, and after massive data manipulation done in python its
15,000 rows (grouped) and 800 columns.I checked a few models such as MLR,logistic regression, random forest, and naive bayes.
Naive Bayes had an accuracy score of 0.98 when (y_pred,y_test) taken,
and score of 0.9967 for the whole dataset (predict, actual),  which i think is impossible.the code is very simple:I know im wrong somewhere/ i dont use something in the correct way.
Please enlighten me."
3922,LinearRegression in Sk-Learn [closed],"['scikit-learn', 'data-science', 'linear-regression']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 4 days ago.I am learning a Data Science course in Udemy and I was get struck when I was learning the Linear Regression method.When i call a Linear Regression method for example: reg = LinearRegression().fit(x_matrix,y) and the output of above code is LinearRegression() without any parameters. But the actual output must contain parameters such as normality, Copy_X etc...
Image of the Error Code"
3923,How can I improve a classification algorithm for dogs and cats?,"['python', 'machine-learning', 'scikit-learn', 'classification', 'sgd']","The following code is a ML algorithm trained to classify between dogs and cats, the database is composed by 25000 images (evenly split) and can be obtained at this Link (if you click it will automatically download it!)Half of the code is basically the tutorial that sentdex has created (Link) while the ""prediction part"" is the application of the SGD classifier from SK Learn. When trying to cross-validate, results are quite poor.Could you please help me understand what can be improved and what has to be changed? Moreover, do you recommend using SK learn for machine learning?"
3924,Can anybody explain me how does this command work? [closed],"['scikit-learn', 'data-science', 'linear-regression']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 4 days ago.X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)"
3925,Python Scikit-Learn Order of Model Prediction Output,"['python', 'scikit-learn', 'linear-regression']",Let's say I have code like the followingIs the output of the prediction in the same order as the data input? This would affect whether the column of residuals is accurate.
3926,Gaussian mixture with 1D data,"['python-3.x', 'scikit-learn', 'gaussian', 'data-fitting']","I'm trying to fit multiple gaussians with GaussianMixture to 1D data (512 points).It seems that the fit is not working properly, here's the plot of the data with the fit (see image below).

The data is simulated and normalized, so I'm passing the correct number of gaussians (peaks) to adjust.I've tried different approaches but couldn't find any solutions even looking at similar questions posted here.What am I doing wrong? Thanks.Code:"
3927,Performing logistic regression analysis in python using sklearn,"['python', 'pandas', 'scikit-learn', 'logistic-regression']","I am trying to perform a logistic regression analysis but I don't know which part am i mistaken in my code. It gives error on the line logistic_regression.fit(X_train, y_train). But it seems okay as i checked from different sources. Can anybody help?
Here is my code:"
3928,Does sklearn consider the probability distribution family of the dependent variable in the plot_partial_dependence,"['python-3.x', 'scikit-learn']","I have been working with sklearn (python) for non-linear models. Especifically, I am working with generalized linear models  (GLM) (i.e.: poisson, binomial distributed data, etc.).Under these GLM, I would like to apply a partial-dependence-plot (PDP) to visualize the importance of the covariates of my model. SKLEARN is a great tool for evaluating and plotting PDP. Nevertheless, I am unsure of its implementation.Therefore, my question regards whether or not sklearn partial_plots does consider the family distribution of the dependent variable in the PDP analysis.For example, given sklearn HistGradientBoostingRegressor example for poisson GLM, a partial plot is presented. Nevertheless, there is no indicative whether the inverse-link function is applied to evaluate the partial dependences in respect to the dependent variable.A simplified snippet is presented below.Below I present an ""ideal"" alternative (future) solution.Obs: this example is not yet implemented...Any suggestions on the subject are appreciated."
3929,Getting TfidfVectorizer (SciPy Sparse Matrix) into a 2D list,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'data-science']","I am currently trying to get the output from a TfIdfVectorizer into a 2D list for other sklearn purposes.The documents are a 1D list (length: 101) of strings, each with sentences that I wish to get the TF-IDF vectors of each of the words. The output for the code above is shown below.There are many more lines of output. I would just like to get the second column of numbers, and put it into a 2D array, without the indices on the left column. Any advice?"
3930,RandomForestClassifier Get Top N Predictions and Respective Probabilities,"['python', 'python-3.x', 'pandas', 'scikit-learn', 'random-forest']","I have a Random Forest model model and have been able to get its top 3 predictions for each entry usingpredictions = model.classes_[numpy.argsort(model.predict_proba(params_only))[:, :-3 - 1:-1]]However, I also want a similar list of the probability NUMBERS that go along with each prediction. The above code only outputs the labels but I also need the probability numbers.The end goal is to get a list of the top1, top2, and top3 predictions as well as top1prob, top2prob, and top3prob lists so that I can feed it into a pandas dataframe. I have done the first part withIf I wasnt clear enough and/or you have any questions, please ask them! Thank you!NOTE: A current solution I have isbut I am not 100% sure if this is accurately giving me what I want or if there is a better strategy."
3931,What is the logic behind sklearn LDA classifier? [closed],"['scikit-learn', 'linear-discriminant']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 4 days ago.I do understand the logic behind LDA as a dimension reduction method. But how does the sklearn LDA model perform classification on data with multiple classes? Does it use other linear methods such as svm or leaner regression to do classification after the data points were projected onto a lower dimension? What's the next step after calculating the projection matrix W by solving the eigenproblem?"
3932,How do I use SciKit's IterativeImputer on a three dimensional dataarray?,"['python', 'scikit-learn', 'interpolation', 'missing-data', 'python-xarray']","I have a netcdf file that has a 3 dimensional variable with some missing values. I need to fill in the missing values before I can do further calculations (linear regression). I don't want to drop the values. When I tried using Scikit's Iterative Imputer, it gave me the following error:Found array with dim 3. Estimator expected <= 2.This is a snippet of my code.I've also tried fit_transform, and I get the same error. hus is dependent on (time,lat,lon)."
3933,Does scikit learn provide an easier solution to getting a confidence level for the coefficient?,"['python', 'scikit-learn', 'linear-regression']","I'm working through Introduction to Statistical Learning, but I'm having trouble coming up with a confidence interval for the coefficient using scikit learn (I'm trying to replicate the code in Python).Using standard deviation and 95% confidence level:Using OLS:I get different results for both so I'm not entirely sure which one is more accurate. Any suggestions? Does scikit learn provide an easier solution to getting a confidence level for the coefficient?"
3934,Faster alternative to using Sklearn Polynomial Features to create interaction variables?,"['python', 'scikit-learn']","I am using Sklearn Polynomial features to create interaction variables for an array of 769 features with around 3,000 rows. On a Google Colab GPU, it takes about 3-8 seconds. Is there any way of speeding this up?Here is the code I am using with the timing:"
3935,Imblearn Pipeline resulting in poor metrics,"['python', 'machine-learning', 'scikit-learn', 'imblearn']","I am working on an imbalanced dataset which is created using the below codeI tried getting rid of the imbalance using SMOTE oversampling and then tried fitting a ML model. This was done using the normal method and then by creating a pipeline.Normal methodOutput - Accuracy: 0.93, Precison: 0.92, Recall: 0.86, F1: 0.89PipelineOutput - Accuracy: 0.96, Precison: 0.19, Recall: 0.84, F1: 0.31What am I doing wrong when using a Pipeline, why is the Precision and F1 score so poor when using a pipeline?"
3936,Boosting algorithm realization in Python [duplicate],"['python', 'scikit-learn', 'boosting']","Using sklearn one can construct Bagging algorithm for non-trees estimator (for example, for SVC). But there is no Boosting realization in sklearn or in any other well known packages. Am I missing something and there is some existing Boosting algorithm with ability to select any other estimators other than trees in standard approach?"
3937,pattern detection using scikit fuzzy [closed],"['python', 'pandas', 'plot', 'scikit-learn', 'fuzzy']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 4 days ago.I am trying to use scikit fuzzy library to find patterns from preliminary results that are shown here. There are three different trends across all 24 cpmbinations. I really appreciate some help to use fuzzy-logic scikit with this data."
3938,How to fix: “error”: “Prediction failed: unknown error.” in custom prediction routine with scikit-learn?,"['python', 'scikit-learn', 'gcloud', 'gcp-ai-platform-training', 'google-cloud-ai']","I am trying to write a custom prediction routine on Google's AI Platform using scikit-learn's MLPClassifier. I have packaged and deployed the model successfully, but when I request online predictions via gcloud ai-platform predict, the error I get the error ""error"": ""Prediction failed: unknown error."" I then went to the console to test my model manually in the ""Test & Use"" section of my model and received the same error.The training vectors are numpy arrays with 6 elements (e.g. [1,2,3,4,5,6]) and the targets are either 0, 1, or 2.Here is my preprocess.py code:Here is my predictor.py code:Here is the code where I train and export my model:setup.py:I have tried serving online predictions with the input.json file looking like thiswith this commandand I get the error above. Can someone please help? I wish Google AI Platform had more informative error messages."
3939,Is variable importance a two way route?,"['python-3.x', 'machine-learning', 'scikit-learn', 'xgboost', 'gbm']","Assuming I have a dataset, dt, with 20 variables, var1, var2, var3, ..., var20And i run a xgboost using:From this model I can get the variable importance using model.feature_importances_My question is, if var2 scores high on the variable importance when Y = dt['var1'] and X = dt.drop(columns='var1'), does this necessarily mean that var1 will also score high on the variable importance when  Y = dt['var2'] and X = dt.drop(columns='var2') ? If not, why ?"
3940,monitoring estimator progress and sklearn wrappers,"['python', 'scikit-learn']","I am implementing my own sklearn estimator class which fits a model by minimising a custom loss function iteratively using SGD. I have written it in such a way that I can supply a validation data set via set_params and it will regularly measure performance on the training and validation sets during the SGD iteration and store the results in a progress_ attribute.This works fine if I instantiate the class myself, but what happens if I want to use it via one of the sklearn wrappers like GridSearchCV? When doing cross-validation the validation set is
different for each fold but I can't see a way to tell GridSearchCV to update the parameters of my estimator accordingly."
3941,Unable to find deployments even after getting a scoring endpoint,"['scikit-learn', 'ibm-cloud', 'linear-regression', 'ibm-watson', 'watson-studio']","I am  trying to deploy my model on watson studio via jupyter notebook.I got a scoring endpoint with a status of deploy_success but was unable to see any deployments
I used watson machine learning python clientthis was the code i usedI was even able to see service under client.deployments.list()
I was unable to connect it via cloudfound.So need help deploying this model"
3942,Random Forest Feature Selection,"['scikit-learn', 'random-forest', 'feature-selection', 'auc']","Am seeing a precise scenarioI am running a random forest model with two independent variables(a,b) where both of them have significant variable importance according to sklearn (b:0.633, a:0.3666) and am getting an AUC of 0.86However, when I am adding a third independent variable 'c' which I feel is very relevant, it shows feature importance as (a: 0.32, b:0.44, c:0.24) but now guess what
AUC is reduced to 0.81.... Isn't it like if a feature  of better importance getting added should improve AUC... Forgive me if am asking a stupid question"
3943,my SVM classifier is acting unexpectedly (only gives 2 values or has a strange 'below chance' accuracy),"['scikit-learn', 'svm', 'libsvm']","I'm applying an SVM to some sparse data and I'm getting a strange result with the following:somehow, the acc_tp and acc_null_tp, despite the 5 different splits and n_repeats, are consistently coming out to 1 of 2 values.Shouldn't the scramble produce stuff below theoretical chance at 0.5 as well as above? (This is actually a 1 vs 3 classification, but apparently it performs many one vs one classifications and averages). I'm also surprised there aren't more values in general.Once we get to the region of expected signal, I get the following:
A strange reflection in the scrambled accuracy below baselineAnyone have any clue why this might be happening?"
3944,Isolation Forest vs Robust Random Cut Forest in outlier detection,"['python', 'scikit-learn', 'amazon-sagemaker', 'outliers', 'anomaly-detection']","I am examining different methods in outlier detection. I came across sklearn's implementation of Isolation Forest and Amazon sagemaker's implementation of RRCF (Robust Random Cut Forest). Both are ensemble methods based on decision trees, aiming to isolate every single point. The more isolation steps there are, the more likely the point is to be an inlier, and the opposite is true.However, even after looking at the original papers of the algorithms, I am failing to understand exactly the difference between both algorithms. In what way do they work differently? Is one of them more efficient than the other?EDIT: I am adding the links to the research papers for more information, as well as some tutorials discussing the topics.Isolation Forest:Paper TutorialRobust Random Cut Forest:Paper Tutorial"
3945,NumPy package gives an array() error how to fix this? [closed],"['python', 'numpy', 'machine-learning', 'scikit-learn', 'artificial-intelligence']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 5 days ago.This is a sample code to perform Binarization of data using Techniques for Data Preprocessing :This code gives an error:How can I make arrays with more than one row ?"
3946,Label Encoding on dependent variable,"['python', 'machine-learning', 'scikit-learn', 'label-encoding']","I am working on Multiclass classification problem that contains more than 500 labels.I am using OneVsRest Classifier.My labels are in the form text.I want to convert this text labels into numbers ,so I am using LabelEncoder(). I converted the labels into numbers using LabelEncoder() and done with my training and testing.I saved the LabelEncoder() object for later use in my Validation data.Now my query is when i will work on new validation set.I am not able to perform 'inverse transform' of the predicted labels.Trianing/Testing:Prediction on Validation set:I am using the above code for prediction.However line marked as ** is now working.
How can i apply inverse transform using saved Label Encoder object."
3947,Predict a categorical column,"['python', 'machine-learning', 'scikit-learn', 'linear-regression', 'random-forest']","I wrote a code that tries to predict the Accident Area (county name in this case) which is a categorical data. There are 10 features. All of them are categorical form. I have tried to convert all the categorical columns including the label column (county), using the .get_dummies(). Then just did the usuals; Split the train test data. Then tried to predict.There are a few problems when I try to attempt. The first thing is that I am not sure if this is the right approach. suggestions needed if there is any other way? If it is the right way, my prediction score is not up to the mark. I got .score() of 82% using RandomForest.score(X_test,y_test). However, got around 30% score when I used DecisionTreeRegression. when i use LinearRegression(), I get a negative number when I evaluate the model using .score()Dataset Link: https://drive.google.com/file/d/1y2YHSZh6M0lQdPdIzZDa5Thzpsvd-O2r/view?usp=sharingThe code is given below:The random Forest gives me 82% score.However, if apply LinearRegression() the score it gives me a negative number:"
3948,Getting Warning: “ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations.” in ElasticNetCV sklear,"['python-3.x', 'pandas', 'scikit-learn', 'regression', 'data-science']","I am trying to run elasticnet regression on a particular dataset. This is my code:I am receiving the following warning and unable to finish execution properly.I have tried increasing tolerance value. However, the problem does not seem to change"
3949,Why scikit-learn is using so much RAM?,"['scikit-learn', 'jupyter-notebook', 'preprocessor']",I'm using scikit-learn to pre-process and normalize data for a PCA. I'm doing it on 1.3 Mb files but it run out 16 GB RAM to do it and then it crashes. I'm using jupyter notebook and this is what i run.
3950,RANSAC algorithm using scikit-learn's RANSACRegressor,"['import', 'scikit-learn', 'regression', 'linear-regression', 'ransac']",I tried to use the code below for fitting a robust regression model using RANSACAnd I get the following error below:Can you help me know what's wrong?
3951,How can I calculate the coherence score of an sklearn LDA model?,"['python', 'scikit-learn', 'gensim', 'lda', 'topic-modeling']","i try to use the methode tmtoolkit.topicmod.evaluate.metric_coherence_gensim  to calculate a coherence score an sklearn LDA  model, But it does not work.Code :Output :"
3952,Will time.process_time() sum all CPU times from subprocess?,"['python-3.x', 'time', 'scikit-learn', 'subprocess']","I want to find a way to get accumulated CPU times of all subprocess in python, I tried time.process_time() in the following code:And got the following result:60.5182340145111160.51864406861.0367555618286161.03741292361.0486302375793460.00870562100000001which indicates that time.process_time() does not contain the processing time used by sub_process. However, if I tried the following code:The result would be:185.017909526824953086.4627873910003I can see from the terminal that when running datasets.make_spd_matrix, there also exist several subprocess, but this time, time.process_time() seems to sum their CPU time all up. Could someone tell me why these two cases are different?"
3953,Predicting the output results based on the given inputs [closed],"['python', 'scikit-learn']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 5 days ago.I have the training data in CSV format in which I have the 5 columns.CSV Download LinkI want to do simple predictions using simple and possible methods. I have spent countless hours and very confused (Basically I want to provide 4 inputs values and predict the 5th output results based on the given inputs). I'm new to Python so would appreciate your help.CSV results are predicted and generated by online drag and drop tool called ai for kids but I want to do the same with the code. then I could give the input of 4 values and get the predict result value, so I could create a simple interface in python and achieve my task.Thank you"
3954,Adding Pandas duplicate method as part of a sklearn pipeline for prediction,"['python', 'machine-learning', 'scikit-learn']",I've created a model and saved my preprocessing steps along with countvectorizer and the algorithm used in a sklearn pipeline and saved it as a pickle file for later predictions. I've also used a method to find and remove duplicate files using Pandas duplicated method. I just wanted to know if I can add the duplication part into the sklearn pipeline?When I loaded my saved pickle file and try to load 5 sample files all of the same content I was still able to see 5 prediction values and instead it has to only one so I was not sure if I'm doing something wrong here.
3955,sum of specific elements in normalized value count in pandas (KNN Classification),"['python', 'pandas', 'scikit-learn', 'count', 'knn']","I am trying some knn-Classification and when testing the model with 30% of the original data,I want to calculate the percentage of correct classification within a +/-3 point range (left side in below output).In other words the sum of the seven floats at the bottom of the below output:To be precise the sum of these floats:Which would be 0,771004So how do I make Python add only those values together?Important is, that these values are not always the seven at the bottom, but may be spread around, depending on the chosen value for k.I think my problem is that I have the logic for sorting and summing up mixed up.This output is generated by this command:"
3956,QDA got a low accuracy using mnist dataset,"['python', 'scikit-learn', 'deep-learning', 'dataset']","So, I'm using QDA to get the accuracy of mnist dataset. The problem is that I got a very low accuracy (0.1425) and I don't know why because I also used other methods such as CNN, MLP and KNN and the accuracies were over 0.97. When I did the training using QDA, I got the warning ""variables are collinear"". Does this have something to do with the low accuracy? I also used the parameter 'tol' but it didn't solved the problem. I'm new to deep learning, so any advices are welcome. Here's my code:"
3957,convert categorical data column to numerical in pandas if the cells in column have many categorical values inside list [closed],"['python', 'pandas', 'scikit-learn', 'sklearn-pandas']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 5 days ago.I am trying to perform one hot encoder or some efficient encoder to convert these categorical data to numeric but stuck due to data inside list and multiple values inside list to use encoder.
I will also love to hear what could be the better approach to perform converting to numeric values of these type of data.Eg: Column name ""A"" has data like this:
Row 1: ['red','orange','blue']
Row 2: ['green','pink','orange']
Need some better and efficient approach to convert these values in numeric."
3958,How to get evaluation metrics of class 0 in cross validation in sklearn,"['python', 'scikit-learn', 'cross-validation']","I have dataset which contains 0 and 1 as the labels. I am using randomforest classifier to do the classification as follows.I think, in default setting, sklearn returns the results I get for class 1. However, I am interested in getting 'precision', 'recall', 'f1', 'roc_auc' for class 0 instead. I was looking how to do this. However, I still could not find a way to do this. Please let me know your thoughts.I am happy to provide more details if needed."
3959,scikit-learn linear regression memory overflow for MNIST,"['python', 'tensorflow', 'machine-learning', 'scikit-learn', 'mnist']","I am trying to perform basic linear regression of MNIST data using the scikit-learn module. It seems to crash with MemoryError. What am I doing wrong? The shape of the training dataset is (60000, 728)"
3960,"ValueError: Error when checking input: expected dense_97_input to have shape (46,) but got array with shape (6,)","['keras', 'scikit-learn', 'jupyter-notebook', 'keras-layer', 'sequential']","Training a dataframe X_train of shape (13708, 46) gives the following error:ValueError: Error when checking input: expected dense_97_input to have shape (46,) but got array with shape (6,)The code for training:What should I do?"
3961,How to scale target values of a Keras autoencoder model using a sklearn pipeline?,"['python', 'tensorflow', 'machine-learning', 'keras', 'scikit-learn']","I'm using sklearn pipelines to build a Keras autoencoder model and use gridsearch to find the best hyperparameters. This works fine if I use a Multilayer Perceptron model for classification; however, in the autoencoder I need the output values to be the same as input. In other words, I am using a StandardScalar instance in the pipeline to scale the input values and therefore this leads to my question: how can I make the StandardScalar instance inside the pipeline to work on both the input data as well as target data, so that they end up to be the same?I'm providing a code snippet as an example."
3962,Keras model gives same output for all inputs,"['python', 'tensorflow', 'machine-learning', 'keras', 'scikit-learn']","I am making a neural network that is supposed to take 480 data points and output 18 data points. The inputs are magnetic field strengths and the outputs are coordinates of objects detected(will be zero if no object is detected) so no data point is really categorical. For some reason, when I train the model I get the same output for every input I try, for example:The code I used to generate this model is:I read that some of the causes of this are having a learning rate that is too high, disabling the ""trainable"" feature of layers, having a small batch size. I tried to decrease my learning rate to 0.0001 and I still got the same result, as far as I can tell all of my layers are trainable and the last thing that could be the problem which is batch size I have not tried yet. I have a couple thousand training samples so maybe this is the problem and am increasing it from 32 to 400 for the new round of training I will do soon, but maybe the issue is somewhere else that I am not seeing?Also I read it's a good idea to use callbacks=['early_stopping_monitor'] is it appropriate in this case?Edit: Also could the kernel_regularizer=regularizers.l2(0.01) term have an effect on this?"
3963,Loading saved model using Pickle - getting error as fit_transform is done in loaded program,"['python', 'machine-learning', 'scikit-learn']","I have created the first program to train the algorithm and save it.Program 1Program 2As I have ""fit_transform"" in program 1 and saved the model, hence in the second program after loading the model I have only transformed the independent variables.I am getting the error message when running the second program
""sklearn.exceptions.NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.""Please suggest. As I understand that I only need to transform and not fit test independent variables."
3964,Unable to install sklearn library using command prompt,"['python', 'scikit-learn', 'pip']","I have to install sklearn python library using command prompt, using the command pip install sklearn but it is showing the following error.Fatal error in launcher: Unable to create process using '""c:\python38\python.exe""  ""C:\Python38\Scripts\pip.exe"" install sklearn': The system cannot find the file specified."
3965,Learning curve function on Scikit learn,"['machine-learning', 'scikit-learn', 'scoring']","I want to plot a learning curve for my regression problem, using RMSE as a ""scoring parameter"".How do I set a different ""scoring"" parameter? Seems that ""scoring"" is deprecated in the learning curve function...Can anyone help me? Or recommend me another function..."
3966,"ValueError: Found input variables with inconsistent numbers of samples: [1, 700]","['python-3.x', 'numpy', 'machine-learning', 'scikit-learn', 'linear-regression']",I'm performing Linear Regression on the data provided by kaggle on Titanic Survivor prediction .Im trying to predict the list of survivors so iam keep getting this error even after i reshape the Y it still shows this error.
3967,TypeError: '<' not supported between instances of 'str' and 'int' during Gridsearch,"['python', 'scikit-learn']","I am trying grid search on my dataset. but i am getting error but no error occured when using cross validation using cross_val_scoreI imported the libraries. I am google colab notebook.Imported the datasetsThen i tried gridsearchcvThe error im getting isNone of the predictors are string so why is the error? how to resolve it?
it works fine when i try with cross_val_search"
3968,Selecting features in python,"['python', 'scikit-learn', 'k-means', 'pca', 'feature-selection']","I am trying to do this algorithm http://venom.cs.utsa.edu/dmz/techrep/2007/CS-TR-2007-011.pdfI don't understand the warnings and the cause that from 2k+ features it only extract the first 2,that's what I did:My question are:"
3969,Invalid Callable Error Raised when Numerically Integrating KDE.fit() function,"['python', 'scikit-learn', 'kernel-density']","I wrote a simple example to try to understand how to numerically integrate the Kernel Density Estimator function. I am using the module sklearn.neighbors.KernelDensity to get the KDE, and then using scipy.integrate.quad to numerically integrate the function:The error that I get is that kde_fit is an invalid callable.How can I numerically integrate kde_fit?"
3970,pyarrow.lib.ArrowNotImplementedError,"['python', 'python-3.x', 'pandas', 'pyspark', 'scikit-learn']","I have a pyspark dataframe with 4 columns as shown below:I need to add another column 'prediction' to the dataframe based on the column 'D'. The model, which predicts, expects a pandas dataframe with single column as input and returns numpy array as output. So, I wrote a Scalar Pandas UDFs to implement it.Final output should look like as follows:But, I get the following error and not sure what was the issue:pyarrow.lib.ArrowNotImplementedError: NumPyConverter doesn't implement <list<item: bool>> conversion"
3971,Error in Bayesian optimization using Hyperopt,"['python', 'optimization', 'scikit-learn', 'valueerror', 'hyperopt']",I am getting value error in the following code when I am using Hyperopt for Bayesian Optimization :The error is as follows:Can anyone help me in solving this error? I tried various times but still this error persists.
3972,ValueError: Unknown optimizer: optimizer,"['machine-learning', 'keras', 'scikit-learn', 'neural-network', 'hyperparameters']","I wanted to do hyperparameter tuning so to do so I applied gridsearchCV but during fitting it, getting the ValueError"
3973,Walkforward Classification Using Pandas and Sklearn,"['python', 'pandas', 'scikit-learn', 'classification']","I have a dataframe called ""df_pct_change"" which has the percentage changes of 8 variables over 5000 days. It has a length of 5000.I would like to:Here is helper function, be warned it has bugs, which explains how I would like the classification to be effected:Here is a buggy line of code which explains how I would like the classifications to be effected:It doesn't save the results to a dataframe.Questions: Where am I going wrong? And, how would you create, using Pandas and SKLearn, a function which conducts walk-forward classification, by iterating through the columns of df_pct_change and saves the predictions to a dataframe?"
3974,When I am fitting a model it does not show the hyperparameters after the model is created,"['machine-learning', 'scikit-learn']",I do not get the hyperparameters displayed when I run the following command.
3975,Difference between r2_score and score() in linear regression,"['python', 'machine-learning', 'scikit-learn', 'linear-regression']",I found the results of score() in LinearRegression is different from r2_score(). I expected them to return the same results.The codes are as below:
3976,two errors with neural network output,"['python', 'python-3.x', 'python-2.7', 'keras', 'scikit-learn']","hi every one
i make a neural network model to predict column labeli make the following stepsi have two problems with this code
First when i run this partit raise error as it didn't identify Val_acc  but it see val_loss > why this?second when i try to get Y_pred to calculate precision, recall, etc
using the following codeit give me y_pred as a continuous variables like (0.093,0.933) not (0 and 1) so it give me error when calculating any metrics >> any one now why these error raised
any help will be appreciated"
3977,How to split a decision tree on a column's value,"['python', 'scikit-learn', 'decision-tree']","I'm trying to solve a problem and I'm unable to get through this phase.I'm working on the 'Breast cancer' dataset.Consider the following two potential splitsFor each of these splits, calculate the resulting mean squared error. What is the mean squared error
impurity of the dataset with no split? (Note: if you’re wondering about the appropriateness of using
the mean-squared-error on binary outcomes, see the optional handout on binary decision trees in your
course handout).
Based on these results, which of these two splits is best?I think it has something to do with the (splitter=) parameter of decision tree classifier model. Can anyone help me out?"
3978,Prediction under SVR support vector regression [closed],"['python', 'pandas', 'matplotlib', 'scikit-learn', 'forecast']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 7 days ago.I am trying to perform predictions by using real data consumption and the svm.SVR function called support vector regression python from sklearn import svm. I train and test enough data but I obtain the following results. I am wondering why predicted data are completely different. What do you recommend to test? thanksenter image description here"
3979,How to One Hot Encode in python for numerical?,"['python', 'pandas', 'encoding', 'scikit-learn', 'one-hot-encoding']",Im new to python and trying to understand how to use scikitlearnI have a dataframe:I am trying to hot encode 3 million rows on the basis of id vs servicesubcodekey such that the final One hot encoding looks like:So as you see for every id which has a service subcode there is a unique encoding and if the id is same then the encoding is the range of service subcode and it is turned on for the digits for example for id 18 if the max service subcode is 4 then encoding is 0101 since 2 and 4 exist for that id.
3980,how does sklearn do Linear regression when p >n?,"['scikit-learn', 'regression']","it's known that when the number of variables (p) is larger than the number of samples (n) the least square estimator is not defined.In sklearn I receive this values:Call [30] should return an error. How does sklearn work when p>n like in this case?EDIT:
It seems that the matrix is filled with some values "
3981,Train from non-numeric data to predict a string column in new data set - Python Machine Learning module,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'prediction']","So my problem is:
For eg existing data set A:Now I have a new data set B:All of the data fields are strings (non-numeric) in A & B datasets.
Obviously A & B datasets are not this small. Data Frame shape 4000x23.I want to predict “segment column” for the new (B) data set using existing (A) dataset above."
3982,Sklearn FeatureUnion not applying steps sequentially like Pipeline?,"['scikit-learn', 'pipeline', 'preprocessor', 'feature-engineering']","I have a feature union constructed like thisThe union works fine, until I add the final VarianceThreshold step, which throws a ""could not convert string to float"" error.What's confusing to me is that I thought feature union handled these steps sequentially, with the output of one step being the input of the next (like a pipeline). In this case the first step should onehot encode the categorical to numeric so it should not matter.Code to reproduce error:Now if comment out the the final variance threshold step from the union and apply it myself after it works fine:What am I missing here? How can I get feature union to apply the steps sequentially? I prefer bundling certain preprocess steps in to a feature union as I can easily do a fit_transform to check the output to that point..."
3983,Why does fit_transform keep throwing an error?,"['python', 'scikit-learn', 'pycharm', 'normalize']",I've really been battling to understand why fit_transform keeps throwing an error. Debugging doesn't help much as it just sends me to the definition of an array and I'm not sure what I'm missing. Any ideas?The error get's thrown at the first line where fit_transform is used:eg. of one row of rawinput data:error thrown:
3984,Euclidean distance of all pandas rows to single row,"['python', 'pandas', 'scikit-learn']","I have a dataset that gives the values of some songs, ie something that looks like:I want to find the songs/ rows that are numerically closest to another song, such as song 0, using the euclidean distance.So I'd like to obtain something like:"
3985,What does setting the 'contamination' parameter to 'auto' in Sklearn Outlier Detection methods do?,"['python', 'scikit-learn', 'statistics', 'outliers', 'anomaly-detection']","I have a dataset where I need to be able to control to what extent the Outlier Detection Model (Isolation Forest, Elliptic Envelope, OneClassSVM...) considers a given point an outlier or not (something similar to the Z-score or IQR-score). This means that I do not want to specify in advance the percentage of outlier points in my dataset, better known as the contamination parameter, but I want this percentage to depend on how ""picky"" I want my model to be. Is this the same as setting the parameter contamination to 'auto'?Here's what the Sci-kit Learn package says about this:
""if ‘auto’, the threshold is determined as in the original paper"".Which original paper does this refer to? And does setting the contamination parameter to 'auto' solve my problem?"
3986,"Trying to implement logistic regression but gridsearchCV shows input variables with inconsistent numbers of samples: [60000, 60001]","['python', 'machine-learning', 'scikit-learn', 'logistic-regression', 'grid-search']","Trying to implement logistic regression but gridsearchCV shows input variables with inconsistent numbers of samples: [60000, 60001]
Here is my code in python 3 environment :The output runtime error is as follow :Please help me debug this code"
3987,Expand a sklearn decision tree classifier [closed],"['python', 'scikit-learn']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 8 days ago.I found a paper named 'Transfer Learning in Decision Trees' that uses a base decision tree classifier and then expands it.
I'm trying to reproduce the results in the paper using a sklearn decision tree classifier as a source classifier from which I'll add/modify nodes and branches.
Sklearn allows me to retrieve the tree object, but I can't find a way to add branches and nodes or modify them. Can someone point me to what I'm missing here ?"
3988,What is .linear_model in sklearn.linear_model,"['python', 'scikit-learn']",I want to know what is the meaning of .linear_model in the following code -My understanding is sklearn is the library/module (both have same meaning) and LogisticRegression is the class inside this module.But I'm not able to understand what .linear_model means?
3989,Sklearn Lasso regularization not dropping out random variables?,"['python', 'scikit-learn', 'logistic-regression', 'lasso-regression']","I've been using SelectFromModel from sklearn to reduce features using LASSO regularization and I'm finding that even when I set max_features to quite low (low enough to negatively impact performance) the random variables are often kept.I generated an example with fake data to illustrate, but I'm seeing similar with actual real data and I am trying to understand why.Even though I've set 20 variables to be informative, and added 2 completely random ones, in many cases this will keep rand_feat2 when selecting the top 10 or even top 5. On a side note I get different results even with random state set....not sure why? But the point is fairly often a random variable will be included as a top 5 feature. I am seeing similar with real world data, where I have to get rid of a huge chunk of the variables to get rid of the random feature....makes me seriously doubt how reliable it is? How do I explain this?EDIT:Adding a screenshot along with sklearn/pandas versions printed... I'm not always getting the random features included, but if I run it a few times it will be there. On my real world dataset at least one is almost always included even after removing about half the variables."
3990,Need help in Binary classification Timeseries problem [closed],"['scikit-learn', 'classification', 'forecasting', 'arima', 'hidden-markov-models']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 8 days ago.I have around 2000 univariate points in a binary classification time series problem. I want to predict the 2001st value (whether it will be 1 or 0). Data is 1s and 0s. I can increase data but that's difficult at this moment.My observations through plotting: After 4-5 1s, 0 has to come in time-series. Attaching the graph here for few values.Which model would be perfect? And How to use that model (Any relevant paper or article or video). What are your views on Hidden Markov Models? I guess ARIMA model assumptions of mean and variance aren't fulfilled here."
3991,1D data passed to a transformer while using scikit-learn Column Transformer in Python,"['python', 'arrays', 'scikit-learn']","I'm using Column Transformer to perform preprocess and regression of a data set with mixed type of data. My label here is customers' response.here is the infothen I created a pipeline and run the data set through,However, I got error after clf.fit(X_train, y_train) saying:I have tried using y = Train_df[['response']] , y = Train_df['response'], either of them worked. Really don't know how to fix it.Please help, thank you guys soo much!"
3992,Python sklearn learning_curve produces different result than standard classification model,"['python', 'scikit-learn', 'random-forest', 'cross-validation']","I've built a RandomForestClassifier that seems to perform pretty well on both my train and test data (train set accuracy = 94%, test set accuracy = 91%).I was hoping to supplement this result by plotting a learning curve of the results to see how these scores changed with the sample size considered. Unfortunately, the score of the validation set never approaches the test set result I found in the original code above (it plateaus under 75%).learning curve resultI can't seem to figure out what causes all scores calculated by the learning_curve() function to be so much lower."
3993,Problem in plotting support vector contours in Jupyter,"['python', 'matplotlib', 'scikit-learn', 'contour']","I am following this script from ScikitLearn to plot the margins and the hyperplane for SVC. I'm executing each line in a different cell and am following the same order, i.e. cell 1 for line 1, cell 2 for line 2, and so on. When I finally get to the plotting part, say on cell k(which is the last cell in my notebook that holds the last 7 lines of the code), I get UserWarning: No contour levels were found within the data range. and no plot as mentioned in the link given above shows up.However, when I execute all the lines in the same cell, the code works as expected. What am I doing wrong here?The code:"
3994,Not giving a perfect linear regression,"['python', 'machine-learning', 'scikit-learn', 'linear-regression']","I am trying to draw a linear regression by scikit module, but it is giving the wrong regression."
3995,Use Gurobi Variable as Input to Sci-kit Learn Model,"['python', 'optimization', 'scikit-learn', 'gurobi']","How do I input Gurobi variables as an input to a sci-kit learn model?
I am trying to solve an optimization problem whose objective function depends on a model I built in sci-kit learn.The code I have looks like:But this is giving me an error:which I think is because the scikit-learn model, ""model,"" needs a string or a number as its input, not a Gurobi variable."
3996,XGBoost get feature importance as a list of columns instead of plot,"['machine-learning', 'scikit-learn', 'xgboost']",I am wondering if you we can get the feature importance as a list of columns instead of a plot. This is what I haveWhich gives me this plotI would like to instead just get a list of the top features since I have over 800 different features.
3997,Is there a catgorical encoder that allows me to specify the starting point(i.e. not 0)? or make specific values off limits?,"['python', 'pandas', 'encoding', 'scikit-learn']",I'm encoding some categories but my program see's 0 as a special value so I do not want it used. I figured out how to retain NaN's when I encode my dataframe but it's starting at 0.How can I avoid it?This is my result:You can see there are 0's in the result which will result in incorrect data when I do df.fillna(0) to convert NaN to 0. Short of building my own encoder is there anything I can do?
3998,Using sample weights in SHAP explainers,"['python', 'scikit-learn', 'regression', 'shap']","I want to calculate SHAP values for a linear model. For the regression, I have to use sample weights.The problem is that I can´t evaluate if the sample weights were actually applied properly calculating the SHAP values.Here is an example.First I calculate a regression without weights.Then I calculate a regression with weights.From was I figured out so far (I tested different combinations using other software packages e.g. R, SPSS to evaluate the results) is the I have to apply the weights to the fit() function and to r2_score() function to get the right result (see the example above). For example, if I only apply the weights to the fit() function but not to the r2_score() function the reported R2 value is wrong (i.e. the model is wrong). If I apply the weights also to the predict() function the R2 value is also wrong (i.e. the model is wrong).However, since I can calculate the SHAP values only in Python there is no way for me to evaluate the results. The question is how should I apply the sample weights to calculate the SHAP value right?Only in the fit function (?):Or also in the explainer() function (?):There might be other possibilities... but I have no idea which one is right.Here is a small data sample."
3999,Why can't sklearn MLPClassifier predict xor?,"['python', 'tensorflow', 'machine-learning', 'scikit-learn', 'neural-network']","In theory, an MLP with a single hidden layer with just 3 neurons is enough to predict xor correctly. It could sometimes fail to converge properly, but 4 neurons are a safe bet.Here's an exampleI've tried to reproduce this using sklearn.neural_network.MLPClassifier:I only get around 0.75 accuracy, while the tensorflow playground model is perfect, any idea what makes the difference?Tried also using tensorflow:With this model I get similar results to the scikit-learn model... So it's not just a scikit-learn issue - am I missing some important hyper-parameter?EditOk, changed the loss to mean squared error instead of cross-entropy, and now I get with the tensorflow example 0.92 accuracy. I guess that's the problem with the MLPClassifier?"
4000,why is Keras accuracy metrics not measuring the same as the sklearn.metrics accuracy?,"['machine-learning', 'keras', 'scikit-learn', 'deep-learning', 'data-science']","I  have a model with this compilation:When i fit the model and call this:it gives me 0.13 of accuracy, but with this:it gives me almost 0.5 of score.y_test is an 2D array with 0 and 1(each instance has almost 2000 classes to be predicted) with 0 and 1, like:I tested acuracy_score from sklearn, and i know that it returns the proportion of the rows of pred that is exactly the same as y_test. So, what is Keras accuracy metrics measuring???"
4001,Why and how the format of the matrix (numpy vs pandas dataframe vs datatable frame) influences the training and test performance time?,"['python', 'pandas', 'numpy', 'scikit-learn']","I load csv file with datatable dataframe. To be able to split the datatable dataframe into train and test using  train_test_split(dt_df,classes) from sklearn.model_selection (from my question ho to split datatable into train and test), I convert the datatable dataframe to either numpy or to pandas dataframe, and i deed my train and test. I tried a third approach which is: when i convert to numpy, just after the split i convert back the result to datatable dataframe. Between these three methods, i get different performance time for the train and the prediction.source code before split method:source code after split method:method 1: convert to numpymethod 2: convert to numpy and return back to datatable dataframe after the split:method 3: convert to pandas dataframeThese 3 methods work fine, but there is a difference in the time performance of the train (ExTrCl.fit) and the prediction (ExTrCl.predict), for a csv file of about 500 Mo I have these results (the average of 10 executions):My question is why and how the format of the matrix influences the train and prediction performance time.Thanks for your help."
4002,Include Labels from SciKit Learn Prediction,"['python', 'python-3.x', 'pandas', 'machine-learning', 'scikit-learn']","I have successfully produced predictions on the data set below, but I am trying to figure out how I can map prediction outputs from the model back to the TEAM labels. I am using Python 3, Pandas and SciKit Learn.sample_data:This is an example of the simple linear regression I set up.Using this, I can produce a prediction array like below:But I am trying to get something that looks like below, so as I feed in new data without SCORE into the model, I can predict SCORE for each team:I am flexible with the format, I just need to match the prediction output to each specific team from the input."
4003,"saving polynomial model , doesn't save polynomial degree","['python-3.x', 'machine-learning', 'scikit-learn']","How can I deal with polynomial degree when I want to save a polynomial model, sicne this info is not being saved!Now, if I try to predict:themodel.predict(X_val), I am receiving:ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6 is different from 2)I have to do:pol_feat = PolynomialFeatures(degree=2)
themodel.predict(pol_feat.fit_transform(X_val))in order to work.
So, how can i store this info in order to be able to use the model for prediction?"
4004,The problem of feature selection in sklearn dataset,"['python', 'scikit-learn']","I'm a beginner.I am now writing a feature selection algorithm under sklearn, and I wrote a simple program to try it out. The goal is to take out certain columns in the dataset, but the result is an error. The following is the code and error message. Please help me,Thank you.What should i do?error message:"
4005,How to best determine the accuracy of a model? Repeated train/test splits or cv?,"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'train-test-split']","I'm creating a classifier that takes vectorized book text as input and as output predicts whether the book is ""good"" or ""bad"".I have 40 books, 27 good and 13 bad. I split each book into 5 records (5 ten-page segments) to increase the amount of data, so 200 records total.Ultimately, I'll fit the model on all the books and use it to predict unlabeled books.What's the best way to estimate the accuracy my model's going to have? I'll also use this estimate for model comparison, tuning, etc.The two options I'm thinking of:I want to estimate the accuracy within a small margin of error as quickly as possible. Repeated train-test splits are slower, since even when I stratify by label (choosing 8 good books and 4 bad books for test) the accuracy for a particular model can vary from 0.6 to 0.8, so I'd have to run a lot to get an accurate estimate.CV, on the other hand, is giving me the same score every time I run it, and seems to line up relatively well with the average accuracies of the models after 100 train-test splits (within 1-1.5%).CV is much faster, so I'd prefer to use it. Does CV make sense to use here?  I'm currently using 5-fold (so it's choosing 8 holdout books each run, or 40 holdout records total).Also, should CV be giving the exact same accuracy every time I run it? (and exact same list of accuracies in the same order, for that matter). I'm shuffling my corpus before putting X, y, and groups into the cross_val_score. Would a ShuffleSplit be preferable? Here's my code:Finally, should I be using stratification? My thought was that I should since I effectively have 40 items to be split between train and test, so the test set (chosen randomly) could reasonably end up being all/mostly good or all/mostly bad, and I didn't think that would be a good test set for representing accuracy."
4006,Unable to pass value to “test_size” dynamically in train_test_split function,"['python', 'scikit-learn']","I have following python code.when it get executed , I am getting following errorHow to pass test size dynamically to train_test_split function in python."
4007,Need Help For Simple Scikit and Numpy Code,"['python', 'scikit-learn']",Tried running this code in jupyter notebook. It takes a long time running: data = pd.read_csv(dataset_url). The two main errors I get are  keyboard interrupt when I interrupt it and timeout error. I am new to ML and any help would be appreciated.
4008,Why does a column of 1s impact the results of a decision tree classifier?,"['python', 'machine-learning', 'scikit-learn', 'decision-tree']","I was testing sklearn's Pipeline on a randomly generated classification problem:This results in an accuracy score of .85. However, when I change the PolynomialFeatures argument include_bias to True, which just inserts a single column of 1s into the array, the accuracy score becomes .90. For visualization, below I have plotted the individual trees for the results when bias is True and when False:These images were generated by plot_tree(pipe['model']).The datasets are the same except when include_bias=True an additional column of 1s is inserted into column 0. So the column indexes for the include_bias=True data correspond to the i + 1 column index in the include_bias=False data. (e.g. with_bias[:, 5] == without_bias[:, 4])Based on my understanding, the column of 1s shouldn't have an impact on the Decision Tree. What am I missing?"
4009,Does Gridsearch CV shuffle the data before creating the folds?,"['scikit-learn', 'shuffle']",I'm used sklearn GridsearchCV to tune hyperparameters but want to know if the dataset I give it will be shuffled before the folds are created. I'd like it to NOT be shuffled but I can't find if it is or isn't in the documentation. Something like train_test_split has a boolean to shuffle or not.
4010,StackingClassifier causing “ValueError: bad input shape” error,"['python', 'numpy', 'scikit-learn', 'classification']","I am running a classifier using scikit's StackingClassifier and I am getting and error that I cannot solve. This is the code:This is the error message that I receive:Any help would be very much appreciated. It might be helpful to note that when I set the ""classifier"" variable equal to the just the RidgeCV() linear model no error is given and the code runs correctly."
4011,"Sklearn ValueError: operands could not be broadcast together with shapes (366345,2) (5,) (366345,2)","['python', 'numpy', 'scikit-learn', 'feature-scaling']","When I try to execute this code -The error appears (for the third line) :ValueError: operands could not be broadcast together with shapes (366345,2) (5,) (366345,2)File Error:File ""/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py"", line 846, in inverse_transform
X *= self.scaleThe dataset I am using has multiple independent variables, but I used the xtrain values without backward elimination (the same error comes there). I also used StandardScaler on my x valuesPython Version - 3.7.6"
4012,How toTrain_test split with OneHot encoded Data?,"['python', 'keras', 'scikit-learn', 'one-hot-encoding', 'train-test-split']","I am dealing with unbalanced data and trying to improve my model by using stratified data. The problem is that I am unsure how to do so exactly. Everything I have tried so far doesn't change anything.It should be something like this:but it doesn't matter if I pass the ""stratify"" parameter or not. My Data is OneHot encoded and y_train looks like this:
[[1. 0.] [1. 0.] [0. 1.] ... [0. 1.] [0. 1.] [1. 0.]]As far as I understand stratify needs my two classes but I am unsure how to do that.EDIT: It doesn't matter if I set stratify = y_train or not because the dimensions of y_train doesn't change.Thanks!"
4013,Imbalanced dataset oversampling using SMOTE-NC,"['python', 'scikit-learn', 'prediction', 'imbalanced-data']","I have a imbalanced dataset with categorical and numerical features , while under and over sampling can be used to resample the data , I wanted to use a KNN based oversampling.
I want to understand what happens to the numerical features , are they also sampled or just copied respective to the categorical entry that is oversampled?"
4014,Is it correct to use a single StandardScaler before splitting data?,"['machine-learning', 'scikit-learn']","I've seen some experiments using two different StandardScaler as follows:I understand that one shouldn't bias the classifier mixing train/test data, but i would like to know if this other scenario is correct or not:Besides, i would like to know how this case extends to KFold cross-validation."
4015,Why k-means in scikit learn have a predict function but DBSCAN/agglomerative doesnt?,"['machine-learning', 'scikit-learn', 'cluster-analysis', 'k-means', 'dbscan']","Scikit-learn implementation of K-means has a predict() function which can be applied on unseen data. Where as DBSCAN and Agglomerative does not have a predict() function.All the three algorithms has fit_predict() which is used to fit the model and then predict. But k-means has predict() which can be directly used on unseen data which is not the case for the other algorithm.I am very much aware that there are clustering algorithms and as per my opinion, predict() should not be there for K-means also.What is the possible intuition/reason behind this discrepancy? is it only because k-means performs ""1NN classification"", so it has a predict() function?"
4016,Feature extraction from LSTM to Sklearn models,"['python', 'tensorflow', 'scikit-learn', 'lstm']","I've a LSTM model and i want to extract features from this LSTM to send it into a Random Forest or a logistic regression on Sklearn.so i want to use x1 as the input of my Random forest.
Any idea ?Thanks :)"
4017,How to show y-ticks for all plots and not just for the leftmost column?,"['python', 'plot', 'scikit-learn']",I'm using scikit-learn's plot_partial_dependence but it shows y-ticker only for the leftmost columns. How can I force it to show y-tickers also for the other columns?
4018,What is the difference between first doing LabelEncoder() and then np_utils.to_categorical() and only doing LabelBinarizer() or only OneHotEncoding?,"['scikit-learn', 'sklearn-pandas']","I'm unable to completely understand when to use which preprocessing method. Which methods are useful in what situations and why? Why do we even have 3 classes that do almost very similar jobs, or don't they?"
4019,Does Scikit-learn support transfer learning?,"['machine-learning', 'scikit-learn']","Does Scikit-learn support transfer learning? Please check the following code.model clf  is gotten by fit(X,y)Can model clf2 learn on the base of clf and transfer learn by fit(X2,y2) ?"
4020,"SKlearn: equivalent of [H,T,outperm] = dendrogram(___)","['python', 'matlab', 'scikit-learn', 'data-science']","Back in the matlab days, one could use the dendrogram function to get a permutation of the data so that the distance matrix looks like a block matrix.https://de.mathworks.com/help/stats/dendrogram.html[H,T,outperm] = dendrogram(___) also returns a vector containing the leaf node number for each object in the original data set, T, and a vector giving the order of the node labels of the leaves as shown in the dendrogram, outperm.It is useful to return T when the number of leaf nodes, P, is less than the total number of data points, so that some leaf nodes in the display correspond to multiple data points.The order of the node labels given in outperm is from left to right for a horizontal dendrogram, and from bottom to top for a vertical dendrogram.I was searching around if sklearn has the same function somewhere, but it looks like it doesn't. I wanted to make a github issue, but the issue said I should rather make a stack overflow post.So - Does anyone know if sklearn already has this functionality?For now, I was able to just do the followingCheers"
4021,Sklearn pipeline not fitted after .fit has been called?,"['python', 'scikit-learn', 'pipeline', 'sklearn-pandas']","I have a simple pipeline like thisI then call pl.fit on my training data, but when I try to check the onehot encoder to get variable names I keep getting an error message that it hasnt been fitted yetAnd checking confirms it has not been fitted. What am I missing?"
4022,How feature selection algorithm SelecFromModel in sklearn work?,"['python', 'python-3.x', 'scikit-learn']",Curious to know what is the algorithm behind the SelecFromModel?
4023,"TypeError, Hands on ML With Sci-Kit Learn Regression","['python', 'machine-learning', 'scikit-learn', 'regression', 'data-science']","Trying to work through the project in Chapter 2 of Hands-on ML with Sci-Kit Learn and Tensorflow, I am being given a type error when trying to run the data through a pipeline prior to building a model.I keep getting a TypeError telling me that fit_transform() takes 2 positional arguments and yet 3 are given. Not sure what I am doing wrong as I am following along the best I can with the exercise. Please advise, let me know or if more information is needed as I tried to stick to the minimum amount of code required to generate the error sustained. Thanks for what insight you may be able to kindly provide.Code is as follows"
4024,Gaussian NB vs LDA in scikit learn,"['python', 'scikit-learn', 'lda', 'naivebayes']","From my understanding, if we only have one feature, then Gaussian NB (naive bayes classification) and LDA (Linear Discriminant Analysis) should give the same result.But I didn't succeed with scikit learn.First I generate some toy dataThen I create a NB model with Gaussian distributionThen a LDA modelNow it is possible to plot the results.But I got the following plotMaybe I didn't these algorithms. Could you explain why the differences?"
4025,Confused about Column Order for sklearn Pipeline Imputer,"['python', 'scikit-learn', 'pipeline']","I have a pipeline that imputes and transforms my dataframe, but the resulting array seems to have the columns re-ordered. I'm struggling to understand how to setup my pipeline so that the order is retained.When I look at the result, the column headers are mismatched to the columns. So something in the preprocessor pipeline is re-ordering the columns and I'm not sure how or where to control that.I feel like this could be an area full of risk for a mistake, especially if I change [cols] to another order --- would imputer possibly accept any order and produce mistakes?"
4026,pipeline regressor ensemble with scikit-learn version 0.19.2,"['python', 'scikit-learn', 'ensemble-learning']","I'm using scikit-learn version 0.19.2 (for onnx conversion compatibility),
and I'm having problems implementing ensemble methods with Pipeline.The code below is trying to implement linear regression from two independent regressors:resulting in errorCan someone help me figuring what went wrong?p.s I'm looking for some Pipeline technique to imitate the sklearn.ensemble.StackingRegressor from version 0.23."
4027,Cannot use One-hot-encoding on 4 columns,"['python', 'scikit-learn']","I have a dataset, in which the first 4 columns are categorial. Thus- I need to hot encode each.
When try to do it, it does not work as expected. However, If I encode only the first 3 or 2 or 1 column(s) it does work.Here is line sample from the data set:Here is my code:Here is the onehotencoding result of the sample line from encoding only the first 3 columns (0,1,2):
[0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 'fri' 86.2 26.2 94.3 5.1 8.2 51.0 6.7 0.0]However, when trying to encode the first 4 columns, when trying to print X[0] (in-order to print the first row - the sample row) I get the error:
IndexError: too many indices for array: array is 0-dimensional, but 1 were indexed"
4028,Scikit-learn fit model with multiple CPUs (Windows),"['python', 'windows', 'scikit-learn', 'multiprocessing']","Im using Scikit-learn for manifold learning with e.g.  sklearn.manifold.LocallyLinearEmbedding on a Windows machine with 2 processors. During fitting the model, the build in parallelization works fine among the cores of one of the processors but the second one is unused.Is there a way to parallelize the fit function to make use of all processors?(I think I found a similar problem but the solution does not work for windows.)"
4029,Classification metrics can't handle a mix of multiclass and continuous-multioutput targets,"['tensorflow', 'scikit-learn', 'one-hot-encoding', 'multiclass-classification']",I am running a BERT pretrained model on a  multiclass dataset for text classification purposes. Since it is multiclass I cannot figure out how to generate a classification report. The solutions I found were this and this. I understand since its a multiclass classification I have to one-hot-encode the test_y values (which I did)but when I doI get still get this error :Why?And if I try to calculate accuracy_score I get 0.0 accuracy: (but my accuracy is around 60%)Why?Details of the model is given below:train_test_splitModel:Model.fitmodel.predictShape of parameters
4030,MeanShift or bandwith estimator taking too long to cluster points of and image,"['python', 'pandas', 'scikit-learn', 'python-imaging-library']","I am not sure if it is the bandwith estimator or Meanshift that is taking ages, I want to know how i can reduce the time, maybe I need to modify the image or maybe the parameters of the the clustering algo.THIS IS THE IMAGE:
THIS IS MY CODE:"
4031,Independent variables/attributes selection paramaters?,"['machine-learning', 'scikit-learn', 'statistics']",Based on what criteria can we decide which independent variables should be selected for calculating the dependent variables in any machine learning model?If I select the independent variables with the coefficient of correlation close to 1 among them then I guess they are not really independent in nature and may have the same effect independent variable calculation.Please enlighten me the process/criteria to consider while selecting the right independent variables.
4032,Vocabulary not fitted or provided,"['python', 'scikit-learn']","I have text data I need to evaluate against my model. I'm trying to vectorize the data but keep getting ""Vocabulary not fitted or provided"" as an error.Here is how I vectorize and train my training data:Now I want to vectorize my test data before making prediction:But the error says:What am I doing wrong?"
4033,Error in installing the scikit-learn-extra,"['python', 'scikit-learn']",these were the error I get while installing the scikit-learn-extra. how should I solve the issue?
4034,grouping a list values based on max value,"['python', 'scikit-learn', 'cluster-analysis', 'k-means']","I'm working on k-mean algorthim to cluster list of number, If i have an array (X)then I run the following codei  got the resultshow to get the max number in each cluster  with value of (i) ? like this"
4035,"valueerror: found input variables with inconsistent numbers of samples: [40, 10]","['python', 'pandas', 'numpy', 'machine-learning', 'scikit-learn']","Here I was creating a model based on multiple linear regression, but now I stuck with an error that is "" value error: found input variables with inconsistent numbers of samples: [40, 10]here I am showing you my code.I am confused, where should I make changes?in case of assigning the dependent and independent variable to x and y. When I checked y.shape it gives one-dimensional array instead of this it should show two-dimensional array."
4036,Clustering data with python,"['python', 'scikit-learn', 'data-science', 'cluster-analysis']","I try to cluster my dataset with python and scikit-learn. It's an exercise of my University. The dataset looks like this:The column ""StationNr"" has the following different values:
[   0, 1200,  900,  100,  400,  300,  600,  200, 1100,  500,
1000,  800,  700] 
If there is a zero in ""StationNr"", it means that the product is good. The other numbers has the meaning, on which station the product declared bad. 
The columns ""A%"" has the following different values and the columns reprensent a processing station:  [ 2.017,  1.767,  0.987, ..., 24.083, 18.977,  4.904] 
There are about 4000 different values for ""A1"". This values are the duration of the processing on the station.Now, I want to know, is there a cluster in duration of processing on stations (A1, A2, A3 ... ,A11 ,A12), for example a single value or the combination of values, that's result is a bad or a good product. Good products has the ""StationNr"" == 0  and bad products has the ""StationNr"" != 0.I select the k-Means algorithm to explore the data and i don't how-to get my exercise in combination with k-Means in python code.Maybe k-Means is not the best algorithm for this case, then I will be happy, if you suggest a better algorithm.I'm very new in this topic and will be happy, if you help me to clustering my data.Best regardsChristian"
4037,AWS Sagemaker Model Error when making Predictor Call,"['python-3.x', 'tensorflow', 'scikit-learn', 'amazon-sagemaker']","I'm trying to make a tensorFlow predictor call but I am getting a ModelError - 502 Bad Gateway. I can't seem to trace back to what is going on in the server that is causing this error. This model was deployed on ml.c4.xlarge instances.As requested from comments, here is the code used to deploy the model:For reference: here is the code in a Sagemaker notebook that is using a .csv file saved in S3 to try and make the prediction:Here is the full error for reference:Finally, a snippet from the CloudWatch Logs if this helps:"
4038,How do I check the average count of words in a dataset?,"['python', 'scikit-learn', 'jupyter-notebook']","I have a dataset that has been trained and validated, and now I want to check the total average amount of words in the dataset per tweet, based on the label. I have a dataset of tweets that contains 5 major emotion classification labels- happy, sadness, anger, fear and love. I heard count can be used to see the amount of words used in a dataset but I encountered an error:however, I got an error that said that 'count' is not defined properly. How do I define the count so it can calculate the average amount of words per label? Thank you!"
4039,Gradient boosting training loss increases at every iteration,"['python', 'machine-learning', 'scikit-learn', 'xgboost']","Every iteration, the train loss is increasing.My input is a large matrix of 0s and 1s (vectorized words, as a sparse matrix), and my targets are integers:Perhaps there's something wrong with my code, but I doubt it. Here it is:My input shapes are:"
4040,How to determine ratio/percentage/contribution of feature for each principal component in PCA in Python?,"['python', 'scikit-learn', 'statistics', 'pca', 'euclidean-distance']","I recently found out about loadings and biplots.  I'm wondering if there is a way to calculate the ratio/percentage/contribution of a particular feature in a particular principal component.I've found this StackOverflow post but it's in R and I'm having difficult understanding how to adapt the code.Below is my implementation but I'm not sure if it is correct. Basically, I'm calculating the loadings, then I scale the loadings so that they sum to 1 (total sum scaling).Is this implementation correct for determining loadings and the contribution of a feature to each principal component?"
4041,No module named 'sklearn.externals.joblib' ERROR,"['python', 'scikit-learn', 'scikit-optimize']","i am trying to implement a bayesian optimization but already starting having an error while importing 'gp_minimize' from scikit-optimize package
from skopt import gp_minimize``
I already downloaded all required packages such joblib and scikit learn and scikit optimize, so can't figure out why it's not working."
4042,Input feature selection on one-hot encoded variables in scikit-learn,"['python', 'machine-learning', 'scikit-learn', 'feature-selection']","I'm trying to do feature selection on some data I have. My issue is because this model will eventually be a part of a web form, I need the input variables to be n number of features (lets say 10). The problem is, I have to one-hot encode my data which means that when I do feature selection it could select less than 15 variables. Is there a way to get around this?Thanks everyone"
4043,Is it possible to encode multiple selected columns at once?,"['python', 'pandas', 'scikit-learn']",I have a list of columns in my large dataframe that are catergorical and I'm trying to encode them because some of the algo's I'm using do not accept strings(knn for example).Here's my code:I got this error:What can I do to only encode those values in my catgoricalValues list while maintaining all other values in my dataframe?
4044,OneHotEncoding multiple columns in dataset at once,"['python', 'scikit-learn', 'one-hot-encoding']","Consider the following simple snipped dataset:I want to onehotencode these columns.My try:At the current state, it only encodes the first column. I can't really understand sklearn documentation for this function ColumnTransformer. How would I select multiple columns to encode all at once?"
4045,Using TensorFlow pre-processing (tf.feature_column) in combination with scikit-learn model,"['python', 'tensorflow', 'machine-learning', 'keras', 'scikit-learn']","For my recent work I have to try different ML models on my given non-linear problem. I used TensorFlow and Keras to build a working version of logistic regression and of a neural network. Now I have to build an SVM classifier for the same problem. Since I could not find a working SVM estimator in TensorFlow, I was thinking of using scikit-learn instead. In my first two models I used tf.feature_column for pre-processing my data (bucketized_column, embedding_column, crossed_column etc.). Since this pre-processing is somehow complex and works pretty well, I was wondering if I can use the TensorFlow pre-processing in combination with scikit-learn.Is this somehow possible? Or could I use scikit-learn somehow inside TensorFlow (similar like Keras), so I can also use TensorBoard for analysing my results?Here an overview of the relevant parts in my code:In the model module I used this function call to get the pre-processed data. feature_columns is an array of the different types of the mappings of tf.feature_column. train_ds etc. are TensorFlow datasets.Creating DenseFeature with the help of array feature_columns:Usage of pre-processing to build Keras model:Fitting of the model:"
4046,How to include SimpleImputer before CountVectorizer in a scikit-learn Pipeline?,"['python', 'machine-learning', 'scikit-learn', 'imputation', 'countvectorizer']","I have a pandas DataFrame that includes a column of text, and I would like to vectorize the text using scikit-learn's CountVectorizer. However, the text includes missing values, and so I would like to impute a constant value before vectorizing.My initial idea was to create a Pipeline of SimpleImputer and CountVectorizer:However, the fit_transform errors because SimpleImputer outputs a 2D array and CountVectorizer requires 1D input. Here's the error message:QUESTION: How can I modify this Pipeline so that it will work?NOTE: I'm aware that I can impute missing values in pandas. However, I would like to accomplish all preprocessing in scikit-learn so that the same preprocessing can be applied to new data using Pipeline."
4047,Why do we use probability to calculate precision recall curve instead of actual class? [closed],"['machine-learning', 'scikit-learn', 'precision-recall']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 9 days ago.If I'm not wrong, We calculate precision and recall values for classifiers by final label predicted. However, theprecision_recall_curve in sklearn uses decision_function instead of final class labels. Does it have any special impact on the final values? Does the extent of confidence impact the curve in any way?"
4048,how to set mlpregressor parameters to get better score,"['python', 'scikit-learn', 'regression', 'mlp']",I'm training a sklearn.neural_network.mlpregressor by a large data of students performance (an excel file with 740 students and 27 columns that are their qualities which are all floats between 0 & 1) and I want to predict their grades. I'm using mlpregressor with these parameters:but I'm getting 0.6 score on training set and 0.3 on test set. How do you think i should change the parameters to get a better score? is mlp a good network to do this?
4049,How to determine best parameters and best score for each scoring metric in GridSearchCV,"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'grid-search']","I am trying to evaluate multiple scoring metrics to determine the best parameters for model performance. i.e., to say:To maximize F1, I should use these parameters. To maximize precision, I
should use these parameters.I am working off the following example from this sklearn pageWhich yields:However, what I would be looking for would be to find these results for each metric, so in this case, for F1 and precisionHow can I achieve getting the best parameters for each type of scoring metric in GridSearchCV?Note - I believe it has something to do with my usage of refit='F1', but am not sure how to use multiple metrics there?"
4050,F1 score in a multi-label classification where the number of labels in one image is sparse and the number of labels between classes is biased,"['scikit-learn', 'deep-learning', 'multilabel-classification']","I use scikit-learn to measure multi-label classification with f-score where labels are imbalanced per image and the number of labels  is low per image.What should I use and why? average = ""micro"" or ""samples""?"
4051,yellowbrick prediction-error plot: getting error at score method,"['python', 'scikit-learn', 'visualization', 'yellowbrick']","I am using Yellowbrick to visualize the prediction error plot but I am getting error at score() method.I am getting following error:X_train, X_test, y_train and y_test are 2D ndarrays that I get after using sklearn's train_test_split() method.Do you know what is the cause of this error?"
4052,Is there any way to decode decision tree probablity output?,"['python-3.x', 'machine-learning', 'scikit-learn', 'decision-tree']",My code is available on google colabCode for indetail informationI have made one algorithm where I use a decision tree for multiple probability predictionI am getting thiswhile trying to decode facing this issueMy output should be :
4053,Sklearn KBinsDiscretizer keep origin column names,"['python', 'python-3.x', 'pandas', 'scikit-learn', 'sklearn-pandas']","I'm working on a machine learning problem and I'm discretizing some continuous variables using Sklearn KBinsDiscretizer.Before being transformed, my X_train.columns returns :After being transformed (and put back as a pandas df), X_train.columns gives :Since I am analysing variables by their original name (A, B, C, ..., J), and have to give feedback about which variables were used for my classification, I'm looking for a way to know which variable is associated to which number of the output. For example, I'm looking to transform my output X_train.columns asI know such a command exist when using the sklearn OneHotEncoder (get_feature_names), but I can't find any way of doing this with KBinsDiscretizer.One of the idea I had to solve the issue was creating one specific discretizer for each variable, then apply to each column the associated discretizer, and rename columns manually before merging everything, but it would be a mess since I have to save my discretizers...Also, even though I'm specifying n_bins = 8, I have 69 output columns from my 10 entries, so 1 entry doesn't always produce 10 outputs, and I can't either use this to set column names back."
4054,Do PMML Models allow for singular prediction based on multiple rows of data (f.e. by aggregation?)?,"['python', 'scikit-learn', 'pmml']","I'm looking for a way to apply some aggregation methods within a PMML model (with no specific example, just to see if it is possible)In the documentation of PMML Transformations page there is a passage on Aggregate, defined as a way to apply six functions: Count, Sum, Avg, Min, Max and Multiset.AggregateDoes this mean that there is a way to generate a transformation inside a PMML model that will be able to collapse multiple rows of input data into a singular row of prediction? I was unable to find such an example (or any example at all), while this post states that only singular row operations are supported within PMML.Searching further, the Sklearn2PMML Library has an ""Aggregator"" method, but this only generates a transformation within a single row, like getting an average out of two columns.This code:Is able to generate a simple transformation, instead of Aggregate function.TL;DR:Example of what I would like to achieve:Is there any way to do it inside the PMML model, or should I try to generate such preprocessing actions before applying the data for prediction?"
4055,TypeError: MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead,"['python', 'scikit-learn']","I am trying to scale a sparse array.:However, I receive this error:I am using:Unless I am missing something, in the sklearn documentation it is said that sparse arrays are supported though:
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.htmlAm I missing something?If not then is MaxAbsScaler equally good?P.S.I have tested MaxAbsScaler and it looks to work well so I am good with that too.
But I am still wondering a bit about the MaxAbsScaler."
4056,"I've had trouble training a model in AWS SageMaker, everything is fine until the model needs to be saved","['amazon-web-services', 'machine-learning', 'scikit-learn', 'upload', 'amazon-sagemaker']","I've had trouble training a model in AWS SageMaker, everything is fine until the model needs to be saved. I have tried with a 500MB dataset and everything works correctly, but when the .csv file occupies 10GB the training job fails. Next I leave my training python file and the error output, the machine used to train was ml.m5.2xlarge with a train_volume_size = 100.File .py to train the model in SageMaker with an output of 10GBWhen the finished the output was the next error"
4057,"Extracting [char,word,func] ngrams features from a dataset and running classification using CountVectorizer in Python","['python-3.x', 'scikit-learn', 'pipeline', 'n-gram', 'countvectorizer']","I have a data set in Urdu language, i want to run Naive Bayes algorithm on the dataset for classification, by using countvectorizer in python i want to extract (char,word,func) ngrams before applying classification algorithms.I have tried ngram_range function using sci_kit library, not sure how pipeline can be used to achieve it.
I am not a developer, just occasionally try to code, so please be nice. Thanks in advance"
4058,Estimate Training Time of scikit multilearn MLKNN algorithm,"['machine-learning', 'scikit-learn']","I am using scikit multilearn MLKNN algorithm to train my multilabel classification problem.
Recently I have trained several models and their training time is followingk= 217, number of samples = 47018 and vector size of each sample = 3000. Total training time was 9069 second (2.5 hours)k=167, number of samples = 28195 and vector size of each sample = 3000 and number of labels=2001. Total training time was 3351 second (Almost 56 mins)Now I have another model which has k= 451, number of samples = 203651 and vector size = 3000. I have already waited for more than 26 hours and it is still fitting aka training.Can anybody explain how much time will need to train that large scale dataset that I have mentioned?"
4059,are we supposed to cover categorical data to dummy variables or hot encode them when using libraries like sklearn,"['python', 'machine-learning', 'scikit-learn', 'classification', 'dummy-variable']","I just have this basic question, when we are using libraries in R or python that deal in classification models like logistic regression, K-nearest, etc, are we suppose to hot encode (create dummy variables) for categorical data points or the algorithm takes that into account already?"
4060,Find max value from grouped list of clusters of similar numbers,"['python', 'scikit-learn', 'hierarchical-clustering']","I'm working on k-mean algorthim to cluster list of number, If i have an array (X)then I run the following codei got the resultshow to get the max number in each cluster ? like this"
4061,Custom Scaling features in Logistic Regression using sklearn,"['python', 'scikit-learn', 'logistic-regression']","I am doing a course on udemy and I am using a custom scaler function they've provided to scale the features that aren't dummies in the dataset but I am receiving an error. I am pretty certain that I've followed the code line for line, but I could have missed somethingI've attached the code and a file to my githubBelow are the functions:Then I pull the columns that I want to scale into a variable and call the function:Then the following error is outputted:This seems weird as I thought ordinarily this was avoided by including self in the parentheses of the functions.If anyone is able to help I'd be most grateful!"
4062,"If no scoring is specified, the estimator passed should have a 'score' method. - error","['python', 'keras', 'scikit-learn']",I am just trying to implement Cross Validation to MLP but this error doesn't let me goThat is the error I got
4063,How to get column names for a one-hot encoder inside a pipeline that has mixed variable types,"['python', 'scikit-learn']",I'm trying to add column names to a dataset I one-hot encoded. I've checked out a lot of different questions but my dataset has two constraints:Thanks all
4064,Loading large trained machine learning model with joblib,"['python', 'scikit-learn', 'pickle', 'random-forest', 'joblib']","I am trying to train the Radom Forest classifier.Here is my code:After completion of training, I dumped the model with the joblib as follow:Later, when I load the model to run it on test dataWhen I run the above line, the Jupuyter notebook's kernel restarts. When I checked the RF_model.pkl file size, it is 422.7MB.I tried this solution, and passed the compress=3 argument to the joblib.dump() method.Even though the size changed to 43MB but still the kernel restarts and I cannot load the model.Note: I am running the Jupyter notebook from a docker container."
4065,Feature Selection in Scikit-learn Encounters Problems with Mixed Variable Types,"['python', 'scikit-learn', 'data-science', 'feature-selection']","I'm currently trying to do feature selection for a dataset I have. There's about 50 variables, 35 of which are categorical each of which are either binary or have < 5 possible values. I'm trying to get ~15 input variables before the preprocessing.I'm trying to use Recursive Feature Elimination with Cross-Validation (RFECV) in scikit-learn. Because there is a mix of continuous and categorical variables, I'm having some problems when I one-hot encode the categoricals that I have two questions about:I'm not going to include the preprocessing, but all it does is impute and one hot encodes with no columns dropped.Here's the two RFECV objects I have:"
4066,ValueError : empty vocabulary; perhaps the documents only contain stop words,"['python', 'pandas', 'dataframe', 'scikit-learn']","For a current project, I am running a script to count the number of words within a Pandas DataFrame with SciKit-Learn. The script is running well when iterating over several DataFrame columns at once (in this case txt_main, txt_pro, txt_con and txt_adviceMgmt).When selecting only one of the columns instead of all four, e.g. through line corpus = row['txt_pro'], I am however receiving the notification ValueError: empty vocabulary; perhaps the documents only contain stop words. This is regardless whether I e.g. selecting txt_pro or txt_con.Where is the thought error that lets the script run when all four columns are selected in the given line (corpus = row[ ]) but yields an error notification if only one column is selected?The script looks as follows:And the correponding JSON file which is basis for the DataFrame has the following structure:The full error message is shown here:
"
4067,Extract Probabilities in Affinity Propagation,"['scikit-learn', 'cluster-analysis']","I would like to try to extract the availability matrix after using Affinity Propagation. Then use these as the probabilities for each point belonging to some cluster. I tried to use the methods for Affinity Propagation, but none were suitable to provide probabilities."
4068,how to extract regression coefficients/durbin watson for PLS (partial least squares) regression,"['python', 'scikit-learn']","(PLS) is implemented in the scikitlearn: http://scikit-learn.org/0.12/auto_examples/plot_pls.html This is OLS results - how do I extract durbin watson for PLS regression?Omnibus:                        0.176   Durbin-Watson:                   2.346
Prob(Omnibus):                  0.916   Jarque-Bera (JB):                0.167
Skew:                           0.141   Prob(JB):                        0.920
Kurtosis:                       2.786   Cond. No.                         176."
4069,"sklearn train_test_split: where to add average=None when we get: ""Target is multiclass but average='binary'… error?","['python', 'pandas', 'scikit-learn']","I know that I should add average=None somewhere but I dont really know, the target variables is a set of numbers:I'm getting this error:Thanks."
4070,Get count vectorizer vocabulary in new dataframe column by applying vectorizer on existing dataframe column using pandas,"['pandas', 'scikit-learn', 'countvectorizer']","I have dataframe column 'review' with content like 'Food was Awesome' and I want a new column which counts the number of repetition of each word.Expecting output like ['Food':1,'was':1,'Awesome':1]
I tried with for loop but its taking too long to executeI would like to do it without for loop."
4071,CAP and ROC Curve in Machine learning,"['machine-learning', 'scikit-learn', 'data-science', 'logistic-regression', 'supervised-learning']",What is ROC and CAP Curve and what is the diffrence between them? And how to select the threshold Value of the model which I want to use
4072,Onehotencoder ValueError: setting an array element with a sequence,"['python', 'scikit-learn']","I am getting ""ValueError: setting an array element with a sequence"" when I am trying to do OneHotEncoder from sklearn library. Can someone help please"
4073,Dataset indices for predicted values is not matching with those for actual values,"['python', 'scikit-learn', 'regression']","I am a python novice who is trying to solve a regression problem with neural networks. I am at the stage where I want to plot the predicted vs actual followed by determining the regression coefficient.Model trainingComparing actual to predicted valuesImage of dataset
"
4074,No unique mode; found 2 equally common values,"['python', 'statistics']","I'm using statistics.mode([1, 1, 2, 2, 3]) to find the mode, but I get:no unique mode; found 2 equally common valuesWhen more than one mode is found, how can I output either 1 or 2?"
4075,"ValueError: Found input variables with inconsistent numbers of samples: [852, 215] while running polynomial regression on python","['python', 'pandas', 'numpy', 'machine-learning', 'scikit-learn']","**when i try to run last cell reg = linear_model.LinearRegression() train_y_ = reg.fit(train_x_poly, train_y)
it shows following error if anyone can help how to solve this error, it looks like i have messed up in train test split **"
4076,How to make prediction based on 1 category with multiple varibles,"['python', 'machine-learning', 'scikit-learn']","What I am trying to do:Here is a question of paper
Here is my sample data, the correlation between variables [max_contours, non_zero_ratio, contours, ratio_a] and the result is around 78% or 80%.However, If I apply ML prediction there is a chance that two checkboxes within a question are predicted to be chosen (or ticked).(There should be 1 answer for each question only)In which way, I should approach this problem ?For your further information about data please click the link below:https://drive.google.com/file/d/1dSasxAk9Mvs5MjsYB-2UXM2HYrmiVvdV/view?usp=sharingWith:x, y, w, h are the coordinate of every checkboxpeaks, gap     are variables that I used to detect checkboxes belongs to 1 question"
4077,"Error: Supported target types are: ('binary', 'multiclass')","['python', 'keras', 'scikit-learn', 'neural-network', 'sequential']","How to handle the error ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous-multioutput' instead ?I tried something with from sklearn.utils.multiclass import type_of_target or x[0],y[0], but without success ...((336, 10), (336, 5))"
4078,Principal Component Analysis - three classes mixed on three seperated groups,"['python', 'machine-learning', 'scikit-learn', 'dataset', 'pca']","I have a Dataset with 3 labels and 27 features. I was trying to use the PCA on it and reduce the dimensions to 2. The results are a bit confusing. Honestly, I didn't expect too good results, but I got the first picture and I was very surprised. Since I have three labels, I thought that I got my three classes pretty clear. However, when I apply the colors, I get the following picture:
I am a bit wondered about the fact that the three classes are totally mixed on three clearly seperated groups. I also tried it in 3D an the results looks exactly the same.
Is there any error in my code or does anyone know a reason why this could happen?"
4079,"GridserachCV, make_scorer in python, for class specific F1","['python', 'scikit-learn', 'cross-validation', 'grid-search']","I am working with a highly imbalanced dataset (more values in class 0 and few in class 1). To analyse the performance of the classifier I am using the F1 metric. I set average = None in the F1 function from scikitlearn, this is because I want to check its performance on class 0 and 1 separately and am only concerned about the classifier's performance on class 1.
 value = f1_score(yTest, y_scores, average=None)
value[1] gives me the required valueNow for hyperparameter tuning using gridserachcv, I create the F1 score in the following way
f1_scorer = make_scorer(f1_score, average=None)
However this gives an array which is not accepted by GridSearchCV(svc_clf, param_grid, cv=nfolds, error_score=0.0, scoring=f1_scorer2)
How do I extract the value at index 1 to be used as the metric in the scoring parameter. This is because I want to place focus on the classifier's performance on class 1 during hyperparameter tuning.
I did try some naive ways of writing f1_scorer2[1] etc but it says '_PredictScorer' object is not subscriptable"
4080,"UFuncTypeError: ufunc 'matmul' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')","['python', 'pandas', 'numpy', 'machine-learning', 'scikit-learn']","Here is my code and there is errorwhile running y_pred lineI am learning machine learning, so please help me"
4081,How can we return the K Nearest neighbour in sklearn?,"['python', 'machine-learning', 'scikit-learn', 'knn']","The KNeighborsClassifier from sklearn predicts the class calculating the k instances that have the least distance, right? But, can we return the instances that he is using to do this prediction?for instance:
if knn calculate that Ana should be a woman due the distance from Mary and Alexa, can we return ['Mary','Alexa']"
4082,sklearn-SVM: Define Different Loss Values for Different Classification Errors,"['python-3.x', 'machine-learning', 'scikit-learn', 'data-science', 'svm']","I have a dataset I want to classify to 4 different classes, using a multi-class SVM. However, I want to give a different weight in the loss-function to different classification errors:
If the classifier identified a class-0 sample as a class-3 sample or vice versa, I need a higher ""cost"" than any other mistake. Something like the following:I saw sklearn's SVM class has a class-weight parameter for the fit method, but I need something similar for the loss. Is it something we can define ourselves?"
4083,How can I get the distance in sklearn.neighbors function,"['python', 'python-3.x', 'scikit-learn', 'similarity', 'nearest-neighbor']","I have some pictures to recognize, one of them is correct. I try to compare them with my correct dataset by scikit-learn knn function.I think the distance between correct picture and correct dataset should be nearest.I can get the correct picture by compare the distance of each picture I need to recognize.My question is how I can get the distance by scikit-learn knn."
4084,sklearn: SVR fails to generalise adder function,"['machine-learning', 'scikit-learn', 'regression', 'artificial-intelligence', 'generalization']",This is my SVR to learn adder function (y=x1 + x2):But the result isn't what expected for:Is it possible for sklearn SVR to generalise the adder function? What should be changed in the code above to make SVR learn x1+x2?
4085,How do I optimize my random forest model in scikit Learn (Python),"['python', 'machine-learning', 'scikit-learn']","I have a random forest model using scikit learn, as seen here:I've been trying to optimize my model, but haven't had any success yet. The highest accuracy rate I have achieved on the test dataset is 78%. Do you have any ideas or steps I could take to improve my model?"
4086,Fitting a 1d vector to SVC linear kernel,"['python', 'python-3.x', 'numpy', 'machine-learning', 'scikit-learn']","I am trying to use SVC with the linear kernel for image recognition task. My current data is a 2x5 matrixMy second row is my X features, which are pixel intensity value from different images.My first row is my Y labels which are which face image the pixel was extracted from.
I am trying to input my data into SVC at any costs, whatever it takes.What i am trying is:I want to test for the recognition rate for those features, but i get the error:TypeError: Singleton array array(162) cannot be considered a valid
collection."
4087,"ValueError: X.shape[1] = 96 should be equal to 2, the number of features at training time","['python', 'scikit-learn', 'classification', 'multilabel-classification']","I am following the tutorial here: https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multilabel.html#sphx-glr-auto-examples-miscellaneous-plot-multilabel-py.I have texts that I want to classify, so I encoded them. On the linesmy X is (188, 96) and my Y is (188, 2).When I try to run clf.predict(test), even though test is (56, 96), I get the error ValueError: X.shape[1] = 96 should be equal to 2, the number of features at training time. Where did the 2 features come from? My Y labels are 2D arrays, but my X has 96 columns."
4088,Making ROC curves with results from cross_validate?,"['matplotlib', 'machine-learning', 'scikit-learn', 'roc']","I am running 5 fold cross validation with a random forest as such:However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py)I tried tweeking the code to make it work for cross_validate but to no avail.How do I make a ROC curve with the 5 results from the cross_validate output being plotted on a single graph?Thanks in advance"
4089,RandomSearchCV with generator as data,"['python-3.x', 'keras', 'scikit-learn']","I want to perform the randomsearch optimization on my timeseries-preiction model and I don't have a fixed dataset but a TimeseriesGenerator to generate data on the fly.When I run this code, I get the following Error:Is it possible to perform hp-optimization with a generator as data?"
4090,Cannot Import Sklearn Library for Python,"['python', 'scikit-learn', 'sklearn-pandas']","I am pretty new to Python and am trying to import train_test_split from sklearn.model_selection for a machine learning project, using the code:I am getting the errorI have tried googling around and have not found a reason for the error and I have downloaded the newest version of sklearn. I do not understand why the library is not importing correctly, nor why the error is for ""__check_build"" when I do not have anything in the code with that name. I am completely stuck. Does anyone know how to fix this problem?"
4091,Python sklearn import error (DLL load failed: The file cannot be accessed by the system.),"['python', 'scikit-learn', 'deep-learning', 'neural-network', 'pip']","I have been trying to load the sklearn module in jupyter-notebook pycharm and Idea all shows the same error. I tried reinstalling the IDE's and also Python (3.7) but the error did not go. I just started learning neural networks and have not been able to solve it on my own.My CodeErrorThe error message was very long, I was not able to fit the full error."
4092,How to predict with the test dataset while using cross validation?,"['python', 'machine-learning', 'scikit-learn', 'random-forest', 'cross-validation']","I would like to use Cross Validation for the prediction model. I would like to keep 20% of my data as a test set, and use the rest of my data to fit my model with Cross Validation.It would like to be as following :And as a machine learning model, I would like to use Random Forest and LightGBM.It gives the result, but I want to predict the y values of X_test data. Could you please help me for it? After that, I will create a model for LightGBM as well.Thanks"
4093,Optimal number of clusters tslearn TimeSeriesKMeans,"['python', 'scikit-learn', 'time-series', 'cluster-analysis']",I have a bunch of timeseries data and want to cluster them using tslearn (tslearn.clustering.TimeSeriesKMeans)Is there a way to find the optimal number of clusters via e.g. an elbow plot or does this not work with timeseries data? Would sklearn help here? Could you please give me a hint how to code that in python?
4094,How does sklearn GaussianNB calculate probabilities when there is a zero frequency problem? How to apply lovelace smoothing with 3 variables?,"['python', 'machine-learning', 'scikit-learn', 'gaussian', 'naivebayes']","I wanna know how has sklearn GaussianNB arrived at the probability figures in the following case:These are the probabilities calculated by the Naive Bayes Model:
I used the following formula to calculate probabilities:These are the probabilties calculated by me before Laplace Smoothing (or Laplace Correction):
As per sklearn GaussianNB, (after Lovelace Smoothing) >>  P(No | overcast,cool) = 0.00505764I tried to get there, using Lovelace Smoothing as I understood it and got this :and also this way:What would be the correct way to apply Lovelace Smoothing in such a scenario?Which method is being used by GaussianNB?"
4095,Skip a pipeline step within GridSearchCV,"['python', 'scikit-learn', 'pipeline', 'grid-search']","I've created the following pipeline:I want to perform a GridSearchCV on which the steps 'feature_select' in pipeline_numeric_feat and in pipeline_category_feat is optionally skiped. For that purpose my param_grid is:However is get the following error: Invalid parameter categroy_pipe for estimator. when run:I know that the problem is in the following line of the param_gridIn particular with the piece of code feature_select.Feature_Selector()Thus, how could I define the 'data_prep__categroy_pipe__features_select' step in the param_grid to be skiped sometimes and not skiped other times."
4096,Pandas: concatenate dataframe with distance matrix,"['python', 'pandas', 'dataframe', 'scikit-learn', 'euclidean-distance']","I tried to concatenate two Pandas DataFrames, but it concatenates wrong.Initial dataset looks like:Then I generate a Euclidean distance between every well from x and y coordinates (last two columns):and receive 65x65 matrix (pandas.core.frame.DataFrame type) where contains the distance between every wellThen I drop extra columns and concatenate two dataframes:As a result I receive not 65 rows x (9 + 65) columns dataframe but 130 rows × 70 columns df like:It looks like some data concatenate in the right but some moved to the bottom. Moreover, strange NaN value popped up.
Please, help me to understand what I am doing wrong."
4097,How to predict y=1/x values in Python? [closed],"['python', 'scikit-learn', 'non-linear-regression']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 15 days ago.I have a data frame named df:x is only for plotting purposes.
I'm trying to predict the y value based on the p values. I am using SVR from sklearn:I have already tried all of kernels but it still doesn't work correct enough.Do you know which sklearn model or other libraries should I use to better fit a model?"
4098,Is there any way to convert Single array to one hot encoding?,"['python-3.x', 'machine-learning', 'scikit-learn', 'one-hot-encoding']",I have taken as input string convert in array but now I want this array to in one hot encoding to run in the model.I am also sharing my google colab for better understanding.Code done on google colab
4099,what should I do if my mutual information and chi2 tests show different results?,"['python', 'machine-learning', 'scikit-learn', 'classification', 'chi-squared']","I'm doing feature selection for my machine learning projects.For a classification problem, I am using both of mutual information and chi-squared test to select the most related features for my target.The two methods are supposed to produce the similar results, but that wasn't the case for my projects.I'm just wondering why?I'm suspecting my chi2 result is not correct. Because my target has 5 levels, and some cells in the contingency table are zeros.So should I combine some target levels?sklearn.feature_selection.mutual_info_classif is the function I'm using for mutual informationsklearn.feature_selection.chi2 is the function I'm using for chi-squared"
4100,How to reshape series to use it in StandardScaler,"['python', 'scikit-learn']","I am trying to perform a time series (LSTM ) on some data that I processed and now I am trying to scale it using the StandScaler from sklearn. Here is my initial preprocessing of data:The output of the train, validation and test sets are (1800,) (699,) and (533,) respectivelyWhen I tried to run a standardscaler by doing:I got the following error:I tried to redo the previous code the following way:But I still get errors: 'Series' object has no attribute 'reshape'I am not sure how to turn this into an acceptable array for scaling. Can somoene please help and let me know how to solve this? Thanks"
4101,Train/test/validate in Scikit learn,['scikit-learn'],"I need to x_train, X_validate, and y_test. This is the code I have so far, but I do not think it is right. Could someone please guide me? I typically only see train and test, not all 3 together.Note: I need to use train_test_split"
4102,What are the ways to choose “Best number of topics” in NMF topic modeling?,"['scikit-learn', 'python-3.7', 'lda', 'topic-modeling', 'nmf']","I am doing topic modeling using the Scikit-learn NMF model.
Just like in LDA there is GridSearchCV to choose the best number of topics, Is there anything like that in NMF also?
Also, I have heard about Davies Bouldin index and other such coefficients, is there any coefficient that can be applied to NMF to select the best number of topics?Thanks in Advance!"
4103,How to plot a survival tree in python,"['python', 'scikit-learn', 'graphviz', 'survival-analysis', 'scikit-survival']",I have developed a survival tree using Survival tree in sksurv.I get the following when I do getstateBut when I use tree.plot_tree I get an error stating the following:AttributeError: 'SurvivalTree' object has no attribute 'criterion'I tried graphviz too and get the same error.So basically I want the survival tree to be plotted for better interpretation.
4104,How can I get TfidfVectorizer in a pipeline? Concatenation axes don't match,"['python', 'scikit-learn']","I'm trying to get a TfidfVectorizer working in a Pipeline, but the pipeline generates an error that the concatenation axes don't match. The TfidfVectorizer seems to work correctly when called outside the pipeline, which is very simple at the moment.  Here's code that will generates the error.Here's the error message, a ValueError:"
4105,How to import SimpleImputer from sklearn?,"['python', 'scikit-learn']","I would like to import SimpleImputer from sklearn, I have tried the following code:However, it gives the following error:The version for my scikit-learn is: 0.23.1Is there any other way to import SimpleImputer or how can I make from sklearn.impute import SimpleImputer code work?"
4106,Linear Regression Model in scikit but I am getting an error,"['scikit-learn', 'linear-regression']",I am trying to fit the training data sets to Linear Regression Model in scikit but I am getting an errorError MessageCan someone help me please?
4107,Why does the accuracy of the model change?,"['python', 'machine-learning', 'scikit-learn', 'linear-regression']","I'm kinda new in the area of ML. There is something I wonder.. when I use  'random_state=10' the variables remain same and nothing changes also it doesn't effect the accuracy of the model.. everythings fine until now.. but when I don't use it, the variables change and it changes the accuracy of the model,  the variables are different now but they're still in the same data frame, I thought the accuracy still would be same.. is that how the things work in ML? or am I missing something? Here is my code."
4108,Multiclass AUC with 95% confidence interval,"['scikit-learn', 'roc', 'auc']","I am currently trying to figure if there is a way to get the 95% CI of the AUC in python. Currently, I have a ypred list that contains the highest probability class predictions between the 4 classes I have(so either a 0/1/2/3 at each position) and a yactual list which contains the actual labels at each position. How exactly do I go about bootstrapping samples for multiple classes?Edit: Currently the way I am calculating the AUC is by doing a one-vs-all scheme, where I take the AUC for each classes versus the rest and averaging those 4 values to get the final AUC."
4109,How to averages models using scikitlearn Cross Validation,"['python', 'scikit-learn', 'cross-validation']","I've always read that Cross Validation can be used to create multiple splits, then average the model to avoid overfitting, but have not been able to find an example doing that.Looking at scikitlearn, I only see examples of cross_val_score that gives you the CV score across multiple splits for you to evaluate whether there is overfitting such as hereHow can one uses CV to actually average the models across the splits ?"
4110,Different values of Numpy.var() and Pandas.var(),"['python', 'pandas', 'numpy', 'scikit-learn', 'statistics']","I am learning a bit about Standard Scaler in datasets. I am noticing a strange behavior which I think might be a syntax or logical error in my code, but can anyone correct me.As we know that when we do StandardScaler, we have a std of 1 and mean of 0 as Var = Stdev^2.Now when I used this in my code with sklearn.preprocessing.StandardScaler, after using fit_transform, the variance was 1. But when I changed the value in original data frame, It was 1.5 for both rows. Can I know why is it and how to make it 1 using StandardScaler. Thanks.Here is my datasetand here is my codeHopefully, you got what I want to say."
4111,Theil sen estimator and linear regression on large 3d numpy array,"['python', 'multidimensional-array', 'scikit-learn', 'scipy', 'regression']","I have a large 3D grid ((100, 375, 700) in which the axis are x (longitude), y (latitude), and years. I want to produce Theil-Sen temporal trends and linear regression trends over all grid cells. In the documentation and in the related stackoverflow questions the regression is always being performed over dataframes. Is it possible to do the regression on a 3D numpy array, or do I need to change the structure to a 2D array or pandas dataframe. Also, I would like to output the significance of the trends. In the pre-arranged functions of SciPy and Scikit-learn, the significance is often not returned.Documentation Theil-Sen estimator: https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.htmlDocumentation Theil-Sen slope: https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.mstats.theilslopes.html"
4112,Custom estimator for GridSearchCV in sklearn,"['python', 'scikit-learn', 'cross-validation', 'gridsearchcv']","I'm trying to implement a custom estimator and optimise four parameters using GridSearchCV. I haven't fully understood this yet and am having some problems (see update below).Here is my estimator with coeffs containing the four parameters I want to optimise. v1, v2, sm are the input data (NumPy arrays).Now I define a function to do the grid search:RunningThis gives me the error message TypeError: fit() missing 1 required positional argument: 'X'.update:
I have updated my code quite a bit and I think I'm getting closer. It would be great to get some feedback on the correctness of this. With some test data, I seem to end up with a and b always being 0.The estimator class:The grid search function:and tuning the model to predict values:Unfortunately, I'm totally lost and I always receive 0 for both a and b which can't be correct. Any support would be really appreciated."
4113,"Python - Unable to import sklearn, but works fine for other packages","['python', 'python-3.x', 'scikit-learn', 'scipy']","Following is the error message I see, can you please let me know what's wrong?The environment details:The command from python terminal: from sklearn.metrics import mean_absolute_errorThis is the complete error message:"
4114,SVR gridsearch runs for very long time,"['python', 'machine-learning', 'scikit-learn', 'svm', 'grid-search']","I'm trying to run a parameter optimization of a SVR algorithm, here is my code:I've already normalized my data by scaling each feature to [0,10]:The data has the following shape (with the target variable included):The output from the gridsearch so far looks like this:As you can see it seems that some tasks take much longer than others, i've been waiting at least 4 hours for these last 5 (266-270). Does anyone know why some of these fits would take so long? If so, is there anything I can do about it?"
4115,What is the use of n_informative in make_classifcation in sklearn,"['scikit-learn', 'classification', 'sklearn-pandas', 'one-class-classification']",the make_classification in sklearn has parameters n_informative. What is the use of that parameter?
4116,Tslearn: import multivariate time series from pandas dataframe,"['python', 'pandas', 'scikit-learn', 'time-series']",I would like to transform the following pandas dataframe to a tslearn format:where index is the index of the time series with associated multiple attributes x and y. I haven't found any method in tslearn to import directly from this data format.Is there an easy way to do it?
4117,polynomial regression and cross validation on data set with more than one variable,"['python', 'machine-learning', 'scikit-learn', 'regression', 'linear-regression']","I have a dataset with 25 columns. Is my code correct?
I got the mean but I'm not sure if I can do this code on more than one independent variable. Also how can test train vs test here? here I test X and Y.
I also get mean that not seems to be normal: -1.1239261032363359e+18Thank you!"
4118,How to get the feature importance in Gaussian Naive Bayes,"['python', 'scikit-learn', 'gaussian', 'naivebayes']",I have a Gaussian naive bayes algorithm running against a dataset. What I need is to to get the feature importance (impactfulness of the features) on the target class.Here's my code:And I tried:and it gives an error:AttributeError: 'GaussianNB' object has no attribute 'coefs_'Can someone please help me?
4119,How to perform nested Cross Validation (LightGBM Regression) with Bayesian Hyperparameter optimization and TimeSeriesSplit?,"['python', 'scikit-learn', 'cross-validation', 'lightgbm', 'hyperopt']",I want to do predictions with a Regression model.I try to optimize my LightGBM model for the best hyperparameters while aiming for the lowest generalization RMSE score without overfitting/underfitting.All examples I've seen use Classifications and split randomly without awareness for Time Series data + use GridSearch which are all not applicable to my problem.How can I get bayesian hyperparameter optimization for my final model while using nested CV and TimeSeriesSplit?My code for simple CV so far:
4120,Python fit_transform return only zeros,"['python', 'scikit-learn', 'linear-regression']","I'm trying to fit_transform an Numpy array, but when run fit_transfort(),
it will fill only zero values.array([-0.23130257,  0.19945477, -0.49045489, -2.40903789,  0.62833204, ........array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0.]])"
4121,Recall scoring metric returns error in TPOT,"['python', 'machine-learning', 'scikit-learn', 'tpot']","I've gotten TPOT code up and running successfully but I wanted to change the scoring metric from accuracy to recall. However, whenever I specify the recall option for scoring TPOT throws its generic error. The weird thing is this runs until it's supposed to end but then throws the error.Here's my code (truncating before this because the rest  is proven to work):And it throws this error:Thanks so much everyone"
4122,How to use Linear regression model output to find matching dataset,"['python-3.x', 'scikit-learn', 'linear-regression']","Using this example I have obtained linear-regression model for my timeseries dataset for one particular day. Now I would like to find the days from archival data whose dataset matches (more than 80%) the regression model.
How can I do that in python?
Should I find the regression model every time I use the application, or I can some way save the results (regression model) and use it to find the matching days from the dataset?I am new these concepts"
4123,Python sklearn - predict single input,"['python', 'pandas', 'numpy', 'scikit-learn']","I'm trying to predict a single array after model trained, but when I change the input, the prediction is the same.

some example ofdata=
[3,9,3,12,3,8,8,13,12,2,0,'42','58',12,12,6,3,6,4,7,10]
[1,4,8,23,8,13,7,9,20,1,3,'50','50',15,16,0,2,4,8,3,1]"
4124,Inaccurate classification predictions by KNN algorithm,"['python', 'machine-learning', 'scikit-learn', 'classification', 'knn']","I am trying to predict whether a tweet goes viral by only considering 3 variables (for the sake of simplicity):This is my code:This individual components (tweet length, no. of followers, no. of friends) of the prediction variable exactly matches one of the real data points that are considered viral (=[1]) in the dataset.Yet, I get a [0] (=not viral) as a prediction though it has an around 80% accuracy rate.Any idea why the algorithm doesn't classify it correctly?P.s. the K (=25) is chosen by trial and error resulting in the highest accuracy score"
4125,"Using cross-validation to determine weights of machine learning algorithms (GridSearchCv,RidgeCV,StackingClassifier)","['python', 'scikit-learn', 'cross-validation', 'gridsearchcv']","My question has to do with GridSearchCV, RidgeCV, and StackingClassifier/Regressor.My question is, what exactly does this mean? Does it break the train data into k folds, and then for each fold, train the final estimator on the training section of the fold, test it on the testing section of the fold, and then take the final estimator weights from the fold with the best score? or what?-To find the best hyperparameters, do they do a CV on all the folds, for each hyperparameter, find the hyperparameters that had the best average score AND THEN AFTER finding the best hyperparameters, train the model with the best hyperparameters, using the WHOLE training set? Or am I looking at it wrong?If anyone could shed some light on this, that would be great. Thanks!"
4126,Python scikit learn pipelines (no transformation on features),"['python', 'scikit-learn']",I am running different machine learning models on my data set. I am using sklearn pipelines to try different transforms on the numeric features to evaluate if one transformation gives better results. The basic structure I am using is simple:I am trying a bunch of transformations but I also want to test the scenario where no transformation are performed on the numeric feature set (i.e. features are used as is). Is there a way to include that within the pipeline? Something like:
4127,How should I choose n_features in FeatureHasher in sklearn?,"['machine-learning', 'hash', 'scikit-learn', 'data-science']","How should I choose n_features  in FeatureHasher in sklearn ?
assume that I have 1000 categories in feature ""case"" and I want to hash them"
4128,TFIDF Vectorizer within SciKit-Learn only returning 5 results,"['python', 'pandas', 'dataframe', 'scikit-learn', 'tfidfvectorizer']",I am currently working with the TFIDF Vectorizer within SciKit-Learn. The Vectorizer is supposed to apply a formula to detect the most frequent word pairs (bigrams) within a Pandas DataFrame.The below code section however only returns the frequency analysis for five bigrams while the dataset includes thousands of bigrams for which the frequencies should be calculated.Does anyone have a smart idea to get rid of my error that limits the number of calculations to 5 responses? I have been researching regarding a solution but have not found the right tweak yet.The relevant code section is shown below:And the output code looks like this:
4129,When to use fit_transform and transform?,"['python-3.x', 'pandas', 'scikit-learn']",For stuff like LabelEncoder and the SimpleImputer from scikit-learn why do we use fit_transform for the X_train DataFrame and why do we use transform for the X_valid DataFrame?egWhat is the difference between the two in terms of how they work?
4130,How can i balance f1_score value over classes in a multiclass classification problem?,"['python', 'scikit-learn', 'dataset', 'classification', 'imbalanced-data']","I have to solve a multiclass classification problem, my goal is to achieve a value of f1_macro greater than 89%.I tried a lot of algorithms, and the best results (f1_macro value) i get is 0.8675071290241037.To understand better what was happening I printed the f1_score of each class in my dataset, there are 4 classes labeled 0,1,2,3 and i found out that all my algorithms show the worse score on class 1.For example with bagging (BaggingClassifier) applied to QuadraticDiscriminantAnalysis  the f1_score of each class is: [0.90454951 0.81434599 0.86750789 0.88362513], using bagging applied to SVC  [0.87156776 0.77124183 0.79685039 0.84953941] and finally using bagging applied to SVC with the option class_weight='Balanced' the f1_score of each class is [0.87216682 0.78688525 0.80060883 0.85294118].
I used also other algorithms, but I think this is enough to make you understand the point.Now my question is: how (and why) can I improve the score on those classes where algorithms go wrong?Is it possible that the problem is that there are different numbers of samples for each class in the dataset? if yes, how can i manage it?I'm supposing that the problem is the number of samples for each class because the situation (after split train-test and after preprocessing) in this:and as you can see, the class that has the lowest f1_score is class 1, which is also the least represented class in the dataset.I thought to use a votingclassfier, but probably it's useless because all my classifiers are ""bad"" on the same classes.If the answer to my problem is to balance my dataset, there is a problem: I tried to balance my dataset using resampling, but Quadratic Discriminant Analysis raises the warning ""collinear variables"", I really don't understand why this happens because before start my algorithm i use PCA, but that's it. So, if the answer is ""balance your dataset"", how can i do it?"
4131,"Error while trying to predict numbers: Expected 2D array, got 1D array instead","['python', 'pandas', 'scikit-learn', 'sklearn-pandas']","I recently started studying ML during a youtube tutorial. Based on what was told in the tutorial, I decided to improve and apply to a kind of Guess Game.The game has multiple scenarios and some numbers which player have to collect before he goes to the next stage. So I thought to apply this to ML and try to see what happens.In my CSV file, I have 16 columns  (stage and 1 to 15 numbers) and lots of rows. So, to predict what is the numbers of the last stage (1988), I directly put into a ""...predict([[1988]]))"" and got theI know it's almost impossible to predict in this case, but my main goal here is to reduce the number of mistakes and see how good ML can be to solve this.Could you guys show me what and where I'm doing wrong? To better explain, code is below:Thank you in advance!"
4132,AttributeError with prediction using SVM model and scikit-learn,"['python', 'scikit-learn', 'svm', 'predict']","I am using a trained SVM model to predict the class label for a new data sample. The data has been processed the exact same way as the training data (same features). I am getting an AttributeError when trying to predict:The data_to_predict is a pandas DataFrame with a single row. I am getting this error:I believe (perhaps wrongly) that the issue is in the shape of the data, since feature_row looks correct when I print it. Here is what I tried so far:Extra Info: I'm not sure if it makes any difference in resolving this issue, but the model involves:Edit: In case someone comes across this thread later. With help from
Victor Luu, we managed to isolate the problem to the pickle file itself. After some further probing, I discovered that I was training and testing the model in two different environments that had slightly different scikit-learn versions (0.23.1 and 0.22.1), once I made sure these matched, the error disappeared."
4133,How to implement Gaussian Naive Bayes in two training sets,"['python', 'pandas', 'numpy', 'scikit-learn', 'seaborn']","how am I supposed to implement Gaussian Naive Bayes, in two training sets.I need:
Create a training set by selecting the rows with id <= 160
Train a Gaussian Naive-Bayes classifier as we saw in class to determine if a campaign will be successful, given the amounts used in each marketing channel
Calculate the fraction of the training set that is correctly classified.and:
Create a test set by selecting the rows with id> 160
Evaluate the performance of the classifier as follows:
What percentage of the test set was classified
correctly (correct answers on the total)? It is desirable that this number reaches at least 80%
What is the ratio of false positives to false negatives?Successful marketing campaign:And my code:"
4134,ValueError: 'pair_grid' is not in list,"['python', 'scikit-learn', 'regression', 'pipeline', 'transformer']","I make a coordinate system on the map so that the axes are parallel to the streets of Manhattan, and then first add the feature “Manhattan distance between the pickup place and dropoff place”, and then the logarithm of this attribute.And then, when fitting the lasso regression, I get an incomprehensible error.Why the program wants the 'pair_grid' in the lasso_best_pipeline? How to fix it?I got a 'pair_grid' in list:"
4135,How do I import sklearn on python 3.5.0 without a Type error?,"['scikit-learn', 'python-3.5']","I installed the prerequisites to install sklearn via this page: https://scikit-learn.org/stable/install.html and everything installed fine. However, when I attempt to import sklearn I receive the error (below).  Any ideas?Install:Error:"
4136,prediction column missing values,"['python', 'pandas', 'scikit-learn']",My code-When I see Test_Result there are values missing in the prediction column. I have checked the y_pred and y_pred_inv and pd.Series(y_pred_inv.flatten()) individually and they don't seem to be missing any values. How else should I debug my code to find my mistake? Also in my 6th and 7th row of satisfation_pred it is giving the wrong answer saying satisfied when it should be neutral or dissatisfied.Attaching the Test_Result below-This is the screenshot of pd.Series(y_pred_inv.flatten())
4137,how to create a dataframe with beta coefficients of regression,"['python', 'pandas', 'dataframe', 'scikit-learn', 'linear-regression']","I have the following code, which takes each column of the data frame HF and fit it to the entire dataframe Index. What I am interested then is to produce a dataframe Betas which contains the regression coefficients:the code runs but what I get is a Betas dataframe with only NaN values. Can you please help understand the problem? Thanks"
4138,facing issues with implimenting polynomial regression: AttributeError: 'PolynomialFeatures' object has no attribute 'predict',"['python', 'pandas', 'scikit-learn']","Following is the code i am trying to impliment.
I am trying to generate a polynomial equation to predict next values in the y arrayI tried searching on google but no good. Can you tell me what is happening, why is the error occuring?"
4139,where can I get the complete example of pmml?,"['scikit-learn', 'pmml']","The model is from sklearn.I'm trying to load above model with Java.My github account is forbidden  to reply in issue issue 197 by pmml's author，so I can not ask him further.I have tried Basic Usagethe full code with above basic usage is https://paste.ubuntu.com/p/8XTCqvshW6/pom.xml is :but I get:unreported exception org.xml.sax.SAXException;must be caught or declared to be throwncould anyone tell me how to solve it?
or where's the complete example of pmml?Thanks for your help."
4140,'SelectKBest' object has no attribute '_validate_data',"['feature-selection', 'scikit-learn']","I wanted to use SelectKBest chi square for feature selection of categorical data.df.shape yields (4000,150)y.shape yields (4000,1)On using the following code:It shows Attribute Error: 'SelectKBest' object has no attribute '_validate_data'Any ideas what might be a solution to this?"
4141,scikit-learn regression with multiple continuous targets,"['python', 'scikit-learn', 'multitargeting']","I want to perform regression on a dataset where the input has multiple features and the output has multiple continuous targets.I've been looking through the sklearn documentation, but the only multi-target examples I've found have either 1) a discrete set of target labels or 2) use a heuristic algorithm like KNN instead of an optimization-based algorithm like regression. Adding regularization would also be great, but I can't find a method even for simple least-squares. This is a really simple, smooth optimization problem so I'd be shocked if it wasn't already implemented somewhere. I'd appreciate it if someone could point me in the right direction!"
4142,Problems using OneHotEncoder from sklearn,"['python', 'machine-learning', 'scikit-learn']","I am attempting to one-hot encode some text, character by character. I have mapped the text to integer values first.Two things:Maybe I should be treating my text as 1985223 samples instead of features? Edit: This seems to be the case. Now I'm just wondering about the intuition behind this.Any help appreciated!"
4143,Error when using pandas trying to retrieve column data for categorical_subset,"['python', 'pandas', 'scikit-learn', 'deep-learning']",I am trying to use Sklearn tools alongside Pandas and np. I am trying to run my code (shown below the error)Please let me know where I've made the mistake and what I can do to fix this!
4144,Use a custom metric function in PySpark Datafame,"['python', 'pandas', 'pyspark', 'scikit-learn']","I have defined a custom function in python to calculate class-wise auc scores in a one-vs-rest fashion. It takes true classes and the probabilities for different classes as input and returns class-wise auc scores.For the sake of simplicity, I am generating some probability values which don't sum up to one.In python, I can use the above function. Can someone help me calculate this in PySpark data frame setting? Changing it to pandas data frame and calculating was failing in this case."
4145,Creating interaction terms quickly without SKLearn,"['python', 'numpy', 'scikit-learn']","I am using the following code to create interaction terms in my data:My problem is that it is extremely slow compared to SKLearn's PolynomialFeatures. How can I speed it up? I can't use SKLearn because there are a few customizations that I would like to make. For example, I would like to make an interaction variable of X1 * X2 but also X1 * (1-X2), etc."
4146,cross_val_score in Ridge/Lasso regression,"['scikit-learn', 'cross-validation']","I am running a Ridge regression in a Jupyter notebook, which I attach here.When applying the funcion cross_val_score to my estimator, what metric does it give me? I obtain 10 values lower than 1 (that come from the cross validation), which are:Thank you!"
4147,RandomForestClassifier - Odd error with trying to identify feature importance in sklearn?,"['python', 'scikit-learn', 'sklearn-pandas']","I'm trying to retrieve the importance of features within a RandomForestClassifier model, retrieving the coef for each feature in the model,I'm running the following code here,but am receiving the following errorWhat exactly am I doing wrong? You can see I fit the model right before looking to identify the importance of features, but it doesn't seem to work as it should,Similarily, I have the code below with a LogisticRegression model and it works fine,"
4148,How to perform feature selection on dataset with categorical and numerical features?,"['python', 'pandas', 'scikit-learn', 'feature-selection', 'rfe']","I am working on a dataset with 30 columns (29 numerical, 1 non-ordinal categorical). I hot-encoded the categorical feature and reached at 35 columns.
To improve training efficiency, I want to perform feature selection on my dataset. However , I am confused with how to handle a dataset with categorical and numerical features combined.Any suggestions? Any help is appreciated."
4149,To Get List of all the Hyperparameters using python for a Machine Learning Algorithm [closed],"['python', 'machine-learning', 'scikit-learn', 'hyperparameters']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 17 days ago.I wanted to get the list of all the Hyperparameters for the Machine Learning Algorithm. I wanted to give it as an input for the Grid Search. I am currently using Jupyter Notebook. Is there any command to get this list of all the hyperparameters ?"
4150,How to load newest pmml model from python3.6.10?,"['java', 'scikit-learn', 'pmml']","Target:1- Train model with Python2- Load model with JavaPython code:
https://paste.ubuntu.com/p/FtGk5Jm9xz/Java code:
https://paste.ubuntu.com/p/sv9DZqqm5s/pom.xmlThe sentense in above java code:Is wrong.I find some hints in Google
But I can not combine the hints with above java code.Could you help me?
Thanks for your help!"
4151,sklearn train_test_split - ValueError: Found input variables with inconsistent numbers of samples,"['python', 'tensorflow', 'keras', 'scikit-learn']","I have a multi-label classification problem, for which I looked online and saw that for one-hot encoding the labels it is best to use the MultiLabelBinarizer.I use this for my labels (which i separate from the dataset itself) as follows:But it throws me this following error:--EDIT: The labels dataset looks as follows (ignore the Interval column, this shouldnt be there and is not actually counted in the rows -- not sure why?):From this we can see that it is a multi-label multi-class classification problem. The shape of the dataset and labels before and after the Binarizer are as follows:"
4152,Grid-Search Voting Classifier containing a Keras model,"['python-3.x', 'keras', 'scikit-learn']","I am trying to train a VotingClassifier containing a Keras model using GridSearchCV.Here is the code:I get the following error:When I train the VotingClassifier outside of GridSearchCV no error occurs, however when I train it within GridSearchCV, I get the error message. This other question, VotingClassifier with pipelines as estimators, has the same error (without using GridSearch) and was fixed by a line asserting that the keras model is a classifier which i have also included:This did not fix the problem here.Any suggestions?"
4153,"How to fit a scikit model, for feature-vectors of varying lengths","['scikit-learn', 'scipy', 'audio-processing', 'librosa', 'sound-recognition']","I'm working on a sound classification project, given a set of audio recordings I try to determine which class a certain recording would fall into. You might compare this to a music genre or topic recognition (of a body of text) problem, my samples are of varying lengths and I need to assign each sample precisely a single label.I represent my features as 2d matrices, where each column represents a frame in the audio file (ex. 0.1 seconds) and each row is a feature pertaining solely to that time frame (ex. MFCC coefficients). Now although my row-count will be fixed, the number of columns will vary depending on the length of the recording.I feed in my training and testing data as numpy arrays, they contain a 2D n x y matrix for each sample, where n is a constant (i.e. 13) and y is a variable, which dependent on the length of the current sample.Unfortunately, scikit-learn doesn't seem to be a big fan of this, time and time again raising me a ValueError: setting an array element with a sequence.. Now I've seen a number of solutions:Now of these three, I would prefer something similar to #1, since it feels like this is the approach scikit is optimized for. Any ideas?"
4154,Machine Learning Error-Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targets,"['python', 'machine-learning', 'scikit-learn']","Getting the erro:File ""C:\Users\Oskar\anaconda3\lib\site-packages\sklearn\metrics_classification.py"", line 90, in _check_targets
""and {1} targets"".format(type_true, type_pred))ValueError: Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targetsWould love some help!"
4155,Get fitted coefficient of linear regression equation,"['python', 'scikit-learn']","I have a dataset with predicted and observed data.
The equation that predicts the data is given by: y = AfT With Af = constant (now at 1.35), T = wave period, g = gravitation 9.81, h = wave height.Id like to use linear regression to find the best fitted coefficient (Af in the equation),
so that the predicted value is closer to the observed data.I now have Af = 1.35 (from suggestion in the literature) results in r^2 = 0.5676
Ideally, I`d use python to find the best fitted coefficient for my data.X = observed/measured values in the field,
y = the predicted values of X using the equationI have difficulties incorporating the actual equation and finding the best fit for Af."
4156,How do I apply dataset that has been One Hot Encoded by scikit-learn to do Decision Trees?,"['python', 'scikit-learn', 'decision-tree', 'one-hot-encoding']","First of all, I would like to apologize if this is a no-brainer, but I am still relatively new at this.I have a dataset that has around 2,000+ rows and few columns. The last column, is the label that I want to predict.For example, the dataset kinda looks like this :The column label (which consists of the label) has been one hot encoded using Scikit-Learn and combined back to the original dataframe. Which looks like this.The column Label is dropped after that. But after that, I literally do not know how to proceed because this is my first time doing a practical approach on this subject.I also have split them to train and test set (based on a internet tutorial that I followed).But right now, when I try below, it doesn't work.It gives an error message belowHow do I approach this problem and proceed properly to classification?"
4157,Why should LabelEncoder from sklearn be used only for the target variable?,"['python', 'machine-learning', 'scikit-learn', 'label-encoding']","I was trying to create a pipeline with a LabelEncoder to transform categorical values.But this is throwing a TypeError:On further reference, I found out that transformers like LabelEncoders are not supposed to be used with features and should only be used on the prediction target.From Documentation:class sklearn.preprocessing.LabelEncoderEncode target labels with value between 0 and n_classes-1.This transformer should be used to encode target values, i.e. y, and not the input X.My question is, why can we not use LabelEncoder on feature variables and are there any other transformers that have a condition like this?"
4158,how to show an image after pca?,"['python', 'scikit-learn', 'pca']",I have a RGB image. I want to apply PCA for image-compression and see the output after the application.Here's what I tried to do:How to recover the picture after the pca.inverse_transform?
4159,Theil Sen estimator for 3D array in Python,"['python', 'arrays', 'multidimensional-array', 'scikit-learn', 'regression']","I have a 3D grid in which the axis are x (longitude), y (latitude), and years. I want to produce Theil-Sen temporal trends over all grid cells. In the documentation and in the related stackoverflow questions the regression is always being performed over dataframes. Is it possible to do the regression on a 3D numpy array, or do I need to change the structure to a 2D array or pandas dataframe.Documentation Theil-Sen estimator:
https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html"
4160,Scikit-learn's DBSCAN is giving me me memory errors,"['python', 'scikit-learn', 'dbscan']","When I cluster on 200.000 x 4 everything seems to be fine and I get good results, but as soon as I reach out to 500.000 x 4 it flops. The entire table is 9.800.000 x 4 so I'm not even close yet.
Looked online for solutions but haven't been able to find any.
I'm not a coder by any means so the lines that are written are not difficult (nor are they written by myself) but not sure where else to go for an answer to my question."
4161,There is no problem Scaling Numerical method to Binary (Categorical) Values? for Clustering,"['python-3.x', 'scikit-learn', 'cluster-analysis', 'unsupervised-learning', 'data-handling']","I have some data sets numerical and categorical data mixed one
(for instance : age / sex / Math_score / take a Math course in Online ...)As we know that Binary data = (Yes or No) or Categorical Data have to use OneHotencoding or LabelEncodingI wonder that if there is a performance problem(maybe it is) or other problems use standardScaler or other methods for handling numerical data to categorical data."
4162,How to integrate the Random Forest Regresssion Machine Learning Model with an Android Application?,"['pandas', 'numpy', 'scikit-learn']","I have a SciKit Random Forest Regression Model. I wanted to integrate it with an android application i.e., I should get the output of my Machine Learning Model in android application by giving the inputs in android application. Will this be possible?"
4163,sklearn RandomForestRegressor incorrect max_samples,"['python', 'machine-learning', 'scikit-learn']","I had tried to use sklearn RandomForestRegressor as a model. My data input contains 58 data points, and I set the parameter 'max_samples' as default, 'bootstrap' as true, meaning that it will use bootstrap and take 'If None (default), then draw X.shape[0] samples.'. Thus, it seems that it should be take 58 data points when training(could have the same data point since it takes with replacement). However, when I use sklearn.tree.plot_tree to draw the tree, it shows up that, when training a tree only about 34 was used. I am confused. So does it mean that the training neglect the duplication data, or it simply just not taking 58 data points? I think the first explanation makes more sense, but if that is true, what is the point to use bootstrap if the model not going to use the duplication data at all? Can someone explain? Thank you."
4164,'list' object is not callable for checking score accuracy?,"['python', 'scikit-learn', 'jupyter', 'svm', 'confusion-matrix']","I am creating a model using SVM. I wanted to save the classifier model and the parameters that was used into an excel and .json file, which will then be opened to see the best model out of all the .json files.However, I got this error when I tried to run the second part of the code:I didn't put anything that has the word 'list' so it shouldn't have been overridden. What makes the score list uncallable? Thank you."
4165,how can i test my model using different dataset in machine learning,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'training-data']",i m new in machine learning and i am create a one small project using CountVectorizer model. i am split my data to 80% -20%. 80% for training model and 20% for testing model. my model work properly run on 20% test data but can i used to test my model on different data set that is similar to training data set?i am using joblib  for dump and load my model.my question is how i directly test my model using different dataset?
4166,sklearn decision tree random state parameter in spark mllib decision tree,"['apache-spark', 'scikit-learn', 'apache-spark-mllib']","I had a sklearn decision tree and I knew that with different values of random state parameter, the result may change. But now I have to use spark mllib instead of sklearn. Is there any parameters in spark mllib decision tree that work in the same way as sklearn random state ?"
4167,"Using Sklearn, Category Predictions not working on the test data","['python', 'pandas', 'numpy', 'scikit-learn']","Dataset: I created a very simple dataset of ""Supplier"", ""Item description"" columns . This dataset has a list of item descriptions and preferred supplier for that itemRequirement: I would like to write a program that will take an ""Item Description"" and predict the ""Supplier"". To keep it very simple, I just have only 5 Unique supplier-Item Description combinations out of the 950 rows in the .txt fileIssue: The accuracy shows up 1 and confusing matrix shows no false positives. But when I give a new data, the prediction is wrong.Steps DoneSo far, things look okTried prediction on a Item Description= 'Lego, Barbie and other Toy Items' ; I was expecting ""Toys R Us""The prediction was wrong, it came up as ""Office Depot"".Can you please let me know
what I am doing wrong here to get the wrong prediction, new_y_pred for the test item description passed in (new_X)This is my first ML code. I have tried debugging this by looking at various articles, but no luck.Thanks== Complete Code, if it is helpful =="
4168,sklearn's imputer reducing columns?,"['python', 'pandas', 'numpy', 'scikit-learn']",I'm wondering if anyone can help explain a weird behavior I'm seeing with sklearn's interativeImputer.I assume the shape would stay the same but the results are:I found this error when I was converting the numpy array sklearn returns back into a pandas dfAny suggestions of what I can do to keep the original shape when using imputer?
4169,concatenate two CSV files using python,"['python-3.x', 'pandas', 'python-2.7', 'scikit-learn']","i have multiple CSV files that contain various patients features and hour measure that represent the time of recording this measurement as followsi want to concatenate the two files to be as followsi use the following codebut unfortunately it create the file as followsSo please id any one could help me how to fix this problem
any help will be appreciated"
4170,Why best_params_ of SVR (Sklearn) return a float number by degree?,"['python', 'scikit-learn']","I am training a regression model on SVR with Sklearn, and using RandomizedSearchCV return these parameters.Why degree is a float number? according with the documentation, degree is an integer. Also, gamma is just for rbf kernel?my code:"
4171,Possible data leakage or overfitting?,"['python', 'pandas', 'scikit-learn']",I'm working on a multi-class classification problem. The data is of the formMy dataset has 7 features and 42000 rows. Around 22000 of them belong to the class Apple and the rest equally divided amongst others. Now I have fit using the random forest classifier.
4172,Different R-squared scores for different times,"['python-3.x', 'scikit-learn', 'linear-regression', 'cross-validation']","I just learned about cross-validation and when I give in different arguments, there are different results.This is the code for building the Regression Model and the R-squared output was about .5 :Now when I give the cross-validation for X_train, X_test, and X(respectively), it shows different R-squared values.Here's the X_test and y_test arguments:The result:Now when I use the arguments, X_train and y_train, different results are outputted.The result:Now, when I input different arguments again; this time X(which in my case is X_rooms) and y, I yet again get different R-squared values.The output:Which one should I use?I know this is a long question so Thanks!!"
4173,Why is the graph not visible even though the code doesn't show any error?,"['python', 'pandas', 'scikit-learn', 'feature-selection', 'missing-features']","I use this code to calculate feature importance but its graph is not plotted. The code didn't give any error. Please, guide me where I have done a mistake."
4174,Pipeline with and without Target-Based Encoding,"['python', 'machine-learning', 'encoding', 'scikit-learn']","I'm confused about the best way to assemble a pipeline if I'm doing both a simple encoder, and a target-encoder. I've found this example here, which illustrates the problem is related to having to pass the target variable along w/ the variable to be encoded.However, instead of directly doing a fit_transform, I'd like to add it as a part of my pipeline so that I can do it within a cross-fold validation scheme.So, the code that doesn't work is:Any ideas on a better way to patch this all together?Edit:The problem lies in passing X into gs.fit(). As is, the code above says: ValueError: A given column is not a column of the dataframeIf I try to get clever and send 'y' along in X, then it tells me ValueError: cannot reindex from a duplicate axis"
4175,What's a good R-squared score? [closed],"['python', 'scikit-learn', 'regression', 'linear-regression', 'train-test-split']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 18 days ago.I ran this Linear Regression code and I got the R-squared score using the .score() method. However, the score is not easily understandable as the score can go into negative numbers. The code can be run on your local file system if sklearn is installed.Code:Thanks!"
4176,How to get test predictions after cross validation,"['python', 'scikit-learn']",My code-I have split my code in this manner-Now I do cross-validation to see the performance of my trained model-This will give me the accuracy of the trained set. How do I evaluate my learnt model on my test dataset now? SInce cross_val_score is not returning an model object?
4177,cross-validation keras -clarification about validation split in model.fit,"['keras', 'scikit-learn', 'cross-validation']","I need some clarification regarding the validation_set when carrying out cross-validation using sci-kit learn on a keras model.I am carrying out a 10x cross_val on my data (code below) and my question specifically is, when running model.fit, do i need to specify a validation split of the TRAIN data or of the TEST data? -- My understanding would be a val_split of the test data, as each different fold will iterate over different combinations of these.The model being trained is a multi-label multi-class classifier.I have early stopping and model checkpoints as i want to save the best performing model too.code:first question: kfold.split takes care of the split and no other splitting prior to that is necessary?
second question: the validation_data argument in model.fit, should be the test data from the for train, test in kfold.split(normCountsscale1, trainingtarget):correct?or should the train data (inputs[train]) have its own validation split i.e validation_split=0.2 argument in model.fit and then test data separately evaluated?Reason for confusion, I have seen training without any validation in cross validation examples, and training where the validation split is specifically the test set.thanks!!"
4178,LogisticRegression predict_probe returns weird probabilities,"['python', 'scikit-learn', 'logistic-regression']","I'm using using Logistic Regression (after standardization) with strong regularization (C=0.01) and Loo
in order to classify this dataset:These are the predict_proba results:Surprisingly (y_pred[:,1] < 0.58) == True) gives me 27/28 i.e. 96% accuracy!
So I'm getting opposite probabilities!Any explanation?This is my code:clf.classes_ is array([0., 1.])"
4179,Matching sequence of two dataframes with similar string parttern keeping index and sequence,"['python', 'pandas', 'scikit-learn']","I have two dataframes df and df1. where I have to match sequence or strings and getting the only matching string sequence with index number of df as output.dfdf1output:I tried with several methods pd.merge, pd.concat, pd.join, also with isin, but, I get the wrong index number.e.g.,"
4180,"Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f9d95dd50f0>, as the constructor either does not set","['keras', 'scikit-learn', 'grid-search', 'gridsearchcv']","I get this runtimeError: Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f9d95dd50f0>, as the constructor either does not set or modifies parameter batch_size
when executing gridsearch in deep learning Jason Bownlee."
4181,How to define strata in StratifiedKfold?,"['python', 'scikit-learn', 'k-fold']","I'm trying to set the columns used for strata in stratifiedkfold but failed to find how to do so. In train_test_split, we can use stratify = (whatever column/subset used to stratify) but it's inconvenient to run train_test_split for 5 / 10 times. Anyone have any idea to do so?"
4182,How to evaluate accuracy of Isolation Forest,"['python', 'scikit-learn', 'unsupervised-learning', 'anomaly-detection']","I'm doing some traffic anomaly detection with the Isolation Forest implementation of Scikit-learn and I need to get some good measures of the accuracy of my model. I'm currently using AUC, AUCPR, Confusion Matrix and classification report. Is there any other measure appropriate for this scenario?
Thanks in advance!"
4183,Using sklearn's mutual_info_classif() to filter features by specific information gain values,"['python', 'scikit-learn', 'classification', 'feature-selection']","I am using sklearn for classification, and currently want to filter a large mixed set of discrete and continuous features by specific range of information gain values (i.e. ""pick only features with information gain ratio between A and B""). Currently, I'm looking at mutual_info_classif() since I read that it gives the equivalent to information gain for this purpose. However, when I tried it using the following code, the resulting score can be greater than 1.Result for colA-colD (with rounding) are 1.29, 0.00, 1.50, 0.58, respectively. Since AFAIK the range of information gain values is between 0 and 1, I wonder if my approach is correct.Beside this, I'd also greatly appreciate suggestions regarding Python libraries for fast computation of information gain."
4184,Error with TfidfVectorizer but ok with CountVectorizer,"['python', 'tensorflow', 'keras', 'scikit-learn']","I have been working on this the whole day but no luckI managed to eliminate the problem in one line of TfidfVectorizerHere is my working codeBut when I change toThe only thing changed is this 2 linesand then I get this errorInvalidArgumentError: indices[1] = [0,997] is out of order. Many sparse ops require sorted indices.
Use tf.sparse.reorder to create a correctly ordered copy.[Op:SerializeManySparse]How to fix that and why it is happening?"
4185,results from PCA and LDA using scikit-learn,"['scikit-learn', 'pca', 'lda', 'linear-discriminant']","I am really confused about this result.. the same dataset for 2 classes and the result from PCA and LDA.. is it reasonable or something may be wrong?
Thanks for any answering!enter image description here"
4186,TypeError: 'GradientBoostingClassifier' object is not callable,"['python', 'scikit-learn', 'shap']",I am not able to plot the features importances with the shap values in my Sklearn GradientBoosting model
4187,Cyclical Loop Between OneHotEncoder and KNNImpute in Scikit-learn,"['python', 'machine-learning', 'scikit-learn', 'preprocessor']","I'm working with a really simple dataset. It has some missing values, both in categorical and numeric features. Because of this, I'm trying to use sklearn.preprocessing.KNNImpute to get the most accurate imputation I can. However, when I run the following code:I get the error: ValueError: could not convert string to float: 'Private'That makes sense, it obviously can't handle categorical data. But when I try to run OneHotEncoder with:It throws the error: ValueError: Input contains NaNI'd prefer to use KNNImpute even with the categorical data as I feel like I'd be losing some accuracy if I just use a ColumnTransform and impute with numeric and categorical data seperately. Is there any way to get OneHotEncoder to ignore these missing values? If not, is using ColumnTransform or a simpler imputer a better way of tackling this problem?Thanks in advance"
4188,DIfferent PCA results between Sklearn and Tensorflow projector,"['python', 'tensorflow', 'scikit-learn', 'pca']","I've a certain data and when I use the Tensorflor projector with default settings, I get the following visual:However, when I use plotly and sklearn to plot the same, I get this:The code I have used for PCA is:Note that my dataset is just a bunch of binary variables. Therefore, I don't believe normalisation is a factor.Why would the TF Projector plot look so much better than mine on the exact same data ?"
4189,Doing transform on test data for selectpercentile,"['python', 'machine-learning', 'scikit-learn']","I have a code-After I have done SelectPercentile(l,percentile).fit_transform(X_train,y_train) I have to do a  transform on the test data also since the number of features will get reduced. How can I avoid the lengthy procedure of first taking the column names and then doing a filter. Is there a sklearn function for this?"
4190,"Expected 2D Array,got 1D array instead.Where's the mistake?","['python', 'scikit-learn', 'svm', 'pca']","I am beginning to learn SVM and PCA.I tried to apply SVM on the Sci-Kit Learn 'load_digits' dataset.When i apply the .fit method to SVC,i get an error:""Expected 2D array, got 1D array instead:
array=[ 1.9142151   0.58897807  1.30203491 ...  1.02259477  1.07605691
-1.25769703].
Reshape your data either using array.reshape(-1, 1) if your data has a single featureor array.reshape(1, -1) if it contains a single sample.""Here is the code i wrote:**Can someone help me out?Thank you in advance."
4191,Is there a way to use multilabel classification but take as correct when the model predicts only one label in keras?,"['python', 'machine-learning', 'keras', 'scikit-learn', 'classification']","I have a dataset of weather forecasts and am trying to make a model that predicts which forecast will be more accurate the next day.In order to do so, my y output is of the form y=[1,0,1,0] because I have the forecasts of 4 different organizations. 1 represents that this is the best forecast for the current record and more 'ones' means that multiple forecasts had the same best prediction.My problem is that I want to create a model that trains on these data but also learns that only predicting 1 value correctly is 100% correct answer as I only need to get as a result one of the best and equal forecasts. I believe that the way I am doing this 'shaves' accuracy from my evaluation. Is there a way to implement this in keras?  The architecture of the neural network is totally experimental and there is no specific reason why I chose it. This is the code I wrote. My train dataset consists of 6463 rows × 505 columns.My accuracy is ~44%"
4192,Get individual models and customized score in GridSearchCV and RandomizedSearchCV [duplicate],"['python', 'scikit-learn', 'cross-validation', 'grid-search', 'gridsearchcv']",GridSearchCV and RandomizedSearchCV has best_estimator_ that :I would like to enrich those limitations withI was thinking of achieving it by :My question is:
4193,How can I use AdaboostClassfier better?,"['python', 'scikit-learn', 'classification', 'adaboost', 'boosting']",I have to solve a multiclass classification problem in python.I started to use ensembles and I started from adaboostclassfier but after a gridsearch I get bad results.What I did is to use the tuned classfier (in the list of classfier that I tried) that shows me the best score as base estimator: a SVC().Then I did gridsearch on the others parameters of AdaBoostClassfier:Now I have 3 questions for you:This is my code:
4194,Using a Pipeline containing ColumnTransformer in SciKit's RFECV,"['python', 'scikit-learn', 'rfe']","I'm trying to do RFECV on the transformed data using SciKit.For that, I create a pipeline and pass the pipeline to the RFECV. It works fine unless I have ColumnTransformer as a pipeline step. It gives me the following error:I have checked the answer for this Question, but I'm not sure if they are applicable here. The code is as follows:Obviously, I can do this transformation step outside the pipeline and then use the transformed X, but I was wondering if there is any workaround.I'd also like to raise this as an RFECV's design issue (it converts X to numpy array first thing, while other approaches with built-in cross-validation e.g. GridSearchCV do not do that)"
4195,How should I solve this DataFrame object is not callable error?,"['python', 'scikit-learn', 'jupyter-notebook', 'anaconda']",I am getting the error:
4196,How to install dask-jobqueue on a HPC,"['python', 'scikit-learn', 'dask', 'hpc', 'job-queue']","I am trying to use dask-jobqueue on a high performing computer (HPC).Following the documentation on dask-jobqueue here, I am trying to install dask-jobqueue on an instance of a Jupyter Notebook in the HPC with the following code:but I am getting the following error:I have attached a screenshot here:
I am not sure what I did wrong and what do I do now? Would really appreciate any help.Many thanks in advance."
4197,How to plot SciKit-Learn linear regression graph,"['python', 'plot', 'scikit-learn', 'linear-regression']",I am new to SciKit-Learn and I have been working on a regression problem (king county csv) on kaggle. I have been training a regression model to predict the price of the house and I wanted to plot the graph but I have no idea how to do so. I am using python 3.6. Any advice or suggestion would be greatly appreciated.
4198,how to use ordinal encoder or hotencoder on numbers that are strings,"['python', 'machine-learning', 'scikit-learn', 'spyder', 'one-hot-encoding']","I have a dataset with a column that contains numbers as strings such as ""one"", ""three"", ""five"", ""five"" and etc. I want to use an ordinal encoder: one will be 0, two will be 1 three will be 3, and so on. How to do it? Also in HotEncoder, I have a sparse option and in ordinal encoder, I don't have this option. Should I need to do sparse here?my code:"
4199,LinAlgError: Singular matrix for finding pvalues in logisticregression,"['python', 'numpy', 'scikit-learn', 'logistic-regression', 'p-value']","I am trying to find the probability of default for credit risk modeling, So I tried to find p-values using this code but I'm getting the below error. Could you please check and suggest me.Output Result:
Here I'm getting the error like the below one, Please check and suggest me"
4200,Class weights worsen my keras classification model,"['python', 'keras', 'scikit-learn', 'deep-learning']","I have a model with that is to classify some data, and it has a target output of 21. It uses the adam optimizer and categorical cross-entropy loss. In an attempt to improve the model loss, I did a visualization of the class frequencies in the data set and found that the top 2 classes have a frequency of about 25,000 and 20,000 while the lowest 2 are about 4, 40. with the other classes ranging from 100, 2000. I realized this is a stark difference in values and attempted to add in-class weights which I extracted using sklearn like so:My y array is in the one-hot encoding style, something like:but my loss worsened and I started getting loss values in the range of 30 - 50, by the 50th epoch. which is horrible when compared to the fact that without the class weights I was getting about 0.4.Is there something wrong with the way I extracted the class weights? or should I not be using class weights entirely?
If not, what should I be using to account for this huge imbalance?
-Thanks"
4201,Sklearn import error: cannot import name check_build,"['python', 'scikit-learn', 'scipy']","I'm trying to import the SGDClassifier library from sklearn and I am getting the following error:I am using python 3.8.3, scikit-learn 0.23.1, scipy 1.5.1, and sklearn 0.0. I reinstalled these packages through pip, but I am still getting the same error."
4202,Recursive RandomizedSearchCV with XGBoost,"['python', 'recursion', 'scikit-learn', 'xgboost', 'hyperparameters']","Does anyone know if it is possible to get a recursive RandomizedSearchCV for an XGBoost model?I think this would be a better option instead of running a GridSearchCV after a simple RandomSearchCV.Maybe I could update the param_distributions of the RandomizedSearchCV, using the hyperparameters of the estimators that have, let's say, the 5 best scores of the output results_cv_, then run it again, and then...But i really don't know how to do it.Any idea?"
4203,Singleton array cannot be considered a valid collection while splitting dataset,"['python', 'scikit-learn']","So i am getting Data from an ES index in a DataFrame. Which has the following columns tags, text and title.And i'm trying to split the Data from this DataFrame using the following Code:but it doesn't work, I get the following errorBut when checking the .shape of texts and tags they're both the same (44908, 1)"
4204,Can sklearn splitter classes return random indexes?,"['scikit-learn', 'cross-validation', 'sklearn-pandas']","I am using Sklearns splitter classes to do K-fold cross validation.  Specifically I am using KFold, StratifiedKFold and GroupKFold.  I have some pandas dataframes that hold my X and y's, so I call the sklearn functions like so:As you can see, I am passing ""dummy data"" to sklearn (np.zeros) because all I thought I cared about was the indexes it would return, so that later, I would use those.  But what appears to happen, is sklearn expects real data, shuffles it, and then returns indexes.  which are always basically in a somewhat sequential order:Basically my train indexes always start at around 0 and with a few exceptions, because they go into test, it may skip some.  What I was hoping is that sklearn would totally randomize and shuffle the ""indexes"", and return to me a randomized index set, that meets the criteria (stratified, group, etc).  Is there a way to do that, so that I can just get the indexes I want from sklearn, or do I have to pass it an array-like object and let it have its way with it?"
4205,ImportError: No module named _multilayer_perceptron,"['flask', 'scikit-learn', 'pickle', 'kaggle']","i was trying to unpickle a model creating and trained on kaggle.com
screen1and i got problems while unpickled my model and i got this error, i tried to reinstall the latest version of SKlearn but whithout any result
screen 2
screen 3"
4206,numpy.int64 TypeError when running a Decision Tree,"['python', 'scikit-learn', 'dataset', 'typeerror', 'decision-tree']","I’ve been trying to implement a Decision Trees Classification example I found online, using the Wine Quality dataset from UCI Machine Learning. I have the general structure of the Tree working, and the output of the code is a pdf of the Decision Tree. However, I’m wondering why I am getting the error: TypeError: can only concatenate str (not numpy.int64) to str? The program was working fine up until the part where I import graphviz, so I’m pretty sure it has to do with that section of code? Thank you"
4207,Simple classification using scikit-learn not working,"['machine-learning', 'scikit-learn', 'classification']","This is the code that I used to solve a classification problem pertaining to credit card fraud detection:For some reason, even if I specify the reshape parameter, the above code is resulting in an error. This is the error:"
4208,Sklearn SVM classifier for sentences / questions,"['python', 'dataframe', 'machine-learning', 'scikit-learn', 'svm']","I'm a beginner in machine learning and testing with some tutorials that I found online https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568. So I want to classify questions with Sklearn and SVM. So there's a level for each question (Bloom's taxonomy). So I want to get the level of the questions when user upload a pdf file. So far I've trained SVM model and did the prediction with test_data. Now I'm having a problem with passing a file / string to predict function in Sklean SVM algorithm.This is a part of my datasetalgorithmI have used this function to extract strings from a pdf filethis is the output of the functionwith sample size [17, 26]And after I pass this output to predict function I get this errorFound input variables with inconsistent numbers of samples: [17, 26]When I pass the X_test data to predict function I get results as expected (Size of the X_test data (26,))How can I pass a string to the predict function ?"
4209,Using predict_proba() instead predict() in Neuraxle Pipeline with OneVsRestClassifier,"['python', 'scikit-learn', 'pipeline', 'neuraxle']","I'm trying to setup a Neuraxle Pipeline that uses sklearns OneVsRestClassifier (OVR).Every valid step in a Neuraxle pipeline has to implement the fit() and transform() methods.In order to use sklearns pipeline steps, Neuraxle uses a SKLearnWrapper that maps the OVRs predict() method to the transform() method of the SKLearnWrapper.Is there a way to modify this behavior so that the predict_proba() method is mapped to the OVRs transform() method instead?Or is there another way of retrieving the calculated probabilities?"
4210,Importing sklearn.family In Python [duplicate],"['python', 'scikit-learn', 'importerror']",I'm currently watching a course on machine learning in Python on Udemy. The presenter uses 'scikit-learn' and instructed that the following command allows you to use a model for making predictions:However when I run it I receive the following error:I tried to manually find where the 'Model' parameter is by searching through:However I couldn't find it. Is the error because the version he is using is outdated or is it something else?
4211,What does “audio_file[6:-16]” means in python,"['python', 'audio', 'scikit-learn', 'librosa']",Here is the code it is code for setting audio files label in python.
4212,"How to find all the True Positives,negatives and False Positives and Negatives considering all the classes","['python', 'machine-learning', 'scikit-learn', 'classification']","I am able to do the same for smaller dataset where I know the desired outputsFor example:
I have the dataset:I am able to write the function for find all the true and false positives and negatives:But what if I intend to find the same for multiple labels,something like this:So I want to find all the TP_total,TN_total,FP_total and FN_total considering each label '1','2','0','3'How can I do that"
4213,Display the prediction for 10 years using polynomial regression on python,"['python', 'scikit-learn', 'regression', 'non-linear-regression']","I built this code using polynomial regression based on the below table (few part of it), and I'm using regression on sklearn from degree 1 until 4 to be able to predict values until 2020.While searched to build it, I got this expression that helps me out to make the prediction for the years lin.predict(poly.fit_transform([[2018]])), but is there a way I can make the prediction until 2020 and plot on the graph with this prediction values?
"
4214,sklearn cross valid / cross predict,"['machine-learning', 'scikit-learn', 'data-science']","I understand that cross_val_predict / cross_val trains n out-of-folds models and then aggragate them to produce the final prediction. This is done on the train phase. Now, I want to use the fitted models to predict the test data. I can use for loop to collect predictions on the test data and aggregate them but first I want to ask if there is a build-in sklearn method for this?"
4215,Nearest Neighbor using customized weights on Python scikit-learn,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'nearest-neighbor']","Good night,I would like to use Nearest Neighbor model for Regression with non-uniform weights. I saw in the User Guide that I can use weights='distance' in the declaration of the model and then the weights would be inverse proportional to the distance, but the results I get were not what I wanted.I saw in the Documentation that I could use a function for the weights (given the distances) used in the prediction, so I have created the follow function:And have declared the method like this:Until that part, everything works fine. But when I tried to predict with the model, I get the error:I did not understand what I did wrong. On the Documentation it is written that my function should has an array of distances as parameter and should return the equivalent weights. What have I done wrong?Thanks in advance."
4216,preserving column names in sklearn preprocessing with normalizer,"['python', 'pandas', 'numpy', 'scikit-learn', 'sklearn-pandas']",I have a pandas dataframe as follows.df has three rows and 4 columns. Now I apply the following preprocessing with normalization.I get an out NumPy array of 3 rows and 3 columns. One column is discarded which contains all nan which is fine.How can I preserve the original column names with this normalization?
4217,How to Encode categorical data into labels for training and testing,"['python', 'pandas', 'machine-learning', 'scikit-learn']",The training dataset has object columns called shops and others. Now for the machine learning model I converted the columns into labels for training purposes. Using the code belowNow the testing dataset also has those categorical columns but with the some columns missing including the target column not relevant here I think. But if I again label the training dataset (unordered) the labels would be different than the one used while training so the model would not work properly . How to solve this problem and get the same encodings while training and testing
4218,Why isn't my subclassed estimator returning parameters?,"['python', 'scikit-learn']","I'm interested in creating a subclass of sklearn's NearestNeighbors to return labels of observations rather than indicies from the training set.  Here is my subclassed estimator NNBR.I can instantiate and use the estimator as I intend to, but when I call get_params(), I am returned an empty dictionaryWhy is my subclass returning an empty dictionary when I use the get_params method?  How can I change my code so that myclf.get_params() returns the same as clf.get_params()?"
4219,Pandas: Combining Actual and Prediction Values,"['python', 'pandas', 'dataframe', 'scikit-learn', 'prediction']","I’m trying to combine the actual target values and predicted target value as a dataframe. However, I’m getting the following error. Not sure why this is happening.Actual Vs Prediction"
4220,Get features for maximum value of Scikit-learn estimator,"['python-3.x', 'machine-learning', 'scikit-learn']","I have the following very simple code trying to model a simple dataset:That gives me the following results:From there, is there a way of computing the values of the features that would give me the maximum label, within the boundaries of the dataset?"
4221,"Random forest, auc for training set and testing set","['python', 'machine-learning', 'scikit-learn', 'random-forest']","I am trying to tune my random forest classifier. When I use GridSearchCV, the best parameters are min_samples_split = 2, min_samples_leaf = 4, max_depth = None, which I think is definitely an overfit. So I plot the auc for both training and testing data:The graph shows that as we increase the max_depth, both train_auc and test_auc will increase; does that graph make sense? Because I think at some point as we increase the depth, the performance of testing data will actually go down."
4222,Is it possible to have a encoder to have a different range / start at a different value?,"['python', 'scikit-learn']","I want to use a encoder for categorical data and I have a process that converts cells below 1 to N/A's. They seem to conflicting because after I convert the category and send it for processing, it turns the 0 of the first item to N/A.Here's a example:the result is: {0: 'cat', 1: 'dog', 2: 'monkey'}This is resulting in cat to be N/A, Is it possible to specify a digit to start from? I'd look for something like this:"
4223,"MemoryError: Unable to allocate MiB for an array with shape and data type, when using anymodel.fit() in sklearn","['python', 'machine-learning', 'scikit-learn']","Getting this memory error. But the book/link I am following doesn't get this error.A part of Code:Error:
MemoryError: Unable to allocate 359. MiB for an array with shape (60000, 784) and data type float64I also get this error when I try to scale the data using StandardScaler's fit_transfromBut works fine in both if I decrease the size of training set (something like : x_train[:1000] ,y_train[:1000])Link for the code in the book here. The error I get is in Line 60 and 63 (In [60] and In [63])The book : Aurélien Géron - Hands-On Machine Learning with Scikit-Learn  Keras  and Tensorflow 2nd Ed (Page : 149 / 1130)Does this has anything to do with my ram? and what does ""Unable to allocate 359"" mean? is it the memory size ?Just in case my specs :
CPU - ryzen 2400g , ram - 8gb (3.1gb is free when using jupyter notebook )"
4224,How can i replace multivariate outliers?,"['python', 'numpy', 'scikit-learn', 'outliers', 'multiclass-classification']","I'm doing multiclass classification in python using scikit learn, pandas and numpy.Is it possibile to replace multivariate outliers?I detected multivariate outliers using different algorithms and then I removed them from my dataset, now what I want to do is to try to replace the outliers and see what is best to do. (what allows me to get a higher score).What i have in mind is to remove the outliers from the dataset, then fit an algorithm on remaining part, transform the outliers via the trained model and then put all together, i tried with the code that follows, but the score i get was terribly low.Probably there are different and better approaches, but I have not found anything on the internet, does anyone know what I can do?It's fine even if you have completely different approaches, I have no more ideas."
4225,Gaussian Process Confidence intervals plotting using python,"['python', 'scikit-learn', 'regression', 'confidence-interval', 'gaussian-process']","I used scikit-learn Gaussian process regression function to make a prediction which works very well. However, when I am trying to calculate confidence intervals and plot it along with estimated value, I am getting the following error: ValueError: Input passed into argument ""'y1'""is not 1-dimensional.Full error displayI don't understand this error and where I made a mistake. Could you please help me with this. Thanks in advance.these are the sample of input and output data that I got from GP library.These above data types are 'Array of floats 64'The code is given as follows,alpha values are calculated by hyperparameter optimisation."
4226,How to check the training accuracy in K fold validation [duplicate],"['tensorflow', 'machine-learning', 'keras', 'scikit-learn', 'classification']","I am using k fold validation in my model in which user scans the historical places and our app will show all the details of it. The technologies i am using are SIFT and MLP. However, I am unable to find the training accuracy in k fold and also one more thing that i am unable to use tensor board in it , because i found on internet that it'll be used in model.fit( ) function but in K fold I am not using Code is shown below ."
4227,XGBoost do not take into account parameters,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'xgboost']","I am using XGBoostClassifier in an imbalanced scenario, so I want to include the sample_weight parameter into the fit() function. I am using Python and the scikit-learn API.The problem is that it seems that XGBoost does not take into account the weight parameter. In addition, even if I say n_jobs=-1, the training is not parallelized at all (only 1 CPU is used).This is the code I am using:The w_yes values is 25 and w_no is 0.25.Even if the weights are quite different (the original dataset is really imbalanced), it seems to produce no effects, such as the n_jobs parameter.Any idea? Do I have to use directly the XGBoost syntax, without the sklearn API?"
4228,Big difference in score (10%) between a split_test_train and a cross validation,"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'train-test-split']","I'm on a classification issue with:
2,500 lines.
25000 columns
88 different classes unevenly distributedAnd then something very strange happened:When I run a dozen different split test trains, I always get scores around 60%...And when I run cross validations, I always get scores around 50%.
Here the screen : 
Moreover it has nothing to do with the unequal distribution of classes because when I put a stratify=y on the TTS I stay around 60% and when I put a StratifiedKFold I stay around 50%.Which score to remember? Why the difference? For me a CV was just a succession of test train splits with different splits from each other, so nothing justifies such a difference in score."
4229,Is it okay to build a model on imbalanced data?,"['machine-learning', 'scikit-learn', 'imbalanced-data']","Background -
The dataset I am working on is highly imbalanced and the number of classes is 543. The data is bounded by date. After exploring the data over a span of 5 years I came to know the imbalance is inherent and its persistent. The test data which the model will get will also be bounded by a date range and it will also have a similar imbalance.The reason for the imbalance in the data is different amount of spend, popularity of a product. Handling imbalance would do injustice to the business.Questions -
In such a case, is it okay to proceed with building model on imbalanced data?The model would be retrained every month on the new data and it would be used for predictions once in a month."
4230,Different F1 scores for different preprocessing techniques- sklearn,"['python', 'scikit-learn', 'statistics']","I am building a classification model using sklearn's GradientBoostingClassifier. For the same model, I tried different preprocessing techniques: StandarScaler, Scale, and Normalizer on the same data but I am getting different f1_scores each time. For StandardScaler, it is highest and lowest for Normalizer. Why is it so? Is there any other technique for which I can get an even higher score?"
4231,How do i generate SAS Proc Logistic output in python?,"['python', 'pyspark', 'scikit-learn', 'sas', 'statsmodels']","I have done enough google search to find a way to generate SAS proc logistic output in PYTHON specially below tables(Effect table and Parameter) but couldn't find. is there any way to generate below tables.
i would really appreciate if anyone can help me out with this problem."
4232,Extract specific information from pandas data frame,"['python-3.x', 'pandas', 'scikit-learn', 'pandas-groupby']","My data frame looks like -My final data frame looks like -logic should be capture all the information which exist in between <p class=""MsoNormal""> and </p>. But not include this string ""&nbsp"". I want to do it in pythons."
4233,Jupyter lab is not printing all parameters form sklearn model completely,"['python', 'scikit-learn', 'jupyter-lab']",I've been using jupyter lab for several month and every time I run a sklearn model the output is like this:Which in the past showed this output:But now it only shows the params that I have actually set:Anyone knows how to make Jupyter show the complete list of params again?Thanks!
4234,How to encode data with multiple class labels?,"['python', 'machine-learning', 'encoding', 'scikit-learn', 'one-hot-encoding']","I have a classification problem with multiple classes, say A, B, C and D. My data has the following y labels:I want to train a Random Forest classifier on these labels. First I need to encode the labels. I first tried LabelEncoder:I also tried OneHotEncoder, but obviously, neither LabelEncoder nor OneHotEncoder would work here. The thing is that I cannot encode data with multiple class labels (e.g. ['A','B','C']). I guess these trivial encoding methods are not the way to go here, so what is the best way to encode my class labels? To clarify, I don't want to treat e.g. ['A','B'] as a completely different class from ['A'] or ['B']. I want it to be a different class but at the same time still inherit features from both A and B classes."
4235,ModuleNotFoundError: No module named 'scipy.io.matlab',"['python', 'scikit-learn', 'scipy', 'python-import', 'importerror']","I've seen so many questions about this error online, but even after reading through all of them, I have no idea why I am still getting this error. I have spicy installed, and I am completely lost as to what to do. All I'm trying to do is get an example working that I found online, but this ModuleError keeps appearing.Here are the imports below:Traceback (most recent call last):Any suggestions would be appreciated so much!! I am using Mac OS, and I have 3.7 python installed through Anaconda."
4236,"sk learn, what to do when the data I want to predict has different distribution with data I have right now","['python', 'scikit-learn', 'random-forest', 'imbalanced-data']","To be specific, I am now working with a data with 100,000 rows and 20 features, my target variable is categorical so I use random forest classifier, Xgboost, LogisticRegression, etc.
I have a binary feature 'A', which in my dataframe only 20% of them are 1. But my future data will all come with feature 'A' == 1. If I train my model with RFC, the importance of feature A is not very important. If I split my train/test set randomly, the AUC of my test set is 0.8, but if I use a subset of my test data with only 'A' == 1, the AUC drop to 0.72.
Any one know what I should do in this situation?
I don't think I should drop all data with 'A' == 0"
4237,How to weight classes using fit_generator() in Keras? [closed],"['python', 'keras']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed last year.I am trying to use keras to fit a CNN model to classify images. The data set has much more images form certain classes, so its unbalanced. I read different thing on how to weight the loss to account for this in Keras, e.g.: 
https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras, which is nicely explained. But, its always explaining for the fit() function, not the fit_generator() one. Indeed, in the fit_generator() function we dont have the 'class_weights' parameter, but instead we have 'weighted_metrics', which I dont understand its description: ""weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.""How can I pass from 'class_weights' to 'weighted_metrics'? Would any one have a simple example?"
4238,StratifiedKFold failing to produce 9 distinct folds - Python 3.7,"['python', 'scikit-learn', 'split']","I am struggling to properly utilize sklearn's StratifiedKFold code.I have an extremely large dataset (X), and subsequent list of classes (y), that is imbalanced. I am looking to break that up into 9 stratified folds.However, The results are not what I am expecting. I am essentially appending the entire dataset each time, and creating 9 folds of the entire dataset. What is quirky, is that I am not looking to get a train and test split for each fold, I just want a stratified split of my data. (i.e., take my data / 9 by maintain the class imbalance).Yields:I have looked at various resources, and cannot figure out how to properly accomplish what I am doing. I am looking for something that effectively creates something to the effect of 9 distinct folds, no overlapping data, with dimensions of approximately 26,774 rows, with each class maintaining its split (about 21,818 of class 0 and 4956 of class 1)UPDATE
I tried using StratifiedShuffleSplit but get the same problem. Each fold is all of the data, not 1/9 of the data."
4239,Detect and Remove Outliers as step of a Pipeline,"['python', 'scikit-learn', 'regression', 'classification']","I have a problem, I'm trying to build my own class to put into a pipeline in python, but it doesn't work.The problem I am trying to solve is a multiclass classification problem.What I want to do this to add a step in the pipeline to detect and remove outliers.
I found this detect and remove outliers in pipeline python which is very similar to what I did.
This is my class:But i get this error in fit_transform return self.fit(X, y, **fit_params).transform(X) TypeError: transform() missing 1 required positional argument: 'y'The following code is the code i use to call this class:"
4240,how to return the features that used in decision tree that created by DecisionTreeClassifier in sklearn,"['scikit-learn', 'decision-tree', 'feature-selection']","i want to do feature selection on my data set by CART and C4.5 decision tree. In such a way that apply decision tree on data set and then extract the features that decision tree algorithm use to create the tree. so i need return the features that use in the created tree. i use ""DecisionTreeClassifier"" in  sklearn.tree module. i need a method or function to give me (return) the features that used in created tree!! to use this features as more important features in main modulation algorithm."
4241,How to install non-negative restriction using dictionary params on sklearn decomposition method?,"['python', 'python-3.x', 'dictionary', 'scikit-learn', 'parameters']","I am using sklearn.decomposition.FactorAnalysis and would like to include some parameters that have real-world meaning, as in none of my Factors can be negative. I have come to understand that you would use the dictionary method to set parameters, but I'm not familiar with how to code this correctly. The code I have is below.Essentially, I just can't figure out how to code params to have the solutions be non-negative.
Thanks for any help or recommendations!"
4242,difference in the calculation of accuracy between keras and scikit-learn,"['machine-learning', 'keras', 'scikit-learn', 'cnn', 'multilabel-classification']","I am currently working on multi-label image classification using the CNN in keras.
In addition to the accuracy of keras, we have also reconfirmed the accuracy of scikit-learn using various evaluation methods (recall, precision, F1 score and accuracy).We found that the accuracy calculated by keras shows about 90%, while scikit-learn shows only about 60%.I do not know why this is happening, so please let me know.Is there something wrong with the keras calculation?We use sigmoid for the activation function, binary_crossentropy for the loss function, and adam for the optimizer.Keras trainingKeras showed 90% (Accuracy).scikit-learn checkOutput(scikit-learn)"
4243,What is the best way in python to create a confusion matrix out of an already aggregated table with counts of actual labels and predicted labels,"['python', 'pandas', 'machine-learning', 'scikit-learn']",What is the best / fastest way to create a confusion matrix in python if the data is available in the following format:
4244,How do you override Google AI platform's standard library's (i.e upgrade scikit-learn) and install other libraries for custom prediction routines?,"['google-cloud-platform', 'scikit-learn', 'gcp-ai-platform-notebook']","I'm currently building a pipeline and trying to see if I can get an ML model deployed in AI platform's prediction service, then use it later on in other projects via the HTTP request that the prediction service offers.However the model that is being used was built using an scikit-learn library that is of a higher version than offered for the prediction runtime version 1.15 (this is the current version supported by google for scikit-learn predictions). This runtime version only supports scikit-learn version 0.20.4 and my model requires 0.23.1. As far as I know, everything else in the custom prediction routine works as intended, but the error received when loading the model () is only ever encountered when the scikit-learn version is older than the model needs.So, I need a way to force the prediction routine to use a particular version of scikit-learn via a pip install or some equivalent - in the past I have done this in Google Dataflow via custom installs in the setup.py file but have yet to succeed achieving this in AI platform custom prediction routines. I assume it can be done?non-working 'setup.py'"
4245,Difference of PLSSVD function from svd in python sklearn,"['python', 'numpy', 'scikit-learn', 'svd']","The descrption of PLSSVD in skearn is:Simply perform a svd on the crosscovariance matrix: X’Y.However, when I compare its result with np.linalg.svd(), I got different results, which confused me. Here is my code:The transformed results X_c, Y_c are different when using PLSSVD or np.linalg.svd, could you tell me what make this differences? Many thanks!"
4246,Improve speed of scikit-learn multinomial logistic regression,"['python', 'scikit-learn']","i am trying to train a logistic regression model in scikit learn and it is taking very long to train, around 2 hours. The size of the dataset is 21613 x 19. I am new to scikit learn, as such i dont know whether my code is wrong or that it just takes very long to train. Any suggestion on how to improve the training speed would be very much appreciated!code used to train is below"
4247,How to deal with SettingWithCopyWarning in Pandas?,"['python', 'pandas', 'dataframe', 'chained-assignment']","I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:I want to know what exactly it means?  Do I need to change something?How should I suspend the warning if I insist to use quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE?"
4248,How to convert a spark ml model to a normal python model (e.g. sklearn model)?,"['python', 'apache-spark', 'scikit-learn']","Because data is huge I can't train that model on my PC , but this model should deploy as a webservice .I have no idea how to convert a spark ml model to sklearn model . I have google this but nothing useful finding ."
4249,How to get the predict probability in Machine Leaning,"['python', 'numpy', 'machine-learning', 'scikit-learn']","I have this ML model trained and dumped so I can use it anywhere. And I need to get not just the score, predict values, but also I need predict_proba value as well.I could get that but the problem is, I was expecting the probabilities to be between 0 and 1, but I get something else like below.And this is the python code I am using.outpu produces by the loaded_model.predict_proba(inputs)How can I convert these values or get an output like a percentage? (eg: 12%, 50%, 96%)"
4250,Combined GridSearch RFECV: fold interaction / data channeling?,"['python', 'scikit-learn', 'gridsearchcv']","I am currently applying a very similar case as in the code example below for the combination of GridSearch CV (similarly RandomizedSearchCV), which works quite well. I am applying in both cases a KFold-CV with shuffle=False to apply a defined grouping. However, I am uncertain about two details on the CV-Folds:1.) When the data from the GridSearchCV is split into the folds, is then only each split data channeled to the RFECV and then the train-set split again accordingly, or how does this work? E.g. ff GS has cv=5, then 4/5 of data is channeled to RFECV and then if cv=5 split again, so that train-set-size in each fold in RFECV will be 4/5*4/5 of the original data size?2.) The split ratio might be also performance influencing? How to know about the best cv-number each and the ratio?The following code section is from this link:
Combining Recursive Feature Elimination and Grid Search in scikit-learn"
4251,SVR hyperparameter selection and visualisation,"['scikit-learn', 'data-visualization', 'svm', 'data-analysis', 'grid-search']","I am just a beginner in data analysis. I want to use 'Cross-validation Grid Search method"" to determine the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM. I don't know where I should put my data on this code, and what data type I should use (training or target data)?For SVRfor cross-validation grid search methods(best parameters):For visualization of parameter effectsWhen I used this code, I found the following output Heatmap plot!
But I am trying to get a Heatmap like this one
"
4252,Why I am getting a high RMSE and R2 score in polynomial regression after data scaling?,"['python', 'scikit-learn', 'regression']","I am trying to do multiple and polynomial linear regression on the ""mpg"" data. Polynomial regression after standardizing the data gives me a very abnormal RMSE and R2 values on test data. While these values for training data is in trend with multiple linear regression outcome. It seems my model is overfitting?Can someone highlight where I am doing wrong?Can I do stasndardizing only those columns where numeric values varies a lot instead of standardizing all columns in X_train? By doing this can I improve my model performance?I tried polynomial regression without standardizing the data and the results are comparable with what i am getting in multiple linear regression."
4253,Size of y is not altered inside of a pipeline when putting it in cross validation,"['scikit-learn', 'cross-validation', 'feature-selection']","I've created the following pipeline for a classifier with the particularity that I use y_train as parameter in the feature_select step. Here is the pipeline structure:For your info, here is what I do in 'feature_select' step with Feature_Selector class:Everything goes fine by putting this pipeline in a bigger one with the estimator:However, if I try to include the full_pipeline in cross_val_score:I get the following error:I've discovered that y_train maintains its origin size inside feature_select step for every fold in cross_val, since I get the following print tracks:As you can see the problem is clear but I have no clue about the solution. How could I do to make the y_train parameter in Feature_Selector creation corresponds to the y generated in cross val process?Thanks in advance!"
4254,plot clusters of kmeans of sparse matrix,"['python', 'matplotlib', 'scikit-learn']","I have a python script which do clustering over a data file which is in svmlight format.
I use the function sklearn.datasets.load_svmlight_file to load the data from the data file.
I know that this function returns a sparse matrix.
I need to scatter plot the clusters, can any body help me please.
This what I have done:"
4255,Optimizing Looping Linear Regression in Python,"['python', 'scikit-learn', 'linear-regression']","I am trying to improve the efficiency of my code, which involves looping the following to find the difference between individual points and a linear regression curve a few hundred times with around 20000 entries for each loop. Each iteration takes more than a second, and along with data processing and copying it over for each iteration takes around ten minutes for the entire program to run, I'm wondering if I'm doing anything inherently wrong that impacts efficiency."
4256,Syntax error in python machine learning sklearn code mosh Python course. Id appreciate if someone could hep me,"['python', 'machine-learning', 'scikit-learn', 'syntax']","I am doing a Python course with Mosh programming and am getting an error with some machine learning codeThe code is this ->I am getting an error that says ->I'm new to these libraries and am able to resolve the issue
Id grateful if you could help me."
4257,How to Use SimpleImputer on a 1D Array?,"['python', 'machine-learning', 'scikit-learn', 'imputation']","I've got a dataset with multiple NaN values in my Dependent Variable column. I've split the set into Dependent and Independent Variables, and I'm currently trying to replace all NaN values in my Dependent Variable column with 0's. However, I'm getting an error when using SimpleImputer for this purpose.Here's my code:Here's the error I get:"
4258,Error: can't multiply sequence by non-int of type 'float',"['python-3.x', 'machine-learning', 'scikit-learn', 'linear-regression', 'sklearn-pandas']","what am i missing here? I've tried fixing it a few times different ways but end up at the same error. Somehow my ""train_x"" and ""train_y"" aren't being converted to arrays so I can multiply them together to create a line.(STACKTRACE)"
4259,Custom Scaler function doubling length (rows) of pandas dataframe in python 3,"['python-3.x', 'pandas', 'scikit-learn']","I am not sure if this is a pandas specific question but I am pretty sure it has something to do with the cat function in this code. I am relatively new to coding so my experience is pretty limited.I am trying to create a class that will allow me to select the columns of the data that I want to scale. The custom function is based on the Standard Scaler model from skLearn. Whenever I try and split my data into training and test sets, I get an error that says the lengths of the dataframes do not match. When I check the size of the dataframes before the scaling they are the same length, however, after they are scaled the inputs are double the length.I am not sure how to include the dataset since it is from a csv file, but it has an index that is a unique ID for each record and four columns:The dataset has about 6000 records in it.Any help would be greatly appreciated.Here is the error that I get:..............in 
train_inputs, test_inputs, train_targets, test_targets = train_test_split(scaled_inputs, targets, train_size = 0.8,
random_state = 42)   File
""/home/sid/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py"",
line 2118, in train_test_split
arrays = indexable(*arrays)   File ""/home/sid/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py"",
line 248, in indexable
check_consistent_length(*result)   File ""/home/sid/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py"",
line 212, in check_consistent_length
"" samples: %r"" % [int(l) for l in lengths]) ValueError: Found input variables with inconsistent numbers of samples: [12302, 6151]"
4260,Feed a scikit classifier with Tensorflow dataset,"['tensorflow', 'scikit-learn']","Is there any simple way to connect a Tensorflow dataset to a scikit classifier (such as SVM), such that the dataset records are read automatically during training by scikit fit function?"
4261,Import error on Google colab after pip install and restarting runtime,"['python', 'import', 'scikit-learn', 'pip', 'google-colaboratory']","I want to use a package (resreg) on Google Colab. I install it in my first cellI get the following output:Then I restart the runtime using
"">Runtime >Restart Runtime""When I try to import the package, I get the following errorWhat could be wrong?"
4262,Get count of matching word in string of pandas column with a predefined list,"['python', 'pandas', 'dataframe', 'scikit-learn', 'dataset']","I have a DataFrame contains index and text columns.For example:Now I have a long list, and I want to match each of the words in text with the list.Let's say:I would want to create a FunctionTransformer to match words in the long_list with each word of the column value, if there is a match, return the count.I did in this way:An example of how I develop my other FunctionTransformer will be:"
4263,Logistic regression estimator always predicts 1 and no 0,"['scikit-learn', 'regression', 'classification', 'logistic-regression']","Hi I have the following problem:When conducting a logistic regression from sklearn.linear_model.LogisticRegression, my log_reg model predicts only 1 (churn = purchase made) and no 0 (churn = no purchase made). Since my data is imbalanced i tried to over-sample the data.Does someone have any tips?"
4264,LabelEncoder with ordered encoding,"['python', 'python-3.x', 'pandas', 'scikit-learn']",I am using label encoderoutputWhich is fine but if I have lot of data than the sequence gets mashed up something like thisAny solution without label encoder which makes sure the sequence is maintained
4265,Does PCA implementation in sklearn have an intercept?,"['python', 'scikit-learn', 'pca']","I'm using the PCA implementation from sklearn and wanted to export the loadings from the fitted model so I could transform anywhere else without using python.However, I came up with an issue when I tried to validate that the dot product of the dataset with the loadings didn't give the same result as the transform function.
Here's an example:I import PCA from sklearn and fit the model with one componentWith the fitted model, I transform the 3 columns dataset into one columnAccording to the sklearn docs I can access de loading in the components_ attribute. When I transform the dataset using the loadings I get a different output.However, the difference between bot output seems to be constantI was taught that PCA doesn't have intercept, but does this mean that the PCA implementation in sklearn includes the intercept? If so, is there a way to access this intercept without calling the difference between the transform funcion and the dot product of the data with the loadings?Note: I know data normalization solves the problem of the intercept but I can't use it in this situation."
4266,Why RandomizedSearchCV reeturn a value of degree or gamma for linear estimator?,"['python', 'scikit-learn']","With Sklearn I am using RandomizedSearchCV, in a specific case the best estimator is:But according to sklearn documentation, degree and gamma is just for rbf and poly kernels.
Why I get linar estimator with gamma and degree values?"
4267,"The target is binary, but I get “ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.”","['python', 'python-3.x', 'pandas', 'machine-learning', 'scikit-learn']","I'm facing an issue in a simple ML model using sklearn KFoldI categorize my target value using the following code:Now, if I open the csv, I can see an added column (length_over, the 18th column, counting from 0) with the binarized variable made by the binarization of the column length. Then, i save the dataset as a new file, and split it to test-validation subsets, using the following code:However, before proceding with models evaluation and comparison, I get the error: Out: ""ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.""I also checked the type of target usingAnd the result is unknownWhat could be the issue? The target is binary when I open the csv, but the function get it as unknown...dtype is int64"
4268,How to evaluate Pytorch model using metrics like precision and recall?,"['python', 'tensorflow', 'scikit-learn', 'metrics', 'tensor']","I have trained a simple Pytorch neural network on some data, and now wish to test and evaluate it using metrics like accuracy, recall, f1 and precision. I searched the Pytorch documentation thoroughly and could not find any classes or functions for these metrics. I then tried converting the predicted labels and the actual labels to numpy arrays and using scikit-learn's metrics, but the predicted labels don't seem to be either 0 or 1 (my labels), but instead continuous values. Because of this scikit-learn metrics don't work.
Fast.ai documentation didn't make much sense either, I could not understand which class to inherit for precision etc (although I was able to calculate accuracy). Any help would be much desperately appreciated."
4269,scikit learn train_test_split() behaving splitting data unexpectedly,"['python', 'scikit-learn', 'python-3.6', 'python-3.7', 'train-test-split']","I'm facing this issue where sklearn's train_test_split() is dividing data sets abruptly in case of large data sets. I'm trying to load the entire data set of 118 MB, and it is assigning test data less than 10 times of what is expected of code.Case 1:  60K datapointsOutput:
(40200, 8) (40200,)
(19800, 8) (19800,)Case 2:109,000 data-pointsOutput:
(109248, 9)
(90552, 8) (90552,)
(1460, 8) (1460,)Anything more than 60K data-points is being abruptly like in case 2 into 90K and  1.4K. I've tried changing random state, removing random state,moving data set to new location but the issue seems same."
4270,why I get in Z1 2 columns instead of 3 and how to fix it using hotEncoder,"['python', 'numpy', 'machine-learning', 'scikit-learn', 'one-hot-encoding']","I'm using hotEncoder for a column with 5 values witch gave me  5 columns (for Z). That's OK
now I have another column with has 3 values but I got 2 columns instead of 3 in Z1
what I need to do in the code to fix that I'll get 3 columns in Z1?also, I would like the explanation for the hotEncoder code. Why I have to use np.hstack here?
Thank you very much!!"
4271,K-means Clustering: How to determine which variables influence a cluster?,"['scikit-learn', 'k-means', 'unsupervised-learning']","I am performing a cluster analysis on 86 different variables, which I managed to reduce to 19  PCs using PCA. Using sk-learn's K-means clustering algorithm, I got 10 clusters. However, I can't figure out which variables are responsible for separating these clusters. How do I determine which the variables that are responsible for a certain cluster."
4272,Create StratifiedK-Folds from a Pandas DataFrame,"['python-3.x', 'pandas', 'scikit-learn']",Let's say that I have a PandasDataFrame df like so:Values are random.
4273,how predict no more than target example?,"['python', 'machine-learning', 'scikit-learn']","I have variable, and I need to predict its value as close as possible, but not greater than it. For example, given y_true = 9000, I want y_pred to be any value  within range [0,9000] as close to 9000 as possible. And if y_true = 8000 respectively y_pred should be [0,8000]. That is, I want to make some kind of restriction on the predicted value. That threshold is individual for each pair of prediction and target variable from the sample. if y_true = [8750,9200,8900,7600] that y_pred should be [<=8750,<=9200,<=8900,<=7600]. The only task is to predict exactly no more and get closer. everywhere zero is considered the correct answer, but I just need to get as close as possible"
4274,Why does scikit-learn silhouette_score return an error for 1 cluster?,"['python', 'scikit-learn']","One cluster (K = 1) is a possible valid, best fit, for different values of K in K-means clustering.
""silhouette_score"" in scikit-learn (v 0.23.1) does not seem to work with one cluster and gives an unexpected error.Here is the code to reproduce:The correct value of silhouette score for 1 cluster should be zero according to this.Am I doing something wrong here?"
4275,"Found array with 0 feature(s) (shape= (2794,0)) while a minimum of 1 is required by check_pair_arrays","['python', 'scikit-learn', 'pca', 'grid-search']","I am trying to get the right hyperparameters for my Kernel PCA. I have a dataset of (4191,193) dimensions.
However, when I run the following code, I get the error stating my array has no features.I have tried debugging but the program enters into sklearn\utils\validation.py file then runs in a loop and stops answering to my debug commands. I hope you guys can help me.X.shape returns (4191,193)
y.shape returns (4191,)The error code I am receiving :"
4276,Why should we use the fit_transform method when we get the same output using transform,"['python', 'machine-learning', 'scikit-learn', 'data-science']","I don't understand why one has to use the fit_transform method when the transform method can give the same the output as using only fit transform method, whats the whole point of fit method?I have printed the x_train and x_test, both of them gave similar output."
4277,Tensorflow implementation of NT_Xent contrastive loss function?,"['python', 'tensorflow', 'scikit-learn', 'backpropagation', 'cosine-similarity']","As the title suggests, I'm trying train a model based on the SimCLR framework (seen in this paper: https://arxiv.org/pdf/2002.05709.pdf - the NT_Xent loss is stated in equation (1) and Algorithm 1).I have managed to create a numpy version of the loss function, but this is not suitable to train the model on, as numpy arrays cannot store the required information for back propagation. I am having difficulty converting my numpy code over to Tensorflow. Here is my numpy version:I am fairly confident that this function produces the correct results (albeit slowly, as I have seen other implementations of it online that were vectorised versions - such as this one for Pytorch: https://github.com/Spijkervet/SimCLR/blob/master/modules/nt_xent.py (my code produces the same result for identical inputs), but I do not see how their version is mathematically equivalent to the formula in the paper, hence why I am trying to build my own).As a first try I have converted the numpy functions to their TF equivalents (tf.concat, tf.reshape, tf.math.exp, tf.range, etc.), but I believe my only/main problem is that sklearn's cosine_similarity function returns a numpy array, and I do not know how to build this function myself in Tensorflow. Any ideas?"
4278,pipe.fit and cross_val_score throws “IndexError” error when applying the OneHotEncoder and CountVectorizer,"['python', 'numpy', 'scikit-learn', 'data-science']","My first attempt in trying out ML. I have an input data where the features are Item Description and Criteria (what is more important - Quality or Price); The Dependent variable (y) is the Seller.Requirement: Given an item and Criteria (quality or price), identify the sellerpython version: 3.7.6""CRITERIA"" column is a category type, so I tried applying the OneHotEncoder for that feature. I applied NLP for the Item description feature. I want to determine the seller based on the Item Description and the Criteria.I am getting an error when doing cross_val_score or pipe.fit
""IndexError: only integers, slices (:), ellipsis (...), numpy.newaxis (None) and integer or boolean arrays are valid indices""If there are some recommendations on how to address this requirement, if the below code is not correct/inefficient, please let me know."
4279,Precision and recall are the same within a model,"['python', 'machine-learning', 'scikit-learn', 'precision', 'precision-recall']","I am working on a multiclassification project and I noticed that no matter what classifier I run the precision and recall are the same within a model.The classification problem has three distinct classes. The volume of the data is rather on the small side with 13k instances divided into test (0.8) and train (0.2).Training data has a shape of (10608, 28) and labels are the shape of (10608, 3) (binarized label).The classification is imbalanced:I am comparing different classifiers, to later focus on the most promising ones. While calculating precision and recall for each model I noticed that they are always the same within a model.Due to how precision and recall are calculated they can be the same when the number of false-negative predictions equals the number of false-positive predictions  FP = FN.Examples:How likely is it that all the models have the same recall and precision within a model? Am I missing something?"
4280,sklearn Voting Ensemble with Pre-Trained Regressor models,"['python', 'scikit-learn', 'regression', 'ensemble-learning']","Can someone tell how to use ensembles voting in sklearn using pre-trained regressors.
I don't want to retrain my models.Thank you."
4281,Sklearn.linear_model.Lasso returns coefficients that are both +0 and -0?,"['machine-learning', 'scikit-learn', 'lasso-regression']","After training an ML model using https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso, when I print out the coefficients, I can see that some of them give 0.00000000e+00 while others are -0.00000000e+00.Is there a reason for this difference? At the end of the day, I am assuming they are both coefficient=0. (?)"
4282,How to one hot encode multiple columns in CSV file using sklearn?,"['python', 'machine-learning', 'scikit-learn', 'data-science']","I have the following code for One hot encoding one column in my CSV file.Now I want to one hot encode more than one columns in that CSV file by using only index number and not name of that column, how can I do that?"
4283,Python sklearn PCA keep lowest variance vectors,"['python', 'scikit-learn', 'pca', 'variance']",I want to use the sklearn pca function to keep the N vectors with the lowest variance rather than highest. I tried to implement this in numpy but it was much slower than sklearns regular PCA. Is it possible ot achieve this with sklearn pca?
4284,Feature union multiple encoders within sklearn pipeline,['scikit-learn'],"I am a data that contains categorical features along with some numeric features. I am using various type of encoders to convert categories into numeric for further analysis. I would like to use the pipeline and concat the result of embeddings together.Here is an exampleI have a label encoderand a target encoderand I have a pipeline similar towhich basically incorporate only one of the encoders. However, I would like to use all the encoders and concatinate the result vertically - but also have the numeric data transformed.therefore, I made a pipeline similar toand I fit the pipeline and it looks likebut when I do transform on the first stepthe output still has the same number of columns ! which is not what I expect.and here is the pipe.steps[0][1]so in sum, what I want is a pipeline to incorporate all categorical embedding, along with the numerical without duplicates or missing columns."
4285,'ValueError: could not convert string to float' when trying to implement a dataset for Random Forest algorithm,"['python', 'scikit-learn', 'dataset', 'data-mining', 'random-forest']","I’m having issues with applying Random Forest algorithm code to a dataset about tennis players at Wimbledon in 2013. I wanted to use the code I found on this website: https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/, but the website doesn’t address how to adjust the code for a dataset with string values. I get the error ‘ValueError: could not convert string to float’ because the names of the players are string variables. Is there a way to solve this issue for both columns that contain lists of names?Here is the code:And here is a screenshot of the dataset:
"
4286,Parameters of sklearn class methods,"['python', 'scikit-learn']","I have 2 small questions. So, after browsing a lot of the documentation of sklearn, I have noticed that many sklearn preprocessing classes such as standard scaler have a .transform method which takes the X values and scales them. However , to ensure that this can be performed in a pipeline, the method also ensures that the y values are taken as input too (albeit ignored if need be)So, question 1: the documentation of these methods looks like this:fit_transform(self, X[, y]).Why is the y in brackets like that with a preceding comma. Is it to signify that this value is not necessary as an input?Question 2: If my thoughts on question one are correct, the fit_transform documentation of SelectKBest looks like this:fit_transform(self, X[, y])In this case, how can y be an optional input, if there is no way to select K features without the y values as targets?"
4287,pipeline and cross validation in python using scikit learn,"['python', 'scikit-learn', 'pipeline', 'cross-validation', 'polynomials']","I had a general doubt for Cross Validation.In the notebook for module 2 it is mentioned that one should use pipelines for Cross Validation in order to prevent data leakage. I understand why , however had a doubt regarding the pipeline function:If I want to use three functions in a pipeline : MinMaxScaler(), PolynomialFeatures(for multiple degrees) and A Ridge in the end(for multiple alpha values). Since I want to find the best model after using multiple param values , I will use the GridSearchCV() function which does cross validation and gives the best model score.However after I intialise a pipeline object with the three functions and insert it in the GridSearchCV() function , how do I insert the multiple degrees and aplha values in the params parameter of the GridSearchCV() function . Do I insert the params as a list of two lists in the order of which the functions have been defined in the pipeline object or do I send a dictionary of two lists, where the keys are the object names of the functions in the pipeline ?????"
4288,How to remove frequent/infrequent features from Sklearn CountVectorizer?,"['pandas', 'scikit-learn']","Is it possible to remove a percentage of features that occur most frequently / infrequently, from the CountVectorizer?So basically organize the features from a greatest to least occurrence distribution and just remove a percentage from the left or right side?"
4289,GridSearchCV fails for own model class,"['python-3.x', 'scikit-learn', 'gridsearchcv']","I'm trying to use a regression model I have implemented in combination with the GridSearchCV class of scikit-learn to optimize the hyper-parameters of my model. My modelclass is nicely build following the suggestions of the scikit-api:The regression-class works as it should, but strangely enough, when I study the behavior of the hyperparameters, I tend to get inconsistencies. It appears one hyper-parameter is correctly applied by GridSearchCV, but the other one is clearly not.So I am wondering, can someone explain to me how gridsearchCV is working (from the technical perspective)? How does it initialise the estimator, how does it run it over the grid?My current assumption of the workings and required use of GridsearchCV is this:However, experience shows me this naive idea is quite wrong, as the hyper-parameter associated with the kernel-function of my regressor is not correctly initialized.
Can anyone provide some insight into what GridSearchCV is truly doing?"
4290,"Unable to find Python libraries like matplotlib,SciKit, Pandas, etc for Solaris 11 compatible","['python', 'pandas', 'numpy', 'scikit-learn', 'solaris']","We have Solaris 11 OS installed with default Python 2.7 along with it.
We are unable to install other packages like Pandas,SciKit,NumPy,etc as many of them are not compatible with Solaris.
Is there any documentation for installation available for adding packages on solaris?
Any reference links would be helpful."
4291,TypeError: argument must be a string or number on column with strings that are numbers,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'one-hot-encoding']","I have a dataset with categories. In column 4 I have 2 values( two and four which are strings). Do you know why I get the error and how to fix it?TypeError: argument must be a string or numberDuring handling of the above exception, another exception occurred:Code:Thank you for the help!"
4292,Measuring memory usage of scikit-learn changes model performance,"['python', 'machine-learning', 'memory', 'scikit-learn', 'memory-profiling']","I am trying to measure memory usage of my models in scikit-learn using memory-profiler python module. However, when I profile memory during training, the model shows different accuracy as when I train it without profiling. (With profiling, the performance is much worse)
The code how I measure it:Does anyone have experience with this? What could be the cause?"
4293,Is there a native 'MinMaxScaler' in tensorflow?,"['tensorflow', 'machine-learning', 'keras', 'scikit-learn']",I need to normalize my data with something similar to sklearn.MinMaxScaler but I need to use native TensorFlow ONLY and to apply it to TensorFlow Dataset API.How can it be done?
4294,Kernel is dead and restarting automatically while running a cell for a training a regression model,"['python', 'pandas', 'scikit-learn', 'jupyter-notebook', 'regression']","Iam doing the Bulldozer price calculation problem, using RandomForestRegressor.After removing all the missing values and converting all data into numeric, I try to fit and train the data into a model. The data set is pretty large about 412698 rows × 57 columns and using a 3gb Ram device.here is my codeThe data set is available in Kaggle and I am also attaching its link..
https://www.kaggle.com/c/bluebook-for-bulldozers/data"
4295,Is it possible to reuse a pipeline component in multiple GridSearch?,"['python', 'scikit-learn', 'grid-search', 'imbalanced-data']","I'm using scikit-learn and imbalanced-learn libraries. Suppose that I have the following pipelines:Then, I'm using both pipelines in a GridSearchCV to test out several hyperparameters, both for SMOTE and the model.In reality I have more than two pipelines, and as I'm currently running the GridSearch, I thought, as SMOTE is repeated during these pipelines, is it possible to configure GridSearchCV in a way so that the SMOTE is only done one time, then passed to both DecisionTreeClassifier and XGBClassifier? Therefore the process doesn't have to do SMOTE multiple times, but only once and then passed to the different models."
4296,Plotting an implicit function on top of scatter plots (decision boundary in logistic regression),"['python', 'matplotlib', 'plot', 'scikit-learn', 'logistic-regression']","I am doing a logistic regression to separate data into two parts in Python. There are 28 features that are derived from 2 original features which were then used to derive others up to 6th powers between them (e.g. x_0^1x_1^5, x_0^6 etc.) The problem is, unlike when the boundary is a line, I could not find how to plot a non-linear boundary on top of scatter plots.My attempt was to solve the boundary equation at each x using scipy.optimize, but the result was highly unsatisfactory:The boundary is missing the top side; maybe I can change the initial value, but this is indeed an inelegant solution. I have also tried using sympy, but I was not able to overlap the scatter plots on top of it. Is there any way to achieve this? I do not mind using other packages if necessary.Another question is, how can this be achieved if I have used sklearn.linear_model instead? I know how to retrieve the coefficients, but I am still not sure about drawing the boundary with the original scatter plot."
4297,Trying to fit_transform on make_column_transformer object,"['python-3.x', 'machine-learning', 'scikit-learn', 'transformation']","I am trying to use sklearn.compose.make_column_transformer and then fit_transform the object on my training & test data. The error I receive isTypeError: All estimators should implement fit and transform, or can be 'drop' or 'passthrough' specifiers. '['inq_refer', 'lead_age']' (type <class 'list'>) doesn't.my code"
4298,"ValueError: operands could not be broadcast together with shapes, but it is just a standardization","['python', 'machine-learning', 'scikit-learn', 'sklearn-pandas', 'valueerror']","I am trying to use my model to predict a new feature with the following numbers:array1=np.array([[5.4629,0.21,1646732.8000,490188.7170,3521510.1690,96.8,27.70,118.30]])array1=scaler.transform(array1)model.predict(array1) ### this one could not be processed yet due to the error from previous command line:ValueError: operands could not be broadcast together with shapes (1,8) (9,) (1,8)The problem is: I am just making a scaler transform using sklearn MinMaxScaler, with what dimentions should match if it is just a standardization? I've reading differents sources and they are always making operations between 2 arrays, so it makes sense at least one dimention match, but in this case I could not understand what it is going on...
Can anyone help me?thanks in advance"
4299,How to use OneHotEncoder encoding over multiple columns?,"['python', 'pandas', 'scikit-learn']","I have a DataFrame of shape (85,78) each row correspondin to a particular area and with values as different restaurant categories like 'Bakery','Cafe','BBQ Joint'etc.Total unique categories = 175.I need to calculate top categories per each area and to do this i want to encode the data set.
But OneHotEncoding gives me a total of 949 rows instead of 175 unique rows.It seems like it is doing fit_transform for each column.pd.get_dummies is doing the same by adding suffix (0,1,2..etc) to each category if repeated along other columns.I want the data frame to have just 175 columns so that I can calculate their sum according to each area."
4300,Suport Vector Machine training :Is sklearn SGDClassifier.partial_fit able to train an SVM incrementally?,"['python', 'machine-learning', 'scikit-learn', 'classification', 'svm']","I am trying to train an SVM model through sklearn to apply as binary classifier to get audio's Ideal Binary Mask(IBM), applied after a neural network that I am developing for my graduation thesis, however, as shown in
!this graph, the accuracy never converges. The mean accuracy is always about 50% it doesn't matter how many audios are used, which is random considering we've got only two choices.As far as I know, SGDClassifier.partial_fit changes the weights in small batches, what would allow us to use different files as batches (since each audio contains thousands of samples for classifications. Is it right?Thanks a lot!"
4301,Pickle Lazy Learners,"['python', 'machine-learning', 'scikit-learn', 'pickle', 'knn']","Does Pickle save training data for lazy learners like KNeighboursClassifier form sci-kit ? If so, can we access this data from a pickle object ? (Asking for data privacy issues)Eg:Does knn_from_pickle or saved_model variables contain Xtrain data ? , Since Knn is a lazy learner and requires distance calculations, when new data arrives, with respect to training data (i.e Xtrain). When I printed knn_from_pickle I was just displayed hyperparameters passed to KNeighboursClassifier algorithm.As I observed that for a 65KB file of data (Xtrain) with all data transformations and taking this entire data for training, when the knn model was fit and serialized like so:space occupied was 238744 bytesWhereas space occupied for pickled objects of other algorithms like Gaussian Naive Bayes was:space occupied was 6074  bytes and for heavy algorithms like Random Forest:space occupied was 48863 bytesSeeing this much space difference between KNN and other algorithm's pickled objects, pickle must be storing training data somehow for KNN. If yes how to access it or how is knn stored in pickle, if no then how is unpickled object (knn_from_pickle) using predict without fit and giving correct answer ?"
4302,Getting an error while training a logistic regression model,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'logistic-regression']","I am trying to fit a logistic regression model to a dataset, and while training the data, I am getting the following error :The code snippet is as follows:The dataset looks like :I looked over the web but couldn't find any relevant solutions.Please help me with this error.
Thank you!"
4303,Is there a way to convert a single LightGBM decision tree (decision rules) to Python code (conditional statements)?,"['python', 'scikit-learn', 'decision-tree', 'lightgbm']",I am trying to convert a single LightGBM decision tree i.e. num_boost_round = 1 with num_leaves = 16 to Python conditional statements. Is there a way to do this? I found a post on stackoverflow about doing this with the sklearn decision tree implementation but that does not directly apply to my case.
4304,i want know Accuracy and confusion matrix for my custom object detection dataset. getting ValueError: could not convert string to float: '889.jpg',"['scikit-learn', 'computer-vision', 'google-colaboratory', 'confusion-matrix', 'faster-rcnn']","what is the problem with this line LR.fit(X_train,y_train).
is that any problem with csv file?"
4305,Passing the tf-idf vocabulary to the following step of the Pipeline,"['python', 'machine-learning', 'scikit-learn', 'pipeline', 'tfidfvectorizer']","I have created a sklearn Pipeline that looks like this:The problem is that the object second_transformer(TransformerMixin) is a custom sklearn Transformer that I have created and which needs the vocabulary of the fitted tfidf_vectorizer to apply the required transformation.
Is there a way to pass the tfidf vocabulary from tfidf_vectorizer to second_transformer when I call pipeline.transform() ? Or at least save the tf-idf vocabulary whenever I run pipeline.fit() ?Thank you very much.I'm using python 3.6"
4306,TypeError: invalid type promotion while fitting a logistic regression model in Scikit-learn,"['python', 'pandas', 'scikit-learn']","I am building a logistic Regression model using Sci-kit Learn. My data is made up mainly of float and int types except for the date column which is of datetime64[ns](Its type was first object, then I converted it usingI did split my data to train and test and when trying to fit the model using logr.fit(X,Y) I get the following error:I am not able to understand what this error is pointing to. However from research, I found that it could be related to the date type but found nothing in the error that points out to date in particular.
Any idea?"
4307,Predict with Ensemble KNN,"['python', 'scikit-learn', 'knn', 'weighted-average', 'ensemble-learning']","I'm newbie in python programming. I want to ask how to use ensemble Weighted mean with k-nearest neighbor in python. I want to combine k from k-nearest neighbor (ex. k = 2,3,5,7,9) using weighted mean to make final model prediction..I'm using sklearn now to predict k value in knn. but I don't know how to ensemble the using Weighted mean"
4308,Automatically find an optimal threshold for multiple datasets,"['python', 'scikit-learn']","I am running an unsupervised model on data with no labels to find outliers, and have 100 datasets in total. The way I do this is, first I run it using a default threshold on each dataset, and second, I change the threshold until I get the desired output (desired- find a maximum of 10 outliers). I want to make this process automatic where the threshold/contamination will be tuned automatically in each dataset, and in the end, I will have 100 datasets with their outlier predictions.An example for one dataset"
4309,Kmeans clustering changes for each training,"['python', 'scikit-learn', 'k-means']","I'm using sklearn Kmeans algorithm for grouping in 4 clusters multiple observations and I have included init_state and seed for obtaining always the same results; but each time that I reload the code in google colab and each time I'm running the training I obtain different results in terms of number of observations in each cluster, here the code:How I can obtain always the same results (in terms of the number of observation in each cluster)?Thank you in advance"
4310,Gaussian Process regression hyparameter optimisation using python Grid search,"['python', 'numpy', 'scikit-learn', 'hyperparameters', 'gaussian-process']","I am started learning Gaussian regression using Sklearn library using my own data points as given below. though I got the result it is inaccurate because I did not do hyperparameter optimisation. I did some couple of google search and written gridsearchcode. But the code is not running as expected.  I don't know where I made my mistake, please help and thanks in advance.The sample of input and output data is given as followsX_tr, y_tr and X_te are the training data points and are reshape values and have a type of 'Array of float64'Here my grid search codeHere is a sample of my code without hyperparameter optimisation:"
4311,Installed scikit-learn doesn't work properly,"['python', 'python-3.x', 'scikit-learn', 'pip', 'sklearn-pandas']","I am getting this error when I run the following code:ImportError: cannot import name '__check_build' from partially initialized module 'sklearn' (most likely due to a circular import).When I check pip freeze scikit-learn is installed.
Also,I tried to uninstall and reinstall sklearn,now I am getting a different error:"
4312,Confidence score for machine learning with SciKit Learn?,"['python', 'machine-learning', 'scikit-learn', 'face-recognition']","I have followed an example of applying SciKit Learning's machine learning to facial recognition.
https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-pyI have been able to adapt the example to my own data successfully.      However, I am lost on one point:after preparing the data, training the model, ultimately, you end up with the line:
Y_pred = clf.predict(X_test_pca)This produces a vector of predictions, one per face.
What I can't figure out is how to get any confidence measurement to correspond with that.The classification method is a forced choice, so that each face passed in MUST be classified as one of the known faces, even if it isn't even close.How can I get a number per face that will reflect how well the result matches the known face?"
4313,"scikit-learn Classifier / Neural Network, How can I set different types of data as input with different dimensions?","['scikit-learn', 'neural-network', 'dataset', 'classification', 'sklearn-pandas']","I have a dataset, where each row represents a process.
A process has an angle, a method used and a table of temperatures in time, finally it has a binary output.https://gyazo.com/2516b581a4355ebb205a5af9e41b9a0cI want to use as inputs of a classifier / neural network in scikit-learn, the angle, method and time/temperature table assigned to the process to obtain the output Y = Outhttps://gyazo.com/28a1a41467724e8e630e6b2fc7008810How can I enter different types of data as input into a scikit-learn classifier / neural network and with different dimensions?If the time/temperature column did not exist, it would be something like this.However, with the time/temperature column, would it be this way?Thank you very much in advance"
4314,how to embed pickle files in a sing executable file using pyinstaller,"['python', 'machine-learning', 'scikit-learn', 'pyinstaller', 'pickle']",I am working on a tester for a chatbot project I traind a bunch of classifiers and dumpped them using sklearn.externals.joblib() function into pickle formatNow I am using pyinstaller in windows 10 to distribute an single excutable file for but the problem is that I should distribute all .pkl files that I am using in the same folder with the excutable file.My question is : is there any method to embed all .pkl files into a single .exe file?
4315,how the kernel works in kernel ridge regression model that used in sklearn library,"['python', 'machine-learning', 'scikit-learn', 'regression']","Can anyone please explain about the internal working of fit() , predict() and _get_kernel() functions in sklearn.kernel_ridge.KernelRidge. In terms of alpha, weight and kernel matrix (K).What will be the shape of the kernel matrix while in training and testing?How the shape of testing data kernel matrix becomes the same as that of the shape of the training?"
4316,How do I properly fit a sci-kit learn model using a pandas dataframe?,"['python', 'pandas', 'machine-learning', 'scikit-learn']","I am trying to create a machine learning program in sci-kit learn. I am using a CSV file to store data, and have decided to use Pandas data frame to import and format this data. I cannot figure out how to fit this data frame with the model.My CSV file has one feature, age, and one target, weight. I am using a linear regression algorithm to predict the weight using the age. I do realize this isn't the best algorithm to use with this data.When I run this code I get the error ""ValueError: Found input variables with inconsistent numbers of samples: [10, 40]""Here is my code:The first 5 lines of my CSV file:"
4317,To use large pkl file for real time use case in production server,"['python-3.x', 'machine-learning', 'scikit-learn', 'large-files']","I have generated pkl file from training using sklearn library and size is around 3.3 GB which will be used for real time use case in our production server, I know to load this pkl using joblib in python but problem it takes too much time, Is there any other way to load pkl file for our real time use case?Note: From sklearn, algorithm used are CountVectorizer, tf=idf and MultinomialNB"
4318,How to choose Strategy in SimpleImputer,"['machine-learning', 'scikit-learn']","How to choose strategy ( mean, median, most_frequent, constant) in a SimpleImputer?
What exactly ""constant"" strategy does ?"
4319,Is there a way to apply OneHot encoded SciPy Sparse Matrix to PCA?,"['python', 'scikit-learn', 'scipy', 'pca']","I have data that has both numeric and cathegoric attributes that I'm trying to apply the PCA analysis. For the cathegoric ones I OneHot encoded them using sklearn.preprocessing OneHotEncoder, but when I apply the matrix to sklearn.decomposition PCA it gives me the following error:TypeError: PCA does not support sparse input. See TruncatedSVD for a possible alternative.I want to use the PCA because my data has also numeric atributes besides the categoric that I'm trying to OneHot encode. I could just either convert the SciPy sparse matrix to the dense NumPy array and append it to my df (I don't know if it'd provide decent results as I don't have much knowledge in statistics) but I wanted to know if there's a way to apply the sparse matrix directly to PCA in case I run into a bigger set of data.Further information:I'm using the ""Horse Colic Dataset"".You can download it here: http://networkrepository.com/horse-colic.phpThe datadict may be obtained here: https://archive.ics.uci.edu/ml/datasets/Horse+Colic"
4320,Incorrect x axis on Matplotlib when doing polynomial linear regression,"['python', 'matplotlib', 'scikit-learn']","The following code results in an x axis that ranges from 8 to 18. The data for the x axis actually ranges from 1,000 to 50 million. I would expect a log scale to show (10,000), (100,000), (1,000,000) (10,000,000) etc.How do i fix the x axis?Plot:"
4321,Feature selection when independent variables are categorical and also target variable is categorical,"['python', 'pandas', 'machine-learning', 'scikit-learn', 'categorical-data']","I have presented a small sample of the dataset that I am working on. My original dataset has around 400 columns for 'Symptoms' and 1 column for 'Disease'. From here the output expected is to find out the top 'N' maybe 10 or some number of 'Symptoms' which are most significant for a particular disease.
My sample dataset is as follows:I have tried using sklearn's SelectKBest but cannot comprehend the results. Also want to know if panda's dataframe.corr function can work in this case"
4322,How to tune hyperparameters over a hyperparameter space using Bayesian Optimization (in Python)?,"['python', 'scikit-learn', 'hyperparameters', 'hyperopt']","I am trying to tune hyperparameters using bayesian optimization for random forest regression over a hyperparameter space using the code below, but I get an error that saysTypeError: init() got an unexpected keyword argument 'min_samples'I got this error when I tried the following code:I have also tried listing the hyperparameters in the objective function using the code below, but I get the following errorTypeError: objective() missing 3 required positional arguments: 'min_samples', 'max_features', and 'max_samples'Can you please advise on what I can do to fix my code?I was able to tune a single hyperparameter using the code below:"
4323,Why does GridSearchCV in Sklearn not choose the best R-squared value?,"['python', 'scikit-learn']","I am trying to tune the parameter for a lasso regression model in Sklearn, but I'm finding that the GridSearchCV does not seem to be choosing the best R-squared parameter. I find that when I test with other parameters for lambda, they have higher R-squared than GridSearchCV returns. Here is my code:This returns a best alpha of 1e4, with R-squared equal to zero. However, when I probe the Lasso model manually, I get a different result:This returns an R-squared value of 0.39 (as well as pretty much any alpha smaller than this). Why would this model not have been chosen by GridSearchCV?"
4324,Converting predictions into categorical from pd.get_dummies,"['python', 'pandas', 'machine-learning', 'scikit-learn']","I have a train_df[y] column like-I converted it into one-hot encoding using-I got-Then I ran a model on it to generate predictions on the train data usingy_pred-Now I want this y_pred to be in a categorical format again.
Example-"
4325,How to convert custom pipeline (categorical get_dummies) with convert_coreml?,"['python', 'scikit-learn', 'onnx-coreml']","I'm trying to save a custom sklearn pipeline as onnx model, but I'm getting errors in the process.sample code:The simple conversion goes well:But the pipeline conversion fails:with the following error:Am I missing something with the customized pipeline and the get_dummies?"
4326,2D local maxima and minima in Python,"['python', 'scikit-learn', 'scipy-optimize', 'scipy.ndimage']","I have a data frame, df, representing a correlation matrix, with this heatmap with example extrema. Every point has, obviously, (x,y,value):
I am looking into getting the local extrema. I looked into argrelextrema, I tried it on individual rows and the results were as expected, but that didn't work for 2D. I have also looked into scipy.signal.find_peaks, but this is for a 1D array.Is there anything in Python that will return the local extrema over/under certain values(threshold)?
Something like an array of (x, y, value)? If not then can you point me in the right direction?"
4327,How do I resolve the “RFECV object has no support_ attribute” Attribute error?,"['python', 'machine-learning', 'scikit-learn', 'sklearn-pandas', 'rfe']","I am trying to pass the sklearn RFECV object and cross validate the scores to return the model performance with the chosen features and feature rankings.However, I get the ""RFECV object has no support_ attribute"" error most likely because I am not fitting it to the data. I need some help in identifying where to fit the data and how to make sure there is no data leakage to the test data set.The original dataset is a timeseries data, so I've split using TimeSeries Split.This code is derived from the RFE tutorial here"
4328,How to use MultinomialNB with MultiOutputClassifier and partial_fit?,"['python', 'scikit-learn']","I could not find the answer to this question anywhere nor an example in scikit learn documentation for my particular case.I want to use MultinomialNB with MultiOutputClassifier and partial_fitI can't figure out the format of the class parameter of partial_fit function  (which is not required for fit() function, that works perfectly)What am I missing here ?Thanks for your helpValueError: Expected array-like (array or non-string sequence), got '1-2'"
4329,How to do feature selection when both the independent variables as well as the target variable are categorical,"['python', 'pandas', 'scikit-learn', 'categorical-data', 'feature-selection']","I have presented a sample of the dataset that I am working on. My original dataset has around 400 columns for 'Symptoms' and 1 column for 'Disease'. From here the output expected is to find out the top 'N' maybe 10 or some number of 'Symptoms' which are most significant for a particular disease.
My sample dataset is as follows:I have tried using sklearn's SelectKBest but cannot comprehend the results. Also want to know if panda's dataframe.corr function can work in this caseCode till now:which outputs something like:which I cannot comprehend. Can someone please help me with a solution or approach for this."
4330,High Score in Train Test Split but Low Score in CV in Python Scikit-Learn,"['python', 'scikit-learn', 'virtual-machine', 'random-forest', 'cross-validation']","I am new in Data Science and have struggled in the problem for the Kaggle's problem. When I use random forest regression for predicting the rating, it is found high Score using Train Test Split but Low Score while using CV Score.https://www.kaggle.com/data13/machine-learning-model-to-predict-app-rating-94"
4331,pytest: Reusing tests of the parent class (which comes from a library) for my project's child class,"['python', 'scikit-learn', 'pytest']","I'm extending Pipeline class of scikit-learn with my custom Pipeline, note the similar name.This custom Pipeline should support all the functionalities of parent Pipeline, plus some more. So it should also pass all the tests written for sklearn/Pipeline, plus some more.My initial attempt was to write the test file like this:Hoping that the child my_project.pipeline will override the sklearn/Pipeline imported in sklearn.tests.test_pipeline, so tests will actually run using child Pipeline. That didn't work.Is there any way to do this in pytest other than copy-pasting testing code from the library into my project?EDIT: apparently this works, but feels a bit hacky. I'd love to have a more ""mainstream"" solution."
4332,Evaluating Active Learning (SGD) Error Graph,"['machine-learning', 'scikit-learn', 'sgd']","I have produced a error graph from using active learning with sgd classifer.See image below.After the 50th query the training error tends to diverge in the first image.
The second image just shows up to the 50th query.Is it safe to assume the model is good at the 50th query and not consider under-fitting ?"
4333,ValueError: Incompatible dimension for X and Y matrices: X.shape[1] == 300 while Y.shape[1] == 100,"['python', 'scikit-learn', 'chatbot', 'chatterbot']","I am working on Telegram chatbot  using chatterbot package.When I am running my chatbot,sometime it goes very well by responding to my question but after 2-3 question it give me below error :When I back tracked the issue, it is failing at this point in my code :I am not able to get the issue.However sometime is worked well without any error but most of the time i get this error."
4334,Error in using Gaussian Process regression in sklearn python,"['python', 'numpy', 'machine-learning', 'scikit-learn', 'gaussian-process']","I am started learning python and trying to implement Gaussian regression using Sklearn library. I tried to follow the examples available here for my own data points. However, I am getting the following example when I am trying to run y_pred, std = model.predict(X_te, return_std=True) this line of code of my problem. The error I got 'XA and XB must have the same number of columns (i.e. feature dimension.)'.I don't know where I made my mistake, please help and thanks in advance.The sample of input and output data is given as followsX_tr, y_tr and X_te are the training data points and are reshape values and have a type of 'Array of float64'Here is a sample of my code:"
4335,Masking targets while training a regressor,"['python', 'machine-learning', 'scikit-learn']","I am building a ML system (with random forests) to predict coordinates (X, Y) of particular objects in an image. The image can contain N or less objects. Since the number of targets should be fixed, they are always N. If an image has less than N objects, I pad those extra empty places with -1, in order to get the same length of targets.What I want to do is to mask those padded values, so that they are not taken into account during the training (similar to what we do in neural networks). How can I do that with scikit-learn?Cheers,"
4336,Confusion about the data location when applying Scikit-learn on cluster (Dask),"['scikit-learn', 'dask', 'slurm', 'dask-dataframe']","I'm currently working on implementing machine learning (Scikit-Learn) from a single machine to a Slurm cluster via dask. According to some tutorials (e.g. https://examples.dask.org/machine-learning/scale-scikit-learn.html), it's quite simple by using job_lib.parallel_backend('dask'). However, the location of the read in data confuses me and none of the tutorials mention it. Should I use dask.dataframe to read in data to make sure it is passed to the cluster or it doesn't matter if I just read in it using pd.dataframe (then the data is stored in the RAM of which machine I run the Jupiter notebook)?Thank you very much."
4337,Saving the preprocessing steps in the end model,"['python', 'machine-learning', 'scikit-learn', 'pickle']","I'm trying to save my text classification model as a pickle file. I have a specific set of preprocessing steps that I wanted to save in my end model to apply it on unseen data for prediction. Currently I tried using a sklearn pipeline which includes preprocessing, applying count vectorizer and applying the algorithm. My question is if this is a right way to save the preprocessing steps in the end model or should I save it as a seperate file. Below is my codeI have a method for preprocessing which I invoke it and I've included this in my sklearn pipeline"
4338,dtype error when fitting Sklearn Pipeline with a TargetEncoder followed by an XGBoost classifier,"['python', 'pandas', 'scikit-learn', 'xgboost']","I am having the following error when trying to fit a pipeline that uses a XGBoost classifier as its final steps:DataFrame.dtypes for data must be int, float or bool.
Did not expect the data types in fields [Categorical Columns here].I am using the following pipeline, with a TargetEncoder to encode the categorical columns:The problem is that, apparently, the TargetEncoder sets object to the data types of the encoded categorical columns. An so, XGBoost throws the error.So, how could I set the data type to, for example, float before it is used by the XGBClassifier object?"
4339,Speed of sklearn.utils._fast_dict.pyx,"['python', 'numpy', 'scikit-learn', 'cython', 'cythonize']","I am trying to build and use sklearn.utils._fast_dict myself but got a speed problem.
First I copied the two files: _fast_dict.pyx and _fast_dict.pxd and rename them as sklearn_fast_dict.pyx and sklearn_fast_dict.pxd.
Then I run setup.py to build sklearn_fast_dict.cp36-win_amd64.pydAnd here are my test codes:Which shows that the python dict is faster than the IntFloatDict
Is there anywhere I did wrong in the building and usage?
Thanks!UPDATE:
I tried to add a function get_values into a similar class Int32Dict(the only difference between the Int32Dict and IntFloatDict is the datatype of keys and values). The function:And normal get_values operation on normal python dict:The get_values function is about three times faster compared with with normal get_values function."
4340,How to add node to a Ball Tree in Scikit-Learn,"['python', 'data-structures', 'scikit-learn']","The sklearn documentation for ball tree does not provide methods to add a new point to prexisting tree.Is this possible for the sklearn implementation of the Tree? If so, how?"
4341,Does cross_val_score not fit the actual input model?,"['python', 'machine-learning', 'scikit-learn', 'svm', 'k-fold']","I am working on a project in which I am dealing with a large dataset.I need to train the SVM classifier within the KFold cross-validation library from Sklearn.I am using the above-mentioned code.
I understand that I am training my classifier within the function of cross_val_score and hence whenever I am trying to call the classifier outside for the prediction on test data, I am getting an error:Is there any other option of doing the same thing in the correct way?Please help me with this issue."
4342,How to split items within one cell in a certain column and implement one-hot encoding?,"['python', 'pandas', 'scikit-learn']","I have a df, one of the column looks like this:How can I split each item in each cell so that I can implement one-hot encoding?I tried:"
4343,Increasing inertia k-means,"['python', 'scikit-learn', 'k-means']","I am trying to perform k-means, with scikit-learn in Python, on the eigenvectors of the Laplacian matrix of my dataset. The thing is, when I attempt to find the optimal number of clusters with the elbow method, I have an increasing inertia as you can see on the figure. Is it normal ?Thank you."
4344,Display feature names in columns after using One Hot encoding,"['pandas', 'numpy', 'scikit-learn', 'numpy-ndarray', 'one-hot-encoding']","I have one column in a csv which are the names of fruits which I want to convert into an array.Sample csv column:There are around 400 fruit names in the columnI have used one hot encoding for the same but unable to display the column names(each fruit name from a row of the csv column)My code till now is:This converts the data of the column into a numpy array but the columns names are coming as [0   1   2   3 .. ..] which I want as each row name of the csv, example [Apple     Banana    Pear    Watermelon .. .. ]How can I retain the column names after using one hot encoding"
4345,Significance of Number of Calls and Reset Call in Ball Tree,"['data-structures', 'scikit-learn', 'tree']",Why does the Scikit Implementation has functions to reset and get number of calls?How are these parameter important in Trees?https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree.reset_n_calls
4346,Python: 'StandardScaler' object has no attribute '_validate_data',"['python', 'pandas', 'scikit-learn', 'standardization']","I recently updated my sklearn. However, since the upgrade I'm getting the error ""'StandardScaler' object has no attribute '_validate_data'"". The following is a snippet of the code:"
4347,Why Do I keep getting an import error when trying initiate the KNeighbors Classifier?,"['python', 'scikit-learn']",I am writing a simple Machine Learning model of the famous iris dataset on my Jupyter notebook but everytime i try to use the KNeighbors Classifier from the neighbors module i keep getting an error ofKNeighborsClassifier' object is not callablehere is my code
4348,Python sklearn cosine-similarity loop for all records,"['python', 'python-3.x', 'pandas', 'scikit-learn', 'cosine-similarity']","i have dataframe named df.
I'm using code below to get the cosine similarity for each row:but the output DataFrame shows the same result for each records where I assume that it refers to the last record:I want the output for all records"
4349,How to Cross Validate SVM with Custom Kernel Function in sklearn,"['python', 'scikit-learn', 'svm', 'categorical-data']","Everyone.Recently, I have got an assignment about the Support Vector Machine. So I decided to do some Custom Kernel Function following the example on sklearn https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py. As you can see from the code below my Custom Kernel function (K_1st) contained 2 additional parameters than the example in sklearn (alpha and gamma)Then I tried to feed this kernel into the model by the Gram matrix with the kernel = ""precomputed"" arguments. I try to fine-tune this model. I try to approach the problem by creating many Gram matrix and use GridSearchCV to loop through the C parameter.But I stumble into the unknown error which I don`t know how to search or do anything :(((. Can anyone help me!I`m run this on Google Colab with no extra setting."
4350,cross_validate and ValueError: The first argument to `Layer.call` must always be passed,"['tensorflow', 'keras', 'scikit-learn']",I am trying to use cross_validate. I had initial hiccup due to pickable and was to get past that. Still I am not able to get the cross_validate to work.Git Link: https://github.com/Neetu162/DeepLearningResearch/blob/76675a79a4922b8bd0d722ab2e4cad448a8d8c76/Demo/classify_demo.py#L106Error:
4351,What is the use of r_ui (true rating) argument in sklearn surprise .predict method?,"['python', 'scikit-learn', 'data-science']","I am learning the collaborative learning algorithm and the sklearn surprise library but I am curious as to what is the use of this parameter in the .predict function.The documentation states:We can now predict ratings by directly calling the predict() method. Let’s say you’re interested in user 196 and item 302 (make sure they’re in the trainset!), and you know that the true rating rui=4:But, the rating is what I am trying to predict. What would be the use of passing it to this function if I already have it?"
4352,"How to export Scikit learn models for hosting in AWS Sagemaker (sklearn model artifacts, .pkl file or .tar.gz)","['amazon-web-services', 'scikit-learn', 'amazon-sagemaker']","I am looking for a solution that allows me to host my trained Sklearn model (that I am satisfied with) on SageMaker without having to retrain it before deploying to an endpoint.On the one hand I have seen specific examples for bring-your-own scikit model that involve containerizing the trained model but - these guides go through the training step and dont specifically show how you can alternatively avoid retraining the model and just deploy. (https://github.com/awslabs/amazon-sagemaker-examples/blob/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb)On the other hand, there are guides that show you how to BYOM only for deploying - but these are specific to MXNet and TensorFlow frameworks. I noticed that the way you export your model artifacts among frameworks differs. I need something specific to Sklearn and how to get to a good point where I have model artifacts in the correct format Sagemaker expects(https://github.com/awslabs/amazon-sagemaker-examples/tree/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/mxnet_mnist_byom)The closest guide I have seen that might work is this one: https://aws.amazon.com/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/However, I dont know what my sklearn ""model artifacts"" includes. I think I need a clear understanding of what sklearn model artifacts looks like and what it includes.Any help is appreciated. The goal is to avoid training in Sagemaker and only deploy my already trained scikit model to an endpoint."
4353,How do I transfer a file from a AWS EC2 Amazon Linux Virtual Env to local (windows)?,"['python-3.x', 'linux', 'amazon-ec2', 'scikit-learn', 'aws-lambda-layers']","I am trying to use Sklearn on a Python Lambda function. Since Sklearn is not native, I need to upload the library and its dependencies in a layer. One such dependency, Numpy, is bigger than I can upload but, fortunately, AWS already provides a Scipy, Numpy layer (AWSLambda-Python38-SciPy1x), so yay! Therefore, I thought I need to create a layer with the remaining libraries and use both layers on my lambda function.
Thus, I've created a layer with the libraries (joblib, threadpoolctl, scikit-learn and sklearn) on my windows, uploaded it and called the function, which didn't work. Mother Google told me it was due to compatibility issues (windows-linux)¹.Finally, I’ve decided to use a Linux Instance, install Python, the remaining libraries and then upload the Lambda layer. I got an ""Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type"" and did the steps described below² (please, be kind to Linux newbs):On my windows cmdISSUE: Now I have the library installed in a Linux machine, but how do I get it to my computer from the local environment? I know how to get it if it is on the instance (outside env)³:If you can help me with the initial problem, that is, running Sklearn on Lambda, with a shorter and painless solution: <3! Otherwise, can you help me get the library.zip file to my windows?Thank you kindly.¹ Unable to import module 'lambda_function': cannot import name 'WinDLL' from 'ctypes' (/var/lang/lib/python3.7/ctypes/__init__.py² https://aws.amazon.com/pt/premiumsupport/knowledge-center/ec2-linux-python3-boto3/³ https://medium.com/@dearsikandarkhan/files-copying-between-aws-ec2-and-local-d07ed205eefa"
4354,Using sklearn with complex values,"['python', 'scikit-learn', 'complex-numbers']",I am trying to perform a kmeans in Python using scikit-learn. The thing is my data are complex values and Python doesn't like it.Is there any mean to use sklearn with complex values ?
4355,Plot Confusion Matrix for multilabel Classifcation Python,"['python', 'scikit-learn', 'decision-tree', 'confusion-matrix', 'multilabel-classification']","I'm looking for someone who can help me to plot my Confusion Matrix. I need this for a  term paper at the university. However I have very little experience in programming.In the pictures you can see the classification report and the structure of my y_test and X_test in my case dtree_predictions.I would be happy if someone can help me, because I have tried so many things but I just don't get a solution, only error messages.next I print the metris of the multilabel confusion matrixand the structure of y_test and dtree_predictons"
4356,Multiclass classification Imbalance in python [closed],"['python', 'machine-learning', 'scikit-learn']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 28 days ago.I have a dataset I'm working on which is of the form:Consider my dataset has a total of 40,000 observations and 22,000 of them are of watermelon class. How can I handle an Imbalanced multi-class problem as shown above in python?"
4357,TypeError: can't pickle _thread.RLock objects while using cross_validate,"['tensorflow', 'keras', 'scikit-learn']",I had a keras model and I converted that to scikit model using kerasclassifier but looks like there is some issue with how I am transforming object.I am getting this error while using the cross_validate function @ https://github.com/Neetu162/DeepLearningResearch/blob/677d1ddeb6f345716a457b977c26cbe14efa7bb0/Demo/classify_demo.py#L83Is there anything incorrect on how I am using the KerasClassifier or cross_validate ?
4358,How to print transformed dataset with ColumnTransformer in Keras with fit_transform?,"['python', 'pandas', 'numpy', 'tensorflow', 'scikit-learn']","I am quite new in Machine Learning. I will appreciate your support.I struggle with the preprocessing step.
Basically, I want display the content of the transformed dataset by using fit_transform. However, I get error (ValueError: Input contains NaN)The process I follow is to first convert categorical data into numeric and normalize numerical data.
Moreover, if you do have a good article on the topic, please share with me.Here is my codeHere is the outputThank you very much"
4359,Does this count as “data leakage”?,"['python', 'machine-learning', 'scikit-learn', 'knn']","I am trying to tune my k-Nearest Neighbours model with Grid Search. But because my data set is wonky, I need to standardize it first.However, I have read in a tutorial that Standardizing before doing k-fold cross validation leads to data leakage, because the validation set influences the training set.I want to ask if this problem exists in my code block below. And if so, how I can avoid this. Thank you!"
4360,Why is bias term not included in Regularization (Regression/Classification)? [duplicate],"['machine-learning', 'scikit-learn', 'regularization']","I read different articles including this answer.  
Why is bias term not included in regularization, in general? I see some of the algorithms such as LinearSVC includes bias term in regularization. Also, is bias term used for only predicting minimum value possible when all weights are zero? What is its significance exactly?"
4361,Is there a way two predict more than one model in GridSearchCV in sci-kit?,"['machine-learning', 'scikit-learn', 'grid-search', 'gridsearchcv']","I am using GridSearchCV from the sk.learn library at the moment. When you call the predict method
of GridSearchCV it always predicts on the test set with the best scoring estimator in the training set.However there are such problems as overfitting and I would like to call the predict method with more models - for example best of three.Is there a way to do this? From the documentation it seems not."
4362,OSError: [WinError 126] The specified module could not be found in Python,"['python', 'python-3.x', 'numpy', 'error-handling', 'scikit-learn']",I am trying to import the following libraries. I get an error message while importing sklearn . I believe it is related to DLL path but i am not able to rectify it . I don't want to manually specify any path in the code. Please helpCode :Error :
4363,GridSearchCV raising SIGABRT(-6) error with n_jobs != 1,"['python', 'tensorflow', 'keras', 'scikit-learn', 'anaconda']","I'm having this issue both on windows and ubuntu:What already been doneOBS:
When running this code at windows, instead it fills the entire both ram and gpu memory freezing the system. With n_jobs=1, the process used average 2 GB ram (with other parameters). The input file is just a 524x254 .csv.Environment: CondaHardwareCode example:Different fileDifferent file"
4364,Predict existing data using scikit learn,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'regression']","My dataset looks like this:What I'm trying to do using python is to predict the value AverageG which is the average of G1, G2, G3.I know that the value of AverageG can be calculated by making the average of G1, G2 and G3 but in my case it has to be predicted by using the library scikit-learn"
4365,Alternative to Alteryx AB Controls Tool in Python,"['python', 'pandas', 'scikit-learn', 'alteryx']","I'm doing a Matched Pairs Design Experiment and want to see the rowIDs matched pairs units as well. There's a tool in Alteryx called AB Controls Tool which according to the documentation takes numeric columns as inputs, uses the KD-Tree Algorithm to create matched pairs, and outputs matched pairs and the distance between the control and treatment unit for each pair. But when I searched the KD-Tree Documentation in scikit-learn I I'm having difficulty in understanding how could I use this function for this application.
I would appreciate a pseudo-code or an actual code which could be used to get matched pairs based on selected metrics, as I can't use Alteryx on my machine."
4366,GridSearch over RegressorChain using Scikit-Learn?,"['python', 'scikit-learn', 'regression', 'gridsearchcv']","I am currently working on a multi-output regression problem, where I am trying to predict multiple output values at once. I am aware there are standard regressors that natively support this task.However, I would like to use a RegressorChain and tune the hyperparameter of the Regressor in the RegressorChain using GridSearchCV. I wrote the following code for this:It tried:and:But I got both times the following ValueError:ValueError: Invalid parameter C for estimator
RegressorChain(base_estimator=SVR(C=1.0, cache_size=200, coef0=0.0,
degree=3, epsilon=0.1,   gamma='auto_deprecated', kernel='rbf',
max_iter=-1, shrinking=True,   tol=0.001, verbose=False),
cv=None, order=None, random_state=None). Check the list of available parameters with estimator.get_params().keys().Does anyone have an idea, how to setup this pipeline correctly? Thank you!"
4367,how to solve this attribution error AttributeError: 'DataFrame' object has no attribute 'as_matrix' (using Python 3.8),"['pandas', 'error-handling', 'scikit-learn', 'jupyter-notebook', 'python-3.8']","Hello guys I am getting (AttributeError: 'DataFrame' object has no attribute 'as_matrix') when I run the code below on jupyter notepad referring to those 2 lines
#create x & y variablesmy whole code is as below"
4368,scikit-learn CountVectorizer return different sizes of vectors,"['python-3.x', 'scikit-learn', 'countvectorizer']","I am trying to have a pre-processing, where I get all the documents, and build a Bag Of Words comparer.In my init method, I create scikit-learn CountVectorizer model, and train it.When I get 2 new documents, I need to return the cosine similarities between them.But self.vectorizer.transform return different sizes of vectors.
This is my code:What am I doing wrong?"
4369,Sklearn RANSAC without intercept,"['python', 'scikit-learn', 'linear-regression']","I am trying to fit a linear model without intercept (forcing the intercept to 0) using sklearn's RANSAC: RANdom SAmple Consensus algorithm. In LinearRegression one can easily set fit_intercept=False. However, this option does not seem to exist in RANSAC's list of possible parameters. Is this functionality not implemented? How should one do it? What are alternatives to sklearn's RANSAC to objectively select inliers and outliers, that allow setting the intercept to 0?The implementation should look like this, but it raises an error:"
4370,Deploy sklearn model with custom transformer,"['python', 'machine-learning', 'scikit-learn']","I have a sklearn pipeline that has been defined in the following way:The structure of my code isI have trained my model and my pipeline is saved in a .joblib file. Now I want to use my model in another project. However, I need to move not only the .joblib file, but the whole tools/transformers.py structure. I think this is kind of difficult to maintain and hard to understand.Is there an easier way to make the pipeline work without the need of moving the code around with the exact same structure?"
4371,How do I install sklearn module properly?,"['python', 'scikit-learn', 'data-science', 'sklearn-pandas']","I'm trying to install sklearn module using pip command but after the installation is completed , all I can see is this folderIn my directory and even the error says module name sklearn not found.
I've tried reinstalling it many times but still I'm not able to see the main sklearn folder in the above directory.Only 1 folder is installed i.e sklearn-0.0-py3.8.egg-info .Can anyone please help?"
4372,H2o parameter search with sklearn RandomizedSearchCV,"['python', 'machine-learning', 'scikit-learn', 'h2o']","I'm trying to fine best parameters for h2o model using sklearn RandomizedSearchCV. Code (taken from this documentation):But it gives me the following error:ValueError: Unexpected __getitem__ selector: <class 'numpy.ndarray'> <class 'numpy.ndarray'>.I tried different datasets and also tried to pass pandas.DataFrame instead of h2o.frame, and it gives the following:AttributeError: 'DataFrame' object has no attribute 'cbind'What is happening? h2o now is not compatible with sklearn?"
4373,X and Y are different sizes (Sci kit),"['python', 'python-3.x', 'pandas', 'scikit-learn', 'scipy']","Code: https://hasteb.in/axafojux.pySo in lines 83-86. You can see that the dimensions are completely different. Which means that I cannot plot them using scatter. However, I do not know what determines these dimensions and why the 1st ""Winnings"" was able to get 2 dimensions while the 2nd one doesn't. I guess that I somehow cut it off when I wanted the lastest year for each player. (Line 12)Not on this site I was told that I had to make a new model for each plot cause it was overlapping or something. But in the middle of changing a few things. I wondered how was that going to work when I did a new model after each train test split. And as I guessed I knew that wasn't my solution.How can I make the X and Y (Record and Outcome) be the same dimensions?Edit: I found a video using X.T[0]. for his .shape being exactly like mine. But it gave me a key error. https://youtu.be/pqNCD_5r0IU?t=3846"
4374,"ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). sklearn","['python', 'pandas', 'scikit-learn', 'jupyter-notebook']","Here is my My code:I'm getting value error:ValueError: Input contains NaN, infinity or a value too large for dtype('float64').And it told me :<ipython-input-1-8b8c4c2d113b> in <module>
62 
63 clf = svm.SVR(kernel=""linear"")
---> 64 clf.fit(X_train,y_train)
65  66 clf.score(X_test,y_test)`this is the link to train.csvI'm using jupyter-notebook, I'm new to sklearn and ml
I attached the CSV file above, thank you for your help"
4375,Getting 'AttributeError: Can't get attribute 'DeprecationDict' on <module 'sklearn.utils.deprecation'' this error while executing ML project,"['python', 'machine-learning', 'scikit-learn', 'pickle']","AttributeError: Can't get attribute 'DeprecationDict' on 
Showing error in model = pickle.load(open('rf_regression_model.pkl', 'rb')) this line."
4376,module 'sklearn.metrics' has no attribute 'plot_roc_curve',"['python', 'machine-learning', 'scikit-learn', 'roc']",I am trying to plot ROC curve for stratifiedKfold validation. Heres the code-I am getting this error-The version is 0.21.3.
4377,Fitting and scoring with training data doesn't get 100% accuracy,"['machine-learning', 'scikit-learn', 'linear-regression']","I wanted to verify that if I tested my model with the same data that I trained it with it would give me an accuracy close to 100%, but that doesn't seem to be the case. (Or maybe this should just not be the case ?)"
4378,Discrepancy between reported n_jobs=1 and CPU-usage in jupyter,"['python', 'machine-learning', 'scikit-learn', 'jupyter-notebook', 'gridsearchcv']","I am running a GridSearchCV on a KneighborsClassifier and put the n_jobs hyperparameter on -1 , so all my CPU's are utilized for the gridsearch. So intuitively I think there should be 8 different combinations of hyperparameters being run parallel.During the run using htop on ubuntu I can see that all 8 of my CPU's are being 99% used by python, so it looks as to be expected.
But jupyter outputs during the runAnd from what I can tell from the code output, it looks like everything is being executed serial instead of parallel.So what is exactly going on here?EDIT: The code"
4379,100% accuracy with decision tree classifier using sklearn,"['python', 'machine-learning', 'scikit-learn', 'decision-tree']","I'm using Decision tree classifier from sklearn,  but I'm getting 100% percent score and I don't know what is wrong. I have tested svm and knn and both give 60% to 80% accuracy and seem ok. Here is my code:and here is the output:
The cross val score for Decision Tree classifier (max_depth=1) is 1.0The cross value score for Decision Tree classifier (max_depth=5) is 0.9996212121212121The cross val score for Decision Tree classifier (max_depth=10) is 1.0The cross val score for Decision Tree classifier (max_depth=20) is 1.0The cross val score for Decision Tree classifier (max_depth=40) is 0.9996212121212121So the best value for the max_depth parameter is 1The accuracy obtained using Decision tree classifier is 100.00000000%"
4380,TypeError: Parameter value is not iterable or distribution,"['python', 'scikit-learn', 'sklearn-pandas', 'matrix-factorization']","I am new to Python and wanted to implement a simple Matrix Factorization Classifier.As I read in another post, there are some possibilities which one can use and I chose sklearn decomposition.NMF: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.htmlUnfortunately I get the following error:I was trying this:The interesting thing is, that I implemented the RandomForestClassifier from Sklearn before,and it works great:I also got this from the sklearn site: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.htmlI was Googling for hours now and cannot find an appropriate solution unfortunately.If somebody could help, I would be very grateful! Best wishes and stay healthy!"
4381,"Should you FIT train, test or all x and y values for a LinearRegression?","['python', 'dataframe', 'scikit-learn', 'model', 'regression']","I have seen so many examples for LinearRegression and all so different. The question is should I fit train, test or all data to the model? Any example had a different way of handling the regression...This is the split of data, no problem here:But when I fit the model, what option should I choose?Also, I must say that the offered plots show the best results using the third method. Is this the correct approach?"
4382,"How can one preprocess training data, use it to train a multilabel decision tree and then convert the resulting tree into PMML?","['python-3.x', 'scikit-learn', 'decision-tree', 'multilabel-classification', 'pmml']","I am part of a team that is creating an app to accompany stroke patients through the recovery process. One component of this is creating an algorithm to suggest treatments based on certain clinical data. I have some clinical data from real and in silico patients where we record the following inputs:I envision some feature engineering, as follows.The training label for each patient is a subset of the following set of treatments:To ensure maximum explainability, I believe that a multilabel decision tree is the way to go. Using scikit-learn, I have created a pipeline of feature engineering followed by the classifier and successfully trained it. The problem is that I also need to export the model to PMML to give to the production team. I tried sklearn2pmml and when that failed, I decided to try to write out the PMML file from scratch. Unfortunately, that too is proving harder than I thought. Here's what I have so far:I wanted to test this by inputting a patient with 1's everywhere and verifying that the patient ends up in node 1, with probabilities 0.166 for TIMES, 0.369 for ECOSS, etc... with probabilities not necessarily adding up to 1 (the treatments are not mutually exclusive).I saved the document into MotorTree.pmml and then in python ranwhich returned the dataframeIt seems there was an automatic scaling to make the probabilities add up to 1, but I don't want this. How can I get around this?If you have gotten all the way to the bottom of this, a hearty thank you, and an even heartier one if you can provide an answer!"
4383,Precomputed distance matrix in DBSCAN,"['python', 'scikit-learn', 'dbscan', 'rapids']","Reading around, I find it is possible to pass a precomputed distance matrix into SKLearn DBSCAN. Unfortunately, I don't know how to pass it for calculation.Say I have a 1D array with 100 elements, with just the names of the nodes. Then I have a 2D matrix, 100x100 with the distance between each element (in the same order).I know I have to call it:db = DBSCAN(eps=2, min_samples=5, metric=""precomputed"")For a distance between nodes of 2 and a minimum of 5 node clusters. Also, use ""precomputed"" to indicate to use the 2D matrix. But how do I pass the info for the calculation?The same question could apply if using RAPIDS CUML DBScan function (GPU accelerated)."
4384,CountVectorizer in sklearn throws “AttributeError: 'numpy.ndarray' object has no attribute 'lower'”,"['python', 'pandas', 'pyspark', 'scikit-learn', 'countvectorizer']","Good day!I am reading spark dataframe of small size using pyspark, then I am collecting it on the driver via toPandas and then I want to apply CountVectorizer sklearn transformator. However it throws exception. Please see example to reproduce below:How should I properly use CountVectorizer in this situation?Thank you."
4385,Pandas Dataframe aggregating Statistics,"['python', 'pandas', 'scikit-learn']","I'm clustering time series data sets using a kmeans algo from sklearn.I want to be able to show the characteristics of each of these clusters using a function like this,This dataframe looks a bit like this,I'm getting errors trying to use the above function to create a statistical picture of the different kmeans clusters. Happy to aggregate the Dose 1 and 2.i.e.Would love a pointer on getting a meaningful output. Trying to determine when the Dose levels are too high, or too low, and flag them using kmeans as the flag identifier.Thanks!"
4386,How can I get recognition accuracy of my trained .clf model in total percentage?,"['python', 'scikit-learn', 'face-recognition', 'estimation']","I have used this code: https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py to train pre-trained faces detector on Labeled Faces in the Wild dataset. So, this is the end of my code:And this is an output:But it is neccessary for me to print a total accuracy in percentage, in this manner: ""Accuracy: ???%"". Please, help me!"
4387,How to measure the computation time of an ensemble learning method in Python?,"['python', 'machine-learning', 'scikit-learn', 'ensemble-learning']","What I understand is that in ensemble learning, there is an ensemble generation phase.Does this include in the computation time?For example, the majority voting technique is training each classifier in the pool considered as part of the computation time?For exampleNote that time_pool  is the training time for each classifier in the ensemble.Is what I did correct?"
4388,Why not random matrix draw from the 1.0 / n_feature scale?,"['python', 'random', 'scikit-learn', 'projection']","As can be seen from here: https://github.com/scikit-learn/scikit-learn/blob/fd237278e895b42abe8d8d09105cbb82dc2cbba7/sklearn/random_projection.py#L163It says:
The components of the random matrix are drawn from
N(0, 1.0 / n_components).I wonder why the scale is not according to the Dimensionality of the original source space. But it is related to the Dimensionality of the target projection space, which means the higher dimension the target space is, the smaller the l2 of random matrix vector should be.It doesn't really make sense to me. I think it should relate to the Dimensionality of the original source space and draw from the 1.0 / n_feature."
4389,Calculating the precision and recall for a specific threshold,"['python', 'scikit-learn', 'data-science', 'precision', 'precision-recall']","I have created a logistic regression model that doesn't perform very well. However, I still calculated the best threshold based on the highest accuracy score. Now, I would like to use that threshold which is 0.04 to calculate the precision and recall. Unfortunately, I cannot find any example on how to determine these values. Could you please help if you know the function I need to use?"
4390,Build feature vectors from nested data,"['python', 'pandas', 'scikit-learn']","I am working with a nested JSON dataset containing various fields with different levels of granularity. The task is to evaluate some classification models to predict whether or not a user will be a fraudster.I am finding cumbersome to build a feature vector given the different level of details provided (e.g the dataframe has one row per customer but some objects have one row per order).
I am sure I might be able to clumsily stitch something toghether in Pandas but I am wondering what would be the most elegant approach to reach a dataset ready to be split between train/test."
4391,Is there a way to extend the training set initially passed to a SVC?,"['scikit-learn', 'svm']","I want to simulate Active Learning in Python. I have an initial training set and a pool of unlabeled potential training data. Now, I want to choose iteratively one single element of my pool, add it to the training set passed to the SVC, and retrain the SVC with the new set.
I am unsure how to do it properly. I could either do (Pseudo-Code):Or:The first would definitely work for me. Every iteration a new SVC is trained with iteratively enlarged training data. But it feels wrong to reinitialize the SVC over and over again.
Regarding the second approach, I am unsure if the SVC retrains from scratch or keeps its state from previous iterations and retrains on top of this state. I do not want that. If this is the case, I thought that there might be an option to add subsequently one element to the old state without passing the entire training data again.But neither do I know how .fit behaves behind the curtain, nor could I find such an option. Is there a ""good"" way to solve my issue?"
4392,SimpleImputer Using both columns to calculate average,"['python', 'scikit-learn']","I am using SimpleImputer and most_frequent to calculate the missing values in my dataset. Problem is that the two columns aren't being treated independently, therefore I am getting the most common value in the first column being used to replace NaN in the second column, which obviously skews my data.code below"
4393,MLPRegressor with partial_fit(),"['python', 'python-3.x', 'scikit-learn']","I'm using a MLPRegressor() with partial_fit(). My dataset has 2 outputs and 2 inputs.
I'm using a sequential model:But it gives me this error at model.partial_fit(X[i], y[i]):What's the problem? How could I solve it?"
4394,Residual plot for residual vs predicted value in Python,"['python', 'machine-learning', 'scikit-learn', 'data-science']","I have run a KNN model. Now i want to plot the residual vs predicted value plot. Every example from different websites shows that i have to first run a linear regression model. But i couldn't understand how to do this. Can anyone help? Thanks in advance.
Here is my model-"
4395,sklearn exception error: fitfailed warning from GridSearchCV in tensorflow,"['python', 'tensorflow', 'error-handling', 'scikit-learn']","I want to optimize hyperparams of CNN model for the image classification tasks (multi- class). To do so, I used gridSearchCV from sklearn but I always have bunch of warnings and values error as follow:to me, this bug may raised from sklearn and I am not sure how can I get rid of it for optimal parameter findings. Is there any way to fix those? Any thought?minimal example:this is just minimal example:from the above-pasted error message, I could assume that error may be traced back from sklearn. Is there any one point me how to fix this? any thoughts?"
4396,Estimator base class wrapping model and preprocessing pipeline,"['python', 'scikit-learn', 'pipeline']","I'm trying to work out how to easily wrap existing estimators along with a preprocessing pipeline and a target encoder, essentially generalizing the idea behind scikit's TransformedTargetRegressor. I have a possible solution but I'm wondering if I'm missing any repercussions of the design that are not immediately obvious. The basic idea is this:And so wrapping CatBoost e.g. would simply beThe crucial part is getting get_params and set_params right such that the wrapped models play nice with scikit's grid search etc., even though the design doesn't follow the official guidelines of having model attributes match its __init__ args.It seems to work, in the sense that getting and setting any of the model or preprocessing pipeline's parameters works, as does cloning:And GridSearchCV also seems to work:So... the question I have is if there is any other way this design may go wrong, e.g. any pending deprecations or planned changes that may break this use of get_params and set_params, e.g.? From what I can see, the only trick necessary to have an arbitrary constructor like this (breaking the official developer guidelines), is that get_params(deep=True) should return all settable parameters, while get_params(deep=False) is used by base.clone() only and needs to return any and all parameters necessary to call the constructor and make a (correct) copy.I know this is a rather long ""question"", but I'd be grateful to know about any caveats I should be aware of regarding the proposed pattern."
4397,Can I use figures from the sciki-learn manual for a thesis,"['python', 'scikit-learn']","I would like to reuse figures from the scikit-learn website/manuel in a thesis. I could not find information about possible copyrights.
Most figures can be reproduced using the added code. In this case I am not aware of the correct copyright laws.
Is it OK to reuse figures, when cited correctly ?"
4398,how to import sklearn.impute on jupyter NoteBook?,"['python', 'scikit-learn', 'jupyter-notebook']","have problem importing impute module from sklearn in jupyter :from sklearn.impute import SimpleImputeI've tried every solution till now,
installing the latest version of both sklearn(0.23.1) and jupyter(6.0.3) notebook
but still have the same errorModuleNotFoundError: No module named 'sklearn.impute'I've already tried the same codes on IDLE and there's no problem
the problem is on jupyterHow can I fix this problem?"
4399,"Randomized Search Value Error: Input contains NaN, infinity or a value too large for dtype('float64'). But data is correct","['python', 'pandas', 'scikit-learn', 'valueerror']","The error in the title allways occures when i do Gridsearch or RandomizedSearch in sklearn in Python.
But i checked my dataframe the X and the y and couldnt find any nans or inf´s. When using the dataset in for training testing a normal regressor/modell it works without anyproblem. So it is occuring only when i do parameter optimization. But i dont get why."
4400,Add top k results to sklearn pipeline?,"['python', 'scikit-learn']","I want to put my model in cloud, but it expects a single joblib file.
I have a model, which predicts the class (multiclass classification).
I currently use np.argsort and get my top k results.How do I add another step in sklearn pipeline which takes the output from the model, so that a single pipeline gives me back top k results?Because AFAIK, joblib can only dump a single pipeline with trained model."
4401,"Reshaping data input for the model from (1, 5) to (1, 3000)","['python', 'numpy', 'scikit-learn', 'reshape']","The training and testing data for the model has a shape of (rows, 3000). I like to call the model to predict A which has a shape of (1, 5). How do I reshape the variable A so the model will take it to return prediction? This is a text classification model, hence the data has been vectorized.ErrorThank you very much."
4402,How to compute split_gain in lightgbm of python,"['python', 'scikit-learn', 'lightgbm']","I wonder how lightgbm compute split_gain.
I saw split_gain = sum_grad / sum_hess at here.but, I saw that is not true.
Source is below.and output isI have tried another conditions, but I didn't know how lgbm comput split gain.let me know please."
4403,include_bias in Polynomial Regression,"['machine-learning', 'scikit-learn']","I'm training a polynomial regression model after adding polynomial features with include_bias=True[1.95551099] [[0.          1.07234332  0.5122747 ]] Question: include_bias essentially adds another feature vector of 1s, for intercept parameter (theta0). so, Im expecting intercept 1.9555 in place of 0. Why does this return 0 for theta0? 
[[0.          1.07234332  0.5122747 ]]"
4404,How to split test and train size,"['python', 'tensorflow', 'testing', 'scikit-learn']","I am trying to feed a CNN model(Human body pose estimation)with a dataset contains 1000 numbers,
first, how can I make sure that the number of my datasets is already enough?
second, how should i split my data to train and test size? (when I put train size = 0.6 and test_size = 0.4 the network doesnt work well and show me NAN for weights and bias and loss value!)"
4405,add a custom distance metric to sklearn grid search for knn classifier,"['scikit-learn', 'knn', 'grid-search']",I need to add the cosine distance to my measure parameters in sklearn gird search. Is there any way that I can implement it myself and add it to the list of distance measures?
4406,pandas to numpy giving none when i try to do dtype.field instead of the number of columns,"['python', 'pandas', 'numpy', 'scikit-learn']",I have a pandas dataframeIt contains 2 columns death and tteNow I convert it to numpy by usingBut next when i try to printI get None instead of the 2 fields i.e the 2 columns.I am new to numpy. Can you guys please help me I know I am making some basic mistake.
4407,when to normalize data with zscore (before or after split),"['apache-spark', 'machine-learning', 'scikit-learn']","I was taking a udemy course, which made a strong case for normalizing only the train data (after the split from test data) since the model will typically used by fresh data, with features of the scale of the original set. And if you scale the test data, then you are not scoring the model properly.On the other hand, what I found was that my two-class logistic regression model (created with Azure Machine Learning Studio) was getting terrible results after Z-Score scaling only the train data.a. Is this a problem only with Azure's tools?
b. What is a good rule of thumb for when feature data needs to be scaled (one, two, or three orders of magnitude in difference)?"
4408,Using RandomizedSearchCV in Multi-label classification,"['python', 'numpy', 'machine-learning', 'scikit-learn', 'multilabel-classification']","I'm working with Multi-label probelm and I started to use sklearn which offers very nice out-of-the-box methods to handle multi-label. I was using MultiOutputClassifier with RandomForestClassifier as an estimator. Example with 4 classes:This code produce one classifier for each label (in this case we will end up with 4 classifiers). My questions are:I also tried the example from there, but it still return only one final classifier.Thank you!"
4409,Problems with Tensorflow Keras wrapper for sklearn calibration model,"['python', 'tensorflow', 'keras', 'scikit-learn']","I have used Keras model objects with CalibratedClassifierCV from sklearn.calibration. I tried using the sklearn wrapper for Keras, but it didn't work. When I run the scipt the  error message is launched: TypeError: can't pickle _thread.RLock objects.script:"
4410,Using Pipeline with GridSearchCV,"['scikit-learn', 'svm', 'pipeline', 'grid-search']","Suppose I have this Pipeline object:To pass the hyperparameters to my Support Vector Classifier (SVC) I could do something like this:Then, I could use GridSearchCV:We know that a linear kernel does not use gamma as a hyperparameter. So, how could I include the linear kernel in this GridSearch?For example, In a simple GridSearch (without Pipeline) I could do:Therefore, I need a working version of this sort of code:Meaning that I want to use as hyperparameters the following combinations:"
4411,Create dynamic list (labels_true and labels_pred) from dictionaries of sets in python,"['python', 'python-3.x', 'list', 'dictionary', 'scikit-learn']","I have to create a list based on the below dictionaries
First dictionary isSecond dictionary isSo, I have to create two lists labels_true which will consists of folder names like 1,2,3 and labels_pred which will check images name from clustering dictionary and check in groundtruth about in which folder that image is located and return that folder number into label_pred list.
For eg, in the below labels_pred we have two 3 in 1 1 because that two images are present in 1st folder in clustering dictionary.Thus, how should I create this dynamic list in python. I am a new in Python. Appreciate your help. Thank you so much."
4412,Multivariate K-means clustering in Python?,"['python', 'scikit-learn', 'cluster-analysis', 'k-means']","I'm not sure if there's existing terminology for this, but I'll try to explain my question below.So I have an existing k-means clustering algo that uses scikit-learn, with about 50 dimensions, corresponding to different dates. The data points are price deviations for each of the dates. I get clusters fine. However, I would like to amend this so that for each data point, along each dimension, we look at 2 variables of interest - let's say price deviations AND absolute price. In terms of the data array I would like to pass into scikit-learn's KMeans function, I would go from something like this:to something like this(obviously these are made up numbers, but I hope you see what I'm getting at)The problem is I believe that the KMeans in the scikit-learn library doesn't take tuples like this - which is understandable, because the traditional L2-norm doesn't make sense in this scenario. I'm wondering if there's a way to do this using a Python library. I could do it manually, of course, but even there I wouldn't know what the best distance norm to use would be. Maybe just an average of the Euclidean distance from the centroid for both values? If so I'd obviously normalize all my variables to mean 0, variance 1.Hope this made sense, and thanks for your help!"
4413,tuple object has no attribute “fit”,"['python-3.x', 'machine-learning', 'scikit-learn', 'tuples']","The error I am receiving is ""'tuple' object has no attribute fit in line 9""I saw the other answers to the same question too they are saying to acess elements using index but I can't understand how to do the same in this question, Pleasse Help me understand it will be a huge help.Thank you and have a great day!!!"
4414,Outlier-Detection in Scikit-learn( Isolation Forest) in a pipeline,"['python', 'machine-learning', 'scikit-learn']","I have encountered the problem, as I can't use the Isolation Forest algorithm in the Sklearn pipeline. I am trying to predict the credit card default using the Kaggle Credit Card Fraud Detection dataset. I am trying to fix everything after data partitioning in order to avoid data leakage. (By using pipelines for every cross-validation as I get an almost 100% F1-score using Logistic Regression in K-fold cross-validation without using pipelines) Most of the machine learning algorithms can be used (Logistic Regression, Random Forest Classifier, etc) but not for some anomaly detection algorithms such as IsolationForest. I wondered how can I fit these anomaly detection algorithms inside the Pipelines. Thanks.Some details for X and Y (Y- 0 as a normal transaction, 1 as fraudulent transaction)"
4415,ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT,"['python', 'machine-learning', 'scikit-learn', 'logistic-regression']","I have a dataset consisting of both numeric and categorical data and I want to predict adverse outcomes for patients based on their medical characteristics. I defined a prediction pipeline for my dataset like so:However, when running this code, I get the following warning message:ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
Increase the number of iterations (max_iter) or scale the data as shown in:
https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)Can someone explain to me what this warning means? I am new to machine learning so am a little lost as to what I can do to improve the prediction model. As you can see from the numeric_transformer, I scaled the data through standardisation. I am also confused as to how the model score is quite high and wether this is a good or bad thing.Thanks in advance!"
4416,Using hold-out-set for validation in RandomizedSearchCV in scikit-learn?,"['python', 'scikit-learn', 'hyperparameters']","Is there any way to do RandomizedSearchCV from scikit-learn, when validation data does already exist as a holdout set? I have tried to concat train and validation data and define the cv parameter to split exactly where the two sets where combined, but could not find a proper syntax that is accepted by RandomizedSearchCV.scikit-learn docu says:The last option should somehow work, I hope, but I don't know in which format I have to hand it over.Any help is appreciated!"
4417,Posterior samples from fitted Gaussian process don't resemble predicted mean,"['python', 'scikit-learn', 'gaussian-process']","I am fitting a Gaussian process based on the second plot shown here and draw samples from it.
However, the drawn samples do not resemble the fitted function and generally looks very different from the predicted mean (specifically, they are not as smooth and have sharp changes) and don't go through (or close) to the given data points (red dots in the plot).Example plot (black line is predicted mean, blue and orange lines are the samples):
After many runs, the result is always similar (even if not exactly the same).
Any ideas what causes that and how I can make the drawn samples more similar to the mean?The code used for generating the plot is"
4418,Clustering product into Product Families with Python,"['python', 'pandas', 'scikit-learn']","I have a dataframe that contains Product ID and Sensors from different stations and Lines of production with values (1: the product passes through the sensor/ or 0: there is no relation between the product and the sensor).
Here is a part of the dataframe:I want to use a clustering methods that can cluster the products in products families according to the process (the sensors).Thank you for your help"
4419,Gaussian process regressor - same std-deviation for all the values sklearn 0.23 version,"['scikit-learn', 'regression', 'prediction', 'gaussian']","Q1) I using Gaussian process regressor on sensor data to estimate human pose. while predicting on the test data I am getting std deviation same for all the values. Is it normal to get standard deviation same as I have used Standscaler() for scaling the data.Q2) Secondly, I want to improve my score on test data, currently it performs poorly with score 0.27.
whenever I am trying to use mixture of kernel my process gets stuck and never returns. Any suggestion, how to resolve this issue.Below is the code."
4420,How to 'leave pair out' where each test pair consists of [0 1],"['python', 'machine-learning', 'scikit-learn', 'cross-validation']","I am building a machine learning classifier and want to use 'leave pair out' cross-validation, with non-overlapping pairs, where each pair contains one instance from the negative class and one from the positive, i.e. the ground truth labels or y values in each test set fold would be  [0 1].I can't work out how to achieve this in Scikit learn. I have 50 instances (with 25 in each class) so I can do:split = KFold(n_splits=50 // 2, shuffle=True, random_state=42) to get non-overlapping pairs but this doesn't give me test sets of [0 1]. I have looked at the documentation for LeavePGroupsOut but this doesn't seem to be what I want.Can anyone point me in the right direction? Thank you!"
4421,GridSearchCV with lightgbm requires fit() method not used?,"['python', 'scikit-learn', 'grid-search', 'lightgbm']",I am trying to carry out a GridSearchCV using sklearn on an LightGBM estimator but am running into problems when building the search.My code to build looks as such:However I am running into the following error:LightGBM however is trained using the train() method and not fit() therefore is this grid search not useable?Thanks
4422,AttributeError: module 'sklearn.utils._joblib' has no attribute 'parallel_backend',"['python', 'python-3.x', 'scikit-learn']","I am working with scikit-learn on a time series forecasting problem. I got this error the other dayFirst I thought there might be something bad with the version of sklearn, so I reinstalled it. It made no difference. I then created a new conda environment (the one you see on the traceback above) and only typed this:That throws the reported error.I suspected and have found that joblib is used diferently as of last year.On the other side, the code I was running executed perfectly on a Jupyter Notebook on the same virtual environment (I am using Anaconda to handle those), and I only got the problem when I moved the code from the notebook to PyCharm.I am using Python 3.7.7 and scikit-learn 0.23.1.Thanks in advance."
4423,"score() missing 1 required positional argument: 'y_true' scikitlearn, crossvalidation","['python', 'scikit-learn']","I am trying to make a simple crossvalidation process by scikitlearn, crossvalidate, and I get the following TypeError:I do not know why, because I am trying to do a Recommender System for tue University using Non negative matrix factorization, which is an unsupervised method ... shouldn`t the code work without y?Code:"
4424,Original space feature importances from principal component (PCA) feature importances,"['python', 'scikit-learn', 'pca', 'feature-selection']","I'm pre-processing my features using PCA to perform classification with the xgboost library, using decision trees as the base learner (like orthogonal features).The xgboost package provides a feature importance assessment (feature_importances_ e.g. via 'gain'); my question is now how I can transform the PC (feature) importances to feature importances in the original feature space.X is my feature matrix and y is my label vector"
4425,How does the predict method work on scikit-learn?,"['python', 'machine-learning', 'scikit-learn', 'supervised-learning']",How does the predict() method in scikit-learn work? Does it return random values or is there a calculation under the hood?
4426,For Loop In Python using sklearn.model_selection.train_test_split,"['python', 'scikit-learn', 'linear-regression']","I need to create a FOR loop in Python that will repeat steps 1-2 1,00 times.I can't seem to grab the R square for the dataset :"
4427,How to pass custom weights to scikit_learn wrappers (e.g. KerasClassifier) in a multilabel classification problem,"['python', 'keras', 'scikit-learn', 'multilabel-classification']","I'm building a chain classifier for a multiclass problem that uses Keras binary Classifier model in a chain. I have 17 labels as classification target and dataset is an imbalanced dataset for these classes. I want to customize weights and train my chain classifier model based on these weights.
Normally, when not using scikit_learn wrappers, I pass the custom weights  to the fit function.This is the code to generate weights for these classes:and here is my model that takes keras model as input and have a chain of binary classifiers.The fit method of chain classifier only take Train features and train labels as input. Is their anyway that I can pass my class weights so that it can be used during training so accuracy of rare classes can be improved?"
4428,Encoding Dataframe features numerically,"['python-3.x', 'pandas', 'dataframe', 'scikit-learn']","I am having a dataframe, with a number of features. There is one particular feature, that is totally dynamic and I aim to encode it. I cannot use one-hot encoding as the unique count of values can change. LabelEncoder can be of use, but can the number of classes/target labels be changed ?Consider an example (the Name feature):I wish to encode it asAnd also keeping in mind that if later on another value different than all these comes up, they automatically get stored in the encoder by the next successive value like even if the next row input isIt gets encoded to and is used asAnd how do I get the original value back ?"
4429,ImportError: cannot import name '_deprecate_positional_args' from 'sklearn.utils.validation',"['python-3.x', 'scikit-learn']",when I try to import deprecate_positional_args fuction from sklearn.utils.validation' I got this import error:The scikit-learn version is 0.21.3
4430,could not convert string to float - float DF with no spaces,"['python', 'machine-learning', 'scikit-learn', 'type-conversion', 'data-science']","I have one error from my sklearn MinMaxScaler process: could not convert string to float
But all my Data Frame is float with no empty spaces:ValueError: could not convert string to float: 'X_train'Can anyone give to me an idea where the problem is?Thank you in advance"
4431,How to extract features or parse from PE header text file and strings in python?,"['python', 'static-analysis', 'feature-extraction']","I was trying to extract features from a static analysis.
I have a PE header in a text file which looks like this:I think, Using data mining from scratch will be very inefficient to parse information and features from this text file. Is there any way to parse this information automatically in python.I have also a text file containing strings of the static analysis. Same question applies for this text file too."
4432,feature extraction for stock prediction,"['python', 'dataset', 'prediction', 'feature-extraction']","I got a dataset of 20 stocks values for each day - 60 days ago. those stocks affect the stock x I want to predict and I got the values of the stock x too. I would like to create a model to predict the stock x and test it with 10 cross-validation.
I'm looking for Features to extract from the data in order to create model.
any ideas?"
4433,Extract features from ResNet,"['deep-learning', 'recurrent-neural-network', 'feature-extraction', 'resnet', 'deep-residual-networks']","Previously, i used one convolution layer for feature extractionI want to extract features from image using ResNet something like that below is just an exampleNetwork.py file"
4434,Audio feature extraction using large window,"['audio', 'signal-processing', 'feature-extraction', 'feature-detection']","I have an audio database consisting of various types of signals and I'm planning to extract features from the audio signal. So I would like to know whether it's a good idea to extract basic audio features (eg MFCC, Energy ) from the audio signal with a large window (Let's say 5s width 1s overlap) rather than using conventional small frame size (in ms). I know that the audio signal exhibits homogeneous behavior in a 5s duration.Thanks"
4435,OpenSMILE Batch Processing,"['audio', 'cmd', 'batch-processing', 'feature-extraction']","I have successfully installed openSMILE for extracting features of a wav file (audio).SMILExtract -C config/chroma_fft.sum.conf -I input.wav -O chroma.csvI have successfully used this cmdline (I am using Windows 10) command to extract the features of a single audio file.
Now I want to compute features of multiple files at once using OpenSmile, rather than feeding 1000s of filenames and then concatenating the resultant CSV.Any help here would be appreciated."
4436,Python featuretools difference by data group,"['python', 'data-science', 'feature-extraction', 'feature-engineering', 'featuretools']","I'm trying to use featuretools to calculate time-series functions. Specifically, I'd like to subtract current(x) from previous(x) by a group-key (user_id), but I'm having trouble in adding this kind of relationship in the entityset.I then try to invoke dfs, but I can't get the relationship right...But what I'd actually want to get is the difference by user. That is, from the last value for each user:How do I get this kind of relationship in featuretools? I've tried several tutorial but to no avail. I'm stumped.Thanks!"
4437,classification model transfer learning for object detection,"['tensorflow', 'feature-extraction', 'transfer-learning', 'efficientnet']","I want to use efficientnet as my feature extractor for objection detection, here efficientnet will output feature vector with shape [batch_size, num_class](trained for classification).While usually objection detection algorithms like yolo process features with shape [batch_size, w, h, channels] because of anchors and feature fusion etc., so I wonder to know if it works to have a transfer learning by use a classification-used pretrained model for an objection detection task?"
4438,Find or match exactly same image from saved database image sets using python include DL/ML (NOT CLASSIFICATION),"['python', 'image', 'matching', 'feature-extraction']","I'm studying about image feature matching with opencv python now. My project is find exactly same image from the already saved DB image data set. The image is kind of products (e.g. front cover of books, package box etc.). I don't need to classification!! I shold find the same package box image from the package boxes using Fingerprint of Things idea.I have written code using cv2 to use ORB and Flann, BF matcher to extract and match feature to find exactly matching for input query image to find exactly same (or high score ordered ranking) from DB image data set.I tried a lot of thing included in cv2, unfortunately, test result (which I know the answer) is not that great enough. Maybe the kind of method such as, CNN or BiGAN can be solution but it is too difficult to find the example code or method even research can be applied my projects.Are there any good idea or comments for this problem? I read about NEC documents about FoT but they said 'our projects work well~' without any helpful idea (I understand their stance for that technique...).Thanks for your help."
4439,Which feature extraction technique works for gender classification using audio?,"['python', 'machine-learning', 'deep-learning', 'neural-network', 'feature-extraction']","I was following this tutorial which taught to classify various spoken words: https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/The feature extraction technique used there was this:But when I used this same feature extraction technique for gender classification,the model was not able to learn the patterns.I wanted to ask why this feature extraction technique did not worked and which feature extraction technique can be used instead (it would be very helpful, if also provided python code of the feature extraction technique)."
4440,Classifying a tennis ball,"['image-processing', 'feature-extraction', 'one-class-classification']","I'm currently working on algorithm to classify tennis balls. It's a binary classifier which answers if the object is a tennis ball or not.Since the code will be executed in CPU, such as Raspberry PI, I'm staying away from neural networks.Also, it will be running in real-time.I'm thinking about using SVM so far.My question is, what feature extraction method should I use? (HOG, GIST, SIFT...)The size of the ball is probably going to be fixed to 25×25×3 or 50×50×3 pixels."
4441,How do I count the number of features matched?,"['python', 'opencv', 'image-processing', 'feature-extraction', 'feature-detection']",I used this FLANN feature matching. I also tried it with both ORB and BRISK descriptor. It showed the results. Now how do I count the number of features matched? I tried print(len(matches) but it gave me 1589. I dont think its 1589 looking at the picture.Feature Matching ResultWill anyone please help me?
4442,How to find correspondences among the features detected in two images?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'correspondence']","I have two planar images taken from different viewpoints. I implemented feature detectors like Harris, Shi-Tomasi, ORB, FAST, MSER, etc. The descriptors I used are ORB and BRISK and feature matching technique s are FLANN and BRUTE. A lot of features were detected ranging from 1000-3000. I want to find the correspondences between two images. Everything I have done is in OpenCV Python. I don't necessarily want to understand the math. I already have the homography. Now I just want a code for calculating the correspondences.Can anyone please help me?"
4443,"Are correspondences, true matches and inliers the same in feature matching?","['python', 'opencv', 'image-processing', 'feature-extraction', 'feature-detection']","There are two planar images taken from different viewpoints. If we implement detector+descriptor+matching then in the result that is obtained, are the correspondence between the two image, the true matches and the inliers same? Is there any difference between them or they the same thing?"
4444,How to count the number of inliers and outliers from this code?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'homography']",I tried this feature matching + homography technique. I did achieve the output. But I want to also calculate the number of inliers and outliers. How do I do it? I have attached the code I used below.P:S: The two planar images I am using are of different viewpoints. The trainImage is the output of using FAST feature detection on one of the images. I used ORB as the descriptor and FLANN as the matching technique.
4445,How to evaluate repeatability and correspondence of feature detector in OpenCV Python?,"['python', 'opencv', 'image-processing', 'feature-extraction', 'feature-detection']","I am comparing the performance of feature detection algorithms on sets of planar images. The conditions under which I am comparing are viewpoint angle change, blur, illumination, jpeg compression and zoom + rotation. I have found that the following code measures the repeatability and correspondence of SIFT and SURF.I want to do the same for feature detection algorithms like Harris Corner Detector, Shi-Tomasi, FAST, ORB, MSER, A-KAZE and BRISK. What is the code for measuring repeatability and correspondence in OpenCV Python?Please help me. I will be very greatful."
4446,How to calculate inliers and outliers from feature matching in OpenCV Python?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'homography']","From the features matched between two images, I want to calculate the number of inliers and outliers.  The images I am using are planar. They were taken from an aerial viewpoint at different angles. They consists of mostly buildings and forests. I will really appreciate it if anyone can give me the code to do it in OpenCV Python. I am new to this so I only understand some basics."
4447,How to calculate true matches in feature matching?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'flann']","I am doing feature matching in OpenCV Python using two images of the same model taken from different viewpoints. The feature matching algorithms I have used till now are FLANN and Brute Force. I have already done feature detection with Harris, Shi-Tomasi, ORB, MSER, etc. The descriptor I am using are ORB and BRISK. After I get the output of feature matching, how do I calculate the true matches and false matches? For me, it is not possible to do it manually as I have limited time and the lines drawn as matches are not very clear. Is there any way to calculate true matches and false matches easily?Please also note that I am a beginner in this field and I don't really know any complex mathematical functions.Any help will be kindly appreciated."
4448,Extract audio jitter and shimmer using librosa [closed],"['python', 'audio', 'feature-extraction', 'librosa']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed 16 days ago.I am using librosa to extract features like MFCC from wav file using librosa in python. I now need to also extract jitter and shimmer. While I understand what these mean, I am not able to find exact ways to get these using librosa for a given wav file. I find no mention of these in the librosa documentation either. Has anyone extracted these features from wav files using librosa or any other python library?
Any help or pointers will really help me move forward.
Thanks!"
4449,Check if All Tiles/Features loaded in polygon/Boundary in MapBOX,"['mapbox', 'mapbox-gl-js', 'feature-extraction', 'turfjs']",I am new to Mapbox. I am getting trouble while reading the features. I am only able to get the features which are listed or shown on current Viewport. I need to ask if there is any event in mapbox to check if all tiles/Features loaded inside my selected polygon so I can read all features between polygon. The code I am using for reading features is
4450,What does “pre-image” mean in this context?,"['python', 'opencv', 'feature-extraction', 'feature-detection']","I am comparing the performance of feature detection algorithms like Harris, ORB, MSER (OpenCV Python). I have two images of the same object taken from different viewpoints. Since I am just a beginner to this area, I am having trouble understanding what ""pre-image"" means in this context. How do I get the ""pre-image""?Detecting regions covariant with a class of transformations has now reached some maturity in the computer vision literature. The requirement for these regions is that they should correspond to the same pre-image for different viewpoints, i.e., their shape is not fixed but automatically adapts, based on the underlying image intensities, so that they are the projection of the same 3D surface patch."
4451,how can i extract features from a hand-written signature image data set using CNN in python?,"['python', 'computer-vision', 'conv-neural-network', 'feature-extraction', 'feature-selection']",Please help me with my task to extract features from a hand-written signature image dataset (CEDAR or GPDS). so that I can apply arithmetic operation (subtraction) on these features.
4452,Feature Engineering: Data become imbalanced after binning,"['machine-learning', 'feature-extraction', 'hidden-markov-models', 'feature-engineering']","I am training an HMM model with some data. To generate observation sequences, I tried different tricks of data binning. The problem is, despite equal-depth binning, other methods usually generate very imbalanced binning results, which means there are much more data in certain bins than in others. This causes the HMM model to weigh more on these bins.Is there a solution to this problem?"
4453,'AttributeError' using tsfresh lib for extract_features,"['python', 'time-series', 'feature-extraction', 'feature-selection', 'tsfresh']",Always have the same error using tsfresh lib. I perform a very simple operation :
4454,How to decrease number of points detected by Harris Corner Detector?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'corner-detection']",I am using Harris Corner Detector for Feature Detection. The code I am using is given below. The result is more than 10000+ keypoints being detected. How do I decrease the number of keypoints detected to around 1000 and make sure that it's precise?
4455,Feature Set Selection generated by an output of another DataFrame,"['python', 'pandas', 'feature-extraction', 'feature-selection']","I have the following table ""df_features"" as an output:I want to iterate through df_features and get every FeatureSet with the ProcessingStatus ""On"" and then get a list of the AttributeHeaders and AttributeNames within these FeatureSets separated by a dot (AttributeHeader.AttributeName). (Except CreationDate)In this case we have two FeatureSets with ProcessingStatus ""On"".If there is no FeatureSet with the ProcessingStatus ""On"", then it should stop.The attributes (Fruit.Apple, Fruit.Banana, ...) are names of columns that exist in the original DataFrame (df) that also holds the data of every Attribute. The DataFrame has around 45 Columns in total.In the next step i want to extract DataFrame (df) to the Attributes of the oldest FeatureSet of df_features and call the extract ""analyze_columns"". (Oldest FeatureSet in this example is First_Set with the CreationDate 01.06.2020)analyze_columnsI'm not sure how to do this in the best and most efficient way."
4456,Feature extraction using deep learning for more than one class,"['python', 'feature-extraction']","I have a time series dataset and I want to extract its features using BRNN or CNN -RNN ( python programming language)First, I train the model with the classification layer and obtain best accuracy.Then, I want to take the features from one of model's hidden layer.But I have 41 class how can I extract the features of all these classes and how I know the extracted features belong to which class?Because I want each class with its extracted features to make some calculation later."
4457,"After clustering on a subset of time series, how can I associate the remaining time series with already created clusters?","['time-series', 'cluster-analysis', 'similarity', 'feature-extraction']","I would like to know if there is a way to associate time series with existing clusters?In practice, I considered a subset of time series and for each I extracted some features (after which I applied the k-means and grouped similar ones) having 6 clusters.Is it possible to insert the remaining time series directly into one of the clusters already created in which there are similar time series?"
4458,No keypoints detected with ORB,"['python', 'opencv', 'image-processing', 'computer-vision', 'feature-extraction']","I was using ORB detector (CV2) for feature detection. But it returned no kps with a small roi (cropped out by YOLOv3 automatically). I thought the reason was the image being too small, so as I debugged, I cropped out that small roi manually and run ORB again, it actually returns a lot of kps...
So could you please give me some opinions on why the ORB detector can give different results with the identical input image?the code is really simple, just"
4459,how to save data image to csv file?,"['python', 'csv', 'opencv', 'image-processing', 'feature-extraction']","I would like to save data images from the extraction feature method to csv dataset file in python. Here is the example of codeI got an error in text syntax. The code, I found from github and using pyqt5 library to open the code, but I don't use it because I can't install the library"
4460,Index image feature vectors into solr,"['indexing', 'solr', 'deep-learning', 'feature-extraction']",We have millons of images that belongs real world objects. We've indexed text features as string value of those object into solr search engine. But now we need a new requirement 'Search by image'. We used pretrained Neural Networks for extracting feature vector of all images. Now here is the question:What is the best practise to index feature vectors into solr. How we can help solr to compare  vectors and get most similar vectors. We've used a solution from github which is a solr plugin but it is very slow. In our opinion it shouldn't be involved in TF/IDF opeartions in search engine. without indexed vectors  Some simple cosine distance measure and rank between them will simple solve the issue.
4461,should I generate visual words in test phase when using fisher vectors?,"['feature-extraction', 'vlfeat', 'gmm', 'vlad-vector']","I'm wondering should I use vl_gmm to get the test features' cluster centers parameters in test phase?Because I saw a code about using fisher vectors, it didn't use vl_gmm to get the cluster parameters, is it right?
Thanks advanced!this is the pseudo-code I saw on the Internet:[means, covariances, priors] = vl_gmm(train_feature, numClusters);FV_train = vl_fisher(train_feature, means, covariances, priors, 'Improved');%get the fisher vectors in test phaseFV_test = vl_fisher(test_feature, means, covariances, priors, 'Improved'); "
4462,"I need to combine the separate[Shape, Margin, Texture] features of a plant into one file for analyzing the model metrics","['machine-learning', 'optimization', 'classification', 'svm', 'feature-extraction']","I have three features of plant
1.Shape
2.Margin
3.Texture
these features are all normalized.
there exist 100 class and for each class there is 16 samples of three features.
i.e. 16x100= 1600 feature instances.
i.e. 1600 samples are available for each shape ,margin and texture feature which are derived from different classes of plants.I tried DNN multiclass classification using the single features, but model accuracy is less.Now i want to make use of three classes together and see if model accuracy improves. for which i want to combine the three feature.
But, i am not getting a clear picture how to put this all together
as for each feature of one plant is defined by a vector of size 64.ex shape feature of one plantsimilarly for the margin and texture too
so any idea about how to combine three features.
if taking mean is an option, then will it cause any problem at last to the feature values at the end??
which model will best help for classification of 100 class with less feature."
4463,Keras Same Feature Extraction from Different Images,"['opencv', 'image-processing', 'keras', 'feature-extraction']","I'm using Keras' pre-trained model for feature extraction in two images, however they gave the same outcome (array_equal = True). I've tried other model like VGG16 and Resnet50 but the results are the same. Am I writing the code wrong or is it the limitation of pre-trained model? Is there anything I can do to extract different features? Thanks!Below are my two images:"
4464,opensmile: Mismatch in input level buffer sizes,"['buffer', 'feature-extraction', 'acoustics', 'opensmiles']","I want to extract egemaps features using opemsmile-2.3.0, and I got a result but with these warning messages:(WARN) [1] in instance 'gemapsv01a_formantVoiced.reader' :
Mismatch in input level buffer sizes (levelconf.nT). Level #0 has size
5 which is smaller than the max. input size of all input levels (150).
This might cause the processing to hang unpredictably or cause
incomplete processing.(WARN) [1] in instance 'gemapsv01a_logSpectralVoiced.reader' :
Mismatch in input level buffer sizes (levelconf.nT). Level #0 has size
5 which is smaller than the max. input size of all input levels (150).
This might cause the processing to hang unpredictably or cause
incomplete processing.(WARN) [1] in instance 'gemapsv01a_logSpectralUnvoiced.reader' :
Mismatch in input level buffer sizes (levelconf.nT). Level #0 has size
5 which is smaller than the max. input size of all input levels (150).
This might cause the processing to hang unpredictably or cause
incomplete processing.(WARN) [1] in instance 'egemapsv01a_logSpectralVoiced.reader' :
Mismatch in input level buffer sizes (levelconf.nT). Level #0 has size
5 which is smaller than the max. input size of all input levels (150).
This might cause the processing to hang unpredictably or cause
incomplete processing.(WARN) [1] in instance 'egemapsv01a_logSpectralUnvoiced.reader' :
Mismatch in input level buffer sizes (levelconf.nT). Level #0 has size
5 which is smaller than the max. input size of all input levels (150).
This might cause the processing to hang unpredictably or cause
incomplete processing.I tried following this guidance from here and the manual, but I cannot figure out how exactly to match the buffer size.When I extract other features (e.g. IS13-ComPare, I don't get these warnings)."
4465,Mask a three dimensional array to perform segmentation,"['python', 'image-processing', 'mask', 'numpy-ndarray', 'feature-extraction']","I'm working with Python. I want to know if there is a python way to mask a 3d array XYZ (volumetric image) to perform a segmentation analysis such as skeletonization.I'm handling a 600x600x300 array so reducing the number of candidates to be analyzed is key to performance. I tried np.array[mask] but the array dimension changes to 1. The where method such as this How to Correctly mask 3D Array with numpy performs the change to one value at the time, but skeletonization needs to analyze the neighbors to be performed.Edit: This is something simple but it might help you to get the idea. It's to create a 3d AOI inside a volume.this is the error due to the 1-d array:ValueError: skeletonize requires a 2D or 3D image as input, got 1D."
4466,How to compare probe face images with gallery images with feature extractor | Python,"['machine-learning', 'compare', 'feature-extraction', 'facenet']","I have a dataset that contains 1500 face images and i have selected 150 images as probe.
Now 150 images are in probe folder and other images are in gallery folder.I have facenet feature extractor which extract features from images and save into .npy array to compute euclidean distance.How i can compare these 150 images with whole gallery folder and draw a accuracy graph of rank-1,5 and 10 and between similar images and compute mAP?"
4467,How do I implement A-KAZE and KAZE in OpenCV Python?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'feature-descriptor']","I am trying to implement KAZE and A-KAZE in opencv python for feature detection on an aerial image. What is the code? Also, what descriptor should go along with it for feature matching?"
4468,Feature Selection in Machine Learning Question,"['machine-learning', 'feature-extraction', 'feature-selection', 'feature-engineering']","I am trying to predict y, a column of 0s and 1s (classification), using features (X). I'm using ML models like XGBoost.One of my features, in reality, is highly predictive, let's call it X1. X1 is a column of -1/0/1. When X1 = 1, 80% of the time y = 1. When X1 = -1, 80% of the time y = 0. When X1 = 0, it has no correlation with y.So in reality, ML aside, any sane person would select this in their model, because if you see X1 = 1 or X1 = -1 you have a 80% chance of predicting whether y is 0 or 1.However X1 is only -1 or 1 about 5% of the time, and is 0 95% of the time. When I run it through feature selection techniques like Sequential Feature Selection, it doesn't get chosen! And I can understand why ML doesn't choose it, because 95% of the time it is a 0 (and thus uncorrelated with y). And so for any score that I've come across, models with X1 don't score well.So my question is more generically, how can one deal with this paradox between ML technique and real-life logic? What can I do differently in ML feature selection/modelling to take advantage of the information embedded in the X1 -1's and 1's, which I know (in reality) are highly predictive? What feature selection technique would have spotted the predictive power of X1, if we didn't know anything about it? So far, all methods that I know of need predictive power to be unconditional. Instead, here X1 is highly predictive conditional on not being 0 (which is only 5% of the time). What methods are out there to capture this?Many thanks for any insight!"
4469,How to count number of features detected by ORB?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'orb']","How do I count the number of features detected through ORB? The openCV python code I used for ORB is:This is the result:
"
4470,How to implement BRISK in openCV python to detect features?,"['python', 'opencv', 'feature-extraction', 'feature-detection', 'mser']","I want to implement BRISK in OpenCV Python for feature detection in drone images. Since BRISK is also a descriptor, I want to use its description features to match two images. How do I do it?"
4471,How to count number of features detected through Harris Corner Detection?,"['python', 'opencv', 'image-processing', 'feature-extraction', 'feature-detection']","I am still new to OpenCV and Python. I am using the following Harris Corner Detection code. As you can see, the result is also given below. Now, how do I count the number of features detected as those red dots? Please note that I am a beginner so you might need to elaborate the answer."
4472,How do I get the number of elements in a list?,"['python', 'list']",Consider the following:How do I get the number of elements in the list items?
4473,How to implement counter in Harris Corner Detection algorithm?,"['python', 'opencv', 'feature-extraction', 'feature-detection']",I need to count the number of features detected by this code. The features appear as red dots in the image. I am extremely new to python and OpenCV. I have no idea how to implement the counter in this code.
4474,Handle variable dimension of hand-crafted feature vector,"['python', 'opencv', 'feature-extraction', 'feature-selection', 'feature-engineering']","I am extracting and processing features from thermal images. I have a sequence of such images and in each image, there are characteristic features (e.g no. of blobs, blob center, area, perimeter, major axis, minor axis etc.) Depending on the number of blobs, the rest of the features can have multiple values as following:I am creating a vector out of these features. Since there can be multiple values for some of the features, the size of the vector will be variable for different images. I have the following questions:I will be using these feature vectors as input to a neural network for a classification problem."
4475,feature extraction and classifiers python email spam detection,"['python', 'python-3.x', 'feature-extraction', 'feature-selection', 'spam-prevention']","Hi im looking to do feature extraction and classifiers for my email spam detection. I have attempted to do the feature extraction which I think is okay, however I would prefer this as a separate function. I was having issues with this however in terms of not being able to get the dictionary data across. Please could someone help with the feature extraction in order for me to then be able to look at classifiers. Please see the code below. I am new to python so apologies.#createDictionary(testingDataLingspamClean)"
4476,How do we extract radiomics features for each independent object in an image?,"['python', 'feature-extraction']",I have a tissue image and there are around 100 cells in it. I also have the corresponding mask image which includes those 100 cells. How do I calculate the radiomics features for each cell?
4477,How to find geometry of an object from point cloud using lidar sensor?,"['feature-extraction', 'point-cloud-library', 'point-clouds', 'open3d']","I am working on a project where an object is passing on a conveyor and point cloud is created by the lidar, I want to extract the length and width of that object, I can extract the approximate width only when the object is passing perpendicular to the lidar FOW, but not the length, and when object is in tilted position neither width nor length is correct.I have tried with PointCloud library and Open3d using Python.Is it possible to extract the geometry when object is in tilted position?How to calculate the length as it depends on the conveyor speed, can the calculation be made independent of the speed?Is there any other library which can help achieving the task, please point to that?Thank You"
4478,"How to show a model an unknown object, teach them what it is with speech recognition so that it learns and recognizes similar object in future?","['deep-learning', 'computer-vision', 'feature-extraction', 'unsupervised-learning', 'object-recognition']","The model is not trained with any dataset beforehand and will only be trained with the object I have shown.  I am thinking of using feature extraction to recognise high level features so that it will recognise similar objects in future, but I do not know how to go about doing that.  Any recommendation of specific tutorial or deep learning library I could use/follow for this problem?"
4479,How to unstack observations and arrange them in columns [duplicate],"['python-3.x', 'pandas', 'feature-extraction', 'feature-selection', 'feature-engineering']","I have a dataframe like below, the row number is identical with all XX YY ZZ variables, how to convert it into You notice that value column is gone, and all values became observations under XX YY ZZ Features."
4480,How to deal with array of string features in traditional machine learning?,"['machine-learning', 'deep-learning', 'feature-extraction', 'feature-engineering']","Let's say we have a dataframe that looks like this:If we are interested in training a classifier that predicts label using age, job, and friends as features, how would we go about transforming the features into a numerical array which can be fed into a model?Hash each element of the list. Using the example dataframe, let's assume our hashing function has the following mapping:Let's further assume that the maximum length of friends is 5. Anything shorter gets zero-padded on the right hand side. If friends size is larger than 5, then the first 5 elements are selected.dataframe after feature transformation would look like this:Consider the following:Compare the features of the first and third record:Both records have the same set of friends, but are ordered differently resulting in a different feature hashing even though they should be the same.To solve the limitation of Approach 1, simply order the hashes from the friends feature. This  would result in the following feature transform (assuming descending order):This approach has a limitation too. Consider the following:Applying feature transform with ordering we get:What is the problem with the above features? Well, the hashes for Netflix and 9gag in rows 1 and 3 have the same index in the array but not in row 4. This would mess up with the training.What if we convert friends into a set of 5 columns and deal with each of the resulting columns just like we deal with any categorical variable?Well, let's assume the friends vocabulary size is large (>100k). It would then be madness to go and create >100k columns where each column is responsible for the hash of the respective vocab element.How about this? Convert each hash to one-hot-vector, and add up all these vectors.In this  case, the feature in row one for example would look like this:Where 01x8 denotes a row of 8 zeros.The problem with this approach is that these vectors will be very huge and sparse.With this approach, we feed each word in the friends array to the embedding layer, then convolve. Similar to the Keras IMDB example: https://keras.io/examples/imdb_cnn/Limitation: requires using deep learning frameworks. I want something which works with traditional machine learning. I want to do logistic regression or decision tree.What are your thoughts on this?"
4481,Extraction of specific pattern using regex python,"['python', 'extract', 'regex-lookarounds', 'regex-group', 'feature-extraction']","I wish to take a string input and extract a specific pattern:For Example :from above string i want to extract  9""""W X 7.5""""H
how can i do it?
Thankyou!"
4482,Comparing features from 2 file geodatabases to identify identical features using select by location,"['python', 'for-loop', 'arcgis', 'feature-extraction', 'feature-selection']","I am having trouble getting this script to run correctly. It selects features from one fgdb and compares it to features in another fgdb to identify identical features and copy the non-identical features into a new fgdb.  My problem I believe lies in the SelectLayerByAttribute_management tool, where the input layer must be a layer and not a feature class.  When I try to use the MakeFeatureByAttribute_management tool on the feature classes in gdb1 within this for loop, it copies every feature instead of only the non-identical features.  Below is a small snippet where I have identified where the problem lies:I have also tried listing/sorting the feature classes before this loop (and inside, which is inefficient but worth the try...see snippet below), but I can't quite get the feature class layers to match up when running the SelectLayerByLocation tool in the loop above.  Any advice is much appreciated!!!"
4483,What's the difference between SideEffect and Condition? how to figure out?,"['nlp', 'feature-extraction', 'language-features', 'aspect']","I'm working on aspect extraction in drug review.There are 7 kind of aspects, but I can't find any role to find the difference between two of them ....Now I can't find out any role to figure out and extract differences between these two type of aspect ."
4484,Fingertip detection OpenCV python using convexity defect,"['image-processing', 'opencv3.0', 'feature-extraction', 'depth', 'convexity-defects']","I have done the following code to detect fingertip. Using a separate dataset it is working. But not working with the NTU dataset. Please help. My code goes here below.In my dataset, I have text files for depth images and from there I am doing depth thresholding. 
The extraction is done. But the convexity defect function is not working I guess.
I need a solution. I have also provided the link to my dataset.Link to my dataset"
4485,How to predict the whole image through trained classifier and visualize it as classified landuse and landcover image(LULC)?,"['machine-learning', 'feature-extraction', 'multilabel-classification', 'pattern-recognition', 'satellite-image']","I am working on LULC classification of sentinel-2 image(4-bands) using Machine Learning algorithms.I  split the whole image into 64*64 patches and assigned class label(builtup,barren land,water and vegetation) to selected patches.For each class patch spectral features(mean,variance values of each band,mean normalized difference vegetation index(NDVI) for each class patch, texture features (grey-level co-occurrence matrix (GLCM) contrast,homogeneity,dissimilarity and correlation for NIR band of each class patch were calculated.I split the features into training and testing dataset and trained random forest classifier on training dataset.The problem is i dont know how to classify the whole image using trained classifier and visualize it as a classified image.''''''"
4486,Detectron2 Panoptic FPN Model Partial Execution - TypeError: 'NoneType' object is not iterable,"['python', 'pytorch', 'typeerror', 'image-segmentation', 'feature-extraction']","I am trying to extract the pre-output feature-map for the Panoptic Output of Detectron2 ResNet50 - based FPN model.Hence, In order to get partial model outputs, I am following the official Detectron2 Modeling Documentation to Partially Execute Models. Please find the code below:Please find the error thrown by the above step shown below:Please let me know what am I doing wrong, and how to fix it. An explanation regarding why did this error come up would also be very helpful.Please let me know if any other information is required."
4487,Function that works with values in a seq(),"['r', 'function', 'dataframe', 'feature-extraction']","I have a df where I want to count per id the amount of times where column c < value. I want to make a df with the results of all the thresholds that I took. Which would result in:I already got the code that counts per id the amount of times where column c < value. However, I can't manage to write a function that applies that code for all the values in a seq() and puts the results in 1 dataframeAny suggestions would be appreciated! Thanks in advance! :D"
4488,Over all features is blue,"['cucumber', 'feature-extraction']","my cucumber shows blue for the features file it needs to be green. how do I do that?
its not working because its blue."
4489,How to do feature extraction on a graph using Pytorch?,"['python', 'graph', 'deep-learning', 'pytorch', 'feature-extraction']","I'm trying to implement a GraphConv network whose goal is to adapt the graph it's being fed to a more suitable one for the target task. To do so, I feed it a deformed grid, compute prediction and compare the result with the a target graph. Which I know is a good graph.Basically, the code looks like this: However I noticed that most GCN in litterature requires a lot of features on every nodes. And I only have  a few (4 features). So is there a way to extract features on a graph as we would do with convolutions and pooling for a CNN on an image ? "
4490,CNN for Image similiarity,"['python', 'tensorflow', 'image-processing', 'keras', 'feature-extraction']","I'm currently working on a machine learning task where the goal is to decide to which of two pictures the food in a third picture tastes more similar. So for each sample we have 3 pictures: A, B and C. The goal is the predict 1 if the food depicted in A tastes more similar to the one depicted in B, and 0 if it tastes more like the one depicted in C. For training I was provided with 10 0000 samples where in each sample the food depicted in A and B taste similar, while the one depicted in C tastes different.Just one example of a training sample:My question is how to tackle the problem. I could use a pretrained CNN and fine tune it to my task, but I don't quite get what the objective would be. How could I use my training data to train a model? As for all the training data the label would be a '1', (first two images of sample are more similar than the first and the third) should I shuffle a couple of the samples, e.g. exchange image B and C such that the label would be '0'?
Or should I completely discard the training data and just use a CNN to extract a feature vector for each image and then based upon the similarity of this vector judge which images are more similiar?"
4491,how to read multiple .csv files conataining feature vectors as input to lstm for implementing cnn lstm classifier?,"['dataset', 'lstm', 'feature-extraction', 'cnn']","I am having a dataset consisting of videos which I preprocessed into frames which were supposed to train model consisting of CNN followed by lstm for binary classification. 
I have used a pre-trained present50 model which generated the features.
In place of generating bumpy array file, I wrote it into a .csv file which is about 3 million features in 2 .csv files which were merged for 2 sets of 17 .csv files.
now the problem is data is not a structured way which is difficult to find the end of one bumpy area list to fetch in while for giving input into lstm for reshaping into 3d tensor.
I am unable to resolve out how to give input into the lstm model from those 34 .csv files togetherly or read each file in place of how we read each column "
4492,Extract feature map from Tensorflow object detection API,"['tensorflow', 'object-detection', 'feature-extraction', 'object-detection-api']",Currently I am using pretrained faster_rcnn_resnet50_coco model from model zoo to detect object . But I want only the feature map which use to generate proposal and classify the object from that model. I want to use that feature map later on another CNN model. I don't want to waste memory by downloading another pretrained restnet50 cnn model and extract feature from there. Thanks in advance.
4493,Halcon - Extract straight edge from XLD,"['line', 'extract', 'feature-extraction', 'halcon']","I have a XLD edge, like the one in red in the sample picture below. I need to extract start/endpoint of straight lines that reppresent it. Hough lines sort of work for this, but the results are not really replicable. minor changes in the contour produce unexpected results. How can the contours be extracted as straight lines? (blue) with start and finish coordinates?
lines shorter than a specified length should not be counted as separate line."
4494,Binary Classification with char/string features,"['machine-learning', 'classification', 'feature-extraction', 'feature-selection', 'feature-engineering']","Im currently working on a binary classification problem with proteins. The goal is to figure out whether or not a mutation will change the proteins function from active to inactive.
The mutation can happen at 4 different but fixed places in the amino acid chain that makes up the protein. So my feature vector consists of a char code of length 4, where each char represents the amino acid at one of the 4 places where a mutation takes place. In total there are 21 possible amino acids.My question is how would I turn this string of 4 chars into something numerical for my classification.
What i tried so far is turning each cahr into the ASCII decimal representing the capital letter for that char (e.g. A->65) but this gave me only mediocre results.I found something about one hot encoding but I don't know how to use it since besides the information about 4 of the total 21 amino acids occur in the mutation also the position at which they occur is important in my case.This is a sample of the training data:"
4495,Is there an alternate way to calculate Solidity in regionprops.Solidity in the image package in Octave?,"['image-processing', 'octave', 'feature-extraction']","I have been trying to use the regionprops function in Octave for Feature extraction from a k-means segmentation of an ultrasound image. The regionprops function has several properties among which is Solidity, which is Area/ConvexArea. I'm on Octave 4.4.1 with the latest image package installed and loaded. However, when I try to use those properties, I get the error property ""Solidity"" not yet implemented. It shows the same error if I try to compute ConvexArea as well. Is there an alternate way to calculate it ? I saw a thread here that shows a method. However, I tried that and couldn't calculate the convex area using the method shown thereCode for reference:"
4496,How can I calculate the number of weights and bias values in a single CNN layer?,"['machine-learning', 'neural-network', 'conv-neural-network', 'feature-extraction']","Given the following image, how can I calculate the number of parameters:CNN LayerThis particular layer consists of 4x4 convolutions and 64 feature maps; how can I complete a calculation that satisfies my initial question?Update - Full Architecture"
4497,skimage.feature.greycomatrix only producing diagonal values,"['computer-vision', 'feature-extraction', 'scikit-image', 'glcm']","I am attempting to produce glcm on a trend-reduced digital elevation model. My current problem is that the output of skimage.feature.greycomatrix(image) only contains values in the diagonal entries of the matrix. The image is quantized prior with the following code: How would I go about making the glcm compute values outside of the diagonal matrix (i.e. just the frequencies of the values themselves), and is there something fundamentally wrong with my approach?"
4498,How to create a N-dimentional feature vector in python,"['python', 'image', 'numpy', 'data-structures', 'feature-extraction']","I have 20 pixel values for an image, and i would like to store them in a 20D feature vector, not a 20 length feature vector. I'm new to Python, so i don't know if in Python a regular array is considered a n-dimentional vector, or i need to vectorize a single array somehow. These are the pixels value:I wish to turn them into a 20d dimentional vector, how would i proceed to do that?"
4499,String data and classification,"['python', 'nlp', 'feature-extraction', 'language-features', 'feature-engineering']","I have a data set that consists of a string and the class it belongs to. The string consists of 4 letters. The class can either be '0' or '1'. Some examples -I am using ''CountVectorizer'' from sklearn.feature_extraction.text to extract features and ComplementNB to train model, but it doesn't work so well, so I want to ask whether there are other approaches to extract more features?Thanks in advance."
4500,how to use tsfresh python package to extract features from time series data?,"['python', 'python-3.x', 'time-series', 'feature-extraction', 'tsfresh']","I have a list of lists where each list represents a time series :i want to extract feature from this dataset with tsfresh package using code:When i'm running it i'm getting Value error which is:EDIT 1:
As suggested i had tried by converting the dataset into data and then tried :but the Value error is sameAny resource or reference will be helpful.Thanks"
4501,How to extract the Freezing Index from Accelerometer data for FoG detecting?,"['python', 'pandas', 'time-series', 'feature-extraction']","I'm working on a project to detect Freezing of Gait episodes for Parkinson's Disease patients, and according to this paper and other ones, extracting the freezing index which is ""The power in the “freeze” band (3-
8Hz) divided by the power in the locomotor band (0.5-3Hz)"" will be a good feature.Here is the explanation: One standard feature which is extracted from the raw signals is the Freezing Index (FI), defined as the ratio between the power contained in the so-called freezing and locomotion frequency bands (3-8 Hz and 0.5-3 Hz respectively). This feature is convenient since it requires only FFT-computation. But I cannot how to implement it in Python.I have a dataframe like this: 

Then I did something like this to extract features from the sensors time-series data:Now, I would like to extract the Freezing Index, How can I do this? and I need to someone to explain it to me because I don't fully understand it! "
4502,Creating a feature vector from single pixels extracted from specific points at an image,"['python', 'image', 'image-processing', 'computer-vision', 'feature-extraction']","i have a face image that i'd like to extract single pixels unit from very specific parts of an image and create a 20-dimensional vector for those points.For instance, this is what i am trying to do, what i've tried so far was using KeyPoints, shapes from cv2 and even points from graphics, none of them worked. Therefore, i wish to try it simply, but i can't figure out how i would do that. Given that a feature vector can be represented from every pixel that an image has, i wish to create a simple 20-D feature vector consisting of only 20 pixels that i find useful at specific locations in the image.
for example: given a 150x150 image, we could suppose the interesting pixels were at the position
125x100
125x50
50x35and etc.I was trying to use PIL, putpoint specifically, i managed to insert white pixels, however the image is being read as black, and i still can't figure how to create a feature vector from the pixels values.When i do this:This happens:I wish to set 20 specific pixels to black, manually. And store them in a 20D feature vector"
4503,CNN feature extraction OCR,"['ocr', 'lstm', 'feature-extraction', 'seq2seq', 'encoder-decoder']","I work on OCR seq2seq system. I have CNN-LSTM-LSTM enkoder-dekoder model. CNN consists of 2 convolutional layers:conv32 -> relu -> bnorm -> conv64 -> relu -> bnorm. So for example whan input image has 600x32px, after the second bnorm layer output will be 150x8x64. Than I Reshape it, so it has 150x512.I would like to ask you, if that reshaped output can be processed by LSTM encoder, or it is necessary something else. I saw that some people put one dense with dimension 512 after reshape layer, and than it is processed with LSTM. When I try it, accuracy doesn't change...so what is the purpose for dense layer after the reshape?Thank you"
4504,audio extraction features kearas.features.melspectrogram to use with conv2D,"['audio', 'keras', 'shapes', 'feature-extraction', 'cnn']","Greetings, 
I'm currently struggling on how to create the right shape of audio extraction to feed into a conv2D layer (Input, first layer) Scenario: 
I want to greate a CNN based on a paper: http://www.ofai.at/~jan.schlueter/pubs/2017_eusipco.pdfThe goal is to extract features out of audio with mel bands to categories them into [0,1]
0= no bird detected in audio , 1= bird detected in audio My problem is: 
I have the mel features extracted an revieve the shape: (700,80) per audio segment but I need a third dimension to feed them into a Conv2D layer. 
I tried to understand an example code to grasp the idea of the added dimension but I'm somehow lost.Example code with print lines and with output: How do I get from mfccs to mfccs_new by only reshaping the information? (ignoring the actual sizes of the arrays) My code (mel spec feature extraction):-> wanted shape (1, 700, 80)  what is contained in the additional dimension? "
4505,"MemoryError: Unable to allocate 115. GiB for an array with shape (1122, 1122, 12288) and data type float64","['python', 'machine-learning', 'computer-vision', 'feature-extraction', 'orb']","I am trying to pass a function that returns a flattened array of images and labels and my OS is windows 10. Moreover, when i try calling the function i the error described in the titleWhat i want to do is: i want to extract features from a dataset with keypoints on them, inside a function and use train_test_split for my dataset, but even if i try to flatten the images with keypoints, it'll get me the error, the only way to flatten are the same images without keypoints.Here's how i was trying:"
4506,Missing keypoints from image,"['python', 'opencv', 'feature-extraction', 'cv2', 'orb']","I'm trying to manually set 22 keypoints on an image so i can extract their features. For this task, i  create an array of keypoints with the coordinates i manually selected and pass the vector as a parameter in orb.compute and them draw the keypoints respectively. My problem is that i set 22 points on different parts of an image, but it won't show more than 14 keypoints in the image.Here's an example of what are the locations that i am expecting to draw 22 keypoints on:What actually happens:Points were changed a bit from one image to another, but it the coordinates in the code are what matters, the images are illustrative example of what im trying to do and what im getting.Here's a reproducible version of my code:So, for some reason i set all these 22 points that can be seen above, but when i run my code, it shows only 14, some are missing
I even tried printing the kp parameter in cv2.drawKeyPoints to make sure, even so:"
4507,How to manually set keypoints and extract features,"['python', 'opencv', 'computer-vision', 'feature-extraction']","I'm using ORB to detect keypoints on a set of images, like in this example:What i want to do is: i want to manually set 22 points at specific coordinates of the image and store the features extracted from these points into feature vectors. For example: After that, store those features into respectively a 22th dimensional vector.The code im currently using to load my images and set the keypoints is this:These are the coordinates i wish to extract features from"
4508,Comparing the two feature sets,"['database', 'classification', 'svm', 'feature-extraction']","I am working on a classification of two feature sets derived from a dataset. We first obtain two feature matrices derived from two feature extraction methods. Now, I need to compare them. However, the recognition accuracy for two feature sets, reaches almost the same recognition accuracy (using 10-fold cross validation by SVM). My question is: Note: I already saw the similar questions in stackoverflow, however, I am looking for another approach. "
4509,Recommendations for clustering/tagging of short and domain-specific texts,"['python', 'nlp', 'cluster-analysis', 'feature-extraction', 'tagging']","I'm looking for some recommendations for a python project.Brief description:
Domain specific customer feedback should be clustered/tagged as fine-grained as possible. The available data come from two sources, in English language but with different characteristics:Source 1: Dedicated SurveyThe customer receives a questionnaire asking him if he has a problem in area XY and can also make a comment.Source 2: Web scrapingFree texts written by users.This information comes from the raw data, so it has not yet been pre-processed.Basics like pre-processing of texts are known to me, as well as other techniques like creating a Word2Vector model.Now I would be interested in your estimations:-Does it make sense to combine this data or would you focus on one source first?-What would be a conceivable approach to cluster the texts and tag/label them in the next step?Of course, it would also be good if already known clusters would automatically get a tag, while unknown ones would be shown for manual labeling.I don't expect a solution, but as I said before, only maybe hints what might work or not work at all. I am working on this topic as part of a bachelor thesis and am looking for new food for thought.I thank you in advance for your answers.Best regards"
4510,Feature importance in regression model,"['machine-learning', 'regression', 'random-forest', 'xgboost', 'feature-extraction']","I am working on a regression problem.
I fitted several algorithms including random forest and XGboost.
Both of them have ways that can show you the feature importance.
However, when I display the features in order, I get a slightly difference order of importance between the two.
Should I assume the more accurate model has the most accurate features in order of importance?"
4511,"Which are the better features to extract from audio for speech recognition, MFCCs or log filterbanks?","['audio', 'speech-recognition', 'feature-extraction', 'mfcc', 'keyword-spotting']","I am currently working on a keyword spotting project in the audio domain. When it comes to feature extraction, there are many approaches, amongst where MFCCs and log filterbanks are most popular. But, amongst these, which one is better? And why is so?Thanks in advance."
4512,Where can I get Code for Complete Local Ternary Patterns,"['image-processing', 'feature-extraction', 'spectrogram']","For feature extraction from images, I used CLBP so far. Since CLTP is more efficient than clbp, I would like to use it instead, where can I get code for it?"
4513,Problem with NormalEstimation for finding Normals in PCL,"['feature-extraction', 'point-cloud-library', 'normals', 'kdtree', 'keypoint']","I am trying to use Normal Estimation for finding normals at points in clouds so that I can pass it to the FPFH keypoint detector. Here is my code:-I am getting the following output:-Loaded 640x480=307200 
    [pcl::KdTreeFLANN::setInputCloud] Cannot create a KDTree with an empty input cloud!
    Normal size: 0As you can see input cloud has 307200 points. I am unable to understand why am I getting the 'empty input cloud' error?Here is my CMakerLists.txt:-"
4514,what is meant by empty rows as feature vectors in text analysis?,"['machine-learning', 'keras', 'nlp', 'text-processing', 'feature-extraction']","I am doing the movie review sentiment analysis using the data available for Kaggle dataset here using Python. https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/dataI do not have errors here but trying to understand why the rows collecting feature vectors are empty for some cases.After preprocessing my text such as removing stop words, missing data removal, removing punctuations I tokenize my text into sequences using the following codes.And when I check how does my X_train looks like I find it to be this way. My question is what should I understand from these numbers. Why are some of them empty?"
4515,"How do I calculate PSD, Median Frequency and Mean Frequency in python?","['python', 'signal-processing', 'fft', 'feature-extraction', 'frequency-analysis']","I am trying to extract frequency features from EMG Data on python with a sliding window. I do not have too much knowledge on frequency analysis, so I apologize in advance if I've got some wrong concepts.I am trying to follow the definitions from this website:MDFmore informationMy questions are whether I am performing the calculations right, and if not how can I proceed to calculate them? Average frequency which is calculated as the sum of product of the EMG power 
spectrum, and the frequency divided by the total sum of the power spectrum. Is this the right way to calculate mean frequency? "
4516,What metric should I use to compare two feature vectors of two images?,"['deep-learning', 'artificial-intelligence', 'similarity', 'feature-extraction', 'cosine-similarity']",I am looking for a way to compare the feature vectors of two images that contain a person in it. The application should be able to output a high similarity score for the same person and a lower score if the people in the photos are different. What I have tried so far: I have extracted two feature vectors using the VGG19 pretrained network. I gave as an input a picture of myself and the same picture but mirrored. I have tried using cosine similarity between these two feature vectors but it gives me a score of only 0.3 similarity. I have tried changing the layer from which I extract the activation map (after each max pool layer) but I can get only 0.5 for the same person but mirrored. It would even give me a higher score for different persons than for the picture with myself compared to the same picture but mirrored. Here you can see an example of an output after the block3_pool/MaxPool layer for both pictures: https://imgur.com/a/NZefxhL. What other metric I could use? I have also tried using Euclidean distance but works even worse than cosine. It gives me a bigger distance between the photos with myself than between photos of other people.Thank you!
4517,How to extract the pixels of a shadow in an image in order to calculate the length?,"['python', 'computer-vision', 'feature-extraction']","I'm taking a picture every minute of a sundial. I want to create a python program which calculates the altitude of the sun based on the length of the shadows captured in the image. Below is such an image.
 I tried to obtain the shadows by using multi-otsu thresholding on the V-channel of the image (I determined this channel contained the most information after plotting histograms of RGB and HSV channels). This results in the following division of the image for two classes:
I'm not sure how to treat the picture further, or if I should've treated it differently to obtain the shadows. I also don't know how to extract the length of the shadows if this is the best extraction of the shadows. I've read about blobs but didn't have success applying them yet. Any pointers on how to proceed / or how to best extract the shadows are appreciated.Here is some of the code:"
4518,Extracting representations from different layers of a network in TensorFlow 2,"['tensorflow2.0', 'feature-extraction', 'pre-trained-model', 'tensorflow-hub']","I have the weights of a custom pre-trained model. I need to extract the representations for different inputs that I pass through the model, across its different layers. What would be the best way of doing this?I am using TensorFlow 2.1.0 and currently load in the weights of the model using either hub.KerasLayer() or tf.saved_model.load() Any help would be greatly appreciated! I am very new to TensorFlow and have no choice but to use it since the weights were acquired from another source."
4519,Tracing the region of an Image that contributes to a location in the CNN feature map [closed],"['numpy', 'deep-learning', 'conv-neural-network', 'feature-extraction', 'vgg-net']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 3 months ago.I(x, y, no of channels) is the image, and Fi(x, y, no of filters ) is the feature map at some layer 'i'.
Given the architecture of a Convolutional Neural Network like VGGNet and a feature map after a certain layer Fi, is there an efficient way to find which pixels of the input image I, that contribute to a location in the feature map? 
I will want to implement this in python."
4520,Getting the column names chosen after a feature selection method,"['machine-learning', 'scikit-learn', 'feature-extraction', 'feature-selection']","Given a simple feature selection code below, I want to know the selected columns after the feature selection (The dataset includes a header V1 ... V20)I appreciate any help. "
4521,When to use which Feature Importance Method?,"['python', 'python-3.x', 'machine-learning', 'feature-extraction', 'feature-selection']","I am trying to find features of importance. I am using different models but each of them give me different results and I cannot wrap my head around why. I looked to see which assumptions work for each model but I am not able to find anything here either.I am using XGBoost, Logistic Regression, RFE, Permutation Importance, and Decision Tree. How can I test which one is the best? Is there any quality metrics I can use? Also, for some models the output does not map to the actual feature name rather to numbers like feature 0, feature 1, etc. How can I map those to my actual features?"
4522,What does Pearson correlation tell when features are uncorrelated,"['machine-learning', 'data-science', 'correlation', 'feature-extraction', 'feature-selection']","I have a dataset (31 features including the class). This dataset is about to be used for a classification problem. I thought to check the correlation between the features using Pearson correlation exists in pandas. When I set the Pearson's threshold > 0.5, I get the following:The result is:It turns out that all 30 features are not correlated at all. What does this mean? Is it always a good indicator that features are independent?Thank you."
4523,What is the most efficient way to store images feature vector?,"['python', 'mysql', 'django', 'storage', 'feature-extraction']","I'm working on project that needs to deal with images, I extract their feature vector instantly when any image uploaded then I store the feature vectors in MySQL database as text per each image.Also I'm using django framework.But looping on each image isn't a big deal as it takes more time as images increase.Also my feature vector in database stored like that:
0.0010601664,0.0003533888,0.8969008,0.0014135552,...Also is there a way to make MySQL database engine calculate similarity from feature vectors?"
4524,Why do I get some features as zeros when using autoencoder for feature extraction?,"['python', 'feature-extraction', 'autoencoder', 'dimensionality-reduction']","I have a dataframe with 67 features x 1031 samples. I would like to reduce the features to say two or three important ones. I am using Autoencoder for this purpose and to my surprise whatever encoding dimension I choose, I always get at least one of the predicted features as a set of zeros.My goal is to demonstrate advantages of Autoencoder over PCA with an example.Expected result: a fully populated output matrix 
Current result: an output matrix with some columns full of zerosHere is my code. And here is the input."
4525,How to select feature sizes,"['python', 'numpy', 'feature-extraction', 'feature-selection']","im trying to replicate an experiment on a paper using SVM, to increment my learning/knownledge on machine learning. In this paper, the author extracts the features and chooses the feature sizes. He, then shows a table where F represents the size of the feature vector and N represents the face imagesHe then works with F >= 9 and N >= 15 parameters.Now, what i want to do is to actually grab the features i extract as he does in the paper. Basically, this is how i extract the features:How do i select the amount of features extracted and stored? or how do i manually store a vector with the amount of features that i need? For instance a feature vector of size 9I'm trying to separate my features this way: Though, my output is:[0. 0. 0. ... 0. 0. 0.]for SVM classification, im trying to use OneVsRestClassifierThen, once i call prediction, i get:"
4526,Phrase extraction in python using CNNs,"['text', 'feature-extraction', 'cnn']","I was working on a problem recently that had a lot to do with automated feature extraction in text, and I decided to take a shot at using deep learning for it.I have read that feature extraction is exceptional using Convolutional Neural Networks, since they share weights among layers and data interaction is global as you go deeper.I was wondering how I would structure a network if I wanted to pass sequential (text) data, and get features (phrases in the text) as my output. Is this possible? If so, what should be my target label(s) and loss function?I was thinking along the lines of building a sentiment classifier and extracting the pre-final layers (which would normally be features in images). My hope is that the features extracted will be text phrases for text input, provided I have an embedding lookup to map them (like a custom word2id) .Is this the correct approach? The exact task I am working on is ABSA (Aspect Based Sentiment Analysis).Any help is appreciated in advance!"
4527,feature selection after extraction process,"['machine-learning', 'feature-extraction', 'feature-selection']","I read that it is recommended to use feature selection after feature extraction process.But there is something missing in all the posts I read:Suppose we have 50 features.Suppose we use feature extraction and we got 3 new extraction features2 questions:Do we need to run the feature selection on:orSuppose we run feature selection on the result of feature extraction and not all the new extracted features where chosen (i.e 2 out of 3) by feature selection algorithm, 
it seems that the output of feature extraction was not good enough if we can drop one of the 'new' dimension ? "
4528,DATA ANALYSIS _Feature extraction for driving data,"['data-analysis', 'feature-extraction']","I wanted a bit of help in a data analysis problem.
 Is there any algorithm/sample code that analyses a set of related data and point out abnormalities(feature differences-rash driving from normal driving) by itself without us setting some constraints?
 I am working on data coming from a vehicle, so I have data like speed,Engine RPM etc in different arrays, right now I am using clustering methods(both 1D&2D) to analyse the data. So are there any better methods available ?So Ideally we would like to give an unsupervised set of data and have the algorithm itself classify it to normal and rash driving."
4529,Python - Matching keypoints using feature vectors,"['python', 'matrix', 'matching', 'feature-extraction']","I'm trying to match keypoints of 2 meshes using their feature vectors and the euclidean distance as a similarity measure,What I tried and here is a simple instance of it, I made an example of the main data:So the point is that for each point in a I have a feature vector for it represented in fv_a and I'm trying to match it with bbut the results are like this:and this is not correct, since 0 should match 0, 1 should match 1, etc..What am I doing wrong, please?and yes, I'm trying to do one-to-one-matching. Any recommendations kindly?"
4530,How can I reduce extract features from a set of Matrices and vectors to be used in Machine Learning in MATLAB,"['matlab', 'matrix', 'vector', 'feature-extraction', 'dimensionality-reduction']","I have a task where I need to train a machine learning model to predict a set of outputs from multiple inputs. My inputs are 1000 iterations of a set of 3x 1 vectors, a set of 3x3 covariance matrices and a set of scalars, while my output is just a set of scalars. I cannot use regression learner app because these inputs need to have the same dimensions, any idea on how to unify them?"
4531,How to use spaCy’s Rule-Based Matching for sentences extraction,"['python', 'nlp', 'spacy', 'feature-extraction', 'information-retrieval']","Good afternoon, I am trying to use  spacy in order to extract french sentences from a list  to another containing a  specific pattern. the negative form  "" n'/ne + verb/aux/ + pas + ADV/ADJ/ROOT/NOUN.I tried to write a small code to test one pattern n'/ne + AUX + ADV but when I  used Spacy, Nothing is done/print. So where I am commiting an errorI do not understand why because I was following a tutorial 
I wanted to check the pattern and if the pattern is present in the sentence , I append it to ""sent_extract"""
4532,Dimensionality reduction using LDA for wavelet scalogram in python,"['python', 'feature-extraction', 'feature-selection', 'lda', 'dimensionality-reduction']","I am trying to reduce dimensionality of multiple scalograms having same dimension of size[5x3844].
How can I apply that using LDA in python ??Any help would be appreciated.code:"
4533,Deep Learning scipy.misc syntax error on imread.image.io,"['python', 'deep-learning', 'scipy', 'feature-extraction']","I have a piece of python code that is intended to extract letters and label each region that contains an image.
I'm using google colabI get the following error:NameError                                 Traceback (most recent call last) in ()
      1----> 2 image = imageio.imread('https://pbs.twimg.com/profile_images/985792111713947648/7YD1ZYpe_400x400.jpg')
     3 
     4 
     5 NameError: name 'imageio' is not defined```Heres the full code:"
4534,Textblob custom feature extractors,"['python-3.x', 'feature-extraction', 'textblob']","My requirement is to write custom feature extractors to satisfy both the words from training data and provide the results. I tried referring the document https://textblob.readthedocs.io/en/dev/classifiers.html#feature-extractors but could not understand.'''
    from textblob import TextBlob'''"
4535,Best way of matching the feature spaces in this classification problem using Tfidf and SVM,"['machine-learning', 'svm', 'feature-extraction', 'feature-selection']","I am training a model to detect spam/ham emails, and feature selecting by doing:The feature space contains both ham and spam features. I am saving the Tfidf model to then be used to predict a totally new, separate email, like this. But, on this new email, only half the number of features are created (because I am not adding spam + ham), and therefore the SVM classifier cannot predict anything.What is the best way of dealing with this, such that I have an equal number of features on the trained Tfidf model AND the new email?"
4536,Deep Learning syntax error in skimage.morphology,"['python', 'deep-learning', 'feature-extraction', 'scikit-image']","i have a piece of python code that is intended to extract letters and label each region that contains an image. edit: I'm using google colabI get the following error:i've tried back tracking versions of python etc but have not found a solution. Any help is apprietiated thanks.
full code:"
4537,Do es anybody how to GulpIO a Custom Video Dataset?,"['video-processing', 'feature-extraction']","I'm trying to use this package https://github.com/jonasrothfuss/videofeatures to extract features from the Protocol 4 of OULU-NPU dataset. However, first I have bring your dataset into the right 'gulp' format. The GulpIO documentation (https://github.com/TwentyBN/GulpIO) says that a have to implemente a custom adapter pattern for my dataset, inheriting from the abstract class:Has anybody implemented something similar for a custom dataset using GulpIO?
I understand I have to iterate the dataset and return a dictionary, however I´m not sure how to do it. Should I use a txt or csv with the path to each items, or should I iterate directly over the video files in the folder? Thank you for your answer"
4538,Feature documentation for a feature engineering python project,"['python', 'feature-extraction', 'documentation-generation']","I am building a python project that has a goal to be the feature engineering project (There will be 6 features that will after be used to train a model).     I was trying to think about how to make a catalog to document the hypothesis and discoveries from each feature. 
I thought about building a notebook, but in the notebook I think is better to leave just for visualization and exploration and have a catalog for the real feature documentation. I would like to receive any advice or ideas about how can I do this catalog.Thanks a lot!"
4539,How to read this Sparse Matrix in Python?,"['python', 'sparse-matrix', 'feature-extraction']","I have a feature vector file which stores feature vector in the following format.The no of non zero elements specifies the total number of non zero elements for that vector. It is the same as the number of ""[Dim 1] [Value 1]"" pairs.
Each ""[Dim 1] [Value 1]"" pair corresponds to a dimension of the vector which is non zero.My questions are
1) Is this a standard representation of feature vector? What is this representation called?
2) How I can load a file with such feature vectors in Python with the help of libraries if required?"
4540,Detect dotted (broken) lines only in an image using OpenCV,"['python', 'python-3.x', 'opencv', 'image-processing', 'feature-extraction']","I am trying to learn techniques on image feature detection. I have managed to detect horizontal line(unbroken/continuous), however I am having trouble detecting all the dotted/broken lines in an image.Here is my test image, as you can see there are dotted lines and some text/boxes etc.So far I have used the following code which detected only one dotted line.My output image is below. As you can see I only managed to detect the last dotted line. I have played around with the parameters rho,theta,min/max line but no luck.Any advice is greatly appreciated :)"
4541,Clustering images e.g. MNIST digits using classical feature extraction tools like openCV,"['opencv', 'image-processing', 'cluster-analysis', 'feature-extraction', 'unsupervised-learning']","This is more conceptual question, but I have been working on using unsupervised ML pipelines (PCA, t-SNE, fully connected and convolutional autoencoders) to perform dimensional reduction / feature extraction for images with complex patterns to attempt to cluster / classify them using KMeans. However, all these unsupervised pipelines have given me poor results to say the least. Ive explored the hyperparameter space for all these techniques (e.g. upto 100 dimensions for PCA / grid search for the autoencoders) to no success. The choice of clustering technique e.g. spectral / agglomerative clustering also doesnt improve the results.I would like to ask if it is possible to use a more classical image analysis workflow such as feature extraction using SIFT / other image feature extraction methods like those in openCV to perform a similar dimensional reduction and clustering of the MNIST digits in a manner like the scikitlearn PCA-KMeans example. Thanks for your help! "
4542,Fast way to feature extraction using python,"['python', 'optimization', 'raster', 'gdal', 'feature-extraction']","I'm testing different methods to extract average and standard deviation from a raster multi-channel image by adding them to a shapefile, but I've noticed that they are much slower than the zonal statistics on QGIS. I used rasterstats, rsgislib, pktools and in my case they took almost 2 days. Considering that some methods like rasterstats and QGIS allow me to extract one band at a time, but they're still very slow. Is there any way to speed up the extraction? Maybe use the GPU. There are programs like Orfeo toolbox that do this in a few minutes. In my case I have a shapefile with a million polygons from which I have to get the information. Maybe GDAL could be optimized for this kind of analysis.My code used so far is rasterstats below but in my case it's still very slow."
4543,Find all combinations of features,"['python', 'machine-learning', 'scikit-learn', 'feature-extraction']","I need to transform my binary coded feature matrix into a matrix that consists of all possible combinations of feature interactions. By all I mean literally all combinations (every set of 2, every set of 3, every set of 4, every set of all, etc).Anyone know if there is a way to do this with sklearn.preprocessing ? Or other libraries?Input this array into some function or method:And get this as OutputEach row in the new matrix represents [x1*x2, x1*x3, x2*x3, x1*x2*x3]"
4544,How to create multi-dimensional matrices as input to neural network?,"['python', 'tensorflow', 'deep-learning', 'feature-extraction']","I am trying to create a neural network with a multi-dimensional input matrix. Input could be of size 2x7, 8x7 or any such dimension with 7 columns. (This input is used in for loop structure shown below)My question is, How to create a training DataFrame that could contain multiple such matrices with different dimensions to feed the neural network? I tried training model on for loop for every matrix but there should be some more suitable method for creating such a dataset.Note: I am trying to get a single input with all such different dimensional matrices and that could easily map to their respective outputs. So input should look like (a, b, 7) where a is the number of data points that are matrices with different lengths of rows, b is the number of rows in that particular matrix and 7 is the number of columns in all the matrices.The data is an example of time-series data that users created by time. So I need to keep each row of single matrices in its order. And the output will generate the next time a user creates a new row. Please understand, the focus of this question is not the model but how to represent this data for the model.Here's the code to my model:My loop to fit model looks like this:I want to create some data structure which contains all my matrices. It would look like a 3-dimensional matrix. And all those matrices have different input shapes as explained above. But have the same output i.e, 1x7. Instead of using the loop structure, I want to pass a single input at the start of training."
4545,'numpy.float64' object is not callable while extracting audios features,"['python', 'numpy', 'audio', 'feature-extraction', 'feature-detection']","I am trying to extract some features in audio files and i have this error : ''numpy.float64' object is not callable'. I have seen this come up in other questions but others answers were in specific context and i don't really undestand what that means!first i use this code to create functions to extracte my features:and after i use these function to extract features on all the audio files like this:but i have this error : I don't undestand , can you help me please?"
4546,Machine Learning - How to extract features from pipeline,"['python', 'nlp', 'classification', 'feature-extraction']","I am totaly new to the field and currently I am stuck. Here is What I want and what I did:I have a Dataframe tht is solit in Train and Test dataset. The Training features are twitter messages, the lables are assigned categories. I set up a tokenizer (called clean_text) that keeps only relevant words and strips the messages down to the core information. The model including a grid search, that looks as follows:The fitting works fine, as well as the evaluation.
Not I am not sure, if the model is set up correctly and if the features are the most used tokens in the messsages (in the above case 50) or if there is an error.Now the question:
Is there a way to print the 50 features and see if they look right?Best
Felix"
4547,Python OpenCV - Canny borders detection,"['python', 'opencv', 'feature-extraction', 'canny-operator']","I m trying to extract borders of a sample (see figure below). The gradient between it and the air seems important so I tried to used OpenCV Canny function, but the result is not satisfying (the second figure)... How I could improve the result?You can find the picture here : https://filesender.renater.fr/?s=download&token=887799f6-f580-4579-8f75-148be4270cb0"
4548,How to extract a feature from OpenCV instead of skimage?,"['python', 'opencv', 'feature-extraction', 'scikit-image']","I'd like to use import cv2 instead of from skimage import feature with local_binary_pattern because GPU doesn't support skimage library and I need to use it in real-time which requires high frame per second.Full my code:for copyright, you can get it herePlease help me or any suggestions?Thank you in advance!"
4549,"When creating a new feature of similarity in ham vs spam case, should I include the similarity of spam with itself in the average of samp similarity?","['nlp', 'feature-extraction', 'feature-engineering']","I want to improve my model by adding a new feature column to my data, the data of ham and spam texts.
I have already created the square Cosine similarity matrix between all the texts, the diagonal of the matrix are 1s = cos(0).I extract all the spam text index in the training data, and I created the column of similarity, for each cell in the column, I add the individual similarity between this text and all the spam and average them.My question: for the text that is ham, it makes sense to do above. But for the text are spam, when calculating the similarity, should I exclude the similarity between itself? Will it causes data leakage?If we have n text of sample size, I represent the similarity value of ham_1 as 
 average(ham_1~spam_1, ham_1~spam_2, ..., ham_1~spam_n)My question is:For spam text spam_5, similarity value = average(spam_5~spam_1, spam_5~spam_2, ..., spam_5~spam_5, ..., spam_5~spam_n)OrFor spam text spam_5, similarity value = average(spam_5~spam_1, spam_5~spam_2, ..., spam_5~spam_5, ..., spam_5~spam_n)"
4550,How to extract CNN activations using keras?,"['python', 'keras', 'conv-neural-network', 'caffe', 'feature-extraction']","I want to extract CNN activations from the first fully connected layer using keras. There's such a function in Caffe, but I cannot use that framework because I'm facing installation problems. I'm reading a research paper that uses those CNN activations but the author is using Caffe. Is there a way to extract those CNN activations, so I can use them as items in transactions by using data mining association rules, apriori algorithm.Of course first I have to extract the k largest magnitudes of CNN activations. So each image will be a transaction, and each activation will be an item. I have the following code so far:"
4551,What is vggish_model.ckpt and vggish_pca_params.npz,"['tensorflow', 'artificial-intelligence', 'feature-extraction']","I am trying to understand some aspects of audio classification and came by ""vggish_model.ckpt"" and ""vggish_pca_params.npz"". I am trying to have a good understanding of these two. Are they part of tensorflow or google audio set? Why do I need to use them when building audio features? I couldn't see any documentation about them!"
4552,TSFRESH - features extracted by a symmetric sliding window,"['python', 'pandas', 'time-series', 'feature-extraction', 'tsfresh']","As raw data we have measurements m_{i,j}, measured every 30 seconds (i=0, 30, 60, 90,...720,..) for every subject j in the dataset.I wish use TSFRESH (package) to extract time-series features, such that for a point of interest at time i, features are calculated based on symmetric rolling window. We wish to calculate the feature vector of time point i,j based on measurements of 3 hours of context before i and 3 hours after i. 
Thus, the 721-dim feature vector represents a point of interest surrounded by 6 hours “context”, i.e. 360  measurements before and 360 measurements after the point of interest.
For every point of interest, features should be extracted based on 721 measurements of m_{i,j}.I've tried using rolling_direction param in roll_time_series(), but the only options are either roll backwards or forwards in “time” - I'm looking for a way to include both ""past"" and ""future"" data in features calculation."
4553,How can i fix dimension error between mfcc and adaboost classifier?,"['python-3.x', 'classification', 'feature-extraction', 'adaboost']","I wanna to feature extract 3450 audio file for train and 690 audio file for test. I wanna to classify this data to 6 classes. Y_train ((3450,6)) and Y_test ((690,6)) are label of train and test data. When i run the following code, I have the dimension error cause X_train after the feature extraction is (25,13). How can i fix this problem? "
4554,Value error: negative dimensions are not allowed when using spafe library,"['python', 'machine-learning', 'audio', 'feature-extraction', 'spectral']",I am working in audio files and I use this code to extract lfcc features:but I have this value error: negative dimensions are not allowedI think that it is because my sig array has negative values. Does someone know of a way to fix it?
4555,Tensorflow audio features extraction process won't run,"['python-3.x', 'tensorflow', 'feature-extraction', 'tf.keras']","I am trying to build my own audio TF dataset using some documentation from devicehive project. I have two PC machines, the same exact project works on one, but it doesn't work on the other! It  gives a warning about TF then says ""Using TensorFlow backend."", as shown below.Any idea about what would be the reason behind this issue?"
4556,import error when using spafe library for feature extraction,"['python', 'machine-learning', 'audio', 'feature-extraction', 'spectral']","I ' am working on audio file and need to use spafe library for lfcc, lpc... and i install the library as mentionned in the site : https://spafe.readthedocs.io/en/latest/But when i try to extract some features , like lfcc, mfcc, lpc, i have import error par example when i use this code : i have this error : I don't undestand because i can import spafe, i have all dependancies the libraries required with the correct versions ( numpy, scipy...)."
4557,What methods are there for extracting featues like local extrema and flat parts from a dataset?,"['c', 'algorithm', 'dataset', 'feature-extraction', 'data-extraction']","I need to extract specific predefined features from a list of datasets. 
Here are two exaple plots of the datasets I am using:My goal is to create an algorithm using C to extract the initial maximum, the time the maximum drops off, the width of the initial peak, the length of the flat part of the curve, etc..I don't need code to get these exact features from this exact dataset. I am asking for current techniques and methods that are being used in this type of problem. Different algorithms and approches that have been proven to work.Any leads in the right direction would be helpful. Thank you!"
4558,any way to select univariate features based on wilcoxon test in R?,"['r', 'statistics', 'r-caret', 'feature-extraction', 'statistical-test']","I intend to use care::sbf to do univariate feature selection, wheres my input is dataframe with mulitple variables (a.k.a, its columns), list of candidate features, and label (a.k.a, categorical variables). After I read caret package documentation, I tried of using sbf, sbfController to do feature selection, but I ran into an error down below:Error in contrasts<-(*tmp*, value = contr.funs[1 + isOF[nn]]) :
  contrasts can be applied only to factors with 2 or more levelscan anyone point me how to resolve this error? what's correct of using caret::sbf to do feature selection? any thought?reproducible example:here is the reproducible example on public gist where I used it as input.my current attempt:I googled this error but still couldn't get over it. Any idea to make the above code work? what's the correct way to do filter selection by using caret::sbf ?what I want is output dataframe must have selected features with its p-value attached to it. So here is my attempt:expected output:I am expecting output dataframe with selected features wheres its p-value returned by wilcox.test should be attached to corresponding features. any idea to make this happen in r? How can I operate feature selection using caret::sbf properly? any thought? here is my R sessioninfo:"
4559,How to get output of intermediate Keras layers in batches?,"['python', 'tensorflow', 'keras', 'deep-learning', 'feature-extraction']","I am not sure how to get output of an intermediate layer in Keras. I have read the other questions on stackoverflow but they seem to be functions with a single sample as input. I want to get output features(at intermediate layer) in batches as well. Here is my model:After training the model, in my code I want to get the output after the first dense layer (784 dimensional). Is this the right way to do it?pred = model.layers[1].predict_generator(data_generator, steps = len(data_generator), verbose = 1)I am new to Keras so I am a little unsure. Do I need to compile the model again after training?"
4560,How to add all of the NLP features to the dataframe,"['python', 'nlp', 'spacy', 'feature-extraction']",I first made dataframethen I was trying to add all of the NLP features to the dataframebut it is giving some error like-Can anyone help me?
4561,Is there a possibility to visualize intermediate layers in Keras?,"['keras', 'feature-extraction', 'visualize', 'densenet']",I am using the DenseNet121 CNN in the Keras library and I would like to visualize the features maps when I predict images. I know that is possible with CNN we have made on our own. Is it the same thing for models available in Keras like DenseNet?
4562,How do I standardize only int64 columns after train-test split?,"['python', 'pandas', 'feature-extraction', 'train-test-split']","I have a dataframe ready for modelling, it contains continuous variables and one-hot-encoded variablesAll the numerical variables are 'int64' while the one-hot-encoded variables are 'uint8'. The binary outcome variable is DEFAULT_PAYMT. I have gone down the usual manner of train test split here, but i wanted to see if i could apply the standardscaler only for the int64 variables (i.e., the variables that were not one-hot-encoded)? Am attempting the following code and seems to work, however, am not sure how to merge the categorical variables (that were not scaled) back into the X_scaled_tr and X_scaled_t arrays. Appreciate any form of help, thank you!"
4563,Feature extraction from Frequency Domain (FFT) of non-stationary shaft,"['python', 'cluster-analysis', 'signal-processing', 'feature-extraction', 'feature-engineering']","After filtering a signal from a non-stationary wind turbine shaft, I am left with 400 Time domain signals and 400 corresponding Fast Fourier Transforms. One FFT is shown below:I want to extract features from these 400 signals/FFTs to be able to perform clustering. I have extracted some features from the time domain already.What are some useful features to be extracted from the frequency domain? I have been looking through other posts and google scholar, but I can't seem to find any methods that stand out for my application."
4564,How to Remove Small Objects from Image after Segmentation,"['java', 'android', 'opencv', 'image-processing', 'feature-extraction']","DescriptionI have a lung cancer CT scan image, that I want to segment and extract the cancerous areas from. I have used Open CV and Java.I have the following image as input:After segmentation with thresholding and watershed method, I get this result:After that, I want to extract the cancerous area from the segmented image, so I have to remove all noise and other objects outside the region of interest (the cancerous nodule). So like shown in image below, I want to extract the cancerous nodule like this:How can I achieve this in android using OpenCV? "
4565,How to develop Time-Series models for multiple time series data columns?,"['python', 'time-series', 'signal-processing', 'statsmodels', 'feature-extraction']","I have multiple files (for now, just 3 files) that contain data collected from 3 sensors. The behavior of the signal is the same as an exponential sine wave and I would like to implement these signals into either ARMA, ARIMA, ARMAX or ARIMAX models into the signals. I have several questions:Question 1:I know how to implement Time Series Models individually but I am concerned about the model parameters like order, I am not sure how to find optimum model order to both models. From what I understood, the orders can be obtained from PACF(for AR model) and ACF(for MA model) where the order values come from the first bar that is closet to the critical limit lines. However, after I watched some videos about getting orders, they mentioned that the tallest bar can be considered an order for the PACF plot and from there I am confused. My question is the following, How can I find optimum order for the time series model? (Please note: I tried to use AIC, however, it takes a lot of time to process and find order where I increase the range of values. The code can be found below)The code below represents how I obtained the orders from observing the plots and inputting the order into the model.Full Code with Plots:Plots of ACF and PACF to get model orderPACF Plot
ACF PlotAR Model Order
MA Model OrderCode Section for finding optimum model order:Question 2:When I tried to implement the time series models into file 1 sensor 2, I get different value for the model order of AR Model (PACF plot), so my second question is, is there a way to implement single Time series models to all sensors and files?Question 3:When it comes to extracting features from the model, How can I approach this?Question 4:If I have to select one model from other Time Series models, how can I decide which model would be appropriate to use for a certain case?"
4566,How to implement feature hashing (hashing trick) for float/int valued features?,"['machine-learning', 'hash', 'feature-extraction']","I know how feature hashing works for categorial features. But what if the datapoint/sample has numerical features in additional to the categorical ones?Example:
height: 1.8Intuitively, I would expect the feature name ""height"" to be hashed and modded with the hashed feature vector length N to determine the corresponding bucket index, and then the value in that bucket is incremented by 1.8 (+/- 1.8, depending on if a sign hash function is also used).Is that how it works?"
4567,How does FAST detector initially find edge candidates? How do any of the edge and feature detectors intially find candidates?,"['opencv', 'computer-vision', 'signal-processing', 'feature-extraction', 'feature-detection']","I've been reading up on how FAST detector work on paper by Edward Rosten and others interpretations of it online on how the algorithm identify edge by comparing the candidate pixel I_p against the points around it + threshold through learned decisions tree and say if it's an edge or not.I get that part but what the all the explanation I've been looking at has been missing is how the algorithm initially finds this Key point candidate initially. This seems to be the case not only for FAST but any other key points detector and feature descriptors. All the papers and explanation of feature extractors/descriptor focuses on is how they are identified once they find candidates and does not explain how they initially go about finding the possible key points to look at.I've looked at the source code on OpenCV here
https://github.com/opencv/opencv/blob/master/modules/features2d/src/fast.cpp but honestly the code is pretty cryptic. I think what it is doing is just doing is a sliding window across the image by the FAST window or kernel size for the detector ring, which is 6 across it's diameter and just moving along to find some candidates.Is this how the detectors find candidates? Does it usually use sliding window? Does it use some other approach? Does other detector use different ways to initially find candidates? Are there other ways or algorithms to find edge or key point candidates?"
4568,sklearn RFE with logistic regression,"['scikit-learn', 'logistic-regression', 'feature-extraction', 'feature-selection', 'rfe']","I am trying to make a logistic regression model with RFE feature selection. And I get:How can I use rfe_model.support_ to extract the list of chosen features (subset the data frame) and make a model with only those features (except manually, by making a for loop ad subsetting the list of features itself)? Is there a more elegant way? Bonus question: Where can I find more info regarding feature selection for logistic regression (not including backward, forwards and stepwise method)? "
4569,Classify hand drawn shapes with opencv,"['opencv', 'feature-extraction', 'opencv-contour']","I have been coding with OpenCV lately,I have drawn on a paper few shapes (circle, triangle , rectangle, square, Parallelogram),
and would like to classify the shapes according to their characteristics.I have searched for contours and drew them, and moreover drew the minAreaRect for each shape.I tried using the function matchShapes but it didn't give me the result I had expected, 
I have tried using Polygon Approximations but it seems it to fail here too.(I went through all questions here before posting this question)Original image

After some processing
What should be the correct approach for classifying hand-drawn shapes?Any tips/suggestions would be great, I'm stuck :/."
4570,PCA analysis with the error memroy allocation is not enough,"['pca', 'feature-extraction', 'feature-selection', 'dimensionality-reduction']","I am going to do the features reduction using PCA. But the original number of features is too large.The related code includes:
For feature_selector libraryFor the usage of PCA:I always got this error:I have no idea how to solve it.By the way, I also want to know how you guys go through the feature reduction with large amount of features.Thanks!"
4571,Featuretools: Using features calculated in train data on new data,"['python-3.x', 'feature-extraction', 'feature-engineering', 'featuretools']","I was wondering how to use features developed in train time for prediction on new data. The dataset in question is the appointment cancellation dataset from Predict appointment no show, GithubConsider the feature locations.PERCENT_TRUE(no_show): the percentage of past appointment cancellations at a given location. Let us say I have new incoming test data with the same locations as in train data for which I already know these values. How can I use this feature in test data? Of course I can merge the test data to the train data and recalculate all the features using featuretools.dfs() but that is time consuming.Is there an easier way?"
4572,Preprocessing images before extracting ResNet features from pretrained model in PyTorch,"['python', 'pytorch', 'feature-extraction', 'resnet']","I need to extract ResNet features (output of final conv layer, just before the global average pooling layer). I found code here and here as followsIn Keras, I can do the same as below:The preprocess_input in Keras subtracts ImageNet mean from images and does some other preprocessing steps. Is it required in pytorch as well? If so, how to do that? Is there any inbuilt function for that?"
4573,Getting feature names from a pipeline with tfidfvectorizer,"['python', 'machine-learning', 'scikit-learn', 'feature-extraction', 'tfidfvectorizer']",I have been trying to get the feature names on my model for quite some time now but have a hard time understanding how to do it. I have tried many posts on here but can't get it to work. Here is my code:loading the classes I need to combine tfidfvectorizer with other featuresThen the code to put everything in a pipeline and run the regressorI can run the model.coef_ to get all the coefficients but I want to see how each item of the TEXT_COLUMN is affected by which weight. I have tried calling get_feature_names() or tried passing them in the pipeline but with no succes (most of google's results are purple by now).Anyone that can give me a bit of guidance how to pass the feature names to the end of the pipeline? The ideal result would be a dataframe with the feature (row from the TEXT_COLUMN) and feature_weight as value.
4574,"Is there any libraries available in python to extract the features of audio in wav format like meanfreq,median,sd,Q25,dfrange,modindex,sp.ent,meanfun","['python-3.x', 'machine-learning', 'audio', 'feature-extraction']","Iam trying to do gender classification using machine learning.I have audios in wav format.I need to extract featuresBelow one is answer I already got from stackoverflow::(but not helpful)I also thought you might be interested in estimation of mean frequency and some other audio parameters without using any special libraries. Let's just use numpy! This should give you much better insight into how such audio features can be calculated. It's based off specprop from seewave package. Check docs for meaning of computed features.This code isn't working and also I need other features like dominant frequencies,mod index,dfrange etc"
4575,How to correctly manage time values as a feature for machine learning?,"['python', 'machine-learning', 'time', 'feature-extraction', 'euclidean-distance']","The dataset contains a column Time that shows what time an incident occurred. The times are in the format hh:mm with 24h format, ex:My first thought was to assign an arithmetic value, ex:But this logic has a major drawback. It does not take into consideration that the times 23:57 and 00:20 are 'close' together, because 23:57 is 1437, while 00:20 is 20.Because of the modular arithmetic nature of the time, I thought of assigning the time two values, like xy on a circle. My int values are between 0 (00:00) and 1440 (24:00).
So the circumference is 2πr = 1440 => r = 230 and for each minute the arc length is 1. So the angle theta: s = r theta => theta = s / r => theta = minutes / rI define time to euclidean coordinates:With this logic if I get the euclidean distance of x,y values, the time 00:20 will be closer to 23:50 than 03:00, unlike my first approach.
Is this approach the correct way to go ?
Are there any other feature extraction methods for time values ?"
4576,Need help on Data extraction from documents,"['feature-extraction', 'extraction', '.doc']",is there a way I can extract data (either in tabular format or text format) from any docs (.doc or .PDF etc) to an excel.
4577,Is 'feature extraction' a core machine learning task?,"['machine-learning', 'deep-learning', 'computer-vision', 'feature-extraction', 'pattern-recognition']","I have been arguing with a friend about ‘feature extraction’. He says the main task of ML is to extract features. But I disagree. In a common-sense feature extraction is not an ML task. If we consider wx+b as the simplest way to represent ML, the task of ML is to find the best w and b. x is the feature. ML tries to find out the best w and b values for a given x, it matches with the training data and thus learns how to find w and b.My friend says it is the core task of ML to extract features. But as I know feature extraction is a data preprocessing task mainly."
4578,How to convert time series to feature vector in python?,"['python-3.x', 'time-series', 'feature-extraction']","i am new to timeseries problems and i had learned about time series analysis in python . but i want to know how to convert time series into feature vector in ?
i had tried nothing till now in practical
Thanks"
4579,Best approach to create inverse data for logistic regression 'predict variable (desired target),"['python-3.x', 'logistic-regression', 'feature-extraction']","I'm trying to think of a quick way to create 'inverse' for the purpose of training a logistic regression model.
I've got sales transactions (i.e isSold=1) and I want to  create the data rows for the isSold=0 equivalent for the 2nd date (as obliviously I dont have the data for that)... Using the example below, 
I'd need to create 4 new rows for 'ABC1','DEF2','GHI3','JKL4' with the below (NOTE that MNO5 isn't required as there is already data available for 2/2/20).Any/ all ideas are welcome - thanks."
4580,Extract object from image and apply it to different image,"['python', 'image', 'numpy', 'feature-extraction']","SHORT BACKGROUND: I am developing a neural network based model to differentiate benign and malignant ovarian tumors. I am working with transvaginal ultrasound images, some of which contains yellow markers as seen in the first image below. Without going into too mach details, there is a risk that the presence of the markers could introduce a bias that the network will be able to exploit. As a way of trying to counter this bias, I will insert semi-random markers to the images during training and validation as a type of image augmentation. PROBLEM:During training, I import the images (stored as JPEG-files) and convert them to NumPy arrays. I have some identical images with/without markers (e.g. first and second image below). After importing the two images as NumPy arrays the marker can be extracted byOption 1: ... subtracting one image from the other (see the third and fourth image below for the result). (Would result in incorrect pixel values for the marker.)Option 2: ... copying the image with markers and setting all cells to zero where the two images (with/without markers) are (almost) equal (see the fifth and sixth image below for the result).a) Let's say, for simplicity, that the only thing that I wanted to achieve was to apply the extracted marker to a different transvaginal ultrasound image. How would I do that?Simply overlaying the marker would create black patches around the marker.Simply adding the pixel values would result in incorrect pixel values for the marker.b)  (given that a) is solved)Let's say that I now also want to be able to insert a marker of a different size and orientation. Orientation: I could rotate and move the marker.
Size: Any suggestion of how I could go about doing this? I still want the marker to have the same properties, i.e. simply stretching is not an option.




"
4581,Titanic Dataset - Feature Engineering - Ticket feature,"['python', 'machine-learning', 'correlation', 'feature-extraction', 'feature-engineering']","I am currently building my first machine learning model using the titanic dataset.
After the data exploration, I decided to focus my attention on the 'Ticket' feature.
One thing I have noticed about this feature is that it is not unique per each passenger; this had led me to believe that other features can be extracted from this variable:The reason I am doing this is because I wanted to explore the nature of the relationship between the extracted features from Ticket and the target Survived (and later decide how to deal with outliers in the SibSp and Parch feature):Link to df: From the above table, I can see that group size 2/3 has almost a 60% and 70% chance of survival. Now, this led me to think that there is a correlation (or at least some sort of relationship between group size and Survived). Therefore, I decided to create a correlation matrix to make sure that is the case.link to df: As I expected there is a correlation between Is_Group and Group_Size (as they have been extracted from the same feature) but there is no correlation between these extracted features and Survived. Hence my, confusion. 
I thought given the high mean values of Survived for Group_Size (2,3) there was a relationship but clearly, I am getting something wrong here.Can anyone help clear this doubt of mine?"
4582,View/download Image Features Vectors in Turi Create,"['image-processing', 'feature-extraction', 'turi']",Is it possible to download the tables of feature vectors that Turi Create extracts after applying an image classifier to extract features?
4583,Dirty Image Quality Assesment Measure,"['image', 'image-processing', 'feature-extraction', 'noise']","I was looking for a way to measure (image quality score) how dirty is the document in image processing. Example images below:a. Dirty Backgrounds w/o TextCan someone give me an initial idea to start my analysis? I was thinking of converting it into a greyscale , then summing up the image intensity levels of the image, then dividing it into 255, then that would be the image quality score? Please help me with this. Thanks."
4584,how can i extract features from a document,"['machine-learning', 'nlp', 'data-science', 'feature-extraction', 'document-classification']","i have a document for which i want to extract features which are important .on that the selection should be such that each category has different columns or (descriptions).suppose if a category has 5 different selections . how will i know which is the important one ??How can i select the features based upon some specific rules so that when any other document is fed into the model ,it should pick the most important categories and their selections even if same category might have different selections as well?"
4585,"How to extract meta-data for str, int type using python in dataframe?","['python-3.x', 'dataframe', 'if-statement', 'metadata', 'feature-extraction']","I have a question about extract meta-feature to make metatable.
Metatable is composed to many statistics values.For example, I have a iris data.
It's consist of float type 4 columns and str type 1 column.
And I can make metatable using feature-extractor.And this is my code example, but str type value can't compute.
So I want to using if-else.Please reply for this problem. 
Thank you."
4586,Preparing MFCC audio feature- Should all WAV files be at same length?,"['machine-learning', 'feature-extraction', 'librosa', 'mfcc']","I would like to prepare an Audio-dataset for a machine learning model.Each .wav file should be represented as an MFCC image.While all of the images will have the same MFCC amount (= 20), the lengths of the .wav
files are between 3-5 seconds.Should I manipulate all the .wav files to have the same length?
Should I normalize the MFCC values (between 0 and 1) prior to plotting?Are there any important steps to do with such data before passing it to a machine learning model?Further reading links would also be appreciated."
4587,Why do Mel-filterbank energies outperform MFCCs for speech commands recognition using CNN?,"['deep-learning', 'conv-neural-network', 'speech-recognition', 'feature-extraction', 'mfcc']","Last month, a user called @jojek told me in a comment the following advice:I can bet that given enough data, CNN on Mel energies will outperform MFCCs. You should try it. It makes more sense to do convolution on Mel spectrogram rather than on decorrelated coefficients.Yes, I tried CNN on Mel-filterbank energies, and it outperformed MFCCs, but I still don't know the reason!Although many tutorials, like this one by Tensorflow, encourage the use of MFCCs for such applications:Because the human ear is more sensitive to some frequencies than others, it's been traditional in speech recognition to do further processing to this representation to turn it into a set of Mel-Frequency Cepstral Coefficients, or MFCCs for short.Also, I want to know if Mel-Filterbank energies outperform MFCCs only with CNN, or this is also true with LSTM, DNN, ... etc. and I would appreciate it if you add a reference.Update 1:While my comment on @Nikolay's answer contains relevant details, I will add it here:Correct me if I’m wrong, since applying DCT on the Mel-filterbank energies, in this case, is equivalent to IDFT, it seems to me that when we keep the 2-13 (inclusive) cepstral coefficients and discard the rest, is equivalent to a low-time liftering to isolate the vocal tract components, and drop the source components (which have e.g. the F0 spike).So, why should I use all the 40 MFCCs since all I care about for the speech command recognition model is the vocal tract components?Update 2Another point of view (link) is:Notice that only 12 of the 26 DCT coefficients are kept. This is because the higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them.References:https://tspace.library.utoronto.ca/bitstream/1807/44123/1/Mohamed_Abdel-rahman_201406_PhD_thesis.pdf"
4588,Features in Images Dataset,"['machine-learning', 'deep-learning', 'conv-neural-network', 'feature-extraction']","As it is known that there are several features in the dataset for the machine learning model. Do the dataset that has only pictures also contain features? As they can't be opened in excel file, do they contain features?My project is on PLANT DISEASE DETECTION USING DEEP LEARNING and my professor is asking about the features in the dataset. I don't know what to say."
4589,How Can I convert the extracted features into a feature vector?,"['machine-learning', 'nlp', 'classification', 'text-mining', 'feature-extraction']","I am working on a Classification Task in Python and since I'm way new into it I have a basic question. Any help would be appreciated. 
Suppose I have considered 2 features for my task: 1-Number of Positive and Negative words 2-Number of interjection word. After this feature extraction, I should convert text data to features vector and pass it to some Machine Learning Algorithms like SVM.  so I want to know :

How Can I convert the extracted features into a feature vector?
"
4590,Combining video feature vectors sequentially based on similiarity using deep learning?,"['machine-learning', 'deep-learning', 'cluster-analysis', 'feature-extraction', 'sequential']","I want to create a Scene Boundary Detection Model. For that input would be a long video and output would be different video segments.I have divided a long video into shots according to camera cut. Now I have feature vectors of each shots and I want to combine some shots sequentially.
These are the feature vectors.(shot1, shot2, shot3, shot4, shot5, shot6, shot7, shot8, shot9, shot10)Then combination be like(shot1, shot2, shot3 | shot4, shot5 | shot6 | shot7, shot8, shot9, shot10)I have heard about spectral clustering but not able to understand properly.
Can someone help me by giving appropriate technique, if deep learning based then well and good else any machine learning based technique, which combines the feature vectors of shots sequentially (it is not time series data it is sequential data) and also number of cluster are unknown too. "
4591,"How do I convert topics for each item in the dataset into a feature vector, considering that each item can have more than 1 topic","['python', 'machine-learning', 'feature-extraction', 'feature-selection', 'feature-engineering']","I have a dataset which contains english statements. Each statement has been assigned a number of topics that the statement is about. The topics could be economy, sports, politics, business, science, etc. Each statement can have more than 1 topic. Some statements can have 1 topic, some can have 2 topics, so on and so forth. For the statement itself I am using TF-IDF vectors to convert statement into a feature vector. However, I am confused how to convert topics into a feature vector for machine learning training."
4592,"how to work with sklearn pipeline, if features are already extracted?","['python', 'scikit-learn', 'feature-extraction']","Hi I'm learning about text classification and if I have a dataset like this one:My question: If I split training and testing set from the dataset, and do the feature extraction separately (I'm working with the word embedding). Is it correct to pass the features from the training and testing dataset (names: feature_array_trainingset and feature_array_testingset) to the pipeline directly this way:  It returns the classification result, but I'm not quite sure whether I'm doing the correct process or not."
4593,How to combine different models in Keras?,"['python-3.x', 'keras', 'conv-neural-network', 'feature-extraction', 'finetunning']","I have a pre-trained network, consist of two parts, the feature extraction, and the similarity learning. The network takes two inputs and predicts the images are same or not. The feature extraction part was VGGNet 16 with all layers freezed. I only extracted the feature vectors and learned the similarity network which consists of the two convolutional layers followed by four dense layers.Note: Removed last layers from the image due to large size. Now, I want to fine-tune the last convolutional block of VGGNet and want to use two different VGGNet feature extractors for each type of image. I have loaded the trained model and created a new model which starts from the Merged_feature_map layer: Now, the new model will only contain similarity network without the feature extraction part. I have loaded two VGGNets for each type of image and unfreeze their last convolutional block as: At that moment, I have 3 models and I want to combine them. The output from both VGG networks should be the input of Merged feature map. How to combine them and make them a single model. It should be like:"
4594,Scientific proof for usage of Var[x] = p*(1 - p) as good feature selection,"['scikit-learn', 'feature-extraction', 'feature-selection']","I am working with scikit-learn and I am trying to find in the papers they published some studies or proofs that the module VarianceTreshold, as seen here: https://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold is a scientific correct way to select features. I am sure it is correct, but for my university assignment I will need some references to why I am using scikit-learns VarianceTreshold for feature selection but I cannot find any references in their papers collection: http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.htmlThe above link is given in the citation section: https://scikit-learn.org/stable/about.html#citing-scikit-learnRight now I am only able to describe what it does but I do not have any studies or evidence that this is actually a successful way of selecting features. I am very thankful for any suggestions, even if they are outside of the scikit-learn world. "
4595,"How to design a shared weight, multi input/output Auto-Encoder network?","['keras', 'conv-neural-network', 'feature-extraction', 'autoencoder', 'siamese-network']","I have two different types of images (camera image and it's corresponding sketch). The goal of the network is to find the similarity between both images. The network consists of a single encoder and a single decoder. The motivation behind the single encoder-decoder is to share the weights between them. When I visualize the network, it show two-different networks. But what I want is a network which takes two inputs, for example, a camera image and a sketch image and returns same images by using a single encoder-decoder. Where I am doing wrong?"
4596,How to retrain Inception V4 model by unsupervised learning?,"['python', 'tensorflow', 'keras', 'feature-extraction', 'unsupervised-learning']","I was trying to retrain Inception-V4 with an image set to unsupervised learning.
First, I have read the pre-trained weights file for the Inception-V4.And I removed final full connection layer and dense layer to make the feature extractor.
Next, i added a new convolution layer to get special dimension feature vector form model.
And I saved a model and read again.The problem is to retrain this model with my image data set.
But I don't know the label for each images.
So, I have to retrain the model by unsupervised learning.
The size of image data set is over 130K.
How can I do retrain the model by unsupervised learning? "
4597,Trained an convoluted autoencoder. Now need help extracting the feature space,"['machine-learning', 'keras', 'feature-extraction', 'autoencoder']","I built an autoencoder using my own data set of about 32k images. I did a 75/25 split for training/testing, and I was able to get results I'm happy with.Now I want to be able to extract the feature space and map them to every image in my dataset and to new data that wasn't tested. I couldn't find a tutorial online that delved into using the encoder as a feature space. All I could find was to build the full network.My code: Here's my net setup if anybody is interested:Then my training:My results:Rather not share the images, but they look well reconstructed.Thank you for all and any help! "
4598,How to use logistic regression to predict whether a rental item will not be returned with variable length data?,"['machine-learning', 'logistic-regression', 'feature-extraction', 'feature-selection']","I am trying to predict whether or not a rental item will be returned.
I have tables forso I suppose I need to join them into one sample for each contract.But the contracts and items are a many to many relationship, and customer and contracts are one to many. So I need to use feature selection to get it all in the right shape, right?I see that I can make the number of items rented on a contract a feature.
But how do I include, for example, three different item ids and their respective quantities in one sample?"
4599,KeyError: 'Entity c does not exist in dfs',"['feature-extraction', 'feature-engineering']","when i try to run this code,i get such error，in function_wrapper(*args, **kwargs)
           38                     ep.on_error(error=e,
           39                                 runtime=runtime)
      ---> 40                 raise e
           41 
           42             # send return valuein function_wrapper(*args, **kwargs)
           30                 # call function
           31                 start = time.time()
      ---> 32                 return_value = func(*args, **kwargs)
           33                 runtime = time.time() - start
           34             except Exception as e:in dfs(entities, relationships, entityset, target_entity, cutoff_time,
  instance_ids, agg_primitives, trans_primitives,
  groupby_trans_primitives, allowed_paths, max_depth, ignore_entities,
  ignore_variables, primitive_options, seed_features, drop_contains,
  drop_exact, where_primitives, max_features, cutoff_time_in_index,
  save_progress, features_only, training_window, approximate,
  chunk_size, n_jobs, dask_kwargs, verbose, return_variable_types,
  progress_callback)
          225     '''
          226     if not isinstance(entityset, EntitySet):
      --> 227         entityset = EntitySet(""dfs"", entities, relationships)
          228 
          229     dfs_object = DeepFeatureSynthesis(target_entity, entityset,in init(self, id, entities, relationships)
           83 
           84         for relationship in relationships:
      ---> 85             parent_variable = self[relationship[0]][relationship[1]]
           86             child_variable = self[relationship[2]][relationship[3]]
           87             self.add_relationship(Relationship(parent_variable,in getitem(self, entity_id)
          124             return self.entity_dict[entity_id]
          125         name = self.id or ""entity set""
      --> 126         raise KeyError('Entity %s does not exist in %s' % (entity_id, name))
          127 
          128     @propertyhowever, this returned KeyError : 'Entity c does not exist in dfs'
any idea what's wrong with my code?"
4600,How to encode extremely high cardinal yet important categorical features in machine learning?,"['python', 'machine-learning', 'classification', 'data-cleaning', 'feature-extraction']","I am using machine learning to classify pairs of data into matching or non-matching classes. In each of those pairs of data I have a feature named company(along with 23 other features) which contains n number of company names on one side and same company's abbreviation on other side. So the pair looks like below:Now as you can see I cannot use one-hot encoding for company since there are innumerable companies(high cardinality) and their abbreviations. In this case how do I encode this categorical feature? 
Also generally in Machine Learning high cardinal features are not that important and can be easily discarded but in my problem it is very much important. Any help on this will be valuable. 
Thanks"
4601,Tensorflow TFX pipeline: where is a right place for features engineering?,"['tensorflow', 'tensorflow2.0', 'feature-extraction', 'feature-engineering', 'tfx']","Let's assume that we have raw data in two tables/files:First table contains e. g. Customer description - name, id, sex, age, etc.Second file contains payment history for each customer.These tables have a different structure.I want to use the data from the second table for generation additional features.The question is where should i do it?OrIn a first case I generate a redundant big table, in the second (if the second case is possible at all) I will slow the pipeline execution.Do you know what is the correct approach to use tfx?Thank you."
4602,How does Weka perform the ZeroR learning scheme for feature selection?,"['weka', 'feature-extraction']","I performed a wrapper random search feature selection with Weka as follows:Evaluator: weka.attributeSelection.WrapperSubsetEval -B weka.classifiers.rules.ZeroR -F 5 -T 0.01 -R 1 --
Search:weka.attributeSelection.RandomSearch -F 25.0 -seed 1My question is about how does it work, since Zero Rule returns just the mean or mode of the features, how can it be used as a predictor of the output variable?
The explorer returns the following:Attribute Subset Evaluator (supervised, Class (numeric): 5 Score):
    Wrapper Subset Evaluator
    Learning scheme: weka.classifiers.rules.ZeroR
    Scheme options:
    Subset evaluation: RMSE
    Number of folds for accuracy estimation: 5How does it work as to perform an RMSE ZeroR learning scheme? I'm not able to get the same features by simple calculation."
4603,Visualize 1D CNN Feature Importance for Time Series Sequences,"['python', 'keras', 'time-series', 'conv-neural-network', 'feature-extraction']","I am trying to extract feature importance from my 1D CNN. Most of the online documentation refers to 2D, 3D, image data and classification problems. I have a multivariate time series that outputs time series sequences. I have tried Shaply and keras_vis, but nothing addresses my issue. One issue is that my input data has 229 features and the first layer of the 1DConv maps 64 filters. Extracting the weights from the first layer, I can determine what filter is contributing to the learning of the layer. However, I cannot translate this to the original output. My question is two-fold (if I can do both):Here is the summary and code used to extract the weights....First Layer of Mean Weights"
4604,How to extract features from only the segmented part of the image for clustering?,"['python', 'machine-learning', 'image-segmentation', 'feature-extraction', 'feature-selection']","I am able to extract and flatten the features from a certain image. However, let's say each of these images has a segmentation mask and I only want to extract features from the segmented part of the image, and then cluster the images based on these features.It does not seem correct to me to extract the features for the whole image, and then apply a mask to zero anything outside the segmentation mask, because segmentation masks could be larger or smaller depending on where the item is in the image.Pseudocode for feature extraction and clustering below (ref: here). How would I add a segmentation mask to this?"
4605,Hot-Encoding only on some elements of a column,"['python', 'pandas', 'feature-extraction', 'categorical-data', 'one-hot-encoding']","On my dataset I have many columns with mixed categorical and numerical values. Basically when the numerical value was not available, a code is assigned, like 'M', 'C', etc.. associated to the reason it was missing.
They have special meaning and peculiar behavior, so I want to cast them as categorical, and keep the rest as numeric.
Minimal example:Question 1.
Q1: How this can be done efficiently? Ideally I need to chain it in a Pipeline, some fit_transform kind ot thing? I have to write from scratch or there is a hack from common libraries to hot-encode a subset of a column, like ['a', 'b', 'else'] ?Question 2.
Q2: How should I fill the 'Nan' for the CName_num? The categorical elements ('a' and 'b' in the example) have behavior that differ from the average of the numerical (actually from any of the numerical). I feel assign 0 or 'mean' is not the right choice, but I ran out of options. I plan to use Random Forest, DNN, or even Regression-like training if it performs decently."
4606,Feature map preprocessing for visualization of intermediate layers of CNN,"['neural-network', 'conv-neural-network', 'feature-extraction']","I am trying to visualize the intermediate layers of my CNN and came to this tutorial in internet. In this code the feaure maps are being preprocessed before ploting: What I don't understand: Why after normalizing, it's getting multiplied by 64 and added to 128?"
4607,How can I generate statistical features from all values in a column from dataframe indexed by a sorted timeseries in Pandas (Python)?,"['python', 'pandas', 'time-series', 'feature-extraction']","I'm given a child birth data from a hospital, and asked to perform certain tasks on it :timestamp ethnicity gender body_massand I need to generate statistical features for every value in 'ethnicity' after every 10minutes.Please help I  am a beginner and stuck on this problem for a day now"
4608,How to find number of lags in autocorrelation and partial autocorrelation?,"['python', 'machine-learning', 'time-series', 'feature-extraction', 'autocorrelation']","I am trying to using the tsfresh package in python in order to extract feature from accelerometer sensor input and I came across several features where they need lag variable:So, I would like to know how to find the lag variable?Kind regards"
4609,Random Matrix Theory — Feature Extraction — Classification,"['machine-learning', 'classification', 'feature-extraction', 'feature-selection']",What would be a great first step/approach towards using random matrix theory to perform feature extraction from a random sample covariance matrix and later on perform event classification?
4610,Extracting feature of a time series signal using wavelet,"['python-3.x', 'artificial-intelligence', 'feature-extraction', 'wavelet']","I am working on a project and should use the wavelet to extract the features from the load time series to reduce the dimension of the input vector of the ANN. for example, I have a data of 40 days(from t to t-960) and make an input vector like [t , t-1,..., t-168] around 10 to 20 features. anybody can give remarks on how can I use Wavelet to achieve this goal?"
4611,Mapping timeseries+static information into an ML model (XGBoost),"['machine-learning', 'time-series', 'xgboost', 'feature-extraction']","So lets say I have multiple probs, where one prob has two input DataFrames:Input:Output:So how can I suitable train XGBoost to be able to make the best usage of one timeseries DataFrame in combination with one static DataFrame (containg additional context information) in one prob? Any help would be appreciated."
4612,Feature selection and reduction of time series data,"['machine-learning', 'time-series', 'feature-extraction', 'feature-selection', 'feature-engineering']","I have a use case of time series data around 25 years with more than 100 features affecting the target. The target is the price which has been converted into categorical to know the downtrend and uptrend. My goal here is to select the best features from 120 to 30 important features that would be impacting the price more.
I have tried a few methods and techniques from this book https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089 but these methods give different features everytime. I would not want to rely on a few methods and reduce the features. I would like to see the evidence from approximately 10 tests that prove that a particular feature is indicative. I have also tried basic stats linear tests but looking for any other techniques that might help. Any help is appreciated!Thanks!"
4613,Pre-rank features in Machine Learning,"['machine-learning', 'feature-extraction', 'feature-selection', 'feature-engineering']","I am working on a text classification problem where I would like to assign ranks or weights to keywords or n-grams before training the model. I know tree-based models can provide me with the importance of the features, but I am looking for something that can assign weights before I train my model(Or I would like some feature to be more important than the others)"
4614,How to make feature vectors size equal for training neural networks?,"['python-3.x', 'wav', 'feature-extraction', 'mfcc']","I am training a neural network, but the feature vectors do not have the same size. This problem may be fixed by adding some zeros or removing some values, but the greater problem would be data loss or generating meaningless data.So, is there any approach to make them equal size, without mentioned weaknesses? Maybe transformation to other dimensions?
I do not want to use random values or ""NA""."
4615,Fast way to get vector of smallest distances in openCV feature matching,"['python', 'opencv', 'feature-extraction', 'feature-detection']","I am using the python version of openCV to develop a ""similarity score"" for two images.
First I find the keypoints and their descriptors for each image (e.g. using ORB or AKAZE).
Then I match these keypoints across the to images and save these matches in a list matches.
For every match x in matches I can get the distance between the corresponding key points via x.distance.My similarity score should now be the sum of the k smallest distances found in the matching algorithm. Obviously, I could get this score as (assuming k < len(matches)):However, this loop is very slow. Is there a vectorised version of .distance that I could use to transform matches directly into a list of floats?Clearly, there are also other ways of doing this. But they are all quite slow (as in, they mean that I loop over the matches in some way).Thank you!"
4616,How to Improve Classification Accuracy with Support Vector Machine,"['matlab', 'machine-learning', 'classification', 'svm', 'feature-extraction']","I have 7 classes of inputs that are related to the brain signals activity (EEG).
When the number of classes is large, the performance of classification algorithms may be affected.
As you can see in the following code, I extracted the features for them and in the first phase I trained my model with 70% of the my data and got 100% accuracy but in the testing phase with the remaining 30% I did not get more than 42.5% accuracy. What is your suggestion to improve the accuracy of my Model?"
4617,R auto compute features log and poly data matrix,"['r', 'formula', 'feature-extraction', 'feature-engineering', 'poly']","I'm writing my self-define function. The function takes a dataframe input with unfixed number of features. Also, the features' type maybe different,i.e. numeric,factor and chr. I want to maximise my likelihood function, which is support on an extended data matrix, with each features' log transformation and up to quadratic orders, e.g. columns interception + log(feature1) + feature1 + feature1^2 +  log(feature2)+..+ feature1*feature2 + ... + feature_{n-1}*feature_nTake bulit-in dataset iris as an example：Code:Out:As we can see, the first 4 features,from Sepal.length to Petal.Width,  are numeric. I want to bulid a model on their log to quadratic orders. So I want to output a data matrix like following:Code:Out:The problem is that using poly to type formula from scratch is not wisely, especially when we have hundreds features! My function should treat dataframe automaticlly. I know model.matrix can extend my original dataset, like iris. model.matrix can even auto dealing with factor and chr features, converting them to dummy variables. And poly can extend feature to high orders but do not provide log transformations.My question is how to get log and up to quadratic order transformation of any given dataframe automatically. I want to share my new model with others, so I think my function should be replicable on any others' data sets."
4618,Explanation of feature descriptors in computer vision and machine learning,"['machine-learning', 'image-processing', 'computer-vision', 'feature-extraction', 'feature-descriptor']","I've started working with computer vision techniques quite a bit, mainly deep learning but I want to try and get a good understanding of the more traditional techniques as well for a good grounding. I have been playing around with some manual feature engineering techniques for classification with RF and SVM classifiers. I've looked at texture representations like HOG and LBP descriptors as well as edge filters, gabor filters and spacial features such as fourier descriptors. What i'm kind of lacking is a good idea of how the different features group and what categories they each belong to. I know some are defined as global and local but what does this mean exactly and which ones? and are there others categories like texture and geometric that I should consider? Any explanation would be useful and much appreciated (i've looked a lot online but it all seems a bit fragmented)Thanks!"
4619,Unable to extract features from time series data using 'extract_features' method from tsfresh package,"['python', 'anaconda', 'time-series', 'feature-extraction', 'argument-matching']","I am running the code in Spyder(3.3.3) from Anaconda3 2019.03(Python 3.7.3 64-bit).
And using tsfresh 0.11.1The code I'm running deals with a huge set of time-series data that has sensor data(data of 17 sensors in 17 different files.(dataset url : https://archive.ics.uci.edu/ml/datasets/Condition+monitoring+of+hydraulic+systems)The problem is that the execution is getting stuck at the feature extraction step. It is using extract_features method from the tsfresh package to extract features from the data. The 'column_id' and 'column_sort' parameters are of 'str' datatype by default. So, it is showing the following error at that step:TypeError: unsupported operand type(s) for /: 'str' and 'int'And when tried to execute only one file out of 17 files, it is showing a different error:TypeError:Cannot cast array data from dtype('float64') to dtype('U32') according to the rule 'safe'The code url:
https://github.com/zhou100/SensorDefaults/blob/master/Detecting%20and%20Compensating%20Sensor%20Faults.ipynbThanks"
4620,Bad formation of VectorAssembler giving unwanted values into features,"['pyspark', 'pyspark-sql', 'feature-extraction', 'apache-spark-ml']","I've used VectorAssembler many times which worked well. But today I got unwanted data into features as shown in below in figure.Input is 4 features without NaN which are from pySpark data frame.Why I've got (5,[0,1] before every rows in features column, is this normal?
Does it affect learning?
"
4621,Best way of extracting features from SuperPixels such as gabor and HOG features,"['computer-vision', 'image-segmentation', 'feature-extraction', 'gabor-filter', 'superpixels']","I've used the slic clustering algorithm to create superpixels of a biomedical image (whole slide image for the biomedical imaging experts). I want to extract different features, texture and spacial for the superpixels to create a feature representation and then feed that into a classifier (SVM, RF) to try and classify each superpixel as I have the labels for each one. The end goal is to to classify every superpixel and then use this to build a segmentation. For each superpixel I draw a bounding box around it with a consistent size across all based on the average height and width of all superpixels since the distribution of sizes is fairly peaked around the average (some will have small parts cut out and others will include some padding. I have a couple of questionsIn regards to the gabor filter for each superpixel I get a gabor feature with a value for each of its individual pixels , I then take the average of these to get a superpixel gabor feature value. Is this the right approach? code belowHow would this work with HOG? would I take the same approach of averaging the feature vector and how would I keep the HOG descriptor from being too large?Would it be sensible to feed the superpixels into a CNN to learn the feature representation?If anyone has worked with this kind of data before any suggestions over other useful image feature descriptors that would be a good approach for the type of data?Any advice on building the features or what type of features to look at for superpixels would be much appreciated!Thanks"
4622,moving every other row to a new column and group pandas python,"['python', 'pandas']","i have an example data set that's much smaller than my actual data set, it is actually a text file and i want to read it in as a pandas table and do something with it:example dataset looks like this:i added a second column called 'value':i want to first move every other row over to the 'value' column :i want to then group by the title name, since there might be more than 1 values for the same title :"
4623,GTSRB Features from pretrained VGG16 - bad results,"['keras', 'feature-extraction', 'vgg-net']","I want to use the pretrained VGG16 net for feature extraction
of the GTSRB dataset.
The extracted features are used as input for a dense-layer with 512 nodes and dropout 0.5, trained for 20 epochs.
The results are very bad, I get only about 0.4 accuracy.
I wonder if the pretrained vgg16 is not suitable as pure feature extractor for the gtsrb data set?Has anyone had similar experiences?"
4624,How to extract features from FFT?,"['python', 'scipy', 'fft', 'feature-extraction']","I am gathering data from X, Y and Z accelerometer sensors sampled at 200 Hz.  The 3 axis are combined into a single signal called 'XYZ_Acc'.  I followed tutorials on how to transform time domain signal into frequency domain using scipy fftpack library.The code I'm using is the below:Then I plot the frequency vs the magnitudeScreenshot:My difficulty now is how to extract features out of this data, such as Irregularity, Fundamental Frequency, Flux...Can someone guide me into the right direction?Update 06/01/2019 - adding more context to my question.I'm relatively new in machine learning, so any feedback is appreciated. X, Y, Z are linear acceleration signals, sampled at 200 Hz from a smart phone. I'm trying to detect road anomalies by analysing spectral and temporal statistics.  Here's a sample of the csv file which is being parsed into a pandas dataframe with the timestamp as the index.In answer to 'francis', two columns are then added via this code:'XYZ_Acc_Mag' is to be used to extract temporal statistics.'XYZ_Acc' is to be used to extract spectral statistics.Data 'XYZ_Acc_Mag' is then re sampled in 0.5 second frequency and temporal stats such as mean, standard-deviation, etc have been extracted in a new dataframe.  Pair plots reveal the anomaly shown at time 11:01:35 in the line plot above.Now back to my original question.  I'm re sampling data 'XYZ_Acc', also at 0.5 seconds, and obtaining the magnitude array 'fft_mag_values'. The question is how do I extract temporal features such as Irregularity, Fundamental Frequency, Flux out of it?  "
4625,i am trying feature extraction and want vertical stack of array but this is error i am getting. ValueError: setting an array element with a sequence,"['python', 'feature-extraction']","this is my code and this line is giving an error unscaled_x = np.vstack((abnormal_features, noabnormal_features)).astype(np.float64)
i tried to solve this but i failed if someone knows hot to get rid of this then please help me outand error is below"
4626,Random Forest in R - Feature extraction method?,"['r', 'machine-learning', 'data-science', 'random-forest', 'feature-extraction']","I'm working on a university project where I need to build a Random Forest model in R to predict if patients have depressive tendencies according to their EEG-data. I already preprocessed the data and built a general model for the forest. Currently, I'm fine-tuning it to receive the best possible prediction Accuracy. If I understand that correctly, I need to do some Feature Extraction (At this moment, I have 1584 Features) and tuning of the hyperparameters. But I'm not sure how to perform the Feature Extraction in R? Right now, I'm doing this:But I get the feeling that this isn't a good approach and very subjective. 
Does anybody have an idea to improve my model? My idea was to perform the Feature Extraction first and then optimizing the hyperparameters. It that common or advised to do it like this? I'm grateful for every input :)EDIT: My specific problem is: I have a way to filter the variables according to their importance. But this approach seems to be very subjective and not quite logical. And my Accuracy varies after every iteration (mostly it stays between 75% - 83% of an Accuracy. But it still takes huge leaps and sometimes falls into values of 58% or 91%.). So my question is if my approach is valid to extract features? Because after extracting the features the changes in Accrucacy are not quite linear (like instead of simply decreasing or increasing, it jumps around after every iteration of the for-loop)?CSV of my dataset: https://drive.google.com/file/d/1k02hyqU51cAy5ka1gs5Ydr5_gOM5vTew/view?usp=sharing
(The last column is my prediction class (0 - Non_Depressive, 1 - Depressive) (All other columns are the EEG Channels, sliced by 99 to have a finer look at their frequency bands. All of these 99 instances are ranges (0 - 0.5, 0.5-1, 1-1.5....))Adding the results of the model:Confusion Matrix and Accuracy (Currently 43 rows with 224 Features)Variable Importance by Rank"
4627,How to get a color-histogramm of the hue-values by using skimage instead OpenCV?,"['python-3.x', 'opencv3.0', 'feature-extraction', 'scikit-image']","I want to get color histograms of RGB images. It should be a global histogram of the hue values in HSV color space, resulting in 256 features per image.My code in skimage is as follows (I'm not sure if this code is correct):I am very unsure whether this is the right way to create color histograms of the hue values.When I print out the histogram print(rgb_to_hsv), I get completely different results than with the following code in OpenCV:Ultimately, I would like to combine the color histogram created with skimage with hog features, which were also extracted by using skimage.Can someone give me a feedback whether my code in skimage is correct and whether I can directly concatenate the color histogram with the hog features?UpdateOutput:[114   6   7   7  10   9  10  16  26  38  73  48  31  26  19  21  18  12
    24  15  14  20  15  10  15   7   7  12  18  13   5   3   8   1   4   1
     0   1   2   1   1   2   0   5   0   0   0   0   1   0   1   0   1   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0
     1   0   0   0   2   0   0   0   0   0   0   1   1   0   0   1   1   0
     4   2   1   2   2   2   4   4   4   3   5   4   3   3   3   1   2   0
     0   1   4   2   0   0   0   2   0   0   0   1   0   2   0   0   0   2
     4   1   1   1   1   4   0   2   2   1   0   1   3   2   1   2   2   1
     1   5   4   1   2   2   3   2   1   1   2   0   3   1   2   2   6   5
     7   9   5   2   4   6  11  14]Output:[109.   0.   0.   0.   1.   1.   0.   0.   0.   0.   2.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   1.   0.   6.   0.   4.   1.   1.   3.   4.   3.
     7.   9.  18.  27.  12.  13.  15.  31.  18.  26.  25.  31.  26.  39.
    69.  90.  29.  24.  16.  13.   5.   4.  45.  10.   6.   6.   8.  11.
     9.   0.   3.   2.   6.   2.   3.   2.   0.   7.   2.   2.   4.   2.
     3.   0.   0.   3.   2.   2.   2.   2.   3.   0.   6.   0.   0.   1.
     0.   2.   1.   0.   1.   0.   2.   1.   2.   1.   3.   5.   5.   4.
     5.   5.   7.   4.   3.   2.   2.   5.   2.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
     0.   0.   0.   0.]Update
I used the skimage version 14.1 for extracting HOG features with the following code:Output:[0.1264151  0.06452642 0.01920705 ... 0.02537039 0.03174189 0.01708564]Code for hue-histogram Output:[309  49  19  21  12  13  16  12   9  14  17  12  12   6   9   7   8   8
     8  12   5   9   5  13   7  11   8  11   7   7   8   6   4  11   5   5
     4   5   4   7   5   6   9   6   9   6   4   7   8  10   6   7  10   8
     6   6  11  10  13   5   5   6   7  11   6   5  12   5   8   5   6   7
    11   3   5  11  14  13   9   5  11   5   5  15   7  11   8   9  11  11
    10   8  10   9   6   8   8   6   8   9  10   6   5   6  10  11  10   7
     9  10   6   8   6  12   5   5  10   8  11  15   9  12   8   4   4   7
     3   7   9   5   5   6   2   3   6   8   6   7   1   8  10   8   4   5
     3   4   4   5   7   3  11   6   9   8   6   6   8  13   6   9  10   9
    12  11  10  13  10   9   8   9  20  13  13  16   9  12  15  17  13  18
    12  14  14  17  11  14  13  23  15  16  17  17  17  20  10  18  17  11
    13   9  24  24  20  10  23  17  15  14  14  10  16  14  15  18  16  12
     9  13  15  18  10   7   7   4   4   9  11   8  13  11   9   7   8   5
     6   7   6  10   3   7   7   5   3   2   3   1   3   1   3   3   1   3
     5   1   2  22]May concate the two features like follows:Would this be correct?"
4628,Conditional Replace Pandas,"['python', 'pandas', 'replace', 'conditional-statements', 'series']","I have a DataFrame, and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:If I copy the channel into a new data frame it's simple:This does exactly what I want, but seems not to work with the channel as part of the original DataFrame."
4629,how to make face graph in elastic bunch graph matching model for face recognition,"['python', 'feature-extraction', 'face-recognition']","As I get from different papers about EBGM, there are two key elements “node” and “edge”.
Node is bunch of jets that each jet is Gabor wavelet of specific area in an image (5 fiducial points).
Edge is average distance between fiducial points.As I know, I should make a dictionary with bellow schema for each face}My questions are•   how to store edges data in this dictionary? As edge is an average of all distance (normally is a number) how should I store it in face set or vector?•   is there a library that already make function for EBGM?i tried to find answer from past question but seems nobody faced with such case, the only post that i could find is below link 
how to apply face keypoint to face recognition?for more information, as school project i should train a face recognition model by EBGM method.i appreciate your help
thank you"
4630,Most of the entries in encoder output of an AutoEncoder is zero while used for feature extraction from high-resolution images,"['python', 'tensorflow', 'keras', 'feature-extraction', 'autoencoder']","I am using a simple Autoencoder to extract meaningful features from high-resolution images (2K).My simple AutoEncoder looks like:Models:fitting the end-2-end autoencoder model:With this encoding_dimension or intermediate hidden size, most of the entries in intermediate representation or encoder output are zero.Can anyone please tell me why most of the entries in encoder output or prediction are zero (i.e. doesn't provide any meaningful features as such)?"
4631,Is there a better way/ function I can use to get the new column other than using apply function here?,"['python-3.x', 'pandas', 'feature-extraction']","I am trying to classify customers into segments based on RFM Scores and below is the function I am using in Python. The dataset contains 950K rows and 4 columns. I am new to python and have been a heavy R user. The below code takes 5 mins to run in python and the similar code takes less than a minute in R.
Any help would be greatly appreciated. 
How can I reduce the runtime here? Is there a better way of doing this?"
4632,PCA analysis incites memory allocation problem. How to solve this without reducing image resolution or number of images,"['python', 'image', 'pca', 'feature-extraction']","using PCA (principle component analysis) to extract features from a set of 4K images giving me the memory errorFile ""/home/paul90/.local/lib/python3.6/site-packages/sklearn/decomposition/_pca.py"", line 369, in fit_transform
    U, S, V = self._fit(X)MemoryError: Unable to allocate array with shape (23339520, 40) and data type float32I am trying to extract 30 features (# of components) from 4K images and getting this error.immatrix = np.array([np.array(Image.open(im, 'r')).flatten() for im in file_list], 'f')x = StandardScaler().fit_transform(immatrix)pc_train = pca.fit_transform(x)Filelist is list of images (currently I have 600 images)I can't reduce the number of images in the list and can't reduce the initial 4K resolution. In this context,how can I solve this memory allocation issue?It will be a great help if anyone can tell me the steps to avoid the memory issues."
4633,Gabor filter with 2 scales and 8 orientations with skimage,"['python-3.x', 'image-processing', 'feature-extraction', 'scikit-image', 'gabor-filter']","I try to understand the Gabor filters and I have problems how to implement this filter with
skimage.
My questions is probably very simple to answer, but I'm a beginner in the field of image processing. I want to extract features from images by a Gabor filter with 2 scales and 8 orientations. 
I don't know how to set the parameters frequency and theta?Do I have to set theta to 22.5?Thank you in advance for every tip."
4634,Selection of most appropriate features from Boolean features data-set,"['machine-learning', 'boolean', 'data-science', 'feature-extraction', 'feature-selection']","I have data-set of about 5 hundred thousand of Boolean features(either 1:present or 0:absent) in which 70% features are just called by 2% samples means that about 70% features contains 98% '0' and 2% '1' values. Now I want to select only most appropriate features.I tried all scikit's feature selection method like: RFE, RFECV, selectkbest etc., but their results are not satisfied. Please anyone can help with the selection of most appropriate Boolean features."
4635,"Scaling multiple features with StandardScaler, before or after concatenation?","['python-3.x', 'svm', 'scale', 'feature-extraction', 'scikit-image']","I have an image data set (with pixel values from 0 to 255), from which I want to extract different features, e.g. HOG features, Gabor filter feature, LBP and color histogram.
I would like to concatenate these features into a single feature vector and then train an SVM with this resulting overall feature vector.I'm using Python and Scikit-Image (Skimage).I am not sure, where I have to use the standard scaler here? For each feature separately, i.e. before all features are concatenated? Or is the standard scaler applied to the concatenated feature vector, i.e. to the resulting overall feature vector?Many thanks for every help"
4636,Queries regarding features selection,"['feature-extraction', 'feature-selection', 'wordnet', 'senti-wordnet']","For choosing the different features, I chose 1.The number of times all words (all distinct words collected from the training set) occur in a review, converted to a feature matrix using TD IDF2.The total frequency of words in a review 3.The polarity of all words (all distinct words collected from the training set) which occur in a review, converted to a feature matrix using CountVectorizerMy queries are:"
4637,How to interpret and view the complete permutation feature plot in jupyter?,"['matplotlib', 'machine-learning', 'deep-learning', 'feature-extraction', 'feature-selection']","I am trying to generate the feature importance plot through Permutation Feature Importance plot. I am trying to kind of make sure whether the features returned through different approaches is stable. To select optimal features. Can we get a p-value or something of that sort which can indicate the feature is significant? If I could do it with PFI, i could be more confident but looks like the results are entirely oppositeHere is my code to generate the plotQuestions1) The feature that I see at the top was non-significant in other approaches (Chi-square,Xgboost Feature importance, Logistic Regression stats model summary etc) but here i see it at the top which I am a bit shocked. Is it ordered in a decreasing order or ascending order?2) I understand PFI randomizes value to see the reduction in model error. If first row (X18) is an important feature, then it's totally opposite of my other approaches. Am I making any mistake here? What should I be looking/checking in a situation like this? Or should I apply PFI only on already selected important features?3) How do I make the jupyter cell to display to all rows. Currently it doesn't show remaining 35 rows as shown below . I have already set pandas_set column width, rows etcCan you help me with this?"
4638,How to fetch values using Permutation Feature Importance,"['machine-learning', 'deep-learning', 'data-mining', 'feature-extraction', 'feature-selection']","I have a dataset with 5K (and 60 features) records focused on binary classification.Please note that this solution doesn't work hereI am trying to generate feature importance using Permutation Feature Importance. However, I get the below error. Can you please look at my code and let me know whether I am making any mistake?I get an error like as shown belowCan you help me resolve this error?"
4639,Extracting feature detail from SelectPercentile: Column names and respective scores,"['python', 'scikit-learn', 'feature-extraction']","I'm trying to match features scores with columns names using a Pipeline with SelectPercentile and RandomForestClassifier.From my initial EDA, the feature importance analysis seems to make sense. Nevertheless I'm not sure if I've done it correctly.My main concern is that I'm getting the original DataFrame column Index ids (to get to know the original column names) via SelectPercentile.scores_ and assuming these features are aligned (when ordered) with RandomForestClassifier.feature_importances_I haven't seen this done anywhere. Am I doing it right?My Pipeline:The Feature analysis bit:And the result:"
4640,What features to be used for clustering points in coordinate system?,"['machine-learning', 'computer-vision', 'conv-neural-network', 'feature-extraction', 'feature-detection']","I have a dataset consisting of 6 labels. You can think of them as points in the coordinate system. The problem is that 2 groups could be in a different location but each group has close to another group in the dataset overall. Since I already have x and y coordinates, using mean, std, min and max values of every point and every axis was the first solution come to my mind.
Here is an example: 1) As you can see, second group of label A is shifted 3 pixels through y-axis and similar for label H and I. I would like to label them in a result of supervised learning algorithm. I have another 8000 coordinate systems like this and all of them have different points and locations, so giving coordinates directly to the system does not work. 2)Generally, label 'H' and label 'Ha' are close to each other, A and I as well and A values are bigger than G values. I have another label named 'O' which appears so many times in the dataset and therefore causes an imbalance problem.Data is just a sample normally values are in a range of 100 and 1600. I thought, as an input to CNN, I could apply padding with different values to each point as color and convert it to image then run object detection algorithms on it. Images would look like this 
contemporary art with squaresWhat kind of features would you use as an input to the clustering algorithm?"
4641,how can I extracted information to csv (I need one column for pages number and one column for extracred info),"['pdf', 'feature-extraction', 'data-extraction']","I am trying to extract (text) from a pdf file using pdfminer and other packages. The extraction has been done and a partiuclar problem occurs when exporting my data as csv. I am supposed to get my data as follows: 1 column for pages numbers; 1 column for extracted text. Nevertheless I have found that my csv file includes many cells (rows). Therefore, i would like to find a code in order to modify the number of cells and make my csv file suitable for future analysis. Kindly check the code related to csv exporter.py below: "
4642,Using cv2.COLOR_BGR2GRAY or color.rgb2gray for hog descriptor from skimage?,"['opencv', 'svm', 'normalization', 'feature-extraction', 'scikit-image']","I want to train a SVM with hog features extracted by the
hog descriptor from skimage.
The images have 3 channels (RGB-images), which I want to transform to grayscale before extracting the hog features.
And here is the problem, when I use the following code from OpenCV then I get features which are not normalized, that means the pixel values are still between 0 and 255.When I use the code from Skimagethen the values seems to be normalized, because the pixel values are approx. between 0 and 1.I tried both versions for hog feature extraction and the results are similar but not the same and when I training SVMs then the results are also similar but not the same. When I train the SVM with the normalized images then the accuracy etc. is a little better, but not much.When I look at the following link from skimage https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html then I assume that the images do not need to be normalized before using the HOG descriptor, since the astronaut image in this link is also not normalized.
Still, I find it confusing. 
Could you confirm or disagree with my assumption that it is better to use OpenCV's code than the code from skimage for transforming from rgb to gray?The full code:orbefore"
4643,sklearn.feature_extraction.FeatureHasher in sklearn 0.22,"['python', 'hash', 'scikit-learn', 'feature-extraction']","I just updated to sklearn 0.22. My code used to work in the previous version, but now it gives an error:Now it gives this error:Any solutions?
Thanks!"
4644,How to use Recursive Feature elimination?,"['machine-learning', 'scikit-learn', 'feature-extraction', 'feature-selection', 'rfe']",I am new to ML and have been trying Feature selection with RFE approach. My dataset has 5K records and its binary classification problem. This is the code that I am following based on a tutorial onlineI encounter the below error. Can someone please help
4645,How to match features based on colour,"['image', 'matlab', 'image-processing', 'feature-extraction', 'mosaic']","I have a new series of images which I want to match with the old series of images taken at the same place but on different day and orientation. However I am unable to match features using most of the state of the art techniques like SIFT. I would like to match two images based on colours, specifically let's say the stone colour from the circle. I have another image of the same picture with the same stone from another angle. How can I use match these two stones irrespective of orientation based on colour"
4646,how to determine new features when doing feature engineering?,"['machine-learning', 'neural-network', 'regression', 'feature-extraction', 'feature-engineering']","I'm working on a project where I need to build a Neural Network Model to compensate the error that a Vehicle GPS make. my Dataset consists of 4 features: longitudinal Acceleration, lateral Acceleration, speed and yaw Rate and my output is the azimuth (angle according to north from the false GPS coordinates to the true GPS coordinates) and distance between the false GPS coordinates the true GPS coordinates. so it's a regression Problem and I thought maybe a Feed Forward Neural Network would perform good in this task but I'm struggling to implement a neural network that can fit my data. My model starts to learn good for the first epochs and then it stucks at a high loss value like it is unable to learn anymore, it even fail to overfit the data, I tried all preprocessing approaches and neural network best practices but that didn't help. My last thoughts were that maybe something is wrong with the data or maybe I need more features to complete such a task so I want to make some feature engineering and that's why I want to ask here how can I make that in my case? I don't know whether it makes sense but I thought maybe I can take the derivative of the acceleration and add that as a features or maybe I can take the square of it or the square of the speed? so I have many Ideas but I'm not sure what to do and how can I have a sense for this so I wish someone here have the experience to help me on this.PS: I don't have an Error in my NN Implementation because I tried to run my model only on 10 examples of the data and the NN achieved to map the relationship between those 10 examples, it was also good when I used 100-1000 examples but when I take more than 3000 examples, the NN starts to learn and then it get stuck at some high loss value that's the reason why I started to think maybe something is wrong with the data or that I need more features to do this"
4647,Feature Hasher Returning Same Pattern for Different Datasets,"['python', 'hash', 'scikit-learn', 'feature-extraction']","I have 20801 observations of a categorical variable with 1128 unique levels.
I used: sklearn.feature_extraction.FeatureHasher(n_features=x)
where x = 5 to 16.Then I recorded the number of non-zero columns in the results and plotted it against the n_features (see the figure below)X-axis shows the x (number of features) used and Y-axis shows the number of non-zero columns in the resultsIt looked a bit suspicious to me. So I made a random categorical dataset with the same dimensions but with different unique levels and tried the hashing on it using x values from 5 to 16. It produced exactly the same graph!How is that possible?"
4648,Extract new feature from sentence column - Python,"['python', 'pandas', 'dataframe', 'machine-learning', 'feature-extraction']","I have two dataframes:city_state dataframeand sentence dataframeI want to extract new feature from sentence dataframe called city. That column city is extracted from sentence if in the sentence contain a name of certain city from column city_state['city'], if it didn't contain a name of certain city its value will be Null.The expected new dataframe will be like this:I have run this codebut the result of this code is like thisIf you have experience on feature engineering with similar case, could you give me some suggestion how to write the right code for expected result.Thank youNote:
This is the full log of the error"
4649,How to display a feature database in Google Earth Engine?,"['javascript', 'user-interface', 'datatables', 'feature-extraction', 'google-earth-engine']","I am very new in EE and JS, and I am trying to write a script to show a table when the user clicks a feature on the map.I have read about DataTable class, but I am not clear with that, or if necessary to define the data table structure manually. I want the script generates automatically a table from the selected feature. Any idea about to solve this?"
4650,AI predict in Tensorflow with a “mean” of a embedded feature,"['tensorflow', 'keras', 'artificial-intelligence', 'feature-extraction', 'feature-selection']","Example: I have a tensorflow model which predicts house prices. I have a hash bucket feature for city:But when I predict I also want to be able to predict with a ""general"" city. Sort like a ""mean"" of all cities.So I get a house price not dependent of which city the house is in.Is this possible?"
4651,How to use sklearn.inspection.permutation_importance for clustering algorithm,"['machine-learning', 'scikit-learn', 'jupyter-notebook', 'cluster-analysis', 'feature-extraction']","In the real problem, I don't have y (true label), I tried to do y=None to make it as an unsupervised learning. But it does not work. I got:Do anyone know how to implement without true label?"
4652,“'str' object is not callable” using DictVectorizer on a column of pandas dataframe,"['pandas', 'feature-extraction', 'dictvectorizer']","I'm developing a machine learning project in which I need to predict the prices of hotel rooms.Since in my data set there are also the names of the hotels, I want to improve the performance of my model by using DictVectorizer on the name feature.However Python keeps presenting the ""str' object is not callable""  error at the line vec= dict.fit_transform(X_data). Can anyone help me with this?The code is in the picture"
4653,New features generated from SymbolicTransformer do not match the rule?,"['python', 'feature-extraction', 'feature-selection', 'gplearn']","I have same features.I put them into SymbolicTransformer, fit.
This is the result of fit :[sqrt(X145),
 log(div(add(div(div(X270, X568), sub(X177, X147)), neg(sqrt(X3))), sqrt(div(log(X240), sub(X32, X481))))),
blablabla...]The column of X145:After transform, the column of new feature generated from sqrt(X145)（the first column of new features） is:I suppose the column sqrt(X145) is:[sqrt(0), sqrt(5), sqrt(2), sqrt(10), sqrt(15)]Obviously, I have some misunderstanding with SymbolicTransformer about generating features. 
Please correct me if I am wrong."
4654,How to use emobase.config file in Opensmile?,"['feature-extraction', 'opensmiles']",I want to extract audio features using opensmile using the predefined emobase.config file. However when I enter a .wav and I get the output for only one framesize.I changed the framemode to fixed and gave the values for fixedstep and fixedsize but it throws error. I don't know if the inbuilt config file like emobase.conf will generates output for the whole video and not frame wise. Is there a way to extract features per frame size for inbuilt config files? I get frame wise output for self generated config files! However for the inbuilt ones I don't know how to go about it! 
4655,How to get the actual names of selected features by LASSO algor,"['machine-learning', 'feature-extraction', 'feature-selection']",is there a way how i can print the actual selected features with their names using Lasso?results I want to see the actual names of the selected features ?
4656,Silhouette extraction from binary image,"['c++', 'opencv', 'image-processing', 'feature-extraction', 'binary-image']","I am working with binary images from CASIA database and opencv in a C++ project. I am looking for a way of extracting only the silhouette(the bounding box containing the silhouette). The original images are 240x320 and my goal is to get only the silhouette in a new image (let’s say 100x50 size).
My first idea would be to get the minimum and maximum position of “white” pixels on rows and columns and get the pixels inside this rectangle in a new image, but I consider this not efficient at all. If you have any suggetion, I would be more than happy to hear it. On the left is the input and on the right is the output."
4657,How to detect position and angle of 2D image in cam feed using web technologies (e.g. JS libraries etc)?,"['opencv', 'feature-extraction', 'feature-detection', 'tracking.js', 'jsfeat']","I'm working on a project where I need to detect the position and angle of a 2D image in a live camera feed, as shown with the moving business card here (scroll down slightly to ""Continuous image tracking""): https://blog.viromedia.com/arkit-2-0-continuous-image-tracking-and-object-detection-with-viroreact-6823b94b0eb1 - except that uses Apple's ARKit in a native app and I need to use web-only solutions (e.g. Javascript libraries etc).Getting camera feed should be straightforward and I think I can get part of the way to a solution using tracking.js Feature Descriptor: https://trackingjs.com/examples/brief.html - though I'm guessing I'd also need to detect the boundaries of the 2D object (e.g. rectangular bounding box).I would have a reference image in advance for comparison with the live feed. My issue is how do I get a measurement of the distance of the 2D object from the camera plus the angle/rotation it's being held at?I've done a load of research but struggling to find a solution. I'm assuming I could use tracking.js, openCV.js or feat.js for something like this, but not sure how to get the right result.Thanks"
4658,How we can convert a time series data into supervised learning problem?,"['machine-learning', 'time-series', 'feature-extraction', 'feature-selection']",I am preparing a data for machine learning model. I want to deal with time series data as normal supervised learning prediction. Let's say I have a data for car speed and I have several cars models such as So I want to deal with several car models in the training so that my machine learning model should predict the speed for Bentley and BMW.I have converted the data for training like this :Is it a correct approach? 
4659,First Derivative of MFCC Coefficients in Matlab,"['matlab', 'signal-processing', 'feature-extraction', 'audio-processing', 'mfcc']",I have extracted mel frequency cepstral coefficient (MFCC) features (in Matlab) for some speech classification. I'm currently thinking of adding the first and second derivatives of MFCC coefficient features. How can we get first and second derivatives from the MFCC?Thanks in Advance
4660,Finding the average of a list,"['python', 'list', 'lambda', 'average', 'reduce']","I have to find the average of a list in Python. This is my code so farI've got it so it adds together the values in the list, but I don't know how to make it divide into them?"
4661,different datatype can effect on normalization of feature selection algorithm,"['machine-learning', 'dataset', 'data-mining', 'feature-extraction', 'feature-selection']","Assume our dataset has 1000 data (matrix rows) and 700 features (matrix column). I want to run various types of feature selection algorithms to find the best features. Features of this dataset are different from each other what I'm trying to say is that some of these data are discrete (for example one of the dataset features is blood type and it has different type of A, B, AB, and o. we transfer this feature to 1,2,3 and 4.) and some of them are continues. Here is my question if I normalize these data then pass it to some feature selection algorithm do I lose information through normalization? if I normalize both discrete and continues feature does it has an effect on the results of the feature selection algorithm?  "
4662,Sound feature attributeError: 'rmse',"['python', 'audio', 'feature-extraction', 'librosa']","In using librosa.feature.rmse for sound feature extraction, I have following:It gives me:What's the right way to get it? Thank you.sample file: https://www2.cs.uic.edu/~i101/SoundFiles/CantinaBand3.wav"
4663,How to extract feature from lists?,"['python', 'pandas', 'scikit-learn', 'feature-extraction', 'sklearn-pandas']","How to extract feature from dataset by python like :I find two ways to slove this problem. 
1) One is:But

 So it is not a good way.2) Another is :Search C and D column to find topK items, and only keep the topK.
But it will lead to the information loss.Is there a better way to solve this problem?"
4664,"Getting TypeError: '(slice(None, None, None), array([0, 1, 2, 3, 4]))' is an invalid key","['python', 'machine-learning', 'feature-extraction', 'feature-selection']","Trying to use BorutaPy for feature selection. but getting a TypeError: '(slice(None, None, None), array([0, 1, 2, 3, 4]))' is an invalid key. I used the breast cancer dataset and did some small tweaking like adding header, feature scaling and missing value handling."
4665,How to handle multi-label categorical feature for binary classification problem?,"['machine-learning', 'data-science', 'feature-extraction', 'feature-engineering']","I have dataset like :How to handle category feature, this table may have others additional features too. One hot encoding lead to consume too much space.because number of rows is around 10 million. Any suggestion would be helpful."
4666,Spectral Centroid - get the same results in matlab and python,"['python', 'matlab', 'feature-extraction', 'spectrogram', 'centroid']","I obtain spectral centroid in matlab using this code:Function FeatureSpectralCentroid I have from this link: https://www.audiocontentanalysis.org/code/audio-features/spectral-centroid/The results are: centroid = [10.6816, 6.0146, 19.6662] and vsc = [40.4499, 27.4300, 41.0146]
I also use a different implementation of spectral centroid and the results are also different. Sometimes scalar, not vector. How to use FeatureSpectralCentroid to get the same results as in spectralCentroid? Can we always get the same results of spectral centroid? "
4667,Save features from hidden layers from an auto-encoder model using keras in runtime,"['python', 'keras', 'feature-extraction', 'autoencoder']","I am training an auto-encoder model and  want to save the features of each image during runtime from encoder part and use it later for feature matching. My model structure is-How can i save the features from encoder part during model fit. Is there any way, please suggest"
4668,Variable image sizes in transfer learning (inception_resnet_v2),"['python', 'conv-neural-network', 'feature-extraction', 'keras-layer', 'tensorflow-hub']","I am running an image feature extraction task with Tensorflow Hub and a feature vector from Inception-Resnet trained on Imagenet (no classification head). From the docs, it is possible to use different image sizes as input. Wondering if would be correct to initialize the feature extractor just once (without specifying input_shape) and then feeding it with different image sizes?
For instance, when running it on a single image (with / without input_shape initialization) the output feature vector is the same:"
4669,How to solve AttributeError: 'int' object has no attribute 'dtype',"['python', 'machine-learning', 'computer-vision', 'feature-extraction']","I want ot extract feature from image,but i found the following error.How to fix this problemattribute error,when i run ""exTrain3000 = getFeature([X_train[:30], 0])[3]"""
4670,How to normalize features after windowing for future test data?,"['machine-learning', 'neural-network', 'signal-processing', 'normalization', 'feature-extraction']","I have an eeg data about 10 minutes. I want to extract some features (e.g. statistical features, power spectrum features, interchannel features, . . .) from this data to apply them to the machine learning algorithms. So I used 3 seconds window with 50% overlap for sliding on the eeg signal. After that because of different ranges of values, I normalized the features with z-score normalization as shown below:value is feature vector and mu and sigma are the mean and standard deviation of that vector. For further use I want to save these parameters (mu, sigma) to normalize the new signals (test data) with the same parameters before applying to the machine learning algorithm, but in this case I would have too many parameters that saved for each window. What should I do now?A possible way is that first (in the training set) extract the features from all of the signal (10 seconds) which is a real time consuming work and then normalize them and save their mu and sigma parameters (for each feature). After that use windowing and also extract the features for windows and normalize them with the saved parameters. It means that saved mu and sigma from all of the signal can be used for train and test data (one time for train phase and save the parameters and always use them), but I'm not sure that this solution is correct !!!"
4671,Explain feature interaction vs feature correlation,"['machine-learning', 'linear-regression', 'feature-extraction', 'feature-selection']",I am confused in the mentioned terminologies in machine learning paradigm? Can anybody drop some kind response here?. I shall be grateful to you..
4672,Extract time features using the periodic normal distribution (von mises) in Python,"['python', 'r', 'pandas', 'feature-extraction']","I am trying to find the mean, variance and confidence interval of the periodic/wrapped normal distribution (von Mises) but within a time interval (as opposed to the traditional interval of pi). I looked at a solution on stack overflow here, its close but I am not sure its exactly what I am looking for.I found exactly what I was looking for here, which uses R (see below an extract of the code). I'm looking to replicate this in Python. Like the library circular, python has a package scipy.stats.vonmises but lies within the interval pi instead of time. Are there any alternative packages that can help?"
4673,FeatureTools GroupBy issue excluding entities,"['python-3.x', 'jupyter-notebook', 'feature-extraction', 'featuretools']","This question is a follow-up of this post:I could solve the first part of the doubt but after that, another arose.I have the following Featuretools entity set:
And I would like to get the groupby_trans_primitives: Diff and TimeSincePrevious(days), but just in the recordings entity, excluding other entities: 'vendedores','produtos',cliente','produto_cliente'I tried the following code to exclude those entities unsuccessfully:Because the code returned de following features:And I don't know why the following features were created as my code specified to exclude those entities:I would appreciate any kind of help! Thank you!"
4674,I got stuck trying to fetch the previous value based on a criteria,"['feature-extraction', 'featuretools']","I'm new to FeatureTools library, and I got stuck trying to create two types of features, both are related to fetching previous values. One is the previous value itself for  'QUANTIDADE', 'VALOR_TOTAL' and 'DATA_NOTA', and the other is the time since the previous observation (days) which has 'DATA_NOTA' as the date field.I don't know if it is possible to do it with FeaturelTools. If someone can help me, I would appreciate it.I have a dataframe (df) as folowing:
When I normalize the above df it takes the following basic schema:As I said, I would like to fetch the previous values and time since the last observation for  'QUANTIDADE', 'VALOR_TOTAL' and 'DATA_NOTA', but when the combination of 'CODIGO_PRODUTO' and 'CODIGO_CLIENTE' matches."
4675,SIFT/SURF and signatures,"['image-processing', 'feature-extraction', 'sift', 'surf', 'handwriting-recognition']",I'm working on a project about offline signature verification and I've tried SIFT/SURF algorithms (OpenCV) for comparisson of 2 signature images.What I've noticed is that when I pass in 2 same pictures I get ~1000 keypoints but when I pass 2 pics of different signatures of same person I get just ~70-80. And when one of the passed pics is a signature of a different person but which has alike style I get ~50-60 keypoints. Some of the points also weren't matching each other at all like they were from 2 different locations.It's clear to me that these algorithms aren't good for my task but I don't quite understand why. Could anyone exaplin the reason to me from the maths/algo point of view?
4676,How to do mean(target) encoding in pyspark,"['python', 'encoding', 'pyspark', 'feature-extraction']","I need to do a mean(target) encoding to all categorical columns in my dataset. To simplify this problem, Let's say there're 2 columns in my dataset, first column is the label column, the second column is a categorical column.e.gSo according to mean encoding strategy: https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0the output should be likeI've tried Koalas to solve this problem, but failed. This is what I've tried:But Koalas doesn't allow cell-level update, meaning I can't modify the value by ks_df.at[i, col_name] = new_valueSo I'm hoping there could be some pyspark solution to this problem."
4677,How to use tsfresh correctly?,"['python', 'pandas', 'csv', 'type-conversion', 'feature-extraction']","I want to use tsfresh to extract automated features with python from my time series csv files. There are about 300 CSV files with one column of about 1500000 entries. The first entry is the label. For testing purposes, i just use 9 csv files and i reduce the entries by a factor of one hundred thousand.To get the right format for tsfresh from the csv, I use the following code. When I enter df1 in extract_features, I get 794 features. ScreenshotAfterwards I would like to reduce the features. I don't get an error but it tells me that the dataframe is empty. !Empty DataframeThank you so much for your help. "
4678,Is there anyway to omit VGG16 target classes?,"['machine-learning', 'feature-extraction', 'vgg-net']","I have a bunch of medical images and I want to do some feature extraction on them. To do that, I intend to use pretrained CNN VGG16. It has a list of target classes which is consists of name of the ordinary objects. How can I change it to do feature extraction instead of object classifying? I mean what change I should perform on its architecture?"
4679,How to see which features are used in the trained machine learning model?,"['h2o', 'feature-extraction', 'automl']","After training a model with autoML tool of H2O, I can see the variable importance with saved_model.varimp_plot(). I am curious about the feature engineering part whic H2O claims to do.I'm trying simple lines of code sapmles in the documentation of H2O.How do I see which features are being used in the trained model? I'd like to see if H2O is extracting and adding new features or not. I've used my_training_frame = your_model.actual_params['training_frame'] as stated in another question but it gives error: ""TypeError: 'property' object has no attribute 'getitem'""."
4680,AttributeError: 'DirectoryIterator' object has no attribute 'images_ids_in_subset',"['python', 'keras', 'feature-extraction', 'resnet']","I try to run this code but still stucked with this error : AttributeError: 'DirectoryIterator' object has no attribute 'images_ids_in_subset'if anyone had this error and fix it, please, let me know how you fixed it.Thanks"
4681,Convert mwArray to std::vector<double>,"['c++', 'matlab', 'opencv', 'shared-libraries', 'feature-extraction']","I am using a feature matching algorithm that is coded in Matlab. I created shared libraries for C++ using the Library Compiler of Matlab.
As a result I am getting a mwArray consisting of n feature points and the point coordinates x,y (n rows, 2 cols). Now I would like to convert the mwArray into a std::vector<double> or even better a std::vector<cv::Point2d> so I can proceed.I tried using the methods GetData() but I don't know which arguments I have to use.
Here is the code:"
4682,How to find the amount of matches from feature extraction?,"['c#', 'opencv', 'emgucv', 'feature-extraction', 'sift']","I want to be able to get the number of matches of features from two images. Unfortunately I can't seem to find how to do it. I use the most recent EMGU package that can be downloaded from NuGet I have used the code from: Feature Matching sample, but the issue is that the method they use to count the values gives an error. It gives an error at this code block:Specifically at mask.GetDAta(i)[0] and it then states that int cannot be converted to bool, but I honestly don't know what to fill in as parameter then. A different solution I have seen is use the nonzero count but that seems incorrect. Furthermore, what is mentioned here EmguCV SURF - Determine matched pairs of points does not work either. Any help is greatly appreciated."
4683,Matlab implementation of extractLBPfeatures,"['matlab', 'image-processing', 'feature-extraction', 'lbph-algorithm']","Matlab's extractLBPfeatures returns 59 uniform local binary patterns. Instead of being integer values, these 59 values are decimal. Anyone please explain the working principle of Matlab's implementation."
4684,Pipeline that cached the results,"['python', 'python-3.x', 'pandas', 'scikit-learn', 'feature-extraction']","I use pandas to do feature extraction for machine learning. 
I hope to achieve the following: Consider I have five data processing steps done sequentially, and I execute it once, the results will be saved automatically. Next time, if I change forth step, the library will automatically start from third step. Would this cache function be supported in Pandas or sklearn.pipeline.Pipeline or other data processing libraries naturally without our need to save them explicitly?"
4685,Compilation time error when declaring SIFT algorithm,"['c++', 'opencv', 'feature-extraction', 'feature-detection', 'sift']","I built opencv with Cmake and tried out a few things like video feed display and loading images, worked fine.Then i tried using the feature detection algorithms (sift, surf etc.) and got different errors. After a bit of research on StackOverflow, i came upon the following documentation: https://docs.opencv.org/master/db/dfa/tutorial_transition_guide.html#tutorial_transition_hints_headersHere it says the good way to declare an algorithm is the following:however this gives me an error during compilation. The error is:I should mention that after I built OpenCV with Cmake, I included the xfeatures2d  files inside the include folder. So i did not build the library with the contrib files. Is this the cause of the error or am i missing something? Because i can open the file and see that xfeatures2d does have a member SIFT, and i assume that it has the create() function since visual assist is listing all the parameters types."
4686,Why VGG16 “model summary” shows less [non]-trainable parameters in a non-shared network scenario for image similarity?,"['keras', 'conv-neural-network', 'feature-extraction', 'transfer-learning', 'vgg-net']","I am working on an image similarity project. There are two types of images(i-e: camera images and another its corresponding sketch or CAD model). The network will be trained to find the similarity/dissimilarity between the given images. For this task, the pipeline work as follows: The VGGNet-16 (without classification head) will be used as a feature extractor. I used two VGGNets(for each type of image).Then I froze all the layers:Extracted feature vectors:This will produce a fixed size feature vector for each type of image. Next, both feature vectors will be combined and put into the similarity network: The summary of the model: As I have used two VGG models one for each type of image. Why the number of non-trained parameters is only 14,714,688. It should be 2 x Non-trainable params. In later stages, I unfroze the last convolutional block of the network and trained the similarity network along with last CONVO block. This also shows the same behavior. Is this the problem with code or in my understanding?Thanks in advance. "
4687,How to detect The errors in my feature extraction algorithm?,"['python', 'pca', 'feature-extraction', 'wavelet-transform']","I am working on sEMG dataset. The dataset comprises of 38 subjects each subject have 30 examples of 5 movements. Each movement is recorded using 8 electrodes so the input is 8 sEMG signals for each movement. 
I am trying to extract features and visualize it using PCA by using two different feature extraction algorithms. 
The first Algorithm is using Windowing on each channel of the 8 channels. the size of the window is 500 with 50% overlap. Then I extract 7 features from each window - Slope sign changes, RMS, VAR, MAV, Zero crossings, Waveform length, and SKEWNESS. The features are then put onto a vector. The second Algorithm I used wavelet packet decomposition with 4 layers then calculated the energy in every node in the 4th layer ( 16 nodes ) and that got to total 16 * 8 feature vector length. The problem is when using PCA to visualize the two algorithms. The PCA returns a weird tangeled line in 3d space.The PCA visualization of Algorithm 2
The PCA visualization of Algorithm 1
I used the built in pca function from scikit and projected the features into 3 dimensions.
What can be the problem with my application of the two algorithms ? "
4688,Feature engineering using Python,"['python-3.x', 'pandas', 'feature-extraction']","I have a pandas data-set in which one of the column is like this:How can I create multiple columns with each genre name and fill 1 if its contains that genre or else 0?Expected Output: Pandas Dataframeso on.I tried using first converting it to list then split it, but it's not pythonic way to do it.Can we do it efficiently using apply function or some other efficient techniques?"
4689,Whats the name of this matrix or table?,"['python', 'machine-learning', 'scikit-learn', 'nlp', 'feature-extraction']","i hope you all will be doing fine.I am having a conceptual problem,I dont know the name of this table and neither i know how can i extract it using scikit-learn.Even, if i knew the correct terminology for this table that would have helped a lot or if someone can tell me, which scikit function to use then it will be awesome.i have googled it a lot e.g using terms like aggregated table, classification reports but couldn't find this type of table.thanks for your time!happy coding!"
4690,Do I need to use StandardScaler after Hog feature extraction scikit-image?,"['scikit-learn', 'svm', 'pca', 'feature-extraction', 'scikit-image']","I have extracted Hog features from the Fashion-MNIST and GTSRB and want to do PCA before training SVMs. The feature vectors only contain values between 0 and 1, therefore I'm not sure if I really should use the StandardScaler before doing PCA?
I have the impression that the feature vectors are already normalized.I tested two variants for each of the two datasets. Once I used the StandardScaler before performing PCA and once I did not. The accuracy of the trained SVM is a little bit better without using StandardScaler.
I tend not to use the StandardScaler, but I am very unsure as it is recommended to convert mean=0 and standard deviation=1. Would not the StandardScaler be more appropriate if I wanted to combine different features vectors with different scales and map them to a consistent scale?"
4691,Features from local binary pattern histogram?,"['machine-learning', 'image-processing', 'classification', 'feature-extraction', 'lbph-algorithm']","I'm trying to determine a correlation amongst some texture samples based on their LBP histograms. Most literature I've been able to find on the subject discusses measuring distances between pairs of histograms (such as Euclidean distance), essentially treating each of the N values of the histogram as a separate feature and trying to cluster within N dimensional space. I would prefer not to treat each value as a separate feature, as I'd like to combine my data with other texture features before my analysis. I'm wondering if there is a non-comparative feature which I could extract from the histograms instead."
4692,What are the correct steps before and between HOG and PCA?,"['normalization', 'pca', 'feature-extraction']","since many days I have a problem to understand what the correct sequence for HOG-feature-extraction and PCA is.I use the following code to extract the HOG-features:The pixel values from the images range from 0 to 255, so I thought I had to normalize the images through dividing the pixel-values by 255. But I'm not sure if this is correct, because transform_sqrt=True does power law compression to normalize image before processing. 
Also, I'm wondering if I need to scale (for example StandardScaler) the HOG-features before using PCA.Can someone tell me which of the following sequences is correct?a) Image -> HOG feature extraction -> PCAb) Image -> Normalization -> HOG feature extraction -> PCAc) Image -> Hog feature extraction -> StandardScaler -> PCAd) Image -> Normalization -> HOG feature extraction -> StandardScaler ->        PCAI really hope someone can help me and thank you in advance"
4693,What techniques can I use to find position relationship in group of elements?,"['neural-network', 'classification', 'feature-extraction']","I have 14,000 tagged documents. These are custom forms that our employees create and fill out. I need to build a model that will be able to classify the types of each input field of the form in order to automate the collection of information in the database.Detection is ready and working flawlessly - I have fields positions and boundings (x, y, width, height) for over 350,000 documents. The type of the fields depends on their position in the form, size, and the fields that are nearby. The number of types is fixed, 56.Obviously, I should use bidirection RNN, but how do I collect features that are responsible for the size ratio and position to each other? Any information would be very helpful."
4694,Scaling before training a SVM with HOG-features?,"['svm', 'normalization', 'scaling', 'feature-extraction']","I'm new in Machine Learning and I would like to train an SVM with HOG-Features from images.The HOG-Features I calculating as follows:The parameter block_norm is by default 'L2-Hys'.My question is, if I have to scale these features, e.g. by using StandardScaler, MinMaxScaler...., before training an SVM with these features?I'm very insecure if I have to scale the features additionally, because transform_sqrt=True does apply a power law compression to normalize the image before processing.Many thanks in advance"
4695,How to build features using pandas column and a dictionary efficiently?,"['python', 'pandas', 'dataframe', 'machine-learning', 'feature-extraction']","I have a machine learning problem where I am calculating bigram Jaccard similarity of a pandas dataframe text column with values of a dictionary. Currently I am storing them as a list and then converting them to columns. This is proving to be very slow in production. Is there a more efficient way to do it?Following are the steps I am currently following:
For each key in dict:
1. Get bigrams for the pandas column and the dict[key]
2. Calculate Jaccard similarity
3. Append to an empty list
4. Store the list in the dataframe
5. Convert the list to columnsUsing the above functions to build the bigram Jaccard similarity features as follows:Sample input:Expected output:"
4696,How to fix Keras Feature Maps Extraction from Conv Layer Error object of type 'InputLayer' has no len()?,"['python', 'keras', 'conv-neural-network', 'feature-extraction']","I'm working on my project using Keras. I have my own CNN model and I like to pull feature maps in every convolution layer in it. However, I experience this error:TypeError: object of type 'InputLayer' has no len()Can some one help me?
Thank youTypeError
  Traceback (most recent call last)
       in ()
       10 # redefine model to output right after the first hidden layer
       11 layer_outputs = [layer.output for layer in model.layers[:10]]
  ---> 12 model1 = Model(inputs=model.inputs, outputs=layer_outputs)
       13 model1.summary()
       14 # load the image with the required shape4 frames
  /usr/local/lib/python3.6/dist-packages/keras/engine/network.py in build_map(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)
     1400 
     1401         # Propagate to all previous tensors connected to this node.
  -> 1402         for i in range(len(node.inbound_layers)):
     1403             x = node.input_tensors[i]
     1404             layer = node.inbound_layers[i]

  TypeError: object of type 'InputLayer' has no len()"
4697,How to create TensorFlow Dataset from 3dimensional Sparse feature cross data,"['tensorflow', 'sparse-matrix', 'feature-extraction', 'tensorflow-datasets']","I have a 3 way feature cross with a small set of discrete values:
feature1 with indexes - 0-4
feature2 with indexes - 0-4
feature3 with indexes - 0-9This triplet of indexes maps to a single integer value.
A given sample can have any number of these up to all 250 values.I am able to use the deprecated:
     dense = tf.SparseTensorValue(indices=tensor, values=values,
                                    dense_shape=[10, 5, 5])So the indexes are triplets of the 3 indexes.  For a single value, there is a 1 dimensional array, with as many values as there are non-zero values.
I would like tensorFlow to create the dense tensor based on the sparse one that I give it with zeros in the spaces.  So at the end the values would always have 250 values.  Is there a better way to approach this?  Do I need to manually populate the zero'ed entries in the values and 3d indices?Here is an example of the data:
1 sample would look like:tensor[0]:[(0, 5, 5),
 (1, 1, 1),
 (1, 4, 4),
 (1, 5, 5),
 (2, 1, 1),
 (2, 2, 2),
 (2, 4, 4),
 (3, 1, 1),
 (3, 2, 2),
 (3, 3, 3),
 (3, 3, 4),
 (3, 4, 4),
 (3, 5, 5),
 (4, 1, 1),
 (6, 2, 2)]values[0]:[1, 1, 4, 3, 1, 4, 4, 2, 3, 1, 1, 1, 1, 1, 1]In this instance there are 15 of the possible 250 non-zero values.But since other examples have 16 samples I can't even stack the individual components together:I am trying this:
tf.InteractiveSession()
wholeRec = tf.stack([prj_count_array, p_pct_idx_array, m_pct_idx_array, target, data_source, m_chan_id, p_chan_id], axis=-1).eval()but getting this:tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [16] != values[1073].shape = [15] [Op:Pack] name: stackI was hoping that the deprecation of the SparseTensorValue, as well as many of the to_dense type api's indicated that TensorFlow, would create any dense data (the missing 0 values to fill out the 250 possibilities)But, instead, this gives me:2019-10-02 09:36:03.479940: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/Users/damonw/PycharmProjects/AVSR-Deep-Speech/Damons_second_try.py"", line 74, in 
    dense_shape=[MAX_PROJ_CNT, MAX_PROJ_IDX, MAX_MOV_IDX]))
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py"", line 128, in init
    indices, name=""indices"", dtype=dtypes.int64)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1087, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1145, in convert_to_tensor_v2
    as_ref=False)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1224, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 305, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant
    allow_broadcast=True)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 254, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/Users/damonw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 115, in convert_to_eager_tensor
    return ops.EagerTensor(value, handle, device, dtype)
ValueError: Can't convert non-rectangular Python sequence to Tensor."
4698,The algorithm behind tsfresh select_features method,"['python', 'time-series', 'feature-extraction', 'feature-selection']","I recently started to use tsfresh library to extract features from time-series data.It's very cool that I can get the bag of features in few lines of code but I have doubt about the logic behind the select_features method. I looked into the official documents and googled it, but I couldn't find which algorithm is used for this. I want to know how it works, so that I can decide what to do on the feature selection phase after data processing in tsfresh."
4699,Colorize Function for descriptor on 3d Mesh,"['python', 'graph', '3d', 'feature-extraction']","The process is like that:I represent 3d mesh as graph data structure (adjacency matrix), and I extracted some features from it, now I would like to visualize these features on the mesh (coloring the vertices that it is related for)I have the access to the XYZ-coordinates,The graph is this for example: and the extracted features are like:or like So the question again: How can I visualize these features on the mesh that in the basic it's a coordinates, edges, faces."
4700,rebuild torch tensor from its batchified version,"['numpy', 'machine-learning', 'pytorch', 'feature-extraction', 'torch']","This is very nice example of how to build a 3D tensor:the output is:ok, now what I want to do is, starting from batched_data I want to build y.
The other way around.
Any good suggestion with a powerful pytorch streamlined code?==== Additional input =====I am using this for RNN and now I have some doubts, becaus eif you consider the following code:The output is:Which I would not expect. I would expect something like: 
[[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20],...."
4701,Tsfresh takess too long that the computer can handle,"['python', 'feature-extraction']",I am trying to use tsfresh feature extraction library in python 3.7.1 using efficient parameters with a test file (24 rows x 366 columns)it never stops and keeps processing and i tried to run same library on a different laptop with installed python 2.17.16 but the tsfresh library did not work. what should i do? 
4702,Feature extraction via tensorflowhub or extract_features.py in Bert? Are these two ways different?,"['feature-extraction', 'python-embedding', 'bert-language-model', 'bert-toolkit']","My task is to classify Chinese sentences. I used bert to extract features and then do training based on the features and sentence labels. I seems that I can obtain the features using tensorflow hubI can also obtain the features using the ""extract_features.py"" So the question is: Are these two methods return the same results? I have tried both methods and I found that the training accuracies based on features obtained through these two methods are significantly different (36% and 63%). I wonder why? I also found ""bert_features_tokens"" in the first part has fixed number of tokens, e.g., n_sentence * 128 * 768, while ""output.jsonl"" in the second part has flexible number of tokens, e.g., n_sentence * flexible_sen_length * 768. Is this the reason?"
4703,Spacy: Should I train the model on single sentence or I can pass two sentence combined?,"['nlp', 'feature-extraction']","I have multiple sentences like the one below in my database: KP Snacks Ltd recalls certain date codes of 4 variants of McCoy’s
  multi bag crisps. KP Snacks Ltd has undertaken a precautionary recall
  of the products listed below as a very small number of these bags of
  crisps may contain small pieces of plastic.Should I first split the sentences or I can just the whole data (2 sentences) to the model?In short, TRAIN_DATA_1 vs TRAIN_DATA_2 which is correct and why?"
4704,How can I extract a feature from a genbank file by label?,"['feature-extraction', 'biopython', 'genbank']","I'm trying to parse a genbank file to find a specific feature.  I can pull it out if I know the feature type (e.g. repeat_region) - eg if I'm looking for this feature:I know that I can find it using:But I don't trust that it will always be a repeat_region.  Instead, I'd like to look for it by label (5' ITR).  I can seem to find a way to parse that from the feature object.  Any suggestions?"
4705,Does the Object Detector in DeepSORT object tracking framework run over each and every frame of a video?,"['deep-learning', 'object-detection', 'feature-extraction', 'video-tracking']","I am trying to track objects using the DeepSORT algorithm described in this paper. What I have understood is that, the there are two deep-learning models at work here. One is the object detector (maybe YoLo etc) and the other is a feature extractor. The object detector tries to detect the presence of the object in a frame, while the feature extractor helps to identify if the current detected object has already been detected previously and if so, it assigns the detected object to the corresponding track.However, one thing I fail to understand is that when does the Object Detector run? Yes, it should run on the first frame, but after that, does it run only after every nth frames? OR does it run on each frame, but only on the apporximate location predicted by the tracker.Thanks."
4706,I would like to consider a feature set(vector) for a data in python for my machine learning algorithm. How can I do it?,"['python', 'csv', 'machine-learning', 'feature-extraction', 'training-data']","I have data in the following formHow do I save this data in excel sheet and how can I train the model using this feature set? Currently I am working on SVM classifier.I have tried saving the feature set list in a dataframe and saving this dataframe to a csv file. But the size and time are getting split into two different columns. The dataframe is getting saved in csv file in the following way:I would like to train and test on the feature vector [size,time] combined. Is it possible and is this a right way? If it is possible, how can I do it?"
4707,CNN Feature Extraction Time,"['python', 'keras', 'feature-extraction']","I have a dataset consist of 260 thousands images that are extracted from several videos. I want to extract features of these images and use them for frame retrieval. I used VGG16 (pretrained on imagenet) that implemented in Keras library with 'avg' pooling in the last convolutional layer. VGG16 gives me a vector consist of 512 number (feature) for each image. The only reason that bothers me is that this scenario is too time-consuming. For my dataset, it took about a day and 6 hours which is too much.
Is this elapsed time normal?
Because of low performance, I switch from VGG16 to DenseNet121 that already implemented in Keras. For this model (now) it took a day and 18 hours to extract features from 33% of my images (about 86000).
I ask again: Is this elapsed time normal?
Is there any way to extract feature faster? Even without using of implemented algorithms?
If you need more clarification, just ask for it. Thank You!"
4708,How to measure similarity of feature descriptor on similar images?,"['matlab', 'image-processing', 'computer-vision', 'feature-extraction', 'feature-detection']","I am trying to assess how similar two bounding boxes within an image are. Based upon some research I have done, I have found that I can do many things such as comparing size, colour, etc.. but one result I found interesting was that I can also compare feature vectors of descriptors. Presently I am running HOG (Histogram of Oriented Gradients) on the two bounding boxes (same object but subsequent frame) and then trying to minimize a distance measure (Matlab pdist2 command) between the feature vectors of each bounding box. Below is what I am computing after I have extracted the feature vectors of each bounding box.However, the results I am getting are really bad and the similarity measure generally returns a poor similarity measure. Is there something wrong with my approach? "
4709,Feature selection using statistical model,"['machine-learning', 'statistics', 'feature-extraction', 'feature-selection']","Problem statement :I am working on a problem where i have to predict if customer will opt for loan or not.I have converted all available data types (object,int) into integer and now my data looks like below.The highlighted column is my Target column where 0 means Yes1 means NoThere are 47 independent column in this data set.I want to do  feature selection on these columns against my Target column!! I started with Z-testWhen i run this function , i always gets different results My question is "
4710,SpaCy NER: Can a same word be part of two different entities?,"['nlp', 'stanford-nlp', 'spacy', 'feature-extraction', 'ner']","For example:Sentence: The best product in the world is Nestle Cookies.Entities:BRAND: NestlePRODUCT: Nestle CookieAre the above entities valid, or should I tag them as:Entities:BRAND: NestlePRODUCT: CookieAnd will it affect model performance?"
4711,LibSVM not converging even after 30 min of GPU server,"['deep-learning', 'libsvm', 'feature-extraction', 'multiclass-classification', 'vgg-net']","I have been trying to train an SVM classifier after feature extraction using VGG16 pretrained model. The SVM is not converging for a dataset containing roughly 10k images even after an hour.I tried the same classifier with reduced dataset and its giving results in time. I took the output from the last convolution layer(Conv5_3 layer) and features were in shape (14,14,512). With 70 images the classifier is converging within a minuteIs it a common problem in classification or will there be any problem with the feature extraction?"
4712,Interpreting time series dimension?,"['matlab', 'time-series', 'feature-extraction']","I am wondering if anyone can explain the interpretation of the size (number of feature) in a time series? For example consider a simple script in MatlabAssume X is a time series with the following output
This generates 2 vectors of length 5 each has 2 rows. Can anyone tell me what is exactly the meaning of first 2 and 5? 
In some websites it says a creating 5 vectors of length 5 and size 2. What does size mean here? 
Is 2 like number of features and 5 is like number of time series. The reason for this confusion is because I do not understand how to interpret following sentence: ""Generate 2 vector-valued sequences of length 5; each vector has size
  2.""
   What do  size 2 and length 5 mean here?"
4713,How to display test images at random from image test folder,"['python', 'opencv', 'jupyter-notebook', 'textures', 'feature-extraction']","I am using Haralick textures for a medical image classification problem.  I want to iterate through a test folder with unlabeled images and print them in a jupyter notebook with prediction labels.cv2.imshow() will output a random image to display, however, when I use plt.imshow() to display in a jupyter notebook the same image is returned.Using pyplot the same image is returned, I would like to return all (or a random subset) of images from the test folder.If anything is not clear, I will clarify.  Thank you.example image"
4714,Encode a categorical feature with multiple categories per example,"['machine-learning', 'scikit-learn', 'feature-extraction', 'categorical-data']","I am working on a dataset which has a feature that has multiple categories for a single example.
The feature looks like this:- The problem is similar to the this question posted:- Encode categorical features with multiple categories per example - sklearnNow, I want to vectorize this feature. One solution is to use MultiLabelBinarizer as suggested in the answer of the above similar question. But, there are around 2000 categories, which results into a sparse and very high dimentional encoded data.Is there any other encoding that can be used? Or any possible solution for this problem. Thanks."
4715,What is difference between Features embedding and Image Features?,"['machine-learning', 'deep-learning', 'conv-neural-network', 'feature-extraction', 'embedding']","I have to calculate the similarity between 2 images, and I was guided to use feature embeddings of images extracted by Auto-encoders rather than Features extracted by CNN.Can I know what is exact difference why Feature embeddings & why it can be used to calculate similarity but not image features extracted by CNN? I have a high-level idea of Image features, that it is a generated data by running a single Foward prop on a pre-trained network (N-1)th layer, not the prediction layer(softmax or sigmoid).And I know word embedding that projecting a dimension of a given word into more convenient feature dimensional space.But what is the intuition of embeddings in Image?When to use one over another ?"
4716,Is there any way to extract and parse the data from a pdf file?,"['python', 'pdf', 'feature-extraction']","I'm trying to extract data from pdf files which have different formats. There might be multiple tables in the pdf and also data out of tables in this format:Feature:  ValueI would like to create a pandas dataframe with all the feature names as columns and append all the values from thousands of pdfs and drop irrelevant info such as addresses and site data. Imagine that the pdfs are like bills with the product bought and they also specify the parameters and info of the product. That's what i have to extract.I have tried PyPDF2, camelot and tabula. All those tools just extract the data to a dataframe in a messy way. I tried to build a function by myself but it only works with a specific pdf structure, and i'm more interested in something general.The code gives you a dataframe without headers and with this structure:0                 1                2Variable: value   Variable: value  Variable: value
Variable: value   Variable: value  Variable: value
Variable: value   Variable: value  Variable: value
Variable: value   Variable: value  Variable: value"
4717,Using Harcascading file along with Your Own Trained SVM file,"['opencv', 'image-processing', 'svm', 'feature-extraction']","Can we use ""Harcascading File"" along with our own Trained SVM file ?
I am confuse at this spot and searched also but I couldn't find satisfactory answer.Here is the code part:I would be obliged if anyone help me out and guide me how to solve this confussion"
4718,Create column headings for 4-gram tokenizer for microsoft malware detection,"['python-3.x', 'feature-extraction']","I am working on the Microsoft malware detection dataset. I am tokenizing the byte file and vectorizing using 4-gram .I plan to give these features column names in the form of '00_00_00_00' thru 'ff_ff_ff_ff'.What is the effective way of generating these column headings ?The most basic ways to do this would be to code 8 for-loops  like what is shown below.The below for-loop  has been executing for the past 8hours and I am not sure how much more longer it will keep executingThe expected column headings would be as follows:
00_00_00_00, 00_00_00_01, 00_00_00_02, 00_00_00_04....ff_ff_ff_f0,ff_ff_ff_f1.... ff_ff_ff_ff.the question is how can I effectively generate these headings or is there another way to accomplish this requirement.What the other ways of generating column names when generating such high dimensional sparse  dataset ?"
4719,What features can be extracted from accelerometer sensor used for Structural Health Monitoring (SHM)?,"['python', 'signal-processing', 'accelerometer', 'feature-extraction', 'health-monitoring']","I am using dataset found online for SHM which uses accelerometer sensor to detect changes in the structure. I know that I can transform the time-domain data into a frequency-domain and into a time-frequency domain (Ex.: Wavelet). So, I have a few questions:Folder May 28:1-105 Damage scenario 1:1-20:  D1 
      21-43:  D2
      44-65:  D3
      66-85:  D4
      86-105: D5
      106-128: undamaged
      181-273: undamagedWhere each file has 15 acceleration sensor data. So, How can I obtain features and form a feature matrix to be used for pattern recognition and machine learning? I am would like to know what is the typical approach in order to form a feature matrix so that it can be used in the machine learning to observe the pattern.link to the dataset: http://users.metropolia.fi/~kullj/JrkwXyZGkhF/wooden_bridge_time_histories/Note: I converted the wav file into a txt file where I got the acceleration data and the sampling frequency"
4720,How to get feature_names after encoding text avg_word to vec?,"['machine-learning', 'nltk', 'feature-extraction']","I am performing analysis on donor_choose data-set.Created a glove words file for essays and encoded essays using average word to vec .
Now, i want to get feature_names.How to perform that?I performed BOW on essays and extracted feature names too.But couldn't perform the same with average word to vec.I extracted features fro BOW through model and get_feature_names().But,how to apply the same on average word to vec,where we are not using any model but the vector of that word.""""""Encoding Essay- Bow""""""vectorizer = CountVectorizer()vectorizer.fit(essay_train) clean_essay_bow_X_train = vectorizer.transform(essay_train)clean_essay_bow_X_test = vectorizer.transform(essay_test)for i in vectorizer.get_feature_names():feature_names_bow.append(i)""""""Encoding Essay- avgw2v"""""" import picklewith open('glove_vectors', 'rb') as f:def avgvectorizer(data):"
4721,How to classify geometric shape of logo images (32x32),"['python', 'opencv', 'image-processing', 'conv-neural-network', 'feature-extraction']","I'm currently working on classifying geometric shapes (circle, triangle, rectangle, square, pentagon) of logo images with 32x32 pixels.I've tried training CNN to classify geometric shape but the accuracy is low. I also have tried using OpenCV library but the logo images doesn't have clear geometric shape and sometimes with text as well. Is there any software or algorithm that can solve this classification problem?Here is my sample logo images: https://ibb.co/album/nBMf1F
I want to classify logo images into geometric shape categories such as circle, triangle, square, and etc. "
4722,(Pandas) How to get count how often the same value as before occured ? (and extract a new Column out of it),"['python', 'pandas', 'feature-extraction']","Im looking for a way to extract a new column out of my Pandas Dataframe, which shows a count of how often the current value occured the same as before (without interruption)e.g. out of a Column like:the following column should be extracted:Something similar a user posted a few years ago:BUT by the code above i am only getting showed the total values of how many times the value occured in a row, (not the counting up to it)
What i need should be a column with the same index as the column ""RiseOrFall"" and with the same amount of rows, like this:"
4723,AttributeError related to multiprocessing when using extract_features from tsfresh,"['python', 'multiprocessing', 'feature-extraction']","I have a data frame which contains multivariate time series data. It has columns: fundId (each entity), periodId(time stamp), v1,  v2, v3 ...I want to extract features from those times series for each fund over time. Like: fundId v1.max, v1.min, v1.median, ..., v2.max, v2.min, v2.median, ...I encountered an AttributeError when use function extract_features from library tsfresh. I'm new to Python but I think it might related to multiprocessing.The trackback shows:Traceback (most recent call last):
  File ""mypath\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File """", line 1, in 
    HFeatures = extract_features(FundCharc, column_id='fundId', column_sort='periodId')File ""mypath\lib\site-packages\tsfresh\feature_extraction\extraction.py"", line 178, in extract_features
    distributor=distributor)File ""mypath\lib\site-packages\tsfresh\feature_extraction\extraction.py"", line 313, in _do_extraction
    progressbar_title=""Feature Extraction"")File ""mypath\lib\site-packages\tsfresh\utilities\distribution.py"", line 349, in init
    self.pool = Pool(processes=n_workers)File ""C:\ProgramData\Miniconda3\lib\multiprocessing\context.py"", line 119, in Pool
    context=self.get_context())File ""C:\ProgramData\Miniconda3\lib\multiprocessing\pool.py"", line 176, in init
    self._repopulate_pool()File ""C:\ProgramData\Miniconda3\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
    w.start()File ""C:\ProgramData\Miniconda3\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)File ""C:\ProgramData\Miniconda3\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)File ""C:\ProgramData\Miniconda3\lib\multiprocessing\popen_spawn_win32.py"", line 33, in init
    prep_data = spawn.get_preparation_data(process_obj._name)File ""C:\ProgramData\Miniconda3\lib\multiprocessing\spawn.py"", line 172, in get_preparation_data
    main_mod_name = getattr(main_module.spec, ""name"", None)AttributeError: module 'main' has no attribute 'spec'"
4724,How to extract color features via histogram from a masked image?,"['python', 'opencv', 'background-color', 'feature-extraction', 'feature-detection']","I extracted an object from an image, so now I have a masked image with a tennis ball and a black background.I want to extract the color features from the tennis ball alone via a histogram. This is the code I have so far, but by the looks of the histogram, the black background dominates the any of the other colors, which makes the histogram ineffective:Is there a way to draw the histogram from the tennis ball BGR color features while disregarding the black background?I'm a python rookie. Thank you."
4725,how to control the number of features [machine learning]?,"['python', 'machine-learning', 'random-forest', 'feature-extraction']","I am writing this machine learning code (classification) to clssify between two classes. I started by having one feature to capture for all my images.for example:
(note: 1 & 0 are for labeling)
class A=[(4295046.0, 1), (4998220.0, 1), (4565017.0, 1), (4078291.0, 1), (4350411.0, 1), (4434050.0, 1), (4201831.0, 1), (4203570.0, 1), (4197025.0, 1), (4110781.0, 1), (4080568.0, 1), (4276499.0, 1), (4363551.0, 1), (4241573.0, 1), (4455070.0, 1), (5682823.0, 1), (5572122.0, 1), (5382890.0, 1), (5217487.0, 1), (4714908.0, 1), (4697137.0, 1), (4057898.0, 1), (4143981.0, 1), (3899129.0, 1), (3830584.0, 1), (3557377.0, 1), (3125518.0, 1), (3197039.0, 1), (3109404.0, 1), (3024219.0, 1), (3066759.0, 1), (2726363.0, 1), (3507626.0, 1), .....etc]class B=[(7179088.0, 0), (7144249.0, 0), (6806806.0, 0), (5080876.0, 0), (5170390.0, 0), (5694876.0, 0), (6210510.0, 0), (5376014.0, 0), (6472171.0, 0), (7112956.0, 0), (7356507.0, 0), (9180030.0, 0), (9183460.0, 0), (9212517.0, 0), (9055663.0, 0), (9053709.0, 0), (9103067.0, 0), (8889903.0, 0), (8328604.0, 0), (8475442.0, 0), (8499221.0, 0), (8752169.0, 0), (8779133.0, 0), (8756789.0, 0), (8990732.0, 0), (9027381.0, 0), (9090035.0, 0), (9343846.0, 0), (9518609.0, 0), (9435149.0, 0), (9365842.0, 0), (9395256.0, 0), (4381880.0, 0), (4749338.0, 0), (5296143.0, 0), (5478942.0, 0), (5610865.0, 0), (5514997.0, 0), (5381010.0, 0), (5090416.0, 0), (4663958.0, 0), (4804526.0, 0), (4743107.0, 0), (4898914.0, 0), (5018503.0, 0), (5778240.0, 0), (5741893.0, 0), (4632926.0, 0), (5208486.0, 0), (5633403.0, 0), (5699410.0, 0), (5748260.0, 0), (5869260.0, 0), ....etc]Question:what to change to add another feature? how the A and B should look when adding the feature and do I change this line when using two features?My guess:class A=[(4295046.0,secons features, 1), (4998220.0,secons features, 1), (4565017.0,secons features, 1), (4078291.0,secons features, 1), (4350411.0,secons features, 1), (4434050.0, 1),......]
is that right? is there better way?"
4726,How to perform downsampling and upweighting technique?,"['machine-learning', 'feature-extraction', 'downsampling', 'oversampling']","I am referring Google's machine learning DataPrep course, in this lecture https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data about solving class imbalanced problem, the technique mentioned is to first downsample and then upweight. This lecture talks about the theory but I couldn't find its practical implementation. Can someone guide?"
4727,Best approach to feature engineering in natural language processing?,"['nlp', 'feature-extraction', 'n-gram', 'information-extraction', 'feature-engineering']","I am trying to cluster a large corpus of documents and would like to also subsequently explain what characterises each cluster in terms of the most common shared keywords or key-phrases in each cluster. To make clustering feasible, I will use a dimensionality reduction method like for example LSA (SVD) or doc2vec.I can see several possible paths to the dimensionality reduced feature matrix:POS-tagging, chunking (shallow parsing) for nounal phrases (NPs), TF-IDF, LSA (SVD)n-grams, TF-IDF, LSA (SVD)doc2vec all the way.What are some of the advantages and disadvantages of these and perhaps other approaches? And what is ultimately the best way to go?"
4728,How to transfer a blob of an image to a white background?,"['python', 'opencv', 'extract', 'feature-extraction', 'feature-detection']","I am attempting to extract the color features of tennis ball in an image. To make it easier, I thought that transferring the tennis ball to a white canvas and then extracting the features would be better. I am extracting the color features via a histogram. I have used a mean shift image segmentation algorithm on a frame from a video where the tennis ball is falling (https://imgur.com/a/lbqOx6S). Originally, I converted an the frame to gray scale and made an histogram from the updated frame, but I realized that it was ineffective because I am trying to extract the color features. Therefore, I am now trying to transfer the the tennis ball to a white canvas and so its easier to extract the color features from the tennis ball into a histogram."
4729,Feature extraction of wav file,"['python', 'wav', 'feature-extraction', 'librosa']",We are trying to extract features from .wav file and always get the same error. We have tried with python 3.6.6 and 3.7.4 version but the error is the same.This is the error we getWe are supposed to get numbers which represent .wav file so we can classify them whether they are cat or dog.
4730,Is it ok to scale labeled/binary data for principal component analysis (pca)?,"['scaling', 'pca', 'feature-extraction']","I have a dataset which have about 20 columns of labeled data (numbered with sklearn.preprocessing.LabelEncoder), 140 binary column (0 and 1) and 3 columns of numerical values. There are about 4400 rows in this dataset and I had a hard time to train a deep neural network on this dataset so I decided to reduce the features and remove unnecessary ones.So here is what I did: I scaled those 3 columns and did the PCA with sklearn.decomposition.PCA() but the result was 99.7% on pca1 and 0.3% on pca2 and other pcas were 0.Then for some random reason, I tried to scale the whole dataset:And tried PCA again and this time, the result was a bit more promising (but not perfect I guess). here are the pcas:So here are my questions:Is it allowed to scale data in such manners? (scaling labeled/binary data)If yes, is such PCA useful for feature extraction? (since the best pca is 2.7%)P.S. I'm pretty new to these stuffs. If I need to provide any other information, please let me know."
4731,Manipulate feature maps while training,"['python', 'tensorflow', 'keras', 'feature-extraction']","In summary, similar to the Faster-RCNN and Mask-RCNN papers that perform ROI pooling on the feature maps, I want to apply some changes to the feature maps of the last convolutional layer of VGG19. First, I want to divide the feature maps to 4 equal parts and put them on top of each other. Then, feed it to a fully-connected layer.What I have:
I am using the example at https://github.com/sugi-chan/fgo-multi-task-keras/blob/master/fgo_multiclass.ipynbbased on https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer, I need to construct the model first:  Problem with this suggestion: I want to extract the feature map and do some process on it while continuing the training phase on the main model (That is called multi_model() in the code I referenced above). However, this suggestion constructs the model with features as its output. Another thing is that I have seen the RoIAlignPooling layer code at https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py but it is hard for me to use it in an easy example like mnist dataset. I appreciate your comments, codes, links to tutorials, etc."
4732,Any workaround to find optimal threshold for filtering raw features based on correlation matrix in R?,"['r', 'correlation', 'data-manipulation', 'feature-extraction']","I intended to extract highly correlated features by measuring its Pearson correlation, and I got a correlation matrix by doing that. However, for filtering high correlated features, I selected correlation coefficient arbitrarily, I don't know the optimal threshold for filtering highly correlated features. I am thinking about to quantify positive and negative correlated features first, then get credible figures to set up a threshold for filtering features. Can anyone point me out how to quantify positive and negative correlated features from the correlation matrix? Is there any efficient way to select the optimal threshold for filtering highly correlated features?reproducible dataHere is the reproducible data that I used whereas row is the number of samples, the column in the number of raw features:my attempt:Here is my attempt to get Pearson correlation matrix and intended to filter out highly correlated features (here I just used correlation coefficient which was chosen arbitrarily):then I selected correlation coefficient arbitrarily as the default threshold for filtering highly correlated features.I think doing this way is not accurate. any idea?I am curious about how to quantify positive and negative correlated features, then find out optimal threshold value for filtering. How can I make this happen? any efficient way to quantify pos/neg correlated features? How can I select optimal correlation coefficient values as the threshold for filtering? any thought? Thanks in advance"
4733,How do I rank a list of features that I have picked out in the given data and rank the features?,"['python', 'machine-learning', 'prediction', 'feature-extraction', 'feature-selection']",I am currently doing ML on house prediction dataset. > datasetHow do I select 10 best features in my dataset for aiding in building a model for predicting SalePrice and how to rank the 10 features ?
4734,Extracting clock hands from an image,"['python', 'opencv', 'computer-vision', 'feature-extraction']","I am currently working on an application to extract the time from an image of an analog clock. I am having a problem in extracting the hands from the image.I am currently using a Hough Circle to extract the clock face, then preforming a canny edge detection and dialating the image. Then finally a Hough Line transform to extract the hands. However, the Hough Line tranform also detects some erronous lines on the edge of the clock and numbers ect along with the clock hands.To fix this i tried, getting the center of the clock and checking if that point lies on the line that was detected, as the hands of the clock always pass through the center. However, the Hough Lines are not always 100% accurate and neither is the center point so not all of them pass through the center. Is there anyway to see if a line passes close to a point?Here is an example image:As can be seen the Hough transform did originally detect the lines, but they did not perfectly pass through the origin. So they were discarded."
4735,How to implement feature extraction in Julia [closed],"['machine-learning', 'julia', 'feature-extraction']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed last year.I am trying to make a binary classifier using machine learning and I am trying to develop other features for my data using correlated features (numerical attributes) I have. I searched much but could not get a block of code that will work with me.
What should i do?I've searched in dimenshionality reduction and found library (Multivariate Statistics) but actually i did not understand and i felt lost :D"
4736,any way to do feature selection for dataframe by computing its mutual information or information gain in R?,"['r', 'dplyr', 'feature-extraction']","I am interested in computing mutual information entropy for all rows of dataframe (a.k.a, features) then do feature selection by looking up its MI value. In my dataset, rows are raw feature set, columns are different groups. I understand the concept of pairwise mutual information (PMI) in NLP but not quite sure MI in R. Essentially I want to make feature selection by computing its mutual information entropy. How can I do that in R? any efficient way to make this happen? Or is there any R package can do this for feature selection? Any thought would be appreciated.reproducible data:here is the reproducible data that can be used:my trivial attempt:but I think this is not a proper way of computing mutual information and make feature selection based on that. Can anyone point me out how to make this happen? Thanksdesired output:basically, in my expected output, original dataframe should be shrinked /filtered the features by looking up its mutual information entropy table. How can I get this done in R? any thoughts?"
4737,Forward Selection using KNN Algorithm in Pandas,"['python', 'pandas', 'feature-extraction', 'feature-selection']","Could anyone help me on how to do a forward selection  from a dataset using KNN Algorithm in Pandas ? https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.htmlI followed this website but it doesn't look like a forward selection and it is not using KNN Algorithm. Is it possible ? If yes, how ? Thanks in advance. "
4738,What is the type of extracted CNN features: global or local feature type or both of them,"['image-processing', 'conv-neural-network', 'feature-extraction']","As it is known that local features describe the local structure of the image contents while the global features describe the image contents as a whole. Convolution neural network which is under deep learning field makes to extract important feature by itself, I want to understand what type of the extracted feature by CNN is they are local or global feature or both of them? and why? is there anyone can help me in some analysis or references answering my question. Thanks."
4739,Get VU meter value from audio using Python,"['python', 'audio', 'feature-extraction', 'meter']","I found many meanings of the Volume Unit meter (VU meter). For example, the average of loudness of sound, the average of frequencies of sound, and the average of power in dB scale.I read audio by using AudioSegment and segmented an audio sound into small windows. Then I got an array of values for each window (I guess that the values that I got are amplitudes).I would like to know the exact meaning of the VU meter and how to get VU value for each window (ex. formula). I also confused between VU meter, Peak Programme Meter (PPM), and RMS. If anyone knows the answer, please help me.Thank you"
4740,"Which algorithm to use for percentage features in my DV and IV, in regression?","['python', 'statistics', 'regression', 'percentage', 'feature-extraction']","I am using regression to analyze server data to find feature importance.Some of my IVs (independent variables) or Xs are in percentages like % of time, % of cores, % of resource used, while others are in numbers like number of bytes, etc.I standardized all my Xs with (X-X_mean)/X_stddev. (Am I wrong in doing so?)Which algorithm should I use in Python in case my IVs are a mix of numeric and %s and I predict Y in the following cases:Case 1: Predict a continuous valued Ya.Will using a Lasso regression suffice?b. How do I interpret the X-coefficient if X is standardized and is a
  numeric value?c. How do I interpret the X-coefficient if X is standardized and is a
  %?Case 2: Predict a %-ed valued Y, like ""% resource used"".a. Should I use Beta-Regression? If so which package in Python offers
  this?b. How do I interpret the X-coefficient if X is standardized and is a
  numeric value?c. How do I interpret the X-coefficient if X is standardized and is a
  %?If I am wrong in standardizing the Xs which are % already, is it fine to use these numbers as 0.30 for 30% so that they fall within the range 0-1? So that means I do not standardize them, I will still standardize the other numeric IVs.Final Aim for both Cases 1 and 2:To find the % of impact of IVs on Y. 
  e.g.: When X1 increases by 1 unit, Y increases by 21% I understand from other posts that we can NEVER add up all coefficients to a total of 100 to assess the % of impact of each and every IV on the DV. I hope I am correct in this regard."
4741,What is the warning 'Empty filters detected in mel frequency basis. ' about?,"['feature-extraction', 'mfcc', 'librosa']",I'm trying to extract MFCC features from an audio file with 13 MFCCs with the below code:But it is showing this warning. What does that mean and how do I get rid of it?
4742,Where is pretrained ResNet101 in Keras and how obtain raw feature?,"['python', 'tensorflow', 'keras', 'feature-extraction', 'resnet']","I need pretrained ResNet101 in Keras but Python gives me error. In the documentation they write (https://keras.io/applications/)
but when I import ResNet101 Python gives the error Moreover, I need the features calculated before the ""pooling"" layer, for example using VGG16 I'd do this:How can I obtain them using ResNet?
Thanks"
4743,"MFCC feature extraction, Librosa","['feature-extraction', 'mfcc', 'librosa']",I want to extract mfcc features of an audio file sampled at 8000 Hz with the frame size of 20 ms and of 10 ms overlap. What must be the parameters for librosa.feature.mfcc() function. Does the code written below specify 20ms chunks with 10ms overlap?The audio file is of 1800 seconds. Does that mean I would get 24 mfccs for all (1800/0.01)-1 chunks of the audio?
4744,Feature selection/extraction in an ANN model for regression,"['machine-learning', 'deep-learning', 'regression', 'feature-extraction', 'feature-selection']","I am trying to fit an ANN model for regression with 15 input parameters.
Some of these parameters are related to each other and the relationship is not linear. Say, one of the input parameters can be expressed as a non-linear function of other parameters. But I don't know these relations exactly because I lack domain knowledge. Is there a way to find these relationships among the input parameters?I have tried finding these relationships with pandas correlation matrix, couldn't draw any conclusion since it talks about the only linear correlation between 2 parameters. Thanks in advance."
4745,Looking for quick-and-dirty image-matching to extract data from game screenshots,"['ocr', 'feature-extraction', 'template-matching']","I'm playing a game where our Guild is constantly sharing teams as screenshots in Discord. I'm wondering if I can find something that will convert those team images into text, using a library of exact images for each of the troops.Online OCR does a marginal job because of the backgrounds behind the text, so it's not a great solution. Also, it basically solves the wrong problem.I've found mention of the OpenCV library (for Python?), but the GitHub projects I've seen are looking at a much broader problem.Here's the algorithm I have in mind:Locate the team in the image and resize to a standard size.Locate the sub-images for each troop.Search the library of template images (about 800 -- a big job to
create!) for each troop and find a sufficiently good match.Look up and return the text associated with each match.Ideally, I would like to point the software to an image file and it would tell me the names of the troops in copyable text.This might be an interesting enough project for me to finally install and learn Python (but that's a whole other question). For now, let's see what already exists that might work."
4746,Convert string features to numeric features in sklearn and pandas,"['pandas', 'machine-learning', 'scikit-learn', 'feature-extraction', 'dictvectorizer']","I'm currently working with sklearn (I'm beginner) and I want to train and test a very naif classifier.The structure of my training and testing data is the following:Where: What I want to do is to convert my context string features in numerical features. Every string field is composed of a maximum of one word.The main goal is to assign a numeric value for every context and word string in such a way to make the system works.
What I thought is that it's possible to define a vocabulary like:and provide this vocabulary to the DictVectorizer, but I don't know how to do this now.What I really want to do is to generate a huge number of binary features: the word “from” immediately preceding the word in question is one feature; the word “available” two positions after the word is another one. But I really don't know how.This is what I tried to do:Obviously did't work. This because the context and word fields can be a very big word like an url.Any suggestions? I accept all kinds of solutions.Thank you very much."
4747,How to extract features with different formats in a text file in python?,"['machine-learning', 'nlp', 'feature-extraction', 'text-extraction']","i have several text files of company invoices which have different kinds of Date formatsdd/mm/yyyymm:dd:yydd,monthname,yyyearname,monthname,ddand so on.
Theres lots of unique patterns that cant be listed here.Problem is i have been using a mix of regex and (mostly) if else string matching to find out these dates but im sure there's a better way to identify them instead of hardcoding the program to find different patternsI would also like to extract other features like 'TOTAL' amount which also has formats likeTotal
$123 Total $123$123
Total$123 TotalHere are some example text filesHeres the code i am using for date extraction and total extractionIs there any way i can use machine learning and train a model to extract these features?"
4748,Get a disparity map from feature extraction of two images,"['matlab', 'computer-vision', 'feature-extraction', 'matlab-cvst', 'disparity-mapping']","i have some understanding problems on how to get the disparity map from two images of one scene. 
I currently can extract some feature points and filter them so just the right correspondences are shown (lets say there are in total 60 feature points i can get).To get the disparity of x1 and x2, i know that i have to do this: My problem is how to proceed from here. Both pictures have like 1000x1500 pixels and i only get the disparity of 60 pixels (because i have i.e. 60 feature points). 
How do i get the other disparities?My current code (in matlab, self written) cant extract more than a cretain amount of features.Should i look for a better extraction alogroithm?
Or is there another way to get the disparity from my current data? 
(i can also calculate the rotation matrix R, the translation matrix T, the essential matrix E, i have the baseline, the calibration matrix of both cameras and so on.)I use the middleburry stereo data set from 2014. 
http://vision.middlebury.edu/stereo/data/scenes2014/Thx in advance for any help:) (sorry if there are spelling errors)"
4749,One dimensional CNNs for time series feature extraction,"['neural-network', 'time-series', 'conv-neural-network', 'feature-extraction']","One dimensional CNNs are used in many tasks for feature extraction of time-series data. In many papers, I have read that one-dimensional CNNs are able to extract the most relevant spatial information from time-series data. What does spatial information mean in one-dimensional data?"
4750,Resnet50 image preprocessing,"['tensorflow', 'feature-extraction', 'resnet', 'tensorflow-hub']","I am using https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3 in order to extract feature vectors from images. However, I am a bit confused when it comes to how to preprocess the images prior to passing them through the module.Based on the related Github explanation, it's said that the following should be done:However, using the aforementioned transformation, the results I am getting suggest that something might be wrong. Moreover, the Resnet paper is saying that the images should be preprocessed by:A 224×224 crop is randomly sampled from an image or its
  horizontal flip, with the per-pixel mean subtracted...Which I can't quite understand what is means. Can someone point me in the right direction?Looking forward to you answers!"
4751,Convert Negative Sobel Gradient Values into 0 to 6.28 Ranges,"['python', 'opencv', 'image-processing', 'feature-extraction', 'sobel']",I've tried to create an Image Processing Program with Twelve Directional Feature Extraction (from this url source https://www.ijcaonline.org/journal/number3/pxc387173.pdf)I used this code to get the gradient resultI tried to get the gradient for this imageAnd I got this resultThe journal expected that the gradient values should be ranging between 0 to 6.28.But I got the negative values gradient. How do I convert them into 0 to 6.28 range?
4752,how to transform Time-series trend into a measurable predictor variable,"['python', 'r', 'machine-learning', 'time-series', 'feature-extraction']","I have a time series data which explains the number of frauds in the transaction over 1 year timeline along with the target variable of fraud or not.X- axis is time-line and Y- axis is number of frauds detected. Do we have any ML model/statistical technique that tries to identify the trend in these frauds and convert into a measurable predictor variable with value like 0 to 1, where values close to 1 are more prone to fraud and vic.,The trends in the frauds over an year is non-linear, so if there is any mathematical transformation i can apply on the time-series so that it can provide me a measurable feature?Any suggestions are much appreciated?I thought of using normal slope techniques where negative slope w.r.t time-line are less fraud and positive slope for more fraud. It only captures linear trend, but need to capture non-linear trend.Edit::I forgot one important point. I will give one scenario to explain this point better. For Financial banks, let’s say I have 1000 banks and each bank has 12 months time period of how many frauds detected per month and corresponding target variable whether that bank has high chances of fraud or not.Now, when I encounter a new bank with corresponding frauds in 12 months, what are the ways to find whether that bank is fraud or not using the 1000 banks fraud pattern?Can we use any time-series approach? I assume, if it is for single bank, time-series handles it as I have multiple banks, I guess using non-linear regression techniques, assuming each month as one feature, training a model might help?  As I can get a polynomial equation which I can use to predict the target?Please share your thoughts as well"
4753,How can i save feature extraction as tuple from .wav file to excel?,"['python-3.x', 'csv', 'wav', 'feature-extraction']","I'm using following code, But my fbank_feat is tuple and i can't save feature extraction in excel file (in line 28 - df). "
4754,How to set Target in the LSTM for video classification,"['python', 'tensorflow', 'keras', 'lstm', 'feature-extraction']","Please see the following code for creating an LSTM network:In the above code, I am creating an LSTM using Keras library of python, my data has a sample of 131 videos belonging to 8 different classes. I have set a frame sequence of 32 frames for each video(thus each video has 32 frames and hence 131 video generated 4192 frames) I extracted features from a pre-trained model of VGG16 for each of these frames.  I created train dataset through adding Each of these extracted features into an array.  it generated a final array of 4192,512 dimensions. the corresponding train_labels holds one hot encoding for each of the eight classes and have a dimension of 4192,8. However since LSTM needs the input shape of (samples, timestamp, and feature) formate, and each of video in my case has a sequence of 32 frames,  so I reshaped the trained data in to [131,32,512] and applied the same reshaping to the train_labels.  However, when I run this I got the following error:If I do not reshape the train_labels and leave it like (4192,8)  the error is :please note that since each of my videos has 32 frame sequence length that i applied this reshaping [131,32,512] to train data and (131, 32, 8) to corresponding labels. I would appreciate any comment or advice to solve this problem"
4755,How to input the sequence of the images in to LSTM network for video classification,"['python', 'keras', 'generator', 'lstm', 'feature-extraction']","I am using LSTM to classify video. I am using Keras python library to create Long Term Short Memory LSTM network.  I understand that LSTM takes the input shape of the data in (sample, timestamp, Features).  I have three class of video and each of these class has 10 video files. This means that I have 10*3=30 samples. 
I have created a sequence of the frame for each of these video files. Each of these sequences consists of 32frames of video files. I use the trained model to extract features, so I feed each of these frames into VGG16 pre-trained model it that generates 512 features for a single frame. so one video files should have an array of (32,512) dimensions. I then append in each of these arrays into a single array for all the 30 samples and save it as numpy array. the final dimension of the array is (960,512).
Now my problem is how should I reshape this array into(sample,timestamp,features) =(20,32,512). This is the snippet of code I used: Please note that x_generator has 640,512 and I wish to convert it as (30,32,512). 
I would appreciate solving my problem."
4756,In python how to replace nan in sparse csr_matrix,"['python', 'sparse-matrix', 'feature-extraction']","I have hstacked a sprase matrix and a dataframe . The resulting csr_matrix is containing NAN.My question is how to update these nan values to 0 .When I pass X_train_1hc to a clasifier I get error Input contains NaN or infinity or a value too large for dtype('float')1.Is there an option/function/hack to replace nan values in a sparse matrix.
This is a conceptual question and hence no data is being provided."
4757,Is it recommended to combine multiple “related” features into one vector in the scenario below?,"['machine-learning', 'vector', 'data-science', 'feature-extraction', 'feature-selection']","I am a beginner/novice at ""practical"" machine learning.I have compiled a very large data set to create a binary classification machine learning model.  The data set has over 80 columns but I'm trying to shrink that column list down.  I've run the data through multiple algorithms (Decition Tree, Random Forest, Gradient Boosting); used various hyper-parameter tuning; and analyzed multiple permutation feature importance (PFI) results to see what features need to be removed.  So far, my accuracy (and other metrics such as F1-score, precision, recall) is hovering anywhere between 70 and 80%.  My question is this:If I have a subset of 2-4 columns whose data is not only related, but dependent on each other
i.e.
- colA won't make much sense without also looking and using colB, colC, etc
- colA won't make much sense without adding/subtracting/dividing with colBIs it possible/recommended to combine these few columns into a vector or another feature?For example, colA plotted as a time series would make a nice non-linear curved line. colB plotted as a time series would also make a nice non-linear curved line.  However, looking at each of these lines won't make much sense until you look at where they intersect (which happens again and again).  So you can see here that the distance between any two points (colA, colB) is really important.BUT BUT when I include a colC which is the result of difference between colA and colC, the PFI analysis kicks colC back as a bad feature that lowers accuracy, etc.Any help with this is greatly appreciated and thank you all in advance for your help.If you need me to provide any more info/example, let me know.  Thanks again."
4758,How can i feature extraction (.wav) form folders and sub-folders for using as input of the neural network?,"['python-3.x', 'wav', 'feature-extraction']","This is my code, please help me to correct:my error is: 'int' object is not subscriptable. How can i correct this?"
4759,Extract Coefficients/Variables from a PLS in R for Decision Tree or Another Type of Model,"['r', 'variables', 'feature-extraction', 'pls']","I ran a partial least squares (PLS) in R and I want to extract the variables so that I can run either a decision tree or random forest or some other type of model.I tried pls1$coefficientsI want the actual variables itself that it created.  For example PCA creates PC1, PC2, etc. I assumed (maybe incorrectly) that PLS does the same."
4760,how to extract features only within a defined ROI?,"['matlab', 'image-processing', 'feature-extraction', 'feature-detection']","I have 3D matrix 320x320x60 (x,y,z), and I am extracting some specific features in each direction, which features are gradient based. Since data is 3D it takes quite long time (around 45 mins) to do the feature extraction on the whole image. which I need only the features obtained from an ROI mask Is it possible to extract features within the mask only? I do not know how it is possible because some of the features in a specific voxel is calculated based on 1st, 2nd, and 3rd-order gradient in each direction, which considers the neighboring voxels too. 
morover, I do not want to crop the image since for reserving the coordinates.your expert opinion is appreciated"
4761,"how to combine lexical, semantic and bow features extracted from tweet into classifier?","['python', 'machine-learning', 'nlp', 'feature-extraction']","i want to union many features groups: lexical, semantic, and bow features extracted from tweets into classifier im working on authorship verification problem in twitter , the code is as following in the folloiwng is my code : the predicted result is F1 score but i got the following error : "
4762,Can I add and remove features manually from CountVectorizer?,"['scikit-learn', 'feature-extraction', 'feature-selection', 'naivebayes', 'countvectorizer']","I'm doing text classificaiton, and using naive bayes with CountVectorizer. I'm looking for away to add and remove features manually. maybe I can remove features through stop_words(is that the best way?) but I couldn't find a way to add features. if I used 'vocabulary' parameter, then there will be no feature extracted from the text other than the ones present in the vocabulary. and that's a problem"
4763,Comparing results of neural net on two subsets of features,"['python', 'neural-network', 'lstm', 'data-science', 'feature-extraction']","I am running a LSTM model on a multivariate time series data set with 24 features. I have ran feature extraction using a few different methods (variance testing, random forest extraction, and Extra Tree Classifier). Different methods have resulted in a slightly different subset of features. I now want to test my LSTM model on all subsets to see which gives the best results.My problem is that the test/train RMSE scores for my 3 models are all very similar, and every time I run my model I get slightly different answers. This question is coming from a person who is naive and still learning the intricacies of neural nets, so please help me understand: in a case like this, how do you go about determining which model is best? Can you do seeding for neural nets? Or some type of averaging over a certain amount of trials?"
4764,Extracting SIFT features of image dataset to be matched,"['python', 'dataset', 'feature-extraction', 'feature-selection', 'sift']","I have image dataset ant want to extract its features in order to be compared with the query image to select the best features inside threshold. I'm able to extract images features and select the best ones in two corresponding images as the following code:I want to compare the query image features with features of all images inside dataset, to select the best ones in order to recognize the specific object. How can I combine all dataset features and compare them with the query image features? can anyone please help me with thanks."
4765,To One-Hot encode or not to One-Hot encode,"['data-science', 'feature-extraction']","My data set has the day of the week number (Mon = 1, Tue = 2, Wed = 3 ...)My data look like thisShall I one-hot encode WeekDay so it will look like this ?I am going to use Random Forest"
4766,Keras Extraction of Informative Features in Text Using Weights,"['python', 'tensorflow', 'keras', 'feature-extraction']","I am working on a text classification project, and I would like to use keras to rank the importance of each word (token). My intuition is that I should be able to sort weights from the Keras model to rank the words.Possibly I am having a simple issue using argsort or tf.math.top_k.The complete code is from PacktI start by using sklearn to compute TF-IDF using the 10,000 most frequent words.I can view the list of words like this:I then build and fit a model using Keras. Keras is using the tensorflow backend. I can then get weights like this:Since the number of rows (10,000) is equal to the number of features, I think I am on the right track. I need to get a list of indices I can use to get feature names: informative_features = vectorizer.get_feature_names()[sorted_indices].I have tried to build a list using two different techniques:tf.nn.top_kI have not determined how to get a list from this result.argsortFunction argsort returns a matrix, but what I need is a one-dimensional list.How can I use weights to rank text features?"
4767,Generating text features with spacy consumes too much time,"['python-3.x', 'feature-extraction', 'spacy', 'natural-language-processing']",I am extracting text features like noun count from text. The following function is consuming too much time. How can I optimize this?Time it took (around 23 mins for 130000 rows) 
4768,"Why is vgg19 much slower than alexnet to train, 50 hours vs 4 hours respectively?","['matlab', 'svm', 'feature-extraction', 'pre-trained-model', 'vgg-net']",I'm trying to use pretrained model vgg19 and alexnet as feature extractor then using svm for classification. I have about 15000 images as input data. I'm using a server hp proliant g7 but vgg19 takes about 50h to train while alexnet only about 4h. Is this normal or I should look into some setup or data issue on my end for the vgg19 case?
4769,How to extract features from a layer of the pretrained ResNet model Keras,"['keras', 'deep-learning', 'conv-neural-network', 'feature-extraction', 'resnet']",I trained a model with Resnet3D and I want to extract the neurons of a layer. I plan to use them with the SVM classifier. How can I extract these weights and put them to the numpy array? Load the weights by keras extract a layer now what should i do?
4770,Extracting texture features from images by GLCM,"['python', 'image-processing', 'feature-extraction', 'scikit-image', 'glcm']",I'm using GLCM to get texture features from images to use them in classification algorithms like knn and decision tree. When I run the greycoprops function it returns an array of 4 elements for each feature as follows. Should I get the average of each feature to be used in my classification or how should I deal with them?
4771,How to avoid inf value while feature extracting in Matlab?,"['matlab', 'signal-processing', 'feature-extraction']","I’m trying to extract features from some EEG signals. One of the trails in my for loop creates inf value and emd doesn’t accept it so it causes an error! Would you please tell me how I can fix it?!
Thanks a lot in advance."
4772,creating a TFIDF vectors outputs a matrix of lesser dimension,"['python', 'feature-extraction', 'feature-selection']","Goal: Use TFIDF on a dataframe with a single textual columnI am working on the quora question similarity problem. I have the following problems:OUTPUTThe actual output I am getting is (1,1) array whereas I expect a sparse matrix having 198102 row"
4773,How to add new features to already trained model without training again on whole dataset?,"['machine-learning', 'neural-network', 'artificial-intelligence', 'feature-extraction', 'transfer-learning']","Suppose, we have following features on which a classification model (Neural Network) is trained to predict whether a customer will buy Milk or not (0 :Will not buy, 1:Will buy) each week(n):Now, after each week, we want to update the data for model by adding whether the customer bought milk in previous week or not (n-1).Without using the time series, how can we update this information in our already trained model every week ?How can we best tackle this problem ?"
4774,Hashing encoder in spark 1.6 for high cardinal categorical variables,"['machine-learning', 'pyspark', 'apache-spark-mllib', 'feature-extraction']",Folks I have a huge dataset which contains high cardinality categorical variable and therefore I would want to use some hashing encoders to transform the categorical variable. I am unable to find any hashing encoder in 1.6. I see that 2.1 and above do tend to possess some hashing techniques. Any ideas as to how I can do this in 1.6?
4775,How to monitor runtime behavior of OpenCL host code,"['opencl', 'feature-extraction']","I need to run a big test suite of OpenCL host codes and kernels, and I need to extract some features from each execution.I 've managed to extract static features from the kernel source files, but now I need to extract information from the host code that runs them as well, such as total bytes transferred between the host and the device, number of times the kernel is applied (# of iterations) etc.Is there something that can help me or do I have to implement something from scratch? I haven't managed to find anything so far, even though it sounds like something that could live somewhere out there... "
4776,extracting frame features for videos using I3D pytorch implementation,"['deep-learning', 'computer-vision', 'pytorch', 'video-processing', 'feature-extraction']","I have a large datasets of videos related to human activities. I have converted the dataset to RGB frames. I want to generate features for these frames from the I3D pytorch architecture. I don't have the flow frames as of now, is it possible to extract features without the flow. Also if anyone can please help me with the process to extract features with I3D. Thank you very much."
4777,What color feature is important for white-like tones?,"['image-processing', 'svm', 'image-segmentation', 'feature-extraction']","I am building an image segmentation model using SVM as a pixel-wise classifier. I am segmentating pressure ulcers. They have different kind of tissues inside. I want to get the region of the ulcer. I have features like RGB intensities, red, green and blue chromatocities. A type of tissue inside the ulcer has white like colors, but most of the ulcer is reddish. I am getting correct results for the reddish colors but not the whites. 
Can anyone point me a feature or set of features that can captures white textures or white color information to include in the feature vector? 
Thanks in advance... "
4778,OpenCV feature-matching algorithm suggestion for boxes on a conveyor,"['opencv', 'image-processing', 'feature-extraction', 'feature-detection']","OverviewI am attempting to build a prototype of a vision system that would apply pattern matching to figure out the orientation of boxes (eg. soap boxes). Image sampleBelow are real-time captured images of soap boxes in actual environment having two of four possible orientations. (Front_Straight and Back_Inverted orientations). The real-time images will be very similar to these (300x200 pixels per image approx.) ____ The template images will be fed to the system in prior and it has to determine the orientation of boxes moving on a conveyor. The boxes on conveyor are guided so that they can take only one of 4 possible orientations Front_Straight, Front_Inverted, Back_Straight and Back_Inverted i.e boxes cannot be angular.  The camera and the conveyor are fixed so the image size of real-time boxes is constant 300px by 200px. (I have used monochrome camera, if needed colour camera can be used too)Some properties of the vision system prototype:Problem StatementI am looking for a light weight yet robust algorithm that can fairly match template image with real-time images of boxes on conveyor to extract the face and orientation. I am new to feature matching so please guide me as to which feature detector and matcher will be most suitable for this particular case. Also please let me know if it is possible to attain 97% plus accuracy using the low-res realtime image as attached."
4779,How to map the coefficient obtained from logistic regression model to the feature names in pyspark,"['pyspark', 'logistic-regression', 'feature-extraction']","I  built a  logistic regression model using a pipeline flow to the one listed by databricks. 
https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.htmlthe features (numeric and string features) were encoded using OneHotEncoderEstimator and then transformed using standard scaler. I would like to know how to map the weights(coefficients) obtained from logistic regression to the feature names in the original dataframe.In other words, how to get the corresponding features to the weights or the coefficients obtained from the modelThank youI have tried to extract the features from the lrModel.schema, which gave a list of structField showing the features I tried to extract the features from the schema and map to the weights but not successfulthe expected outcome from the extraction a list of tuples(feature weight, feature name)"
4780,"Finding human speech/word in long audio. Which is good STFT or MFCC for feature extrection. And also which algorithm is preferable..CNN ,SVM?","['machine-learning', 'deep-learning', 'signal-processing', 'data-science', 'feature-extraction']","SO i am doing sound classification, in which i have to find out the human speech in long audio, weather specific audio contain human sound or not...SO which feature Extraction method is good for this (STFT,MFCC) and which algorithm is suitable for this problem (SVM,CNN,RNN) .."
4781,How to enhance feature detection using ORB in Opencv?,"['android', 'c++', 'opencv', 'feature-extraction', 'feature-detection']","I'm developing an Android application to match two images using ORB feature detection .The processing and matching logic is called in java using JNI functions.The problem is that the feature detections works well for some images, but fails in some images and some cases.Here is an example of images that fails in some unknown conditionsAfter some thoughts and discussions, I figured out that the problem is that the problem is the lack of features that's why the program fails. Someone in the opencv community tried this image and it gave him 60 keypoints which all of them doesn't survive the RobustMatcher tests.So I need to enhance to features in this image in order to make the matching work.In addition to equalizeHist, what can I do ?I hope you can help me with some suggestions and maybe some examples guys."
4782,Feature Engineering: What we call the process that generate values of features from dataset?,"['machine-learning', 'feature-extraction']",I know that the feature generation means generating new features from the original features and feature extraction means selecting features from the a set of features. So what we call the process that we get the value of each featrue from dataset?
4783,Compute future features with featuretools,"['python', 'pandas', 'feature-extraction', 'featuretools']","I'm trying to use featuretools to generate a feature matrix to train on past data and predict some future data. So this is my setup:And I generate a feature matrix as follows:However what this gives me are two rows (where time is 4, after the curoff) where all values are NAN. The desired behaviour is to fill the values of these rows as well (but computing the aggregations based only on past data). Is this possible with featuretools?"
4784,Using TfidVectorizer() but get error 'ValueError: X has 3 features per sample; expecting 3231926',"['python', 'machine-learning', 'scikit-learn', 'feature-extraction']","My project is building classifier to classify url which is safe to visit and this is dataset for training model using SVMLight format""http://www.sysnet.ucsd.edu/projects/url/#datasets""
(How can I read this datasets as text ?)I trying to convert url input to feature vector using TfidfVectorizer() but I get this error when predict.I'm try to using and SGDClassifier can predict input but alway classify in ""1"" class (Safe to visit url)I try to input malicious url like ""http://www.batmanporn.com"" , ""http://www.virusparty.com"" but still classify in ""1"" classI need to convert input to feature vector  with expecting feature and my classifier can predict as well"
4785,how to do segmentation and feature extraction for this image?,"['matlab', 'image-segmentation', 'feature-extraction']",I have problem with this image and I have more than one images like this. I got zeros value for this image. Can you teach or guide me on how to do segmentation and feature extraction on this image?
4786,Additional audio feature extraction tips,"['python', 'machine-learning', 'feature-extraction']","I'm trying to create a speech emotion recognition model using Keras, I've done all of the code and have trained the model. It sits around 50% validation and is overfitting. When i use model.predict() with unseen data it seems to have a hard time distinguishing between 'neutral', 'calm', 'happy' and 'suprised', but seems to be able to predict 'angry' correctly in the majority of cases - i assume because there's a clear difference in pitch or something. I'm thinking it could possibly be that i'm not getting enough features from these emotions, which would help the model distinguish between them. Currently i am using Librosa and coverting audio to MFCC's. Is there any other way, even using Librosa, that i can do to extract features for the model to help it better distinguish between the 'neutral', 'calm', 'happy', 'surprised' etc?some feature extraction code:Also, this is with 1400 samples. "
4787,How to Extract the following Frequency-domain Features in Python?,"['python', 'feature-extraction', 'spectrum', 'frequency-domain']","So this is a very basic question and I only have a beginner level understanding of signal processing. I have a 1.02 second accelerometer data sampled at 32000 Hz. I am looking to extract the following frequency domain features after having performed FFT in python - Mean Freq, Median Freq, Power Spectrum Deformation, Spectrum energy, Spectral Kurtosis, Spectral Skewness, Spectral Entropy, RMSF (Root Mean Square Freq.), RVF (Root Variance Frequency), Power Cepstrum.The csv file containing data has four columns: Time, X Axis Value, Y Axis Value, Z Axis Value (The accelerometer is a triaxial one). So far on python, I have been able to visualize the time domain data, apply convolution filter to it, applied FFT and generated a Spectogram that shows an interesting shock enter image description hereenter image description hereenter image description hereenter image description hereIf my code is correct and the generated FFT and spectrogram are good, then how can I graphically compute the previously mentioned frequency domain features?enter image description here
enter image description here"
4788,tsfresh extract_features runtime error “freeze_support”,"['python', 'time-series', 'feature-extraction']","I recently installed the tsfresh package to extract features of my timeseries data. I tried to run the example in the documentation and got the following error:I'm a bit confused since it's literally the example code:I get the same error when i try the function with my own data.
What am i doing wrong?"
4789,Original features percentage composing PC1 in PCA,"['python', 'plot', 'bar-chart', 'pca', 'feature-extraction']","I used PCA to find 60 PC's:Now, I'm trying to find the contributions of my features (the columns of X data) to PC1 and PC2. For example, for PC1, I'd like to show a bar plot of the percentage of each of the top 10 features, where the x-axis would have the labels of the features.something like this:
https://i.stack.imgur.com/Hz24K.pngI'm trying to do this in python, but can't figure out how to find the features names. For example what I did is sorting to find the top 10 features variables composing PC1 as:Which results inand plotting it:but this results in X-axis of just numbers 0 to 9 (corresponds to 10 elements). How could I retrieve the names and put labels on the plot bar?https://i.stack.imgur.com/iFdDt.pngThanks so much!"
4790,Contributions of variables to PC in python [closed],"['python', 'plot', 'bar-chart', 'pca', 'feature-extraction']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed last year.I used PCA to find 60 PC's:Now, I'm trying to find the contributions of my features (the columns of X data) to PC1 and PC2. For example, for PC1, I'd like to show a bar plot of the percentage of each of the top 10 features, where the x-axis would have the labels of the features.something like this:
I'm trying to do this in python, but can't figure out how to find the features names. For example what I did is sorting to find the top 10 features variables composing PC1 as:Which results inand plotting it:but this results in X-axis of just numbers 0 to 9 (corresponds to 10 elements). How could I retrieve the names and put labels on the plot bar?Thanks so much!"
4791,Image analysis/feature extraction using PCA,"['python', 'image-processing', 'classification', 'conv-neural-network', 'feature-extraction']","PCA for feature extraction. Hey all: I read a few papers on using PCA for feature extraction, and then using a neural network to classify the images. But I realized that PCA takes 2D data while convolutional nets take 3D data. For now, I can reshape my images to 2D, and run PCA, but I don't know how to input the result into a convolutional net. Thanks in advance."
4792,Understanding of Histogram-of-Oriented-Gradients,"['image-processing', 'computer-vision', 'object-detection', 'feature-extraction']","I am learning about HOG and I understand it from here. A well-explained page with an example. I am not understanding this concept that how it worksA 16×16 block has 4 histograms which can be concatenated to form a 36
  x 1 element vector and it can be normalized just the way a 3×1 vector
  is normalized.How this 36*1 came and how we calculated it? and is it compulsory that we always need 9 bin vector? Is it a fixed size for HOG?came?"
4793,Local Binary Patterns features using matlab 'extractLBPFeatures',"['matlab', 'feature-extraction', 'pattern-recognition', 'lbph-algorithm']","I am trying to understand the extractLBPFeatures function to extract the 59 uniform features, here is a simple code that extracts the 59 features from an imageFrom what I understand according to LBP and Uniform LBP , the LBP is like histogram that counts the frequency of the pixels, but instead of intensity, the binary code of each centre pixel (converted into a new digit),  so we would get bins from 0 to 59 with a number of occurrences of that each digit Here are the LBP features, but shouldn't the numbers be real numbers since it is counting the frequency or occurrences of each pixel?
"
4794,Local Binary features with unit16 images vs unit8 images,"['matlab', 'feature-extraction', 'lbph-algorithm']","I have unit16 image dataset, and I want to extract LBP features from them using  Matlab function extractLBPFeatures, which uses uniform LBP (59 features). 
so when I apply it on unit 16 image  it gives the below: but with unit8 images it giveswhat could be the reason? Is it because that the LBP uses histogram that can be seen as a 256-dimensional (0 - 255), if so, how to deal with unit16 imageshere is the code"
4795,Local Binary Patterns features,"['matlab', 'classification', 'feature-extraction', 'pattern-recognition']","I have a dataset of images that differ in sizes, and when I extract the local binary pattern (uniform local binary 59 feature)  from them I get several features as zeros, is it acceptable as features, if not, and how to deal with it
"
4796,How to apply ransac on segments and plotting them,"['opencv', 'image-processing', 'computer-vision', 'feature-extraction', 'feature-detection']","I am trying to implement https://rd.springer.com/chapter/10.1007/978-3-319-68505-2_6 , but facing issues to filter and plot the results.I am using https://docs.opencv.org/3.4.2/d1/de0/tutorial_py_feature_homography.html to filter the matches.When i am running cv2.findHomography(sr[i], de[i], cv2.RANSAC,5.0) ,python crashes everytime."
4797,Extract keypoint coordinates and draw lines using cv::line,"['python', 'opencv', 'computer-vision', 'feature-extraction']",I found How to get pixel coordinates from Feature Matching in OpenCV Python to extract matched features coordinates and How to draw a line on an image in OpenCV? to draw lines b/w the two points as in the attached image. I am not getting how to use this in my case.enter image description here
4798,Predicting rare events and their strength with LSTM autoencoder,"['deep-learning', 'time-series', 'lstm', 'feature-extraction', 'autoencoder']","I’m currently creating and LSTM to predict rare events. I’ve seen this paper which suggest: first an autoencoder LSTM for extracting features and second to use the embeddings for a second LSTM that will make the actual prediction. According to them, the autoencoder extract features (this is usually true) which are then useful for the prediction layers to predict.In my case, I need to predict if it would be or not an extreme event (this is the most important thing) and then how strong is gonna be. Following their advice, I’ve created the model, but instead of adding one LSTM from embeddings to predictions I add two. One for binary prediction (It is, or it is not), ending with a sigmoid layer, and the second one for predicting how strong will be. Then I have three losses. The reconstruction loss (MSE), the prediction loss (MSE), and the binary loss (Binary Entropy).The thing is that I’m not sure that is learning anything… the binary loss keeps in 0.5, and even the reconstruction loss is not really good. And of course, the bad thing is that the time series is plenty of 0, and some numbers from 1 to 10, so definitely MSE is not a good metric.What do you think about this approach?Thanks,"
4799,How to select most important features? Feature Engineering,"['python-3.x', 'cluster-analysis', 'data-science', 'feature-extraction', 'hierarchical-clustering']","I used the function for gower distance from this link: https://sourceforge.net/projects/gower-distance-4python/files/. My data (df) is such that each row is a trade, and each of the columns are features. Since it contains a lot of categorical data, I then converted the data using gower distance to measure ""similarity""... I hope this is correct (as below..):I then plot the hierarchical_cluster from above into a dendogram: I cannot show it, since I do not have enough  privilege points, but on the dendogram I can see separate colors.
What is the main discriminator separating them?
How can I find this out?
How can I use PCA to extract useful features?
Do I pass my 'hierarchal_cluster' into a PCA function?
Something like the below..?  "
4800,Finding cumulative features in dataframe?,"['python', 'dataframe', 'feature-extraction', 'cumulative-sum', 'cumulative-frequency']","I have a datframe with around 200 features and 3000 rows. These data samples are logged in different time, basically one per month, as shown in the below example in “col101”:Within these features some of are cumulative data so that in every month their values have been increased. For example, col2 and col100 are the cumulative features in my dataframe. So I want to add one more column for each cumulative feature, with the difference with respect to the previous month. So my desired dataframe should be something like this:Now, I have two problems here: 1) how can I automatically recognize those cumulative features with 200 features? and how to add that extra feature (e.g., col22c and col100c) for each cumulative attribute? Does anyone know how I can handle this?"
4801,How to pass the HH subband of an image to greycomatrix function?,"['python', 'opencv', 'feature-extraction', 'scikit-image', 'glcm']","I'm using scikit-images's function greycomatrix, to which I want to give as input one of the components obtained from wavelet transform after haar filtering.If I pass HH to greycomatrix I get this error:ValueError: Float images are not supported by greycomatrix. Convert the image to an unsigned integer type.I tried to convert the image using this code:But I got this error:raise ValueError(""Images of type float must be between -1 and 1."")
  ValueError: Images of type float must be between -1 and 1."
4802,Compare original to modified images,"['python', 'image-processing', 'feature-extraction']","I am working on a project in which I want to compare a non-modified original picture against a dataset that contains images of which some are small to medium alterations of the original image. These alterations can go from simple color changes, gradients, lighting, flipping/rotating the image to even modifications done by a professional in Photoshop and used for a movie poster.My goal is to identify, with rather good accuracy, if the original image has been used in one of the images.I have already tried many different approaches:However, I always have the feeling like all the above have some shortcomings in terms of accuracy and performance.Therefore I was wondering if someone knows a good Python project (Github, website,...) that will allow me to achieve my goal."
4803,Why feature extraction of text don't return all possible feature names?,"['python', 'scikit-learn', 'nlp', 'pytorch', 'feature-extraction']","Here is the snippet of code from the book 
Natural Language Processing with PyTorch:The value of vocab : Why is not there an 'a' among the extracted feature names? If it is excluded as too common word automatically, why ""an"" is not excluded for the same reasons? How to make .get_feature_names() filter other words as well?"
4804,How can i use opencv to get product images from market leaflet?,"['python', 'opencv', 'machine-learning', 'computer-vision', 'feature-extraction']","How can i process a market leaflet with Opencv to extract products image?
For an example, I have this market leaflet:Example of market leafletResult that I expectI tried using some image processing techniques like bilateralFilter, adaptiveThreshold, findContours, but is not so good, because I have to change some parameters to work in others market leaflets.I was thinking in use ORB, SURF and/or FREAK and scrape some product image to extract features and then build a database of this features.Example of my algo to extract product images from market leaflets:Example image inputresult output"
4805,fit_transform output vs components_ in sklearn.decomposition.DictionaryLearning,"['python', 'scikit-learn', 'feature-extraction', 'dimensionality-reduction', 'feature-engineering']","I'm trying to understand the values returned by the fit_transform method and the components_ array in sklearn.decomposition.DictionaryLearning.From the documentation it seems fit_transform returns the sparsely represented input data and components_ contain the atoms.I ran sklearn.decomposition.DictionaryLearning on the digits data set using this code:When I plot X_dict returned by fit_transform (which according to the docs is the transformed input data) using this code :this is what I get:
When I plot the contents of components_ (which should print the atoms) using this code:this is what I get:Now my question is, is this correct? From my understanding of dictionary learning, I feel the first image looks more like the atoms and the latter seems more like the transformed input.Could somebody please explain the difference between atoms and the transformed data and tell me which of these images is which?"
4806,How to extract specific features of same person using different image?,"['keras', 'deep-learning', 'feature-extraction', 'training-data', 'pre-trained-model']","The aim of my project is extracting specific facial features on mobile phone. This a verification application using user's face. Given two different images of the same person, extracting the features as close as possible.Right now, I use the pretrained model and weights of VGGFace team as a feature extractor, you can download the model in here. However, when I extracted features based on the model, the result was not good enough, I described what I did and what I want as below:I extract features from Emma Watson' images, image_1 returns feature_1, image2 returns feature_2 and so on (vector length = 2048). If feature[i] > 0.0, convert it to 1. for i in range(0, 2048):
  if feature1[0][i] > 0.0:
    feature1[0][i] = 1Then, I compare the two features vector using Hamming distance. Hamming distance is just a naive way to compare, in real project, I will quantize those features before comparing. However, the distance between 2 images of Emma still large even though I use 2 neural facial expression images (same emotion, different emotion type return worse result).My question is how could I train the model to extract features of target user. Imaging, Emma is a target user, and her phone only need to extract her features. When someone try to unlock Emma's phone, her phone extract this person's face then compare with saved Emma's features. In addition, I don't want to train a model to classify 2 classes Emma and not Emma. The thing I need is comparing extracted features.To sum up, If we compare features from different images of the same person, the distance (differences) should be ""close"" (small). If we compare features from different images of different people, the distance should be ""far"" (large).Thank you so much."
4807,Create difference columns from one hot encoded columns,"['python', 'pandas', 'feature-extraction']","I'm trying to create some extra features on a data set. I want to get a spatial context from the features I already have one hot encoded. So for example, I have this:I want to create some new columns against the values here:I'm hoping there is an easy way to do this, to calculate changes from the last value of the column and output that to a corresponding column. Any help is appreciated, thanks."
4808,How do I handle string feature in Recursive feature elimination or model training using SVM?,"['python', 'pandas', 'scikit-learn', 'feature-extraction']","I have data which looks like thisSo it contains string as well as numerical features. I first want to perform Feature elimination and then SVM on it. Here is my code to do it.But as column status has string value, it is giving - ValueError: could not convert string to float: 'S'Having such string feature is obvious. What is the standard practice to handle this kind of scenario?"
4809,Mapping Histograms on the image,"['computer-vision', 'histogram', 'feature-extraction']",Features extracted using the HoG method is a single vector. Is there any way to map these histograms on the image and use the mapped image for the further processing to extract the features.
4810,Discrete wavelet transform for image texture analysis,"['image-processing', 'classification', 'feature-extraction', 'wavelet', 'haar-wavelet']","I am planning to use the discrete wavelet transform to extract textural features from grayscale images for classification purpose. However, I am not sure which type of wavelet should I choose? most of the studies I read, using  Haar or Daubechies wavelets when extracting features from images.
So, is there a way to determine which wavelet is suitable?"
4811,Using Featuretools to aggregate per time time of day,"['feature-extraction', 'feature-engineering', 'featuretools']","I'm wondering if there's any way to calculate all the same variables I already am using deep feature synthesis (ie counts, sums, mean, etc) for different time segments within a day? I.e. count of morning events (hours 0-12) as a separate variable from evening events (13-24).Also, within the same vein, what would be the easiest to get counts by day of week, day of month, day of year, etc. Custom aggregate primitives?"
4812,What is the best window size (in seconds) and hop size (in seconds) for a audio sample which has 3 second length?,"['voice-recognition', 'feature-extraction']",I have some voice samples with 3s length size for an audio feature extraction project. First I select 0.5s window size and 0.2 hop size but I doubt how to select best window size and hop size for better results. 
4813,Display extracted feature vector from trained layer of the model as an image,"['python', 'numpy', 'keras', 'feature-extraction', 'keras-layer']","I am using  Transfer learning for recognizing objects. I used trained VGG16 model as the base model and added my classifier on top of it using Keras. I then trained the model on my data, the model works well. I want to see the feature generated by the intermediate layers of the model for the given data. I used the following code for this purpose:In the code, I used np.expand_dims to add one extra dimension for the batch as the input matrix to the network should be  of the form (batchsize, height, width, channels). This code works fine. The shape of the feature vector is 1, 224, 224, 64.Now I wish to display this as image, for this I understand there is an additional dimension added as batch so I should remove it. Following this I used the following lines of the code:However it throws an error: ""Invalid dimensions for image data""I wonder how can I display the extracted feature vector as an image. Any suggestion please. "
4814,TypeError: fit_transform() missing 1 required positional argument: 'raw_documents',"['python', 'machine-learning', 'scikit-learn', 'feature-extraction', 'tfidfvectorizer']","I'm trying to  do feature extraction text with  Sklearn, however I'm getting error Type error:fit_transform() missing 1 required positional argument:
  'raw_documents'It seems I have to make complete some arguments with missing raw document, but i cannot find what is the caused the error, here's my code:When running in the Jupyter notebook console I get the following errors:"
4815,Fusing intermediate outputs of CNN and GRU models,"['conv-neural-network', 'recurrent-neural-network', 'feature-extraction']","In my research work, I have used a two stream model for activity recognition. First stream is based on CNN, while second stream is based on GRU. I have extracted features from the last fully connected layers of both the streams, and then train an SVM classifier for classification.Is it also possible to combine output/feature maps from the intermediate layers of both the streams, and then train an SVM?Thanks"
4816,MGCA technique for speech features extraction shows this error (IndexError: list index out of range),"['python-3.x', 'numpy', 'speech-recognition', 'feature-extraction']","By executing this program for speech features extraction from wav file
  , i got problem in code ,error say IndexError: list index out of rangeFile
  ""C:/Users/KALEEM/PycharmProjects/Speech_Processing/2-Speech_Signal_Processing_and_Classification-master/feature_extraction_techniques/mgca.py"",
  line 77, in 
      mel_Generalized()   File ""C:/Users/KALEEM/PycharmProjects/Speech_Processing/2-Speech_Signal_Processing_and_Classification-master/feature_extraction_techniques/mgca.py"",
  line 74, in mel_Generalized
      mgca_feature_extraction(wav)   File ""C:/Users/KALEEM/PycharmProjects/Speech_Processing/2-Speech_Signal_Processing_and_Classification-master/feature_extraction_techniques/mgca.py"",
  line 66, in mgca_feature_extraction
      writeFeatures(mgca_features,wav)   File ""C:/Users/KALEEM/PycharmProjects/Speech_Processing/2-Speech_Signal_Processing_and_Classification-master/feature_extraction_techniques/mgca.py"",
  line 46, in writeFeatures
      wav = makeFormat(wav)   File ""C:/Users/KALEEM/PycharmProjects/Speech_Processing/2-Speech_Signal_Processing_and_Classification-master/feature_extraction_techniques/mgca.py"",
  line 53, in makeFormat
      wav = wav.split('/')[1].split('-')[1] IndexError: list index out of rangeProcess finished with exit code 1"
4817,Feature Hashing on multiple categorical features(columns),"['python', 'pandas', 'dataframe', 'scikit-learn', 'feature-extraction']",I would like to hash feature ‘Genre’ into 6 columns and separately feature ‘Publisher’ into another six columns. I want something like below:The following code does what I want to do This works for the above two feature but If I have lets say 40 categorical features then this approach would be tedious. Is there any other way to do? 
4818,I have built an image search using VGG16. It takes 4 mins to go through the search. what are the techniques that I can use to shorten this time?,"['image-processing', 'feature-extraction', 'google-image-search', 'vgg-net']","I have built an image search using VGG16 engine, I have a data set of about 20,000 images. It takes 4 mins to go through the search. what are the techniques that I can use to shorten this time?"
4819,"ValueError: operands could not be broadcast together with shapes > (400,2) (400,)","['python-3.x', 'numpy', 'machine-learning', 'speech-recognition', 'feature-extraction']","Dear concerns : I am extracting features from wav , using PLP , this (
  Pyhton 3.6 -Anaconda Spyder ) after execute i am facing error in this
  lineFile
  ""C:\ProgramData\Anaconda3\lib\site-packages\sidekit\frontend\features.py"",
  line 399, in power_spectrum ahan = framed[start:stop, :] * windowValueError: operands could not be broadcast together with shapes
  (400,2) (400,)"
4820,Data preparation before RFECV or any other feature selection,"['python', 'scikit-learn', 'feature-extraction', 'rfe', 'normalization']",I'm trying to figure out if it is wise to remove highly correlated and negatively correlated features before feature selection. Here's a snapshot of my codeSo i've tried it with and without dropping the correlated features and have gotten completely different features. Does RFECV and other features selection (dimensionality reduction methods) take into account these highly correlated features? Am i doing the right thing here? Lastly if removing high threshold features is a good idea should i scale before doing this. Thank you.Kevin 
4821,Can you input multiple time series datasets for a single column ID in TSFRESH,"['python', 'machine-learning', 'feature-extraction', 'feature-detection', 'automl']","As far as I'm aware, TSFRESH expects a number of column IDs (entities) with one set of continual time series data each. If I've got a number of different discrete datasets of time series data for each entity, can TSFRESH use them? These datasets are from the same sensor but are essentially repeats of the same event multiple times."
4822,Significance of sobel's scale when searching Harris corners,"['c++', 'opencv', 'feature-extraction', 'feature-detection', 'sobel']","For function cornerEigenValsVec in corner.cpp, I am stuck on understanding effects of local variable scale passing to Sobel(from line 257 to line 263):To my understanding, scale will be 1/(255*12) if src is of CV_8UC1. Applying 1/255 will normalize pixels' intensity to [0,1], but how about additional scale 1/12? What is its effect?"
4823,audio feature extraction in Java,"['java', 'audio', 'feature-extraction']","I want to write a program that extracts certain features from speech in Java. Such features would include syllable length, pause length, breath length (if possible). What is a good starting point? I have seen things related to FFT and MFCC, but I don't know where to start coding. Are there any existing libraries that can help?"
4824,python feature extraction: AttributeError: 'list' object has no attribute 'lower',"['python', 'scikit-learn', 'nltk', 'feature-extraction']",if am writing this ::i am getting this error ::
4825,"When building Feature based grammar, why do I get “invalid syntax” error?","['python-3.x', 'nltk', 'grammar', 'feature-extraction', 'nltk-book']","Why do I get ""invalid syntax"" in the line with the % start S?"
4826,How is hashing implemented in SGNN (Self-Governing Neural Networks)?,"['tensorflow', 'machine-learning', 'scikit-learn', 'feature-extraction', 'language-features']","So I've read the paper named Self-Governing Neural Networks for On-Device Short Text Classification which presents an embedding-free approach to projecting words into a neural representation. To quote them: The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and complex networks with huge parameters. [...] our method is a truly embedding-free approach unlike majority of the widely-used state-of-the-art deep learning techniques in NLPBasically, from what I understand, they proceed as follow: What doesn't work in all of this, and where my question lies, is in how they could end up with binary features from the sparse projection (the hashing). They seem to be saying that the hashing is done at the same time of computing the features, which is confusing, I would have expected the hashing to come in the order I wrote above as in 1-2-3 steps, but their steps 1 and 2 seems to be somehow merged. My confusion arises mostly from the paragraphs starting with the phrase ""On-the-fly Computation."" at page 888 (PDF's page 2) of the paper in the right column. Here is an image depicting the passage that confuses me: I'd like to convey my school project to a success (trying to mix BERT with SGNNs instead of using word embeddings). So, how would you demystify that? More precisely, how could a similar random hashing projection be achieved with scikit-learn, or TensorFlow, or with PyTorch? Trying to connect the dots here, I've significantly researched but their paper doesn't give implementation details, which is what I'd like to reproduce. I at least know that the SGNN uses 80 fourten-dimensionnal LSHes on character-level n-grams of words (is my understanding right in the first place?).Thanks!EDIT: after starting to code, I realized that the output of scikit-learn's SparseRandomProjection() looks like this: For now, this looks fine, it's closer to binary but it would still be castable to an integer instead of a float by using the good ratio in the first place. I still wonder about the skip-gram thing, I assume n-gram of characters of words for now but it's probably wrong. Will post code soon to GitHub. EDIT #2: I coded something here, but with n-grams instead of skip-grams: https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer"
4827,How can i improve performance of My model without Normalization?,"['python', 'scikit-learn', 'svm', 'data-science', 'feature-extraction']",I'm new to Python and data science. I'm working on Electricity Load Forecasting. When I use Normalisation the results are Good. I get very Small MAE and MSE. But when I apply SVR on the same data without Normalisation I get very High MSE and MAE value.Normalising my Data hereShould I Use Feature Extraction?  I Have obtained SVR Parameters from Grid search?
4828,getting back feature extraction result in a csv,"['python-3.x', 'machine-learning', 'feature-extraction', 'training-data']","I have a csv dataset and had applied feature extraction such as wavelet, kurtosis so that later do machine learning. I am new to python and machine learning so not able to understand how to get back the feature extracted csv file.DatasetI am unable to get back the feature extracted csv file. what should I do?This is the code which i wrote.
I want that i get back features.csv as a result but it is not writing into it."
4829,R: Trouble coding loop function for feature extraction?,"['r', 'for-loop', 'dplyr', 'feature-extraction']","I have two vectors:I need help writing a loop function for the following workflow:This is the result I want to achieve:A new column named ""d"" in the Events dataframe derived from aggregating values in column ""x"" in the Activities dataframe.However, this is as far as I've gotten:Here's my reproduceable data:How can I best accomplish my objective with R?"
4830,How to extract the random shift intensity difference a 3D array elements in MATLAB?,"['matlab', 'image-segmentation', 'feature-extraction']","I am doing image segmentation on 3D medical images. In one of the related papers, authors have extracted a feature that I do not understand how should I calculate it,  f(x, u) = I(x + u) − I(x) is the random shift intensity difference between voxel x and the offset u in mm^3. If we consider that the pixel_spacing of the medical image is 0.36x0.36x0.7 mm^3, how I can extract and obtain this feature for the whole volume in an memory and computationally efficient way for u=2 mm^3? Your help is appreciated."
4831,Predicting customer behavior from previous transactions,"['machine-learning', 'statistics', 'feature-extraction', 'feature-selection']",I am working on a dataset where the customers previous transactions are to be used to predict the future transaction given the transaction history of every customer. The dataset does not come with the labels of the prediction and are to be created. Can you please suggest me the citeria's required to build the predictive labels from the values in the dataset. Given are 
4832,Create features based on cutoff times in featuretools,"['python', 'feature-extraction', 'featuretools']","Im using featuretools and I need to create a feature that uses the cutoff time for its calculation.My entityset consist in a client table and a subscription table (it has more but for the question only these are necessary):client tablesubstription table I created the entity set using client_id as key and setting start_date as time_index Out:Now, I need to create a feature that estimates the time between the cutoff time (i.e. 01/01/2018) and the closest end_plan_date for each client. In algebraic form the calculation should betime_remaining_in_plan = max(subscription.end_plan_date - cutoff_time)Also I need to calculate the amount of time since the client started:time_since_start = cutoff_time - client.start_dateIn my example the expected output for those features should look like this (im assuming the time differences in days, but it could be months also, also im using a time range for the cutoff times):Is there a way to use featuretools to create custom primitives (aggregation or transformation) or seed features that can generate this result??Thanks!!"
4833,How to get the number of days in given month for a pandas dataframe column? [duplicate],"['python', 'pandas', 'feature-extraction', 'feature-selection', 'feature-engineering']","Trying to encode cyclical features for a ML algorithm, where the timestamp feature is very important as feature. I want to transform the day_in_month ('day' column of cyclic_df) into a cyclical variable, so that the 1st of a month is after the last day of a the previous. So 01. February (01.02) is nearer to 31 January (31.01) and thus the difference between the 2 days, if you consider just the day column, is 1 and not 30! The problem is with that 30 with which I divide. Not every month has 30 days, there are months with 30, 31, 28 or 29 days. In each row in cyclical_df, I have a column 'month', a column 'year', and a column 'day'. So theoritically, there should be a solution to read the right number of days for that given month. How can I replace that 30 (line 5 & line 6 in code above), with the right variables, so it reads from the other columns the year and month, and replaces with the right value, and not always 30? PS: It would be very nice, if someone could tell me, if I am doing right for the minute, hour and month, also available in the code above. EDIT (after comments):
Yes, I have a 'year' column. And changing the two line to:I get following error:"
4834,How to pass arguments dynamically to the anova test?,"['python', 'pandas', 'scipy', 'feature-extraction', 'anova']","I have a df, from this df i want to pass arguments for Anova test. but the problem is here df values are dynamic. How to pass arguments into scipy.stats.f_oneway this.For Example:I have to pass the values like below,In the above approach i have to store the values of each type into separate variable. But I want to avoid this. because my values are dynamic. For example, the above sample df has only 4 types,here this 4 is dynamic, it can be anything at running time.So far I can get the values into list using below.But I don't know how to pass this value into scipy.stats.f_oneway. Please give a good approach to solve this."
4835,Feature extraction with R,"['r', 'feature-extraction', 'data-cleaning']",My question is about feature extraction.I would like to build a dataframe from my text.My data is:My expected output is : Thank you in advance for your answers or any other suggestions.
4836,Get Bounding Polygon from contour images,"['opencv', 'tensorflow', 'feature-extraction', 'bounding-box', 'cv2']","I have a dataset of contour images.
In my dataset, each image contain SINGLE object (on black background) which corresponds to a contour-image (i.e. image corresponding to a particular detected contour earlier), retrieved earlier.
I just have these images, and no other contour information.
I need to get contour polygon (height, width, polygon coordinates) for each image so that I can use this dataset for training in Tensorflow models.Will running cv2.findContours() make sense (as each image is already a single contour) or is there another faster way to extract the bounding polygon from the contour images ?Thank you so much in advance."
4837,How can I get a TF feature column from a dataframe column?,"['python', 'tensorflow', 'feature-extraction']","I have a dataframe:
I want to create a feature column out of the integer ""length"" column of this dataframe. How can I achieve that?"
4838,Essentia MusicExtractor python: Warning from AudioLoader,"['python', 'audio', 'feature-extraction']","I am working with the Essentia's MusicExtractor to extract a bunch of features for different time frames on a relatively large music collection. Example: To extract all standard features (plus statistics) for the time frame [3.0, 6.0] and the audio file example.mp3, i do:Depending on the audio file and probably on the time frame, i sometimes get the following warning:[ WARNING  ] AudioLoader: invalid frame, skipping it: Invalid data
  found when processing inputThe features are calculated anyway, so i am not sure if it really is a problem. But if it is a problem then what does it mean and why does it happen?  "
4839,shape of input to calculate information gain,"['machine-learning', 'scikit-learn', 'feature-extraction', 'feature-selection', 'entropy']","I want to calculate the information gain on 20_newsgroup data set.I am using the code here(also I put a copy of the code down of the question).As you see the input to the algorithm is X,y
My confusion is that, X is going to be a matrix with documents in rows and features as column. (according to 20_newsgroup it is 11314,1000 
 in case i only considered 1000 features).but according to the concept of information gain, it should calculate information gain for each feature.(So I was expecting to see the code in a way loop through each feature, so the input to the function be a matrix where rows are features and columns are class)But X is not feature here but X stands for documents, and I can not see the part in the code that take care of this part! ( I mean considering each document, and then going through each feature of that document; like looping through rows but at the same time looping through columns as the features are stored in columns).I have read this and this and many similar questions but they are not clear in terms of input matrix shape.this is the code for reading 20_newsgroup:(X_vec.shape) is (11314,1000) which is not features in the 20_newsgroup data set. I am thinking am I calculating Information gain in a incorrect way?This is the code for Information gain:"
4840,R feature extraction for text,"['r', 'text', 'nlp', 'text-mining', 'feature-extraction']","My question is about text mining, and text processing.I would like to build a dataframe from my text.My data is:My expected output is : My code is:So I would like to extract the reference in a single lineThank you in advance if you have solution for concatenate many citation in one column separated by coma for one article.Thank you :)"
4841,How to apply tsne() to MATLAB tabular data?,"['matlab', 'machine-learning', 'feature-extraction', 'feature-selection']","I have a 33000 x 1975 table in MATLAB, obviously requiring dimensionality reduction before I do any further analysis. The features are the 1975 columns and the rows are instances of the data. I tried using tsne() function on the MATLAB table, but it seems tsne() only works on numeric arrays. The thing is that is there a way to apply tsne on my MATLAB table. The table consists of both numeric as well as string data types, so table2array() doesn't work in my case for converting the table to a numeric array.
Moreover, it seems from the MATHWORKS documentation, as applied to the fisheriris dataset as an example, that tsne() takes the feature columns as the function argument. So, I would need to separate the predictors from the resonses, which shouldn't be a problem. But, initially, it seems confusing as to how I can proceed further for using the tsne. Any suggestions in this regard would be highly appreciated."
4842,Why is this Deprication Warning halting code execution?,"['python', 'scikit-learn', 'feature-extraction']","I tried to use the TfidifVectorizer and CountVectorizer from the Sci-Kit Learn package, but when I import them:
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizerI get the following warning message:/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Mapping, defaultdictMy code stops running after this even though the message is just a warning, indicating that an error arises (even though no error is reported back). I suppose this is the true reason I am asking about the warning because it is all that I have to go off of.
Is this a bug with SKLearn? Since the update to python 3.7, are the developers behind? Any suggestions on whether I should report this, or how to revert to python 3.6 using anaconda to get around this would be greatly appreciated. Thank you!"
4843,Why feature vectors have a lot of zero values in Keras VGG16 model's output?,"['python', 'tensorflow', 'keras', 'deep-learning', 'feature-extraction']","I am trying to extract the features from the last layer of VGG16 model in Keras using the following code:feature variable supposes to be the feature vector but it has a lot of zeros, I think because of relu layer. In Matlab, for example, the extracted features vector seems to have both positive and negative values, how I can get the same with keras model?The matlab code is:the two output vectors feature and trainingFeatures as following (python output to the left and Matlab's to the right
And here is the tested image:"
4844,"In Reinforcement learning using feature approximation, does one have a single set of weights or a set of weights for each action?","['feature-extraction', 'reinforcement-learning']","This question is an attempt to reframe this question to make it clearer.This slide shows an equation for Q(state, action) in terms of a set of weights and feature functions.These discussions (The Basic Update Rule and Linear Value Function Approximation) show a set of weights for each action.The reason they are different is that the first slide assumes you can anticipate the result of performing an action and then find features for the resulting states. (Note that the feature functions are functions of both the current state and the anticipated action.) In that case, the same set of weights can be applied to all the resulting features. But in some cases, one can't anticipate the effect of an action. Then what does one do? Even if one has perfect weights, one can't apply them to the results of applying the actions if one can't anticipate those results. My guess is that the second pair of slides deals with that problem. Instead of performing an action and then applying weights to the features of the resulting states, compute features of the current state and apply possibly different weights for each action.Those are two very different ways of doing feature-based approximation. Are they both valid? The first one makes sense in situations, e.g., like Taxi, in which one can effectively simulate what the environment will do at each action. But in some cases, e.g., cart-pole, that's not possible/feasible. Then it would seem you need a separate set of weights for each action.Is this the right way to think about it, or am I missing something?Thanks."
4845,AssertionError: Index is not unique on dataframe (Entity cust) using Featuretools,"['python', 'python-3.x', 'feature-extraction', 'featuretools']",I have a dataframe as belowI am creating an entity as below using featuretoolsbut i get the erroryet customerid is the identifier 
4846,Optical character recognition - how it works?,"['image-processing', 'computer-vision', 'feature-extraction']","I was interested in symbol recognition recently and I start to read about it in the Internet. I got more information about preprocessing and segmentation stages, but all of it is just prestage for transformation from image to string. And all notes from Internet led me to using ready solution, like Tesseract, which do all works behind interface. However, I interested in detailed description of this process and I want to get all steps of this transformation.
Can anybody give me some links to exhaustive literature or articles about this theme? For example, Tesseract image_to_string() function algorithm. I will thankful for any help"
4847,Feature scaling difference between “normalize” and “Normalizer”,"['machine-learning', 'cluster-analysis', 'data-science', 'feature-extraction', 'unsupervised-learning']",can someone help me with the difference in feature scaling using normalize vs normalizer as in 
4848,How to find Top features from Naive Bayes using sklearn pipeline,"['scikit-learn', 'pipeline', 'feature-extraction', 'naivebayes']","How to find Top features from Naive Bayes using sklearn pipelineHi all,I am trying to apply  Naive Bayes(MultinomialNB ) using pipelines and i came up with the code. However I am interested in finding top 10 positve and negative words , but not able to succeed. when I searched , I got the code for finding top features which i mentioned below. However when i tried using the code using pipeline i am getting the error which i mentioned below. I tried searching exhaustively , but got the code without using pipeline.But when i use the code with my output from pipeline, it is not working.  COuld you please help me on how to find feature importance from pipeline output.FEAURE IMPORTANCE:-Regards,
Shree"
4849,Aggregating over an Event table based on time-window periods in configured in another table,"['sql', 'join', 'google-bigquery', 'aggregate-functions', 'feature-extraction']","I have three tables, UpEvent, DownEvent and AnalysisWindowI want to do analysis at each AnalysisWindow in order to aggregate the UpEvent's and DownEvent's that occurred between the defined window.So for each AnalysisWindow record I would end up with 1 feature row:My first thought was to do something likeWhich obviously doesn't work.Am I approaching this problem incorrectly? I want to do a windowed analysis of the tables at various windows that I configure and get 1 aggregate record per window"
4850,Cropping Image to Object Region's Area,"['python', 'image-segmentation', 'feature-extraction', 'scikit-image', 'glcm']",I'm going to apply texture recognition to different flower pictures and I'm considering GLCM for feature extraction. Before applying GLCM I should convert RGB images to gray images. I also want to subtract background information from gray images to apply GLCM only flower object's pixels to remove noise. I'm considering using skimage. How can I subtract the background pixels from a gray image to ensure that majority of the pixels belong to flower object?  
4851,MATLAB - No SURF features detected in uint16 images,"['matlab', 'image-processing', 'feature-extraction', 'surf', 'image-registration']","I am using SURF features for image registration. I have images of type uint16.pts1 = detectSURFFeatures(image)When I used the above function on uint16 images, the function returned 0 SURF points. As a workaround, I used to convert the images to uint8, after which the function was returning a good amount of detected SURF points. Thus, the function worked on uint8 images, but not on uint16.Note, that the function doesn't show any error. It executes successfully, but the SURFPoints object returned has 0 SURFPoints.Why does this happen? The documentation of the above function says that the image can be single, double, uint8, uint16, int16 and logical. Then why is the function not working on the uint16 images I am using?I experimented by converting the image to double data type. It didn't work. It worked only when I convert the image to uint8.Sorry I cannot share the images since they are confidential data, but did anybody here faced similar issues?I could have continued with the conversion to uint8, but that is posing some other problem, hence I want to know the solution to the fundamental issue.I am using MATLAB 2017."
4852,Difference between grayscale images represented by 3 channels and 1 channel in CNN?,"['image-processing', 'machine-learning', 'conv-neural-network', 'feature-extraction']","I am classifying images where the color doesn't play any role. So, I grayscaled my images and got just grayscale images. But they are represented by 3 channels. I know, that I can use the following formula: to convert RGB image to grayscale image and get grayscale image with just one channel. My question is, is there a difference between using grayscale images represented by 3 channels or by 1 channel in CNN (not in code or filters but in results)? Do the results vary depending on the number of channels, even when the images are the same in both cases? "
4853,Android OpenCV - Recognizing parts of a building,"['java', 'android', 'opencv', 'feature-extraction', 'opencv4android']","I am creating an app to recognize a building or parts of a building to overlay certain highlights.
At first I thought about using Vuforia and Unity like everyone else does, but I feel like it does not give me the freedom I need, especially with the free version.My logic goes a bit deeper than just using a target image, so my idea was to use Android Studio and OpenCV.I am at a point, where I can show feature matching with steps likeCalib3d.findHomography(pts1Mat, pts2Mat, Calib3d.RANSAC, 10, outPutMask, 2000, 0.995);to get good matches and then useFeatures2d.drawMatches(imgFromFile, keyPoints1, imgFromFrame, keyPoints2, better_matches_mat, outputImg);But at this moment I am kind of out of ideas on how to translate the seemingly easy python code you often find to android/java.Things I need to do at this point:Some of my codeI hope some of you can help me to figure out my next steps and how I should continue.
Thanks in advance."
4854,Extracting Information of some columns from a large file based on ID in the another file [closed],"['linux', 'awk', 'feature-extraction']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I have a large text file (about 15G) like below:I also have another text file containing subset of ID from column 5 as below:I would like to extract the related information of columns 1, 2, 4, 5, 8 and 9 for the names within the second text file. I tried some simple grep and awk command, but didn’t work. Could you please kindly help me out with this issue?Thanks"
4855,How to implement GLCM algorithm in python(opencv) for image feature extraction?,"['image', 'image-processing', 'feature-extraction', 'skin', 'glcm']","I need to implement an algorithm in python or with use openCV.
An algorithm which helps in features extraction of an image.
Images which I'm going to use here is skin images.
Please guide me to build this code."
4856,Is there any reason why my results are the same for the most informative feature for binary classification?,"['python', 'binary', 'classification', 'feature-extraction']","Below is the codes to extract most informative feature for binary classification 
for each classes:Below is the output of the method most_informative_feature_for_binary_classification:When the program first run (it will show classes 2 and 3 informative feature):When the program run the second time (it will show classes 3 and 4 informative feature):When the program run the third time (it will show classes 4 and 5 informative feature):If you guys realize the above results, they are all the same. Please help me check the codes for the most_informative_feature_binary_classification method codes. Thanks :((Example of counter.json:Below are the classifiers that have made use of the most_informative_feature_binary_classification method:Naive bayes classifier:Logistic Regression classifier:"
4857,How to extract LBP features from datasets in Matlab?,"['matlab', 'image-processing', 'machine-learning', 'feature-extraction', 'lbph-algorithm']","I've learnt about how to extract features from a single image as described in this example: https://www.mathworks.com/help/vision/ref/extractlbpfeatures.htmlNow I am working with datasets of 1000 images for my matlab project to extract features of bicycle, car and motorbike. I have three separate folders including bicycle, car and motorbike in my dataset. During execution, I am getting error saying,What should I do? Below is my sample code ==>Please help me with my sample code implemented."
4858,Extract features from Colour-only images,"['opencv', 'image-processing', 'colors', 'computer-vision', 'feature-extraction']","I want to extract features for A.I. based colour classification. I'm aware of using histogram of colour spaces as vector of features, however using multiple colour space histograms will make my neural network inputs way too large. A single RGB colour space will give me (3x256 = 768 features). Using more than one colour space for different variations of the same image is just too many features. I'm avoiding using the average of the histograms because I would like to know the colour distribution of the image. The images just consist of colour, no edges or objects present. Any suggestions for what algorithms to use is welcomed.These are examples of the colour images that I want to work with
"
4859,How to extract features using date range within a month?,"['python', 'python-3.x', 'pandas', 'feature-extraction']","I would like to extract features from a datetime column for a day/date for example between day 1 to 10, the output is stored under a column calledearly_month
  as 1 or 0 otherwise.The following question I posted earlier gave me a solution using indexer_between_time in order to use time ranges.How to extract features using time range?
I am using the following code to extract days of the month from date.Thank you."
4860,need normalization before SelectKBest in python,"['python', 'feature-extraction']",I need to select some features from dataset for a regression task. But the numerical values are from different ranges.To increase the performance of regression model do I need to normalize X before SelectKBest method?
4861,sklearn encode hierarchical feature for learning,"['python', 'machine-learning', 'scikit-learn', 'feature-extraction', 'categorical-data']","Does sklearn have an efficient way to encode hierarchical features with many unique values?Here is the context: I have a car price dataset and I want to build a (regression and random forest) model to predict car price. In the dataset, two of the features are categorical: maker and model. For example, the maker includes Nissian, Honda, etc., and the model includes Skyline, Accord, etc.. Each model belongs to a maker, so it forms a hierarchy between these two features.There are 50 different makes and 900 different models in the dataset. I tried to use sklearn's LabelEncoder and OneHotEncoder to encode these two features, which results in a very large sparse matrix with more than 900 dummy variables in each row. Obviously this does use the hierarchy between maker and model, and it would lead to the learning model inefficient. Is there a better way to encode these two features by sklearn?"
4862,From featurers to words python (“reverse” bag of words),"['python', 'scikit-learn', 'feature-extraction']","Using sklearn I've created a BOW with 200 features in Python, which are easily extracted. But, how can I reverse it? That is, go from a vector with 200 0's or 1's to the corresponding words? Since the vocabulary is a dictionary, thus not ordered, I am not sure which word each element in the feature list corresponds to. Also, if the first element in my 200 dimensional vector corresponds to the first word in the dictionary, how do I then extract a word from the dictionary via index?The BOW is created this waythus ""features"" is a matrix (n,200) matrix (n being the number of sentence)."
4863,SLAM system that uses deep learned features?,"['computer-vision', 'conv-neural-network', 'feature-extraction', 'feature-detection', 'slam']","Has anybody tried developing a SLAM system that uses deep learned features instead of the classical AKAZE/ORB/SURF features? Scanning recent Computer Vision conferences, there seem to be quite a few reports of successful usage of neural nets to extract features and descriptors, and benchmarks indicate that they may be more robust than their classical computer vision equivalent. I suspect that extraction speed is an issue, but assuming one has a decent GPU (e.g. NVidia 1050), is it even feasible to build a real-time SLAM system running say at 30FPS on 640x480 grayscale images with deep-learned features?"
4864,how to matching different numbers of features?,"['image', 'matching', 'feature-extraction', 'fingerprint', 'feature-selection']","I'm working on a fingerprint matching program. The extracted features are minutiae, each of these features contains the location (x , y) and angle of direction (o).
The number of features extracted from images varies.How can I match different numbers of features?If I use the Euclidean distance, the vector of features must be equal in dimensions."
4865,Encode categorical features with multiple categories per example - sklearn,"['pandas', 'machine-learning', 'scikit-learn', 'feature-extraction', 'categorical-data']","I'm working on a movie dataset which contains genre as a feature. The examples in the dataset may belong to multiple genres at the same time. So, they contain a list of genre labels.The data looks like this-I want to vectorize this feature. I have tried LabelEncoder and OneHotEncoder, but they can't seem to handle these lists directly.I could vectorize this manually, but I have other similar features that contain too many categories. For those I'd prefer some way to use the FeatureHasher class directly.Is there some way to get these encoder classes to work on such a feature? Or is there a better way to represent such a feature that will make encoding easier? I'd gladly welcome any suggestions."
4866,What does mean affinity='precomputed' in Feature Agglomeration dimensionality reduction?,"['scikit-learn', 'feature-extraction', 'feature-selection', 'dimensionality-reduction']","What does affinity='precomputed' mean in feature agglomeration dimensionality reduction (scikit-learn) and how is it used?
I got much better results than by using other affinity options (such as 'euclidean', 'l1', 'l2' or 'manhattan'), however, I'm not sure what this 'precomputed' actually means and whether I have to provide something ""precomputed"" to the feature agglomeration algorithm?
What does ""precomputed"" actually means?I haven't passed anything other than original data preprocessed (scaled), numpy array. After fit_transform with feature agglomeration, result was passed to Birch clustering algorithm and I got much better results than with other affinities mentioned. Results are comparable with PCA, but  with much lower memory consumption overhead, so I would use feature agglomeration as dimensionality reduction, but I'm concerned that I did it wrong?"
4867,Calculating same features with multiple training windows in Featuretools,"['python', 'pandas', 'feature-extraction', 'feature-engineering', 'featuretools']",Featuretools supports already handling of multiple cutoff times https://docs.featuretools.com/automated_feature_engineering/handling_time.htmlBut as you see for one ID multiple points in time a pandas multi index is generated. How (maybe via a pivot?) can I instead get all the MIN/MAX/... generated columns prefixed with last_x_days_MIN/MAX/... so get additional features per cutoff window?
4868,Featuretools relationship with non unique join key,"['python', 'feature-extraction', 'feature-engineering', 'featuretools']","Assuming I have two tables, one with metadata about a customer with field customer_id and an events table recorded from website clickstream events with fields customer_id, date. Obviously, the second table might have several non unique events (unfortunately date is really only a date not a timestamp).When trying to create https://docs.featuretools.com/loading_data/using_entitysets.html it fails with:How can I either make it unique or make it work?"
4869,Featuretools categorical handling,"['python', 'pandas', 'feature-extraction', 'feature-engineering', 'featuretools']","Featuretools offers integrated functionality to handle categorical variablesHowever should these be strings or pandas.Category types for optimal compatibility with Featuretools?Also, is it required to manually specify all columns like in
https://github.com/Featuretools/predict-appointment-noshow/blob/master/Tutorial.ipynb or will they be inferred automatically from fitting pandas datatypes"
4870,Featuretools handling of multiple join keys,"['python', 'feature-extraction', 'feature-engineering', 'featuretools']",How are one to many relationships with multiple join keys represented in featuretools? Is there some integrated approach - or should the join keys manually be concatenated into a single column?
4871,Can Canny edge detection be used in Hog feature extraction?,"['opencv', 'feature-extraction', 'feature-detection', 'canny-operator', 'sobel']",As I studied so far Canny is an edge detection algorithm and Hog is a feature extraction method. In openCV I saw some implementation of Hog feature extraction with Sobel kernels:Instead of using Sobel is there a way to use Canny alogorithm to calculate gradients for HOG? My goal is to detect cloths in images.
4872,How to choose the weight for a weighted average?,"['feature-extraction', 'weighted-average']","I'm conducting a feature extraction process for a machine learning problem and I came across with an issue.Consider a set of products. Each product is rated as either 0 or 1, which maps to bad or good, respectively. Now I want to compute, for each unique product, a rating score in the [0, n] interval, where n is an integer number greater than 0. The total ratings for each product are obviously different so a simple average will originate issues such as:Even though the ratio a) is higher, ratio b) gives much more confidence to an user. For this reason, I need a weighted average.The problem is what weight to choose. The products' frequency varies from around 100 to 100k.My first approach was the following:At first this sounded like a good solution, but looking at a real example it might not be as good as it looks:Such result suggests that product b) is a much better alternative than product a) but looking at the original ratios that might not be the case. I would like to know an effective (if there is one) way to calculate the perfect weight or other similar suggestions."
4873,How to get featuremap of convolutional layer in keras,"['python', 'machine-learning', 'keras', 'conv-neural-network', 'feature-extraction']","I have a model that I loaded using Keras. I need to be able to find individual feature maps (print values of each feature map). I was able to print weights. Following is my code:The model consists of one convlayer which has total 384 neurons. First 128 have filter size 3, next 4 and last 128 have filter size 5. Then, there are relu and maxpool layers and then it is fed into softmax layer. I want to be able to find outputs (values not shapes) of convlayer, relu and maxpool. I have seen codes online but I'm unable to comprehend on how to map them to my situation."
4874,Understanding the output of mfcc,"['python', 'audio', 'artificial-intelligence', 'feature-extraction', 'mfcc']","I would like to get the MFCC of the following sound.wav file which is 48 seconds long.I understand that the data * frame = length of audio.But when I compute the MFCC as shown above and get its shape, this is the result: (20, 2086)What do those numbers represent?
How can I calculate the time of the audio just by its MFCC?I'm trying to calculate the average MFCC per ms of audio.Any help is appreciated! Thank you :)"
4875,tsfresh multivariate time series forecasting with multiple time series,"['python', 'pandas', 'machine-learning', 'feature-extraction', 'feature-selection']","I'm looking to use tsfresh library for feature selection on a multivariate time series forecasting problem with multiple time series. My problem is similar to that addressed in https://github.com/blue-yonder/tsfresh/blob/master/notebooks/timeseries_forecasting_google_stock.ipynb, but with a few adjustments (I've listed the adjustments in the context of the linked problem). 1) Rather than making predictions for only the Google stock, I'd like to make predictions for a variety of stocks (e.g a set of tech stocks). So there would be a single model able to make predictions for the entire set of stocks.2) Rather than just predicting the next time step (like how well the stock will perform tomorrow), I'd like to predict how much the stock will go up in the next week / two weeks/ etc. 3) I'd like to include data from other stocks (maybe like some predefined set for each stock) for feature selection.4) Rather than just using the high value as a feature, I'd like to use all of the values (high, low, etc.)I've tried and failed to implement this with tsfresh. Can it be done, and if so, how? Here's code that I've written to calculate my own features with a rolling methodwhere full_data is a flattened dataframe with columns: date, stock_id, and a few time series (high, low, etc.)"
4876,How do I pair multiplication across dataframe efficiently,"['python', 'pandas', 'dataframe', 'feature-extraction']","I want to do feature engineering using multiple numeric features, the idea is do pair multiplication across dataframe, preferred answer is something that available on machine learning library, such as TensorFlow, Keras, TPOT, H20, etc (I don't know the scientific name of this process), but is fine to do this without library.Here's my simplified datasetHere's what I needWhat I didThis is prone to mistakes for tons of features. How to do this automatically?"
4877,How to compute the co-occurrence matrix in Java?,"['java', 'image-processing', 'feature-extraction', 'glcm']","I have a question about the co-occurrence formula. How can I implement in Java the calculation of the GLCM of an image? Concretely, I'm trying to figure out how to compute the number of times a pixel has intensity x and the pixel at its immediate right has intensity y. I also need to store the obtained value in the x-th row and y-th column of the resulting co-occurrence matrix.The expected behavior is shown below:Here's what I got so far:CODE (NOT complete yet)OUTPUT (ERROR)I would prefer not to use third-party libraries such as OpenCV or jfeaturelib."
4878,Pre-processing features before applying cross-validation without leakage,"['cross-validation', 'h2o', 'feature-extraction']","I want to do some pre-processing (scaling, feature engineering, for instance target encoding) with cross-validation. I know that the best and theoretically right way to do this is to pre-process the data separately for each train/test step of the cross-validation. However, I am using H2O, which, unless I'm mistaken, doesn't allow me to create a pre-processing pipeline. A h2o documentation page on target encoding offers a workaround to avoid leakage from training folds to the validation fold: ""The target average is calculated on the out of fold data to prevent overfitting."" So the pre-processing on a given fold excludes data from this fold.  It looks to me that this avoids leakage from the training sample to the test sample, but the opposite seems not true. Can I safely use this workaround (provided the #observations >> #features)  or should I be looking for another framework allowing pre-processing pipelines (or doing cross validation by hand) ?  "
4879,Can I use one Examples for multiple Scenario Outline,"['cucumber', 'feature-extraction', 'python-behave']","This is the code I am using:But I got error that first Scenario Outline do not have Examples.
Point is: this Examples are repeated, so I am asking can I use one Examples table for multiple Scenario Outlines?"
4880,How to detect shape in openCV?,"['c++', 'opencv', 'feature-extraction', 'feature-detection']",I'd like to detect a custom shape using a cameraStream + openCV.Now I'd like to detect the position of the shape in an real life image. Also I need to get some feature points of the shape displayed in this image:So I'm pretty sure just detecting contours will not make the full deal. Also HoughTransform will not be a really good method because tbh the contour could be more an crazy ellipse rather than a circle. Another disadvantage is that detecting the blue marked points will be nearly impossible.I'm pretty sure I need to make use of feature matching or keypoint detection but tbh I do not really got any idea how to implement this feature in my camera stream nor how to detect the blue marked points as key points.Note: detectShape will be called for every frame of the camera stream and it's result will be displayed as camera frame.
4881,Performing Feature Matching of Images in Java,"['java', 'opencv', 'feature-extraction', 'feature-detection']","I'm trying to perform feature matching on images in Java. The code example for this I found is this one. I fixed the FeatureDetector deprecation via this answer.However, when I use this code, it does not detect a sub image being part of the bigger image when they clearly are. Here is my adapted code example:My test is the following:The example images are the following:Since the lower image is cut out of the upper one it should definitely be found but the match returns false.Furthermore there are still deprecations left in the code (related question):Can someone fix the deprecations and/or the general code itself to make feature detection work properly because I can't get it to detect anything correctly?"
4882,Calculate features at multiple training windows in Featuretools,"['feature-extraction', 'featuretools']","I have a table with customers and transactions. Is there a way how to get features that would be filtered for last 3/6/9/12 months? I would like to automatically generate features:I've tried using the training_window =[""1 month"", ""3 months""],, but it does not seem to return multiple features for each window.Example:Do I have to do individual windows separately and then merge the results?"
4883,Using hashing trick for new incoming data,"['python', 'pandas', 'feature-extraction', 'dummy-variable', 'one-hot-encoding']",Is there anyway of using hashing trick after I train and deploy my model? Assume I have the following data and I tased the  Cat feature as follows:How can use the taser to hash incoming new data? I am looking for something like:Should I just build a dictionary from the hashing process during training and then just map the new incoming data to the build dictionary? Is there a better way? 
4884,Extracting quantitative Information out of Strings [closed],"['python', 'regex', 'pandas', 'feature-extraction', 'text-extraction']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I am analyzing the Open Food Facts dataset.
The dataset is very messy and has a column called 'quantity' with entries like the following:  '100 g ',
'5 oz (142 g)',
'12 oz',
'200 g ',
'12 oz (340 g)',
'10 f oz (296ml) ',
'750 ml',
'1 l',
'250 ml',
'8 OZ',
'10.5 oz (750 g)',
'1 gallon (3.78 L) ',
'27 OZ (1 LB 11 OZ) 765g  ',
'75 cl',As you can see the values and units of measurement are all over the place! Sometimes the quantity is given in two different measurements...
My goal is to create a new column 'quantity_in_g' in my pandas data frame where I extract the information out of the string and create an integer Value based on the number of grams from the 'quantity' column.
So if the quantity column has '200 g' I want the integer 200 and if it says '1 kg' I want the integer 1000. I would also like to convert the other units of measurements to grams. For '2 oz' I want the integer 56 and for 1 L I would like to get 1000.
Could someone help me to convert this column?
I would really appreciate it!
Thanks in advance"
4885,Using openSMILE with audio stream,"['audio', 'feature-extraction']",I'm trying to use OpenSmile as feature extractor(using emobase2010.conf) and do some classification with that features.What i'm curious about is whether if i can use stream of audio already made to list as input(I'm using ROS communication to get audio stream).At manual of openSMILE it only has example of using .wav as input.Or is there anyway to extract 1582 features(like emobase2010.conf) from audio other then using openSMILE?
4886,Py (os.path): is there a max. size for a string before an automated breakline / return?,"['python-2.7', 'subprocess', 'feature-extraction', 'os.path']","I hope I am able to describe my problem well, sorry in advance if it's complicated.Question:
Does Python (or the os.path calls) automatically insert a return after an amount of characters?Background:
I try to extract acoustic features from .wav files with the tool openSMILE.
For this purpose I pass the strings of the path (inputfile and outputfile)
via subprocess.The SMILExtract call takes 3 arguments (-C for config; -I for inputfile -O for output file). I prepare these 3 Arguments with string operations and save the arguments in a list which gets passed to the subprocess call. In the console the output of the print command (print ' '.join(...)) looks alright (example below)However when I try to run the code with the subprocess call I get an exception. For debugging purposes I copied the output of the print to a text Editor and it appears that a Return gets entered, it looks like this"
4887,Bag of Words (BOW) vs N-gram (sklearn CountVectorizer) - text documents classification,"['python', 'scikit-learn', 'feature-extraction', 'feature-selection', 'n-gram']","As far as I know, in Bag Of Words method, features are a set of words and their frequency counts in a document. In another hand, N-grams, for example unigrams does exactly the same, but it does not take into consideration the frequency of occurance of a word.I want to use sklearn and CountVectorizer to implement both BOW and n-gram methods.For BOW my code looks like this:Is is enought to set 'binary' parameter to True to perform n-gram feature selection?What are the advantages of n-gram over the BOW method?"
4888,Deciding n_features value for scikit learn FeatureHasher,"['machine-learning', 'scikit-learn', 'feature-extraction']","I have a categorical column with 4000 unique levels.When using sklearn.feature_extraction.FeatureHasher for encoding , that column
What should be the n_features value to avoid collision.?"
4889,How to determine the line width in OpenCV's drawMatches,"['python', 'opencv', 'match', 'feature-extraction']","I'm using OpenCV's drawMatches() to draw the matches between keypoints. Is there a way we can specify the width of the lines drawn, as in my case they appear very thin.Thanks."
4890,Handle mismatch in number of features in Training Data and Prediction Data,"['machine-learning', 'data-science', 'feature-extraction', 'feature-selection']","I have 6 text features (say f1,f2,..,f6) available for the data on which I have trained a model. But when this model is deployed and a new data point comes, for which I have to make prediction using this model, it has only 2 features (f1, and f2). So, there is the problem of feature mismatch. How can I tackle this problem?
I have a few thoughts, but that are not very efficient."
4891,How to read or append a specified file without extracting it using python?,"['python', 'zip', 'feature-extraction']",Is it possible to read a file inside a zip folder without extracting it.
4892,How to do probit feature engineering from numerical data (cdf and pdf style) on pandas,"['python', 'pandas', 'dataframe', 'feature-extraction']","This question is based on my current understanding (edit for more exact statistical terminology is very Wellcome). In my assumption, probit is the right terminology. I want to do probit_pdf and probit_cdfprobit_pdf is the probability of the variable is equal certain value
 probit_cdf is the probability of the variable less or same with valueHere's my dataTo make the question clearer, I give example for few Id'sprobit_pdf sample, for Id = 1 :
Here's the expected output, because probability of Value = 2 is 0.40 (4in 10), so the probit_pdf is 0.40.probit_cdf sample, for Id = 5:
And because probability of Value >= 5 is 0.90 (9in 10), so the probit_cdf is 0.90So my expected output is"
4893,How to count feature duplication (or Ridit feature engineering) individually on pandas,"['python', 'pandas', 'dataframe', 'feature-extraction']","This seems have multiple purpose by my machine learning project, it can be count duplication, and can be used as feature extraction as well, luckily can be use to both numerical and categoric, Ridit AnalysysMy data seems to much duplication, and I want to check this. Here's my dataHere's what I wantWhat I did isBut I looking for better alternative for faster way, especially if we have tons of features"
4894,How to save extracted pitch values in a csv file?,"['python', 'audio', 'signal-processing', 'feature-extraction', 'feature-selection']","Well I think I should mention it that it's the very first time I'm trying Audio signal processing in Python. I have an audio data set and I am extracting pitch features using Aubio library, and MFCC feature using the python_speech_features library in Python. The thing is, for a single audio file, I am getting around 84 valued vector for the pitch and 12 valued feature vector for MFCC. Image of extracted pitch feature vector
So how do I save all these so many values in a single csv file? I have around 700 audio files separated in different directories wrt to emotions. Should I take the mean of all of these values and save them wrt the audio file in a csv? Like this:Also, how would I used these values for classification then? 
Any help would be much appreciated, Thanks."
4895,Can any machine learning algorithm find this pattern: x1 < x2 without generating a new feature (e.g. x1-x2) first?,"['machine-learning', 'scikit-learn', 'feature-extraction', 'pattern-recognition']","If I had 2 features x1 and x2 where I know that the pattern is: Can any machine learning algorithm find such a pattern? What algorithm would that be?I know that I could create a third feature x3 = x1-x2. Then feature x3 can easily be used by some machine learning algorithms. For example a decision tree can solve the problem 100% using x3 and just 3 nodes (1 decision and 2 leaf nodes). But, is it possible to solve this without creating new features? This seems like a problem that should be easily solved 100% if a machine learning algorithm could only find such a pattern.I tried MLP and SVM with different kernels, including svg kernel and the results are not great. As an example of what I tried, here is the scikit-learn code where the SVM could only get a score of 0.992:Output running the code:This is an oversimplification of my problem. The real problem may have hundreds of features and different patterns, not just x1 < x2. However, to start with it would help a lot to know how to solve for this simple pattern."
4896,Machine Learning - Feature Generation using dataframes with different sizes,"['pandas', 'dataframe', 'machine-learning', 'scikit-learn', 'feature-extraction']","I have multiple CSV files with 18 columns of sensordata of a production cycle ordered by time. Each CSV file represents one product (smartphone), which production was either sucessful (1) or unsucessful (0). I converted each CSV to a dataframe and brought them together in a dictionary. The CSV files have different numbers of rows.My question is now, if I do have to compress them into one single row with the result of either 1 or 0 at the end to compare different machine learning algorithm (like the multiple logistic regression). For my algorithm, each input is a dataframe, and the output is a label. Concatenating all the rows side by side into one single row could create feature vectors of different lengths.For example: I have 7 CSV files converted into 7 dataframes and put them together in one dataframe with 7 rows (a single row for every dataframe). If I have to compresss one DataFrame to a single row, could you tell me how to do so?Or is it possible to tell the algorithm, that it has to consider every row of a whole dataframe (30000 rows).Thank you very much!  "
4897,How would you describe the connectivity of coordinates?,"['machine-learning', 'feature-extraction']","I'm trying to get a Human pose from their body joints. 
The number of body joints coordinates is 14 for one person (ex. ankle, knee, hip etc)And I need to give their connectivity between the coordinates (ex. ankle-knee, knee-hip) as an input of DNN model. I used to use the relative coordinates (ex.x1-x2, y1-y2) for giving the direction between joints, but there were limitations to increase predict performance. I want to get a fresh and creative idea you have.
If you have any ideas, let's share.  Thanks"
4898,best-found PCA estimator to be used as the estimator in RFECV,"['scikit-learn', 'regression', 'feature-extraction', 'feature-selection']","This works (mostly from the demo sample at sklearn):And this works:but this gives me the error ""RuntimeError: The classifier does not expose ""coef_"" or ""feature_importances_"" attributes"" on the line ""selector1 = selector1.fit""How do I get my best-found PCA estimator to be used as the estimator in RFECV?"
4899,GPU performance during feature extraction (Tesla K80),"['python-3.x', 'keras', 'gpu', 'feature-extraction', 'tesla']","I am using the following code to extract features from about 4000 images divided over 30 classes. Although, my entire dataset is much larger and is up to 80,000 images. When looking at my GPU memory this bit of code works in Keras (2.1.2) for the 4000 images but takes almost up all my 5gig video RAM of my Tesla K80. I was wondering if I could improve my performance by changing the batch_size or is the way this code works just to heavy for my GPU and should I rewrite it?Thanks!"
4900,Understanding Feature Extraction and Feature Vectors in Image Processing?,"['matlab', 'image-processing', 'feature-extraction']","I am working on a small project in Matlab just because of my interest in image processing and I have not studied a degree or a course related to image processing. I want to understand a small concept about feature extraction and feature vectors. I have read some articles about that and in general I can understand that, but my question is:For example, I want to extract some information from different objects of a binary image, the information is about length, width and distance between the objects. In one application I want to extract the features on which I want to apply some algorithms to compute width of all the objects and ignore the length and distance. Can we name this as feature extraction regarding the width? And storing them in different vectors as Feature Vectors? It makes me think that, I might be complicating the simple things. Should I use some other terminologies for this instead of feature extraction and feature vectors? Please suggest me if I am going in the right direction or not?Thank you!"
4901,Extracting Haralick features from GLCM. Why do I get multiple values for each feature?,"['python', 'image-processing', 'feature-extraction', 'scikit-image', 'glcm']","I have seen this paper yesterday. In this paper the features are taken as contrast, local homogeneity and energy, which all are a single values (as per my knowledge) but according to skimage fuction greycomatrix, the parameters passed to these that are distances and angles (which can be more than one).
Here is my code:What confuses me is if I generate a glcm of contrast property it will be of 3x4 size but according to the paper it is a single value and even if I consider all 3x4 values of all the properties as a feature, I bet it will have a over-fitting problem for svm model. "
4902,GLCM Texture analysis in Sentinel-1 SNAP toolbox outputs texture with min and max pixel values not between 0 and 1,"['image-processing', 'feature-extraction', 'glcm']",I have implemented GLCM Texture analysis on the Sentinel-1 SAR imagery. The imagery is high resolution. The parameters for the GLCM texture analysis are:Window size: 5x5Quantizer: Probablistic Quantizer Quantization: 64 bit Angle: 0 degree Displacement: 1The output is 10 different texture images. However the range of pixel values is not between 0 and 1. The range for every texture is between different min and max values. I believe this should be between 0 and 1 as it is a probabilistic analysis with GLCM that is being calculated for every pixel. Am I missing a step?
4903,proper method to save serialized data incrementally,"['python', 'memory', 'scikit-learn', 'pickle', 'feature-extraction']","This must be a very standard problem that also must have a standard solution:What is the correct way to incrementally save feature vectors extract from data, rather than accumulating all vectors form the entire dataset and then saving all of them at once?In more detail:I have written a script for extracting custum text features (e.g. next_token, prefix-3, is_number) form text documents. After extraction is done I end up with one big list of scipy sparse vectors. Finally I pickle that list to space efficiently store and time efficiently load it when I want to train a model. But the problem is, that I am limited by my ram here. I can make that list of vectors only so big before it or the pickling exceeds my ram.Of course incrementally appending string representations of these vectors would be possible. One could accumulate k vectors, append them to a text file and clear the list again for the next k vectors. But storing vectors and string would be space inefficient and require parsing the representations upon loading. That does not sound like a good solution.
I could also pickle sets of k vectors and end up with a whole bunch of pickle-files of k vectors. But that sounds messy.So this must be a standard problem with a more elegant solution. What is the right method to solve this? Is there maybe even some existing functionality in scikit-learn for this kind of thing already, that I overlooked?I found this: How to load one line at a time from a pickle file?But it does not work with Python3."
4904,Combining Text and Numerical Columns for ML Algorithm,"['python', 'pandas', 'machine-learning', 'nlp', 'feature-extraction']","Here I'm working with a Sentiment Classification problem, where I have to predict whether the tweets are positive, negative or neutral. Here's a glimpse of my dataset:However text is the column which I'm fitting in my TfIdf_Vectorizer and using logreg to predict the sentiment. However I'm getting a very low accuracy of ~68%, which turns out to be a pure NLP problem. However the other features will surely increase my accuracy if I can somehow use them.I'm interested in knowing how can I combine the other numerical as well as textual columns like negativereason as features with my text column, to increase my accuracy. Or is there any method of stacking that can be done here? Like combining the predictions of Tfidf and then once again doing prediction with rest numerical columns?TL;DR How to deal with numerical as well as textual columns as features to make a good prediction?"
4905,How can we reduce the size of HOG descriptor changing the size of cells and bloks,"['matlab', 'image-processing', 'feature-extraction', 'descriptor']","I have a question about reducing the size of HOG descriptor i would like without using methods such as PCA .. to reduce the size of this descriptor and obtain a 16 elements in this vector using a specific number of cells and blocks. Idf you have any idea i will be thankfull.
Thanks a lot "
4906,Feature matrix from pipeline,"['python-3.x', 'scikit-learn', 'feature-extraction']",Given the example from scikit learn examples using feature union having a pipeline like below. How is it possible to get the dimensions of the whole feature matrix after the pipeline executed?
4907,Excluding the current row from feature engineering in Python featuretools,"['python', 'pandas', 'datetime', 'feature-extraction', 'featuretools']","I'm generating historical features for the current row with featuretools. For example, the number of transactions made in the last hour during a session.Package featuretools includes parameter cutoff_time to exclude all rows that come after cutoff_time in time.I set cutoff_time as time_index value - 1 second, so I expect the features to be based on historical data minus the current row. This allows including the response variable from historical rows.The problem is, when this parameter does not equal the time_index variable, I get a bunch of NaNs in the original and generated features.Example:Output (excerpt):Column sessions.SUM(transactions.amount) is supposed to be >= 0. Original features session_id  product_id  amount are all NaN as well.If transactions_df['cutoff_time'] = transactions_df['transaction_time'] (no time delta), this code works but includes the current row.What is the right way to calculate aggregates and transformations that would exclude the current row from calculations?"
4908,Selecting a Specific Number of Features via Sklearn's RFECV (Recursive Feature Elimination with Cross-validation),"['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'feature-extraction']","I'm wondering if it is possible for Sklearn's RFECV to select a fixed number of the most important features. For example, working on a dataset with 617 features, I have been trying to use RFECV to see which 5 of those features are the most significant. However, RFECV does not have the parameter 'n_features_to_select', unlike RFE (which confuses me). How should I deal with this?"
4909,Feature extraction for Timeseries LSTM,"['python', 'keras', 'lstm', 'feature-extraction']","I want to feed a timeseries into an LSTM to perform a forecast.
Lets say I have 10000 samples. Now in order to feed the timeseries into my LSTM I reshape it to (samples,timesteps,features). In my case I use timesteps=50 to create subsequences and perform a forecast of t+1. So I end up with x.shape=(9950,50,1). So far so good.My ModelNOW I want to create artificial features e.g. I want to use the fft of the signal as a feature. How I can feed it into my LSTM? Is it legitimate to just compute the fft, append it to the Dataframe and reshape all together so I end up with (9950,50,2)??Questions are basically:Thanks in advance"
4910,how do i mask green pixels using color thresholder app in Matlab?,"['matlab', 'image-processing', 'matlab-figure', 'image-segmentation', 'feature-extraction']","i am doing plant disease detection and classification. first, i do preprocessing step, then image segmentation. In segmentation, i do background removal in RGB image using color thresholder app in matlab. Then, i will do masking green pixels, the pixels where the level of green is higher than red and blue are identified and removed by application of the mask. This is based on the fact that these green pixels most probably represent healthy areas in the leaves. Thus, after removing the background and green pixels, the region left in an image is the region of our interest. **How do i mask green pixels and how do i use threshold values get from
  background removing using color thresholder app in matlab?Please, answer me! This is my main code:And, this code is background removing using color thresholder in matlab:
    % Auto-generated by colorThresholder app on 26-Mar-2018
    %-------------------------------------------------------"
4911,A good way to identify cars at night in a video,"['python', 'opencv', 'object-detection', 'contour', 'feature-extraction']","I'm trying to identify car contours at night in a video (Video Link is the link and you can download it from HERE). I know that object detection based on R-CNN or YOLO can do this job. However, I want something more simple and more faster beacause all I want is to identify moving cars in real-time. (And I don't have a decent GPU.) I can do it pretty well in the day time using the background subtruction method to find the contours of cars:
Because the light condition in the day time is rather stable, the big contours in the foremask are almost all cars. By setting a threshold of the contours' size, I can easily get the cars' contours.
However, things are much different and complicated at night mainly because of the lights of cars. See pictures below:The lights on the ground also have high contrast to the background so they are also contours in the foreground mask. In order to drop those lights, I'm trying to find the differences between light contours and car contours. By far, I've extracted the contour's area, centroid, perimeter, convexity, hight and width of bounding rectangle as features for evaluation. Here is the code:However, I don't see much difference in the features I extracted between lights and cars. Some large lights on the ground can be differed with area and preimeter but it's still hard to differ small lights. Could somebody give me some instructions on this? Maybe some more valuable features or another different method?Thank for @ZdaR's advice. It makes me consider of using cv2.cvtColor to switch the frame image into another colorspace. The reason for doing that is to make the color difference between headlight itself and light on the ground more obvious so we can detect headlights more precisely. See the differece after switching colourspace: ORIGIN(the colour of light on the ground is similar to car light itself):AFTER SWITCHING(one becomes blue and the other becomes red):So what I'm doing now is 1.Switch the colorspace; 2.Filter the swithced frame with certain colour filter (filter out blues, yellows and keep the red in order to keep the car headlights only.)3.Feed the flitered frame to background subtruction model and get the foreground mask then dilation.Here is the code for doing that:And I can get this foreground mask with headlights (and some noise too):Based on this step, I can detect the cars' headlights pretty accurately and drop out the light on the ground:However, I still don't know how to identify the car based on these headlights."
4912,I want to get high quality feature points only,"['c++', 'opencv', 'feature-extraction', 'feature-detection']","I'm currently working on real-time feature matching using OpenCV3.4.0, c++ in QT creator.My code matches features between the first frame that I got by webcam and current frame input from webcam.But the code returns so many matched points that I cannot distinguish which one matches which.So, are there any methods to get high-quality matched points only?And how can I get each matched point's pixel coordinates in QT creator just like MATLAB?"
4913,Want to detect features and match the feature in 2 different frames,"['c++', 'opencv', 'feature-extraction', 'feature-detection']","I'm currently using OpenCV 3.4.0, c++ in QT creator.
I've tried sample code in this page
https://docs.opencv.org/2.4/doc/tutorials/features2d/feature_description/feature_description.htmlbut the code kept returning errorI have imported all the necessary modules I think including xfeatures2dWhat is the problem?And are there any other sample codes that I can try?"
4914,How to extract these 6 symbols (signatures) from paper (opencv),"['opencv', 'feature-extraction', 'opencv-contour']","I have an image:and I'm trying extract the signs one by one.
I tried findContours() but I got a lot of internal contours. Is there any way to do this?"
4915,R extracting job titles from list of sentences,"['r', 'list', 'nlp', 'feature-extraction', 'sentence-similarity']","I am currently writing a script (In R) to extract job titles from sentences which are from biographies of various corporate executives. I have a list of job titles and a list of sentences from the biography and I was wondering how I would go about extracting a list of the job titles in each sentence. I've tried breaking the sentences down into words (same with the job titles) and matching them using %in% and it works but it doesn't pull out whole phrases (Board of Directors is just ""Board"" and ""Directors"" I can't include ""of"" as a bunch of unwanted of's will be pulled out""So essentially I would like some code to look through each of the sentences, match it to the job_title and pull out the job title so that the result is:
a list of ""board of directors"" & ""VP of sales.
Many thanks!"
4916,How do I know which features are selected with SelectKBest?,"['python', 'machine-learning', 'scikit-learn', 'feature-extraction', 'feature-selection']","Some features are selected after running SelectKBest and the result is returned as an array, so I have no idea which features they are since my training set has thousands of features. 
I want to locate and pick out these features in my test set and remove the rest. Is there any convenient way to do so? Thanks!The codes are like:And the result is:"
4917,Inception V3 Image Classification,"['machine-learning', 'computer-vision', 'feature-extraction', 'convolutional-neural-network']","How can I understand what features is the Google Inception V3 model using to classify a set of images, what features or pixels of the images are more significant for classifying them? For instance, if the classifier were to distinguish between a Cheetah and a Leopard, it would probably do so by judging based on their spots. How can I determine what aspects of my images the classifier values most?"
4918,How to represent samples that can belong to multiple categories of a categorical feature,"['tensorflow', 'machine-learning', 'classification', 'feature-extraction']","For example, say I have these data points:The categorical feature column would be [red, blue, green, yellow, orange], but each sample can belong to multiple categories (such as (red, green)).One approach would be to represent each category (color) as it's own column, and then perform a binary encoding on top of that (1 or 0 for true or false).Would this be the best approach in Tensorflow, or is there a better way to do this?"
4919,TypeError: bad argument type for built-in operation using Otsu threshold,"['python', 'feature-extraction', 'threshold']","I'm trying to apply pre-processing filters like Otsu threshold then skeltonization to pass through feature extraction algorithms.i get the errorTypeError: bad argument type for built-in operationat line  im = cv2.imread(img, 0)here is the codethe complete error Message"
4920,Feature extraction using Keras does not include class labels,"['machine-learning', 'keras', 'feature-extraction', 'convolutional-neural-network']","I use Keras for applying image classification on MNIST dataset, implementation is available here. I use this implementation but with adding the following method to extract features from trained images:The output file does not contain class labels, i.e., each row in the file is a set of features but without a class label. Is there a way to modify this method so that it also adds the class label of each extracted feature (i.e. each row)?"
4921,Can we use Principal Components(PCA) with other features? [closed],"['python', 'machine-learning', 'pca', 'feature-extraction', 'feature-selection']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 2 years ago.I have a dataset of 10 features.  Three of these are categorical; when I apply one-hot encoding to these three, they blow up into 96 features. I reduced these 96 features into 20 by PCA.I plan to use the 20 principal components and the remaining 7 features as my final feature set. Is this a good idea: to combine principal components with actual features?"
4922,how to pass equation variable as a feature vector to a training classifier in python,"['python', 'vector', 'classification', 'feature-extraction']","I'm using few equations as centroid skewness and kurtosis to extract features from images how to pass the return of each variable as one feature vector to the classifier ""feature.append(image_statistics(img))""def image_statistics(img)"
4923,Image texture with skimage,"['python', 'image-processing', 'feature-extraction', 'scikit-image', 'glcm']","I'm trying to get texture properties from a GLCM I created using greycomatrix from skimage.feature. My input data is an image with multiple bands and I want the texture properties for each pixel (resulting in an image with the dimensions cols x rows x (properties *bands)), as it can be achieved using ENVI. But I'm too new to this to come to grips with greycomatrix and greycoprops. This is what I tried:Unfortunately, this gives me a 1 x 4 matrix per prop, which I guess is one value per angle FOR THE WHOLE IMAGE, but this is not what I want. I need it per pixel, like contrast for each single pixel, computed from its respective surroundings. What am I missing?"
4924,What's the best way to select features independent of the model being used?,"['python', 'tensorflow', 'regression', 'feature-extraction', 'feature-selection']","I am using tensorflow's DNNRegressor to model a multivariate regression problem. I want to form an optimal feature-set from a mixed bag of categorical and continuous features. What would be the best way to proceed? The reason, I want this approach to be independent of the model is because I couldn't find much about feature selection/evaluation in direct context of tensorflow."
4925,Feature extraction from trained images via CNN into dataset,"['tensorflow', 'machine-learning', 'neural-network', 'feature-extraction', 'convolutional-neural-network']","My goal is to utilize ConvNN to extract the important features given a folder of images of the two types, say dog and cat. From the implementation of the MNIST found here, I want to know if where and how I can extract the features from the trained images into a dataset so that I can use it as an input dataset for another classifier. I have spent a long time trying to do that by myself but I couldn't do so. Any help is highly appreciated.Thank you."
4926,How to visualize fhog (not HOG),"['matlab', 'computer-vision', 'feature-extraction']","My MATLAB code uses fhog (instead of Hog) to extract features. However, I want to visualize the HOG features used on the image patch. I know extractHOGFeatures or VLFeat is used if we use HOG available in MATLAB. But how do I visualize fhog?Since Piotr's Image & Video Toolbox (which has fhog) is widely used in MATLAB now and I frequently need it, it would be great if someone can tell me how to visualize fhog extracted features.The code of fhog can be found at here:The code snippet is as follows:Edit: hogDraw exists but writing the following:gives me an error :"
4927,Spark ML convert Map of counts to feature,"['feature-extraction', 'apache-spark-ml']","I have a Scala Map of seenCounts in specific places, eg.:How should I convert such type of data to features for machine learning?Currently I construct a List[String] of items and use CountVectorizer to convert it to feature, however I am loosing information of how frequent particular place is. I would like not to loose this information."
4928,Librosa feature extraction methods with PySpark,"['apache-spark', 'pyspark', 'feature-extraction', 'spectrogram', 'librosa']","I've been searching long time but can't see any implementation about music feature extraction techniques (like spectral centroid, spectral bandwidth etc.) integrated with Apache Spark. I am working with these feature extraction techniques and the process takes a lot of time for music. I want to parallelize and accelerate this process by using Spark. I did some works but couldn't get any speed up. I want to get arithmetic mean and standard deviation of spectral centroid method. This is what I've done so far.The output of the program is below.So, even though I parallelized the array y (the array of music signal), I can't speed up the process. It takes longer time. I couldn't understand why. I am newbie with Spark concept. I thought to use GPU for this process but couldn't implement that either. Can anyone help me to understand what I am doing wrong? "
4929,“numpy.ndarray' object has no attribute 'get_support” error message after running SelectKBest in Scikit Learn,"['scikit-learn', 'feature-extraction']","I met a question related to this old one: The easiest way for getting feature names after running SelectKBest in Scikit LearnWhen trying to use ""get_support()"" to get the selected features, I got the error message:numpy.ndarray' object has no attribute 'get_supportI would greatly appreciate your kind help!Jeff"
4930,There are deep learning methods for string similarity in machine translation?,"['matlab', 'string-comparison', 'feature-extraction', 'machine-translation']","I am interested in machine translation and more specific I would like to examine the similarity between two strings. I would like to know if there are deep learning methods for text feature extraction. I already tried the famous statistics methods like cosine similarity, Levenstein distance, word frequency and others.Thank you"
4931,Extract main feature of paragraphs using word2vec,"['python', 'word2vec', 'feature-extraction']","I just got a hold of Google's word2vec model and am quite new to the concept. i am trying to extract the main feature of a paragraph using the following method.But am getting the following error where it says that the paragraph checked is not a word in the vocabulary.KeyError: 'word \'The Republic of Ghana is a country in West Africa. It borders Côte d\'Ivoire (also known as Ivory Coast) to the west, Burkina Faso to the north, Togo to the east, and the Gulf of Guinea to the south. The word ""Ghana"" means ""Warrior King"", Jackson, John G. Introduction to African Civilizations, 2001. Page 201.  and was the source of the name ""Guinea"" (via French Guinoye) used to refer to the West African coast (as in Gulf of Guinea).\' not in vocabulary'Now I am aware that the most_similar() function expects a single array. But I would like to know how this can be translated to extract one main feature or word that displays the  main concept of the paragraph using the word2vec model.ModifiedI modified the above code to pass the word_array into the most_similar() method and I' getting the following error.Traceback (most recent call last):
    File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/new_approach.py"", line 108, in 
      print(model.wv.most_similar(positive=word_array, topn=1))
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 361, in most_similar
      for word, weight in positive + negative:
  ValueError: too many values to unpack (expected 2)Modified ImplementationAny suggestions in this regard are much appreciated."
4932,Why harris matrix is positive semi-definite,"['image-processing', 'computer-vision', 'feature-extraction']","I'm learning ""Harris Corner Detector"" algorithm,
and stuck here that why Harris matrix is positive semi-definite.Since Harris matrix's trace is positive, so I can tell Harris matrix's two eigenvalues are all positive or one positive and one negative.So, how to derivate Harris matrix is positive semi-definite?"
4933,"Extract info from json file, Python","['python', 'json', 'midi', 'feature-extraction', 'mido']","I am trying to extract information out of a json file that I dumped. I used mido module to get the information I need, and the only way I've found to get these features is by dumping it as a json. But, after some search, I've tried and not managed to extract and store the info in a python array*(numpy array)*.
Below, you see the sample code. I am pretty sure that double [[ on the begining of the file is making all this trouble.So, to close this up, how could I extract this info? Is json even a good approach? And if so, how could I actually extract this info? Or a way to go inside the [[ ]].
Thank you very much for your time, W7 p3.6+."
4934,How to extract keypoints from Harris Corner Detector using Opencv,"['feature-extraction', 'keypoint', 'corner-detection']",I could do them using SURF quite easily but I want to do it using Harris corner detector.-Having problem with this part (How do i extract the keypoints from above Harris detector)
4935,Features extraction using Matlab,"['matlab', 'statistics', 'matlab-figure', 'feature-extraction', 'feature-selection']","I am new in matlab software so may be my question is simple but I did not found an answer for my my question, I calculated some statistical features for several time series signals , Then I calulate the average of each one of the feature ex: average of mean, average of median, now I want to draw these results (av. of mean ,....) as a new signal , How I can that ? "
4936,Extracting features from the bottleneck layer in Keras Autoencoder,"['keras', 'feature-extraction', 'autoencoder']","I am sequentially asking you for the autoencoder stuff past weeks.
The question today is as follows;
how to obtain features from the bottleneck layer?I have referred this website.
https://github.com/keras-team/keras/issues/2495The error message I got was shown here;
UserWarning: Update your Model call to the Keras 2 API: Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)
  Model(input=[inputs], output=[intermediate_layer])Also, I have tried to extract the features by using this method (go see the link below) and it did not work either.
https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layerAny comments should be helpful.
Thank you!"
4937,Find the column name which has the maximum value for each row,"['python', 'pandas', 'dataframe', 'max']","I have a DataFrame like this one:In here, I want to ask how to get column name which has maximum value for each row, the desired output is like this:"
4938,Can 1D CNNs infer a feature from two other included features?,"['deep-learning', 'convolution', 'feature-extraction', 'feature-selection', 'convolutional-neural-network']","I'm using a 1D CNN on temporal data. Let's say that I have two features A and B. The ratio between A and B (i.e. A/B) is important - let's call this feature C. I'm wondering if I need to explicitly calculate and include feature C, or can the CNN theoretically infer feature C from the given features A and B?I understand that in deep learning, it's best to exclude highly-correlated features (such as feature C), but I don't understand why."
4939,What is the best feature detection?,"['matlab', 'image-processing', 'feature-extraction', 'feature-detection']","We have two images, one as a reference and another image which we want to align like the reference using Matlab. In order to do this, we need to find similar points in both images and than calculate the kernel matrix using the Least Square method for the transformation like this code:In this code we don't care much about the speed, the most important thing to us is the accuracy.1) In here we are using the Surf method and we try to understand if this is the best method for this task, or maybe we should use other feature detection like HOG or FAST etc?2) We try to understand what are the differences between each feature detector and when to use each one of them?Thanks in advance."
4940,How to create a multidimensional picture from a moving camera,"['image', 'opencv', 'image-processing', 'computer-vision', 'feature-extraction']","I am looking for a smart way to solve the following problem:I have a camera, which is mounted to a car. The car is driving on a road.
What I want is a top-view image of the road surface, with all images aligned, but not stitched together.The camera can maybe sees the next 20 meters, but a new photo is taken every 2 meters (depending on velocity).My take on the problem is to calculate the top view with a perspective-transformation and after this to do a feature based matching.
However the feature based matching is pretty slow (and a road is not the best place to search for features).
I was wondering if there are faster methods around that I don't know about?"
4941,Long time for Image Feature Extraction using Wavelets,"['python', 'image-processing', 'machine-learning', 'feature-extraction', 'wavelet-transform']","I have a few directories each with 220 images (borders taken and denoised already) from wich I need to extract features for ML. I wrote a python script that works with a directory of 6 images. Converts each image into a Numpy Array and uses the array to generate a Wavelet, using PyWavelets. The output file is a 17,2MB and takes about 4 minutes. Seems ok to me.But when I run the same script (changing only the string parameters so the directory is the big one) it seems to stay in infinite loop or something. I realize that 220 images would take a a lot longer but the it is running for 2:30 hours and counting. Does't seem right.Plus, when I try to see the file size (via context menu), it changes between 2,2MB to 17,2MB and back to zero. My real question is: Do I keep waiting?The code: Notice that I am using joblib for paralellism. Thaks for any thoughts"
4942,feature vector of image using tensorflow,"['tensorflow', 'feature-extraction']","I am trying to extract the feature vector of the image. I came across this link which seems to do exactly what I need.Link: https://www.tensorflow.org/hub/modules/google/imagenet/resnet_v1_101/feature_vector/1I followed their instructions and have the above method. However, running this code gives me the output as follows:I know to actually evaluate the value, we need a session. I have been working on this for a while now, but have not been able to figure out what changes are needed to actually compute the value of the feature variable. Can anyone please advice me regarding this issue? "
4943,How to process feature vectors with different dimension in machine learning?,"['python-3.x', 'machine-learning', 'logistic-regression', 'feature-extraction']","I'm a beginner in machine learning, and I'm trying to use a data set to train a log linear classifier. The data set contains five features, and each feature is a vector, but the dimension of the features are different. The dimensions are 3, 1, 6, 2, and 2 respectively. I tried PCA method to reduce the dimensions to 1 with scikit-learn, but it didn't works well. So how do I process the features to fit a log linear classifier model like logistic regression?"
4944,Matlab - Plot histogram of oriented gradients with external feature Vector,"['matlab', 'image-processing', 'plot', 'feature-extraction']","A hardware design does calculate a normalised hog feature vector, which I want to visualise with the Matlab Hog library.Is there a function in Matlab, which is able to plot a given feature vector?Edit: I'm open for alternative approaches, perhabs on different platforms"
4945,What is Non Redundant LBP in texture description?,"['c++', 'image-processing', 'opencv3.0', 'feature-extraction', 'lbph-algorithm']",I want to know about non redundant local binary pattern for texture description. What is the difference between original LBP and non-redundant LBP in texture description?Can someone clarify the above mentioned topic through a good example?
4946,Grouped Feature Matrix in Python #2- Follow Up,"['python', 'excel', 'pandas', 'feature-extraction']","It's not too different from before. We can start with the sample data:DataFrame1:DataFrame2:I am looking for a way to make a new excel file that looks like this (Expected Outcome):(Note: It doesn't need to have the head-titles of the No., I just did that for clarity and later explanation).Basically what I have done is, very similar to the other, looks for each name, and then for each name it looks to see how many distinct No.'s it has. It then selects for people who have a certain amount of distinct No.'s. Now, I have a set of ""Comments"" and ""Reports"" I wish to look for 
({Irrelevant, Whatever, Regardless} and {Awesome, Solid, Perfect} respectively [note: this is only a subset of Comments/Reports]) and for these I want to have a 1 or 0 if it appears but only for each No. Put another way, I want for each No. to have a ""group"" of columns titled {Irrelevant, Whatever, Regardless} and {Awesome, Solid, Perfect} and for each value I want a 1 if it appeared for the person for that Specific No. and a 0 if it didn't.In this matrix, for example, we only see John because he is the only one with more than 1 distinct No. In the first group of columns only Irrelevant and Awesome have values of 1 whereas the rest have 0 and in the second group only Regardless and Perfect will have 1s. What it did was it listed all of my desired Comments/Reports ({Irrelevant, Whatever, Regardless} and {Awesome, Solid, Perfect}) for only one No. and then found out if each appeared or not (1 or 0). It then repeated all the desired Comments/Reports in a new ""group"" of columns for a new No. and for this new No. found out which Comments/Reports now appeared.Let me know if anything is unclear and I truly do appreciate your help.Thank you."
4947,How to count the ID with the same prefix and store the total number in another column,"['pandas', 'classification', 'feature-extraction', 'tabular']","I have a dataset in which I noticed that the ID comes with info for classification. Basically, the last 2 digits of ID stand for their sub-ID (01, 02, 03, etc) in the same family. Below is an example. I am trying to get another column (the 2nd column) to store the information of how many sub-IDs we have for the same family. e.g., 22302 belongs to family 223, which has 3 members: 22301, 22302, and 22303. So that I have a new feature for classification modeling. Not sure if there is a better idea to extract information. Anyway, can someone let me know how to extract the number in the same class (as shown the 2nd column)ID  Same class23401   122302   343201   1144501  2144502  222301   322303   3"
4948,Histogram calculation for variance local binary pattern (VLBP) in OpenCV C++,"['c++', 'image-processing', 'opencv3.0', 'feature-extraction', 'lbph-algorithm']",I want to calculate the histogram for Variance local binary pattern of a gray scale image using OpenCV C++.Can someone explain me how to exactly find histogram for variance LBP in OpenCV C++ and what exactly it means?Also please provide some links that are useful in this case. 
4949,Feature extraction from AlexNet fc7 layer in MATLAB,"['matlab', 'machine-learning', 'deep-learning', 'feature-extraction']","I have this AlexNet model in MATLAB:I'm using it to learn features from sequencies of frames from videos of different classes. So i need to extract learned features from the 'fc7' layer of this model to save these features as a vector and pass it to an LSTM layer.
The training process of this model for transfer learning its ok, all right.I divided my data set in a x_train and a x_test sets using splitEachLabel() in my imageDatastore(), and using the function augmentedImageSource() to resize all the images for the network. Everything ok!But when i try yo use this snippet of code shown bellow to resize images from my imageDatastore to be readed by the function activations(), to save the features as a vector, i'm getting an error:Function activations:The error:Someone help me, please!
Thanks for the support!"
4950,What other feature extractors can be used with the BagOfFeatures function in MATLAB?,"['image', 'matlab', 'feature-extraction', 'sift']",I'm developing a program that uses BagOfFeatures but I want to try it with different feature extractors such as SIFT and HOG to compare the results. Is there any way I can integrate this extractors and use them as the extractor for BagOfFeatures?
4951,Recoding Catigorical Variables in Python,"['python', 'scikit-learn', 'feature-extraction', 'categorical-data']","I've been trying to learn Python 3.6 using the Anaconda distribution. I've hit a snag with the content of the online course I'm using, and could use some help working through some error messages. I'd ask the instructors of the course, but they don't seem very responsive to questions from students.I've been having some trouble working with the three dominant classes used to recode categorical data. As I understand it, there are three classes drawn from the scikitlearn package used for recoding variables: LabelEncoder, OneHotEncoder and LabelBinarizer. I have attempted to employ each to recode a categorical variable inside a dataset, but keep getting errors for each.Please pardon my relative noobness for the samples codes. As one might have guessed by the baseness of my question, I am not well versed in python.The object X contains a few columns, the first being a categorical string I need to convert (If someone could also tell me how to insert tables, that'd be helpful. Do I have to use HTML?):""Fish""    1    5    3
""Dog""    2    6    9
""Dog""    8    8    8
""Cat""    5    7    6
""Cat""    6    6    6Label Encoder AttemptBelow is the code I attempted to implement, and the resulting error message I received for the object X, which has roughly the properties I described above.What is throwing me is I thought the above code was clearly defining what y is, the first column of X. OneHotEncoderLabel BinarizerI've found this one the hardest to understand, and actually couldn't make an attempt based on the structure of the dataset.Any guidance or suggestions you could provide would be endlessly helpful."
4952,ORB arm compute library,"['arm', 'computer-vision', 'gpu', 'feature-extraction', 'feature-detection']","The ARM compute library has an attractive set of mature, tested functions. Has anyone managed to move ORB feature detection and description over to an ARM Mali GPU? I have found a CPU only implementation that achieves real-time performance here, but surely a GPU implementation would bring better performance?"
4953,How to extract the finger vein clearly from camera or picture by using opencv,"['c++', 'algorithm', 'opencv', 'computer-vision', 'feature-extraction']","I'm using Opencv extract the finger vein from a camera, first I use the GaussianBlur to smooth the image and then use the adpateThreshold to get the binary image, but the result isn't good, because the camera has a lot of salt noise and it made each frame are difference(for the brightness), the binary image is very unstable(I also tried the medianBlur but the result isn't good too), so I think to extract the vein from camera can't simply use the threshold method. could anyone give me some suggestion about how to extract a clear and stable finger vein by using opencv?
thanks a lot.Photo here"
4954,Machine Learning using Multiple Features - Text Processing,"['machine-learning', 'nlp', 'text-processing', 'feature-extraction', 'feature-engineering']","I have data like following:I went through http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing but I could only find information to vectorize col3 and pass it on for classification. In my scenario, I have numerical information in col1 and col2 as well.If without vectorizing I pass col1, 2 and 3 I get an error for col3 as it is String.If I vectorize col3, the output is a sparse matrix. I need to add col1 and col2 to the vectorized data. How do I do that?I am using scikit-learn."
4955,Find points to split n-dimensional time series based on labeled data,"['machine-learning', 'time-series', 'feature-extraction']","I'm at the starting point of a bigger project to identify common patterns in time series.
The goal is to automatically find split points in time series which splits the series into commonly used patterns. (Later I want to split the time series based on the split points to use the time series in between independently.)One time series consists of:For example, it could look like this:My best bet is to solve this problem with Machine Learning because I need a general approach to detect the patterns based on the user selection beforehand.
Therefore I have a lot of labeled data where the split points are already set manually by the user.Currently, I have two ideas to solve this problem:I prefer 1. because I think it's more important to find out what defines a split point. 
I'm curious about if neuronal networks are well suited for this task?I ask the question not to get a solution for the problem, I just want to get a second opinion on this. I'm relatively new to Machine Learning, that's why it's a bit overwhelming to find a good starting point for this problem. I'm very happy with any ideas, techniques and useful resources which could cover this problem and can give me a good starting point."
4956,Fast Correlation Filter Selection,"['r', 'feature-extraction']","I want to implement FCFS on my training set to select the most relevant and unredundant dependent variables using the r package (Biocomb)  then test the selected feature on svm to ensure their efficiency on the classifier performance(svm) but i didn't understand the parameter attrs.nominal Do i have to add all nominal variables in my dataset in order to discretize them ? Any explanation will be appreciated.
Error in [.data.frame(data.validation, , 72) : 
  undefined columns selected
Error during wrapup: cannot open the connection"
4957,How to find the matched SIFT features that are spatially consistent?,"['image-processing', 'computer-vision', 'feature-extraction', 'sift']","I have extracted DenseSIFT from the query and database image and quantized by kmeans using VLFeat. The challenge is to find those SIFT features that quantized to the same visual words and be spatially consistent (have a similar position to object centers). I have tried few techniques:I am struggling with it for many days, and I hope experts can guide me with this. What are the possible solutions or algorithms that I can use for solving this?"
4958,Use Python to Make a Matrix (Feature Matrix?),"['python', 'excel', 'pandas', 'dataframe', 'feature-extraction']","Let's say I have an excel file that has columns like:And another excel file that reads:What I want to do is make this into a new excel sheet or just a matrix that reads something like:So that in the end I get a matrix that has the names, and the following columns list out the food options and the values that the names are assigned (1 or 0) are for whether or not the item was bought for that day. Notice that in the matrix that I described above only values for the first date was found even though each person had a different initial date. More columns would describe the other dates.Please help."
4959,Late fusion for the CNN features,"['matlab', 'machine-learning', 'computer-vision', 'conv-neural-network', 'feature-extraction']","I am working on early and late fusion of CNN features. I have taken features from multiple layer of CNN. For the early fusion I have captured the feature of three different layers and then horizontally concatenate them F= [F1' F2' F3']; For the late Fusion I was reading this paper. They have mentioned to do supervised learning twice. But couldn't understand the way.For example this is the image taken from the above mentioned paper. 
The first image have three different features and for first supervised learning  the labels lets say will be 1 in 4 class image set. The output for example is [1 1 3]. Lets say the third classifier has wrong result. 
Then my question is then the  multimodal feature concatenation is like [1 1 3] with the label 1 lets say for class 1 image?"
4960,Feature Extraction and higher sensitivity,"['python', 'machine-learning', 'logistic-regression', 'feature-extraction']","When conducting feature extraction (PCA and LDA) on the WBCD dataset followed by logistic regression, I get an improved sensitivity but varying accuracies. I have been trying to find literature that can explain/ has looked into how feature extraction can improve a classifiers sensitivity, but I can't find anything."
4961,How to obtain Probabilities from classifier?,"['machine-learning', 'classification', 'feature-extraction']","We are writing a classification code in python, we want to obtain the probabilities of sample belonging to each class as our output . How do we obtain probabilities instead of class label from the classifier?
We want to know the probability of sample belonging to each class..
(For eg: If there are 9 classes, we need probability of sample belonging to class one, class two and so on..) and then we require to place them in class according to probability prediction and check the accuracy.
we are classifying data samples according to their features extracted (like opcode, API calls etc.). 
We dont want to use Weka for classification.edit: We are planning to use probabilistic classifier like NaiveBayes.
Libraries used are Numpy, scikitlearn
Also, is there a specific format in which features should be obtained?"
4962,How can give varying number of inputs to a neural network?,"['tensorflow', 'machine-learning', 'neural-network', 'feature-extraction']","I am new to Machine Learning, and looking for a solution to my problem.I have to recognize characters in Telugu. For which I have been given a dataset with series of (X, Y) coordinates which when plotted obtains a graph which forms the character.For each character, the seperate file is provided which contain (X, Y) coordinates which form that character when plotted. The number of points (ie: (x,y)) varies from character to character.I am asked by my supervisor to give X and Y coordinates as features to the neural net. How can I do that? as the number of neurons required for character-1 and character-2 vary.
I am not supposed to use CNN but should do as he said.(He's quite rigid)example:points [x,y]character-1 [1,2],[2,3],[4,5] character-2 [2,6],[7,8],[9,10],[11,14],[10,16]"
4963,How to count vowels and consonants in pandas dataframe (both uppercase and lowercase)?,"['python', 'regex', 'pandas', 'text', 'feature-extraction']",Here's my dataHere's my expected output
4964,How to count non-alphanumeric characters on pandas dataframe,"['python', 'pandas', 'text', 'feature-extraction', 'non-alphanumeric']","Here's my dataHere's my expected outputI am only count non-alphanumeric like ! @ # & ( ) % – [ { } ] : ; ', ? / *  space and number is not count     "
4965,How to count uppercase and lowercase on pandas dataframe,"['python', 'string', 'pandas', 'text', 'feature-extraction']",Here's my dataHere's my expected output            
4966,Extract feature vector from 2d image in numpy,"['python', 'numpy', 'machine-learning', 'classification', 'feature-extraction']","I have a series of 2d images of two types, either a star or a pentagon. My aim is to classify all of these images respectively. I have 30 star images and 30 pentagon images. An example of each image is shown side by side here:Before I apply the KNN classification algorithm, I need to extract a feature vector from all the images. The feature vectors must all be of the same size however the 2d images all vary in size. I have extracted read in my image and I get back a 2d array with zeros and ones.My question is how do I process image in order produce a meaningful feature vector that contains enough information to allow me to do the classification. It has to be a single vector per image which I will use for training and testing."
4967,Feature extraction with Gabor filters,"['matlab', 'feature-extraction', 'feature-selection', 'gabor-filter']","I am currently working on feature extraction system with use of Gabor filters in Matlab. For Gabor filters and convolution of image I am using code found here
https://www.mathworks.com/matlabcentral/fileexchange/44630-gabor-feature-extraction ,just slightly adjusted for my own use. Faces for this system are obtained from AT&T face database http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
I use 9 subjects from the database. Each subject has 10 images belonging to him/her. With use of Viola Jones I crop the face image and resize it to 128x128px.This function creates Gabor filter bank. Where u=5, v=8, m,n = 39. So resulting gaborArray consists of 40 different Gabor filters of 5 scales and 8 orientations. I assume these 2 functions are working as intended since they are from approved external source.This is 2nd part where I load my database and convolve each image with Gabor Bank stored in gaborArray which was obtain from previous function. Since I work with 9 subjects x 10 images result is of cellsize 90 x gaborAbs + 1 for class (1-9)This is the part where I probably made mistake in implementation.With help from Gabor feature extraction  I made few changes to previous function to compute Local Energyand to compute Mean AmplitudeBoth vektors of Local energy and Mean Amplitude have length of 41 where last index is class of subject. However when either is used in my face recognition Knn based system I end up with 10% recognition rate in some cases. I tried to plot Local energy feature vektor using andrewsplot in matlab with this as result. Each line represents 1 subject so there are together 90 lines, where lines with same color belong to same class(same person on image). As we can see they are stacked on top of each other with little to no difference  SO finally, and I am sorry for this lenghtly description I ask of you. "
4968,C#: Improving edge point extraction algorithm,"['c#', 'feature-extraction', 'image-recognition']","I am working on an edge points extraction algorithm.
I have a List of points representing a blob (a group of connected pixels) and I want to extract edge points.
I have an example algorithm below, but I am wondering if there is a faster way.
I am using class for BlobPoint, because there is more to it than shown in the example below.
BlobPoint index value represent index value in the original image (image is 1D array of pixels, hence required width information).
This algorithm is build on idea that if a pixel on right or bottom or left or top does not exist in the list, then this point is an edge point.
The list will usually contain between 20 000 to 1 000 000 elements.
yr and yl are added for clarity.My solution:
Ok, I have done some testing and the method above is way too slow for my needs.
I will leave question as this might be of use to someone who looks for faster iteration method.
This is a solution I came up with, but it is still a bit slow:"
4969,extracting features for HOG,"['python', 'numpy', 'feature-extraction']",I am trying to extract features from a dataset that contains images. For this i will be using the HOG concept. But while running the code i am getting this errorThis is my code:-
4970,Using Pyradiomics to calculate shape features on meshes instead of matrices?,"['mesh', 'shapes', 'feature-extraction', 'imaging', 'stl-format']","I am trying to compute some mesh features for 3D models that I created using numpy-stl. I would like to compute all of the features given within pyradiomics, but I am not sure how to use them on just the meshes without them having all of the extra binary image, and matrix information? Unless there is a better program t use for shape feature extraction? Also, in the documentation, it says that there are some features you need to enable C extensions for. How can you do that in your python script?"
4971,How to combine LabelBinarizer and OneHotEncoder in pipeline in python for categorical variables?,"['python', 'machine-learning', 'scikit-learn', 'preprocessor', 'feature-extraction']","I have looked up for the right tutorials and Q/A on stackoverflow for the last few days without finding the right guide, primarily because examples showing use case of LabelBinarizer or OneHotEncoder don't show how it's incorporated into pipeline, and vice versa.I have a dataset with 4 variables:num1 and num2 are numeric variables, cate1 and cate2 are categorical variables. I understand I need to encode the categorical variables somehow before fitting a ML algorithm, but I am not quite sure how to do that in pipeline after multiple tries.This gives me error ValueError: could not convert string to float: 'Cat'Replacing the last 4th line with thiswill give me the same ValueError: could not convert string to float: 'Cat'.Replacing the last 4th line with thiswill give me a different error TypeError: fit_transform() takes 2 positional arguments but 3 were given.Replacing the last 4th line with this will give me this error TypeError: fit_transform() takes 2 positional arguments but 3 were given."
4972,Lower accuracy of VGG16 using data augmentation in Keras,"['neural-network', 'keras', 'conv-neural-network', 'feature-extraction']","I have a question regarding feature extraction with data augmentation in Keras.  I am building a dog breed classifier.By feature extraction, I am referring to extending the model, (conv_base, VGG16) by adding Dense layers on top, and running the whole thing end to end on the input data. This will allow me to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. Training Set:  6680 images belonging to 133 classesValidation Set: 835 images belonging to 133 classesTest Set: 836 images belonging to 133 classesI was able to successfully implement data augmentation and feature extraction independently of one another but when I try combining the 2, my accuracy is coming out incredibly small for some reason.  Why is this?  Am I doing something majorly wrong with my approach?Output looks like this:"
4973,Converting images of alphabet to feature vector,"['python', 'ocr', 'feature-extraction']","I want to write a python code for Persian letter recognition. I have a dataset of  Farsi alphabet that has 15 instances from each class. There are 19 classes.
Actually I don't have much experience in python. I almost know what are the steps theoretically but I dont know the coding.
Fisrt I want to convert images to feature vectors, but I don't know how to do this:/ I've searched a lot but I couldn't find anything useful.Any help would be highly appreciated.
"
4974,Unable to use FeatureUnion to combine processed numeric and categorical features in Python,"['pandas', 'machine-learning', 'scikit-learn', 'normalization', 'feature-extraction']","I am trying to use Age and Gender to predict Med, but I am new to Pipeline and FeatureUnion of Scikit-learn, and encountered some issue. I read through some tutorial and answer, and that's how I wrote the codes below, but I don't have a good grasp on how to feed the split data into the pipeline functions.Errors:Edits:givesAfter changing into 'model__', I am getting the new error:Edits 2:Errors:"
4975,how to solve this error with lambda and sorted method when i try to make sentiment analysis (POS or NEG text)?,"['python-3.x', 'nltk', 'sentiment-analysis', 'feature-extraction', 'text-classification']","Input code:   best = sorted(word_scores.items(), key=lambda w, s: s, reverse=True)[:10000]Result:  How do I solve it?"
4976,OpenCV brief descriptors less than keypoints,"['opencv', 'feature-extraction', 'feature-detection']","I'm using OpenCV to detect features and compute descriptors. For feature detection I'm using FAST:For descriptors I'm using BRIEF:After that, I'd like to order keypoints based on their response and store just a certain number of them:I don't know why I'm giving 100 keypoints to the DescriptorExtractor, but I'm recieving 55 descriptors.I'd be very grateful if you could explain me what is happening.Thanks."
4977,Sklearn: What is the equivalent of check_estimator for TransformerMixin?,"['python', 'python-3.x', 'machine-learning', 'scikit-learn', 'feature-extraction']","I'm trying to create a custom feature extractor using FunctionTransformer for personal use. I'd like to make it as compliant with the scikit-learn API as possible to avoid warnings and errors down the line, but I can't get it to pass the check_estimator test.However, I noticed that a provided feature extractor, DictVectorizer doesn't pass the test either despite being a child of BaseEstimator. My understanding is that the TransformerMixin seems to be causing the issue.Is there an equivalent of check_estimator that works with TransformerMixin? "
4978,"OpenCV Error: Bad argument (bytes must be 16, 32, or 64) in BriefDescriptorExtractorImpl","['c++', 'image-processing', 'feature-extraction', 'feature-detection', 'opencv3.2']","I am trying to detect feature points using SURF and descriptor extractor using BRIEF.But I am getting error can anyone explain about this error and also how to solve it.
I am using OpenCV3.2"
4979,Unable to parse a list of values into a list of strings,"['python', 'pandas', 'machine-learning', 'feature-extraction']","So I need to parse a list of values in python and one-hot encode them for feature engineering. Following is the value from one sample of the 'amenities' column of my featureset.The problem here is that this has both curly braces '{}', as well as values which should be in double quotes but are not  (see: Kitchen, Heating in the example above). If I could convert the above to a string, then I know how to remove the braces and split them into a list.I need to convert the above into a list of items where the values that are not in double quotes become strings."
4980,Scaling data real time for LibSVM,"['svm', 'scaling', 'libsvm', 'feature-extraction']","I am using LibSVM to classify data. I train and test the classifier with linearly scaled feature data on the interval [-1 1]. After establishing a model which produces acceptable accuracy, I want to classify new data which arrives periodically, almost in real time.I don't know how to rescale the feature columns of the 'real time' data on an interval of [-1 1] since I'm only generating 1 row of features for this input data. If I were to store the min/max values of the testing/training set data feature columns (in order to scale new data), this presents the possibility that if the new real time data does not fall into this min/max range, thus the model is no longer valid as I would have to re-scale all prior data to accommodate for the new min/max and generate a new model.I have thought about using other scaling techniques such as mean normalization, but I have read that SVM works particularly well with linearly scaled features so I am hesitant to apply another methodology.How does one deal with the rescaling of new features to a linear interval, when the new features are a single row vector, and could have higher/lower feature values than the max/min feature values used in rescaling the training data? This is the equation I'm using to rescale the training/testing feature set.Even if one were to use another feature scaling technique (such as mean normalization), with each additional 'real time' classification, would it be prudent to recalculate the mean, min and max for ALL (new, test and train) data before rescaling, or is it acceptable to use the stored scaling values from training/testing for new samples -- until a ""re-training"" the classifier to account for all the newly acquired data were to occur. All in all, I think what I'm having trouble with is: how does one deal with linear feature scaling in an 'online' classification problem?"
4981,Feature matching with flann in opencv,"['python', 'opencv', 'image-processing', 'feature-extraction', 'flann']","I am working on an image search project for which i have defined/extracted the key point features using my own algorithm. Initially i extracted only single feature and tried to match using cv2.FlannBasedMatcher() and it worked fine which i have implemented as below:But now i have one more feature descriptor for each key point along with previous one but of different length.
So now my feature descriptor has shape like this:Now since each point's feature descriptor is a list two lists(descriptors) with different length that is (10, 7, ) so in this case i am getting error: setting an array element with a sequence.while converting feature descriptor to numpy array of float datatype:I understand the reason of this error is different length of lists, so i wonder What would be the right way to implement the same?"
4982,How to split a record into multiple record based on start and end date R,"['r', 'feature-extraction']","I would try explaining my problem by taking a sample dataNow I want to outcome to be something like this:Current Approach which I'm thinking of implementing:I've created a dataframe which has 14 records with every month start date and end date for year 2017 and 2018 like:I've made a new column for year and month:If the start date year, Month are same as that of end date year, month then next same start and end date would be copied to the new dataframe likeIf the Start date year, Month are not same then it appends I couldn't find any similar questions, I've gone through this link, but not useful.if there is any better approach do let me know."
4983,HIstogram based feature extraction in an Image,"['image', 'image-processing', 'histogram', 'feature-extraction']","I have an image which is subdivided into twelve ROIs. ROI is further divided into 10*10 pixel blocks. Now I want to compute features such as local contrast, minimum brightness ,sharpness, hue and saturation in each of these blocks , and plot 5 histograms(corresponding to each feature) for each of the ROI. I want to finally combine all histograms into one extended descriptor vector and use it for classification. Can someone please help me with a step wise approach?"
4984,pandas: efficiently apply function that uses as input the whole dataframe,"['python', 'pandas', 'time-series', 'feature-extraction']","I have a pandas dataframe that models product purchases according to dates. I want to add features of how many purchases happened yesterday, last week etc. Is there an elegant and efficient way to do that? Now I am doing a loop, which takes a lot of time..Given the data: to obtain the sales of the previous days and sum of sales of previous two days I loop:Everything in the loop is time consuming, but I can not think of a better way. "
4985,Tensorflow feature column for variable list of values,"['tensorflow', 'machine-learning', 'neural-network', 'feature-extraction']","From the TensorFlow docs it's clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For exampleLet's say ""kitchenware"" maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [""kitchenware"",""electronics""] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is.The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!).What is the correct way to implement this?"
4986,How to extract SIFT features from an image with normalized float values between 0 and 1?,"['image-processing', 'feature-extraction', 'feature-detection', 'sift', 'cv2']",I am using cv2 to compute SIFT features. The values of gray image is between 0 and 1 (continuous float values). The problem is that I am getting the following error if I don't save the type as uint8:after saving as uint8: I am getting a complete blank image. Could someone please suggest a solution?
4987,feature hashing on image features (ORB) - scikit,"['python', 'scikit-learn', 'svm', 'feature-extraction']","I am trying to train an SVM using ORB features for an image. Each ORB point has 32 integer values showing the intensity for the keypoint. The number of keypoints is variable, so I am looking into using feature hashing.here is how the desc_dict looks like I am confused about how to pass this to the FeatureHasher. Should I convert the array into a catenated string? Sorry I am a newbie to this, and could use some guidance."
4988,Extracting Features from the image manually,"['machine-learning', 'feature-extraction', 'feature-detection', 'feature-selection']",I am working on image classification problem. How to find out specific features from the image manually that will help to build a DNN? Consider an image of a man talking on phone while driving for classification as distracted.
4989,"Python_packages, MATLAB and yaafe_extensions directories not created after installing YAAFE","['audio', 'installation', 'feature-extraction']","I'm following this page to install Yaafe. After 'make install' the following directories should be found under my installation directory. But only the first 3 are present. MATLAB, python_packages, and yaafe_extensions directories are not present. Does anyone know what could be the reason?"
4990,How to check what features are extracted while training and testing a CNN model for image classification?,"['python', 'machine-learning', 'conv-neural-network', 'feature-extraction', 'scientific-computing']","I'm using CNN for training and testing images of seeds. I want to know:How do I define my classifier to extract only specific features?The above code is for training the classifier using CNN. how to visually represent the output at each layer while training.
Also how to deploy my trained model into a protocolbuffer(.pb) file for using it in my android project "
4991,Counting occurrence of multiple words Python,"['python', 'pandas', 'dictionary', 'feature-extraction']","I am quite new to Python, so sorry for my not-knowing. I want to create, for example, a table in which the rows are documents and the columns are labels. A document feature matrix if you will. Every label represents an arbitrary number of words (usually 1, 2 or 3 words). They are stored in a dict:Now, I have a number of documents (represented as strings in a df) and I want to find out how often the words from one label appear in a document. For the following:I would like to count the amount of times the values occur somewhat close to each other in a document, for example with this regex I created:Desired output:Edit: If only 1 or 2 words appear in the document, it should get a score of 0. It is important that all words appear in the document! :) By somewhat close to each other, I mean that if it says ""This is a text. These sentences don't contain interesting words. My job is in civil service. The previous sentence does contain interesting words"", 'job' 'civil' and 'service' appear quite close to each other in the entire document, so the document gets a score of 1 for item_4276.Right now, all I got is this, but it is far from satisfactory:In the end I would like to filter the matrix with scores to only have counts of between 5-10 for every document. 
So all in all, written compactly (but obviously not working-ly :-) ):"
4992,Apply object mask to LBP calculation,"['python', 'opencv', 'image-processing', 'feature-extraction', 'lbph-algorithm']","I see lots of articles applying lbp for texture based image classification. I just wonder three things about this technique that I couldn't find clear answers from Google:How the algorithm calculates lbp for border pixels of an image that don't have enough neighbor pixels around them. If we have eight neighbor pixels then the central pixel will have 256 patterns (59 if using uniform). But if we increase the neighbor pixel size (e.g. 8 or 10), then number of patterns will also increase, is that right? In that case, how it impacts to histogram calculation? How can we calculate lbp for the object only. Particularly, if we want to compare objects in images, we only need to calculate lbp and histogram for the objects. I have tried to this idea by using opencv histogram (which supports mask and numpy histogram doesn't support mask) to lbp output but it didn't work. Any idea about how to filter lbp array based on mask and then we can find histogram afterward. Thank you. "
4993,Calculation of feature vectors using HAAR feature extraction algorithm in Python,"['opencv', 'feature-extraction', 'haar-classifier']",OpenCV has the implementation of HOG feature extraction algorithm.Can anyone tell me how can I get HAAR feature vectors in python?
4994,How can I one hot encode a subset of columns?,"['python', 'pandas', 'scikit-learn', 'feature-extraction']","I have a data set which has some categorical columns. Here is a small sample:Here, the dow and precip are categorical, where as the others are continuous.  Is there a way I can create a OneHotEncoder for just those columns?  I don't want to use pd.get_dummies because that won't put the data in the proper format unless of each dow and precip are in the new data."
4995,Batch Processing of files from a directory in openSmile toolkit,"['linux', 'speech-recognition', 'batch-processing', 'feature-extraction']",I am trying to read speech files (.wav) from a directory one by one and extract the feature and store it filewise. I am unable to do so. Any help will be appreciated.I am new to Linux and OpenSmile.
4996,more than 40 features causing error in kernelpca scikit learn,"['python', 'numpy', 'scipy', 'scikit-learn', 'feature-extraction']","When I put the number of features for kernel PCA above 40 it gives an error like so:Below is the code I used:I am puzzled as to why this is happening, could it be due to the size of array (41,77760)?"
4997,Scala Dataframe window lag function on condition,"['scala', 'dataframe', 'spark-dataframe', 'lag', 'feature-extraction']","I have a dataframe with following records.I would like to get the lag features from the above data frame but with a condition on ""id"" column.Is it possible to give condition on lag feature without changing the records?. I have tired like belowThe above code will result in giving the lag features based on grouped id,I would  like to consider the values with ""-"" are child's and they need the parents lag feature. Eg : Row with id A-1(child) needs the value of A(parent) appeared before it.The expected result should be like below."
4998,How do I properly combine numerical features with text (bag of words) in Spark?,"['scala', 'apache-spark', 'machine-learning', 'apache-spark-mllib', 'feature-extraction']","My question is similar to this one but for Spark and the original question does not have a satisfactory answer.I am using a Spark 2.2 LinearSVC model with tweet data as input: a tweet's text (that has been pre-processed) as hash-tfidf and also its month as follows:If there are 30,000 words features won't these swamp the month? Or is VectorAssembler smart enough to handle this. (And if possible how do I get the best features of this model?)"
4999,How to extract feature vector for image when using CNN in Keras,"['keras', 'feature-extraction']","I am doing a binary classification problem, my model architecture is as followI need for each image on a test set, I get a 128-D feature vector collected from FC layer use for SVM classification. More detail, from model.add(Dense(128)). Can you please show me how to solve this problem? Thank you! "
5000,How can Color Histogram be performed on KNN to classify colors?,"['opencv', 'image-processing', 'machine-learning', 'feature-extraction', 'knn']","General Topic: Color Matching Application by Feature Extraction with ClassificationI decided to use ""Color Histogram"" and ""K Nearest Neighbor"" for classifying the colors.I can get the image color histograms and I can deploy them in csv file with label to train KNN. This step is done! However, I can not figure out that how can send test image color histogram values to KNN for classifying it? There are a bunch of matrix values in test image color histogram. So my question is that how can send this point cloud (color histogram values of a test image) to KNN for classifying it?"
5001,Using SVM with HOG Features to Classify Vehicles,"['c++', 'opencv', 'machine-learning', 'svm', 'feature-extraction']","My goal here is to classify between SUVs and sedans using SVMs and HOG features. First I read 86 training images, calculate the HOG features for each of them, and put them in a training Mat that is of size 86xdescriptorSize called HOGFeat_train.Next I create a labels_mat of 86 labels for the supervised learning portion of the SVM (I know this way is impractical and time consuming, which I'll fix later). 1 means SUV, and a -1 means a sedan. Not sure about these SVM Parameters but I've tried different varieties and values but all results are the same.Next I read 10 test images the same way I did with the train images, and compute the HOG features again. After the HOG features are computed they are placed into 1 row x descriptorSized HOGFeat_test Mat, and then I use svm->predict on that HOGFeat_test Mat which should return a value of -1 to denote a sedan or 1 to denote an SUV. The following image shows the result, a test image, and the HOGFeat_train Mat in case it's useful to anyone. The result (Sedan, -8.412e08) is always the same no matter what values or parameters or images I use. The result is not a -1 or a 1 but -800000000000 and I'm assuming a negative value corresponds to a -1, but most importantly I'd like to know why the result isn't changing. Does anyone have any insight of this? Thanks. EDIT----------------------------------------I removed all of the ones from float labels[86] and simply left it as float labels[86]; //{1, 1, -1, etc...}This showed no difference in the SVM result and it was still able to train successfully. This tells me that my labels arent going through the svm->train function or something. I will continue to investigate. "
5002,Horizontal Stack 2d Numpy Array with 3d Numpy Array,"['python', 'numpy', 'machine-learning', 'dataset', 'feature-extraction']","I'm generating my feature dataset for machine learning, and I have a 2d numpy array X where X.shape = (n, d) - n samples, d features.Now I generate a new feature with one-hot-encoding - f where f.shape = (n, 1, k) - n samples, k labels. What would be the best way for me to add this new feature to my existing feature dataset?"
5003,How to save a frequency distribution plot?,"['python', 'matplotlib', 'text-mining', 'streamlit']","I was trying to save a frequency distribution plot in Python using fdist.to_file().
After getting an error, I was searching in the forum and found this post: How to save a nltk FreqDist plot?. However, I have two problems here:I didn't want to use this method because I am using Streamlit. So I thought it would be bad for others to find things in the code. I mean, I would do something like this:in line 10. And then, in line 500 I would do:So I was wondering if there is another option to do this.In the end, what I need is an alternative because the line that will save the image will be separated from the ones that generate it."
5004,Tidytext word count analysis resulted wrong word [closed],"['r', 'text-mining', 'word-count', 'tidytext']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 3 days ago.I am running biomedical literature text mining using the tidytext package. After the tokenization steps, I have run the following codeenter image description here
The word textless appears to be one of the most commonly mentioned words (had a count of 13,838). This cannot be right by any means. None of my 16,000 abstracts has this word. I can possibly remove this word by creating custom_stop_words. However, it is quite puzzling to me why this word appears to have such a huge count. Has anyone experienced this?"
5005,Automate a function for many txt files,"['r', 'text-mining']","I have 600 txt files to clean, analyze, and write a one line output appended to a csv file. I am able to do this for one file at a time by copying and pasting the name of each txt file, one at a time. I would like to automate this so my code will see all 600 txt files in the directory, then run the code for each of them. Ultimately I should get a csv report with 600 lines summarizing the data of each report.I'm definitely a newbie, so it's cool if you talk down if you have a suggestion. I think the help I need comes in the first 3 lines, and the final line. I've posted all of my code just in case."
5006,Text mining in r - Finding most frequently occurring word from a column of string in a data frame in r,"['r', 'dataframe', 'text-mining']","Is there a way to find out most frequently words used in a column of strings in a data frame in r? I came across lots of  functions for doing that using text corpus but none for a dataf rame. I need to do it for a data frame so that i can create ""Metadata"" for the products. Below is an example of the data i have and the result i am trying to achieve. Any help is highly appreciated. Thanks!Product data for a grocerNow i want to find the most frequently occurring word from the ""combineall"" column and list those in a new column next to it. Basically i am trying to create metadata from the product description. Thanks again!"
5007,Please help me understand why I am getting an error from removewords [closed],"['r', 'text-mining']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 7 days ago.Could someone help me identify why I am getting an error here? How would I define removewords?Error in tm_map.SimpleCorpus(myCorpus, removewords, myStopwords) :
object 'removewords' not found"
5008,How to automate text extraction from PDFs?,['text-mining'],"I want to extract specific information about an animal (the Red Panda is here used as an example) from a large number of PDFs. Example questions include: What is the general description of the Red Panda? What is its lifespan?The link below is a PDF file that contains information on the Red Panda. Without having to read through the entire PDF, I seek to answer: What do red pandas eat?https://www.researchgate.net/profile/Achyut_Aryal/publication/258515861_Summer_Diet_and_Distribution_of_the_Red_Panda_Ailurus_fulgens_fulgens_in_Dhorpatan_Hunting_Reserve_Nepal/links/00b49528c53254a89b000000/Summer-Diet-and-Distribution-of-the-Red-Panda-Ailurus-fulgens-fulgens-in-Dhorpatan-Hunting-Reserve-Nepal.pdfIdeally, this software or automated process should be able to search and extract relevant text sections on the diet of the Red Panda either by sentences or in paragraphs  in an output file, such as in Microsoft Excel format. I should be able to answer the question “What do Red Pandas eat?” just from reading the extracted text.  Any help or information is much appreciated."
5009,I want to remove “ (string )” from a text using regex [duplicate],"['python', 'regex', 'text-mining']","i want to convert :  Bowmore 46 year old (distilled 1964), 42.9%into              :  Bowmore 46 year old , 42.9%"
5010,"R wordcloud error, invalid cex, unsure of reason","['r', 'text-mining', 'word-cloud']","I can't quite figure out whats wrong, perhaps cause I don't completely understand text mining in the first place. The syntax tends to confuse me... but I digress, this is following a tutorial yet I've come up with an error. the one in particular well,,As the title implies I'm trying to use the wordcloud. However I turn up with this error when I use it. If you're curious about what data I have, I'm using the same database referenced in this post which the kind lads there helped me within SQL. But now I'm using R.... here is the code in question.any help is appreciated!"
5011,missing library from Github,"['python', 'machine-learning', 'text-mining']","I am reading ""Text Analytics with Python, A Practitioner’s Guide to Natural Language Processing""
Second Edition by Dipanjan Sarkar.The book points to the author's GitHub, which looks empty https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition (not sure why!)I googled and I found it in Apress' GitHub https://github.com/Apress/text-analytics-w-python-2e but it misses very importat code like the model_evaluation_utils.pyIs the author not sharing model_evaluation_utils.py on purpose? (per the book all code should be available...)"
5012,How to process many txt files with my code in R,"['r', 'text-mining']","I'm quite a novice, but I've successfully managed to make some code do what I want.
Right now my code does what I want for one file at a time.
I want to make my code automate this process for 600 files.
I kind of have an idea, that I need to put the list of files in a vector, then maybe use lapply and a function, but I'm not sure how to do this. The syntax and code are beyond me at the moment.
Here's my code...#Packages are callled#this is my code to run the code on a bunch of text files. Obviously it's unfinished, and I'm not sure if this is the right approach. Where do I put this? Will it even work?##this bit cleans the document##import key words and run analysis on frequency for the document#write the summary for each company to a csv"
5013,Automate text mining of many .txt files in R,"['r', 'text-mining']","I'm working in R Studio. I'm a novice.
I have about 1200 text files to analyze with R.
For each one, a one line summary is written into a csv report.
I can do this for one file at a time by manually copying and pasting the file name into my code, and running the code one report at a time.
I want R to look at the directory of text files and run the code for each one.
this eludes me.I know I can get a file list and put it in a vector.I'm not sure how to automatically have my code run for all the files listed in the vector.
Any help is appreciated. Thanks."
5014,Extract text from cells marked by regions,"['r', 'text', 'unicode', 'text-mining', 'stringr']","I don't know how else to describe this problem. I apologize for the most vague title ever.this is what the data looks like[us]Deftek
[jp]<U+306F><U+3061><U+307F><U+3064> (Honey)
Hampern
[jp]<U+3067><U+3055><U+3093><U+3068> (Descente)
[jp]<U+5E73><U+30DC><U+30E0> (Hirabomb)
[jp]<U+30A2><U+30AD><U+30E9> (Akira)
Balls Out
[jp]Teguru
[jp]MeltySo the names Hampern and Balls Out extract just fine, but the others I cannot extract anything from."
5015,How to tokenize my dataset in R using the tidytext library?,"['r', 'text', 'text-mining', 'tidytext']","I have been trying to follow Text Mining with R by Julia Silge, however, I cannot tokenize my dataset with the unnest_tokens function.Here are the packages I have loaded:Here is the dataset I tried to use which is online, so the results should be reproducible:And here is where everything falls apart.Input:Output:Error in tbl[[input]] : subscript out of boundsFrom what I have read about this error, in Rstudio, the issue is that the dataset needs to be a matrix, so I tried transforming the dataset into a matrix table and I received the same error message.Input:Output:Any recommendations for what next steps I could take or maybe some good Text mining sources I could use as I continue to dive into this would be very much appreciated."
5016,Is there a function for selecting LSA component for a regression in R?,"['r', 'text-mining', 'pca', 'lsa', 'latent-semantic-analysis']","I'm working on a project involving text-data and I'm building a model (logistic regression) for interpreting some customer's opinion about the company they work for. I've decided to use LSA for reducing the number of predictors in the regression, but I'm not able to find a function for selecting the right components according to the results of my regression (via cross-validation). I know there is a similar function for dealing with PCA but I can't find something equivalent for Latent Semantic Analysis.
The function used for PCA is called prc() and it's contained into the pls library."
5017,Data extracting form Instagram,"['python', 'instagram', 'data-mining', 'text-mining', 'network-analysis']","I am going to extract different collaborative healthcare networks in s Instagram and exploring their actors and attributes. To do this, I am looking for posts that include #collaborativehealthcare and #collaborativehealth.
When I search and extract all captions, I couldn't see the exact name of the actor. For instance, I can see a specialist share a new treatment method so that we can conclude there is a collaboration network among professionals and eve professionals and patients through sharing information and giving feedback.  Or we can see a post for a particular workshop/seminar/ course so that we can appreciate the collaboration among different professionals and even healthcare students and professionals. Now, I am looking for a data extracting method to analyze the posts and extract such networks and the actors that we need!
Would you please help me to know what data extracting method could be useful?Best
Tahi"
5018,Text and data mining for Scientific Research (Biology),"['data-science', 'data-mining', 'text-mining']","I’m a newbie biology researcher who’s venturing into text and data mining of academic search engines (e.g. Google Scholar, Wiley Online, Web of Science, etc). I want to automate the process of extracting information of species from scientific literature (usually PDFs). I would like to generate an Excel file of short text sections that contain the specific information.Let’s use Bigeye Tuna as an example. This is what I have done so far:The output from Agent Ransack is a long list of text sections from within the PDFs. The text sections consist of irregular length of texts (e.g. some are complete sentences, some are text columns from tables, some are just a couple of words). Ideally, I would like the output to be text extracts from the PDFs of concise sentences that tells me the different sizes, color, etc recorded for this tuna.How should I move on from here or what can I improve on? I feel like there may be tools / softwares out there that I’m unaware of that are able to meet my research objective. Separately, there are figures and tables within the PDFs (e.g. images of maps or graphs). I would like to extract those as well but have no idea how to do so.Thank you so much!"
5019,Why Python gensim package gives better coherence scores compared to TexmineR in R,"['python', 'r', 'text-mining', 'lda', 'topic-modeling']","I am really confused to see the results on the coherence scores using Python Gensim and R TexmineR packages.I've trained both models for the same number of topics (from 5 to 15). Using the first one the average score is around 4.0, for the second one is not more than 2.0. So, for me the models coming from R are not to use as their coherence score is too low.I would like to use R as I am more experienced in this language. But if I don't find a solution I will have to switch to Python.Just wanted to know if you have any idea on how to solve this problem? Is it because gensim Python provide better quality algorithm?Ps. I've tried training LDA in R with different parameters to make sure the source of the problem is not situated inthere.Thank you in advance for your support and help.All the best,
Evangelia"
5020,How to extract comment using PSAW and save them in a dataframe?,"['python', 'pandas', 'text-mining', 'reddit', 'praw']","I am usinggen1 = api.search_comments(subreddit=""Fitness"", q=""dumbbell"", limit=100)dt = pd.DataFrame([obj.d_ for obj in gen1])but this dataframe does not include the comment body ..
is there a function such as comment.body for PSAW like there is in PRAW.
I couldn't find a way to extract N number of comments with a Keyword in PRAW."
5021,Login twitter using Request,"['python', 'web-scraping', 'beautifulsoup', 'python-requests', 'text-mining']","I tried to login into twitter using Request, whenever i scrape the authenticity token it comes back as a None so i can't extract value for it.It returns the error below:"
5022,Scrape data from NCBI books section?,"['python', 'text-mining', 'biopython', 'ncbi']","I'm currently writing a program which requires me to scrape articles from the NCBI.I'm using the Entrez Utilities to do this (https://www.ncbi.nlm.nih.gov/books/NBK25497/).I have figured out how to do this with PubMed data, namely by using handle = Entrez.efetch(db='pubmed', id=pmid, retmode='text', rettype='abstract').However, I want to scrape data from the books section of the NCBI because the pubmed section contains incomplete articles (compare for example https://pubmed.ncbi.nlm.nih.gov/20301533/ vs https://www.ncbi.nlm.nih.gov/books/NBK1359/).I have a list of all the GeneReviews ID's (e.g NB1359, NB1400 etc.) in a text file but I'm not sure how to go about scraping this data because handle = Entrez.esearch(db='books', term=""NB1359"", retmode='text') does not return the text in the article."
5023,"using the Stanford NLP package in R, installing it and putting it in the library","['r', 'stanford-nlp', 'text-mining']","I'm trying to get the Stanford NLP library going in R, and I can't import the library. To be clear, it works fine when I do this:install.packages(""coreNLP"")But when I do thislibrary(coreNLP)I get the following error message:Unable to find any JVMs matching version ""(null)"".
No Java runtime present, try --request to install.
Error: package or namespace load failed for ‘coreNLP’:
.onLoad failed in loadNamespace() for 'rJava', details:
call: fun(libname, pkgname)
error: JVM could not be found
In addition: Warning messages:
1: In system(""/usr/libexec/java_home"", intern = TRUE) :
running command '/usr/libexec/java_home' had status 1
2: In fun(libname, pkgname) :
Cannot find JVM library 'NA/lib/server/libjvm.dylib'
Install Java and/or check JAVA_HOME (if in doubt, do NOT set it, it will be detected)I think it has something to do with Java, perhaps even rJava. But I don't know what to do from here. I'm running this on a Mac (Mojave). And again, this is in R.I've tried things likeinstall.packages('devtools')
devtools::install_github(""statsmaths/coreNLP"")Sys.getenv(""LD_LIBRARY_PATH"")
""/usr/local/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/""from other Stackoverflow pages, but I'm pretty much just cutting and pasting stuff in. Obviously, I'm not very skilled with R..."
5024,Removing Stop words from a list of strings in R,"['r', 'dplyr', 'text-mining', 'tidytext']",Sample dataDput code of my dataI want to remove the stop words from the above data set using tidytext::stop_words$word and also retain the same columns in the output. Along with this how can I remove punctuation in tidytext package?Note: I don't want to change my dataset into corpus
5025,Assessing LDA predictions with textmineR in R - Calculating perplexity?,"['r', 'text-mining', 'lda', 'topic-modeling', 'perplexity']","I am working on a LDA model with textmineR, have calculated coherence, log-likelihood measures and optimized my model.As a last step I would like to see how well the model predicts topics on unseen data. Thus, I am using the predict() function from the textminer package in combination with GIBBS sampling on my testset-sample.
This results in predicted ""Theta"" values for each document in my testset-sample.While I have read in another post that perplexity-calculations are not available with the texminer package (See this post here: How do i measure perplexity scores on a LDA model made with the textmineR package in R?), I am now wondering what the purpose of the prediction function is then for? Especially with a large dataset of over 100.000 Documents it is hard to just visually assess whether the prediction has performed well or not.I do not want to use perplexity for model selection (I am using coherence/log-likelihood instead), but as far as I understand, perplexity would help me to understand how well the prediction is and how ""surprised"" the model is with new, previously unseen data.Since this does not seem to be available for textmineR, I am not sure how to assess the model prediction. Is there anything else that I could use to measure the prediction quality of my textminer model?Thank you!"
5026,Mapping the topic of the review in R,"['r', 'dplyr', 'text-mining', 'tm', 'tidytext']","I have two data sets, Review Data & Topic DataDput code of my Review DataDput code of my Topic DataDput of my Desired Output, I want to look up the words which are appearing in Topic Data and map the same to the Review Data"
5027,PANDAS find exact given string/word from a column,"['python', 'pandas', 'text-mining']","So, I have a pandas column name Notes which contains a sentence or explanation of some event. I am trying find some given words from that column and when I find that word I am adding that to the next column as TypeThe problem is for some specific word for example Liar, Lies its picking up word like familiar and families because they both have liar and lies in them.As you can see from above only the second sentence is correct. How do I only pick up separate word like liar, lies and not families or familiar.This was my approach,Appreciate any help. Thanks "
5028,I have a problem with filter stopword operator in rapidminer,"['text-mining', 'rapidminer', 'rosette']","I am working on a sentiment analysis project in Persian language and I use rapidminer to this purpose.. I installed the rosette extension for some text preproccesing purpose in this language such as tokenization.
I have a problem with filter stop word (dictionary) operator... when I apply this operator to my data set (after tokenization) I receive only tokenized data set without filtering stop words... Do you know what is the cause of this problem?"
5029,R- Word co-occurrence frequency within paragraph,"['r', 'text-mining', 'quanteda']","The dataset contains text data of 26 news articles.
I would like to count word co-occurrence frequency within each paragraph, but it seems that my codes below are doing within a document (a whole article). 
Can you designate the level (sentence, paragraph...) for calculating co-occurrence frequency with fcm()? 
Or is there any other package to do so?"
5030,Merge two dataframes in R column-wise and sort columns by one value,"['r', 'dataframe', 'merge', 'text-mining']","I have two dataframes in R that look like the following examples:The names in Dataframe 2 are sorted by their value in Dataframe 1 - these are the top terms I got from the textmineR package with the GetTopTerms function from my model. However, I do not know how I can combine the phi values I have with each word that the value belongs to. In other words, what I want as an output is a combination of the two dataframes above - where the phi value is listed from highest to lowest in each single column as seen below: Is there an easy function to merge these two tables as seen above as well as to sort each phi-value from lowest to highest while merging. Thank you!"
5031,Analyzing Twitter Followers for my research,"['r', 'twitter', 'text-mining']","I need to analyze the followers of an organization Twitter account to identify who the followers are (description in their profiles), what they tweet about, etc. Is there any R (or python) library (package) or any other software with which I would be able to analyze the Twitter followers?Thanks
Iman"
5032,R: How to delete words other than specific words in a corpus,"['r', 'text-mining', 'corpus']","In the corpus ""tkn_pb"" , I would like to delete all words except for some keywords I chose (ex. ""attack"" and ""gunman""). Is it possicle to do this?"
5033,Extract values and attributes from a list and convert them into a dataframe in R,"['r', 'list', 'dataframe', 'lapply', 'text-mining']","I got the following list for my model:
Is there a way of how to select $theta and all its attributes and save them as a data frame? In other words, I want to extract this part from the list: and have a dataframe that looks like this (the column order does not matter): I have tried lapply and many other suggestions that I found in terms of list extraction  but failed to extract the part shown above.Thanks a lot!"
5034,"How to clean abbreviations containing a “period-punctuation” (“e.g.”, “st.”, “rd.”) but leave the “.” at the end of a sentence?","['r', 'regex', 'text-mining', 'topic-modeling']","I am working on a sentence-level LDA in R and am currently trying to split my text data into individual sentences with the sent_detect() function from the openNLP package. However, my text data contains a lot of abbreviations that have a ""period symbol"" but do not mark the end of a sentence. Here are some examples: ""st. patricks day"", ""oxford st."", ""blue rd."", ""e.g.""Is there a way to create a gsub() function to account for such 2-character abbreviations and remove their "".""-symbol so that it is not wrongly detected by the sent_detect() function? Unfortunately, these abbreviations are not always in between two words but sometimes they could indeed also mark the end of a sentence:Example:""I really liked Oxford st."" - the ""st."" marks the end of a sentence and the ""."" should remain.vs""Oxford st. was very busy."" - the ""st."" does not stand at the end of a sentence, thus, the "".""-symbol should be replaced. I am not sure whether there is a solution for this, but maybe someone else who is more familiar with sentence-level analysis knows a way of how to deal with such issues.
Thank you!"
5035,Text Mining in R: Counting 2-3 word phrases,"['r', 'text-mining']","I found a very useful piece of code within Stackoverflow - Finding 2 & 3 word Phrases Using R TM Package
(credit @patrick perry) to show the frequency of 2 and 3 word phrases within a corpus:How do you ensure that frequency counts of phrases like ""the tin"" are not also included in the frequency count of ""the tin woodman"" or the ""tin woodman""?Thanks"
5036,Python - Exception handling for regex functions,"['python', 'regex', 'exception', 'match', 'text-mining']","First time to the site, searched everywhere for an appropriate answer, let me know if my format is not correct.QUESTION:
How do I apply exception handling when applying regex formulas to a Python dataframe?EXAMPLE:
I am trying to obtain the second element after a re.split is applied. If the split delimiter is not present, Python returns an error - how do I overcome that?SCRIPT: WHAT I WANT:
Preferably in the same line where the re.split happens, return the original string ""abc123"" (rather than an error) even when the ""-"" is not present and the split cannot happen.THANKS!"
5037,Merge several txt. files with multiple lines to one csv file (1 line = 1 document) for Topic Modeling,"['python', 'export-to-csv', 'text-mining', 'lda', 'topic-modeling']","I have 30 text files so far which all have multiple lines. I want to apply a LDA Model based on this tutorial .
So, for me it should look this:But the whole text of a specific document has to be on one line.I tried this post and for some reason it keeps saying: csv_output.writerow(row[1] for row in csv_text) IndexError: list index out of range . Any thoughts? I named the documents in a same way and edited the range, of course.Basically, I don't care if we can solve this problem with python or not. I'm just done with my nerves so I really appreciate every help"
5038,How to turn txt file to nice dataframe,"['r', 'text-mining']","I have a txt file containing Track ID, Song ID, Artist Name and Song name. I'd like to convert it into a dataframe in R to do some analysis. What would be a good function to use to separate the data? Below is the top row of the dataset. Thanks!"
5039,How to extract matching values from a column in a dataframe when semicolons are present in R?,"['r', 'subset', 'text-mining']","I have a large dataframe of published articles for which I would like to extract all articles relating to a few authors specified in a separate list. The authors in the dataframe are grouped together in one column separated by a ; . Not all authors need to match, I would like to extract any article which has one author matched to the list. An example is below.I would expect to return; However with my large dataframe this command does not work to return all AU, it only returns rows which have a single AU not multiple ones. Here is a dput from my larger dataframe of 5 rows"
5040,Remove empty rows within a dataframe and check similarity,"['python', 'regex', 'pandas', 'text-mining']","I am having some difficulties to select not empty fields using regex (findall) within my dataframe, looking for words contained into a text source:I will need to look for words that ends with ful in my text string, then looking for words that ends with full in my dataset. (from csv/txt file).
I need to extract words ending with ful in text, then look at both DF_Text (thus Author) which contains words ending with ful and appending results in a list.    My question is: how can I remove empty rows([]) from the analysis (NaN) and report the author names (e.g. 563, 21) related to?
I will be happy to provide further information, in case it would be not clear. "
5041,Text comparison based on numbers/digits,"['python', 'text-mining']","I would need to compare texts by extracting only numbers from the following two texts:  However, it seems also to be relevant the next words (for example trillion /tn, billion). 
Do you know how I could get this information?I have tried withthen to compare them, but it gives me not all numbers in texts. Expected output: "
5042,How to export and save a vector from R,"['r', 'text-mining']","I'm struggling to get any code to run to save the vector a to save as a csv file. I'm keen to see what it looks like after I merge @65-70 text files together. My code to read in the files and merge is:I can run head(a,n) to see some of the data but tried to run different code to save it so I can examine if there any patterns.This didn't work:brings up this error:Any help appreciated.Update - used code to collapse and tried to save but still errors.Ran head to see some of the data
    head(corpus, 10)The xxxxx elements are respondent responses so redacted. I want to save this group of text files to check for patterns in the data. The other text left in are the standard quetsions/text in the files I want to try and exclude."
5043,Store multiple corpus via for loop by different names,"['r', 'for-loop', 'text-mining', 'tm', 'corpus']","I have multiple text documents per ticker which I want to store as an individual corpus. 
I've read about creating ''lists in lists'', but this doesn't work for me. For example, ''text mining and termdocumentmatrix'' give the following error: no applicable method for 'TermDocumentMatrix' applied to an object of class ""list.I could possibly put everything within the for loop, but that's not what I want since I want some flexibility to play with the corpus.Could someone help me out how I can effectively work around this problem? My code is below. Thank you in advance!"
5044,Combining two columns to create a datetime object,"['python', 'pandas', 'string', 'text-mining']","I would need to create a datetime by combining two columns in a dataframe. 
My original dataset has these columns: I would like to create a new column which combines Date + Time, i.e. which creates a datetime object:I used but the output gave me today's date plus values from column Time 2020-05-29 00:22:00.
How could I get the right date by using the columns Date and Time above?"
5045,Extract words from dataframe formatted text column,"['python', 'pandas', 'text-mining']","I need to create a new column from another one. 
The dataset is created by this code (I extracted only a few rows): which generates the following I need to create two new columns that contain age information in the format: At the end I should haveI do not know how to extract the first three words, then only the first, to create new columns. I have tried withbut it is wrong (ValueError: Must have equal len keys and value when setting with an iterable). Help will be appreciated."
5046,"“None of [Float64Index([nan, nan], dtype='float64')] are in the [index]” setting col A value if col B contains string","['python', 'pandas', 'nlp', 'text-mining']","I have a dataframe (called corpus) with one column (tweet) and 2 rows:I have a list (called vocab) of unique words in the column:I want to add a new column for each word in vocab. I want all values for the new columns to be zero, except for when the tweet contains the word, in which case I want the value of the word column to be 1.So I tried running the code below:...and the following error was displayed:How can I check to see if the tweet contains the word, and then if so, set the value of the new column for the word to 1?"
5047,Text Mining in R: Creating a Corpus creates unusual text,"['r', 'text-mining']","I'm reading in a single text file and my code below. It reads in fine but places a \t in random places throughout the corpus.Examples:
Original in text file
5.  If you are responding as an individual,.....
In Corpus
""5.\tIf you are responding as an individual,...or
Q1. What lessons can we learn from elsewhere....
""Q1.\tWhat lessons can we learn from elsewhere.....It seems like a tab is being translated into a \t in the corpusAny ideas how to fix this?Thanks"
5048,Character-Matrix - how to operate by row?,"['r', 'matrix', 'text-mining']","I have a matrix of character dataI want to paste the rows together and trim them to get the vector.I solved it this way, but I think there must be something much simpler, either in Base R or Tidyverse.Can you show me a more direct way to get this answer?"
5049,Text Mining in R - how to exclude full phrases/sentences from text analysis,"['r', 'text-mining']","I'm working on around 160 separate responses to a survey. I've ran R code to produce, after cleaning, a simple wordcloud and some sensitivity analysis.
However, the analysis includes all the text from the template which the respondents were asked to complete, such as the introductory text, instructions, name, company, address, along with all the section headings and questions presented.
As all of this template text will be repeated for each of the 160 responses, it is skewing the frequency of words in the responses.
Is there a method in R to exclude not just single words (as per stopwords or creating mystopwords) but full sentances or phrases from the analysis so all text in the template can be ignored and not included as part of the set of responses?
I am unable to share responses due to confidentality but I can share the blank template which contains all the text I wish to exclude from analysis, available at https://www.economy-ni.gov.uk/energy-strategy-call-for-evidence.Dummy data in R:[50] ""2. Energy in Northern Ireland""
  [51] ""Q1. What lessons can we learn from elsewhere in addressing energy within an""
  [52] ""overarching climate action framework?""
  [53] ""Q2. What are the key considerations for decarbonising Northern Ireland’s energy""
  [54] ""sector given existing linkages to other jurisdictions?""
  [55] ""Q3. To what extent should Northern Ireland implement the key energy-related""
  [56] ""recommendations from the CCC ‘Reducing Emissions in Northern Ireland’""
  [57] ""report?""
  [58] ""Q4. Do you agree with the 30-year timeframe? If not, please state your preferred""
  [59] ""approach and reasons.""
  [60] “Respondent response text xxxxxxxx  blahblahvlahblah”
  [61] ""3. The Energy Transition in Northern Ireland""
  [62] ""Q5. What are the unique characteristics of Northern Ireland that need to be""
  [63] ""considered in a net zero carbon energy transition?""
  [64] ""Q6. Is your organisation undertaking or planning to undertake projects to support""
  [65] ""the energy transition? If so, please provide further details.""
  [66] “Respondent response text xxxxxxxx  blahblahvlahblah”                                         Within this subset of the data, I wish to exclude standard text in rows 50 to 59 and again rows 61 to 65 as these contains the same text through all responses i.e. each row across all responses will begin with the same text each time.The rows begining Respondent response text are only dummy data for illustration purposes and will be different each time a section/questions are being repsonded to. In this example, I would be seeking to keep the text in rows 60 and 66 for the analysis. The standard text, to exclude, will not always be the in same row number across each response/file. This is caused by the responses running over multiple rows.Thanks
Stephen"
5050,Text Mining in R - How to separate sections of text based on headings for separate text analysis,"['r', 'text-mining']","I'm working on around 160 separate responses to a surve. I've ran R code to produce, after cleaning, a simple wordcloud and some sensitivity analysis.However, the template respondents have completed is split into sections. For example, the structure of the template is 
Heading 2 - Energy, then a set of questions, 
then Heading 3 - Energy Transition, another set of questions 
then Heading 4 - Consumers and questions next. 
This repeats up to 12 sections. Instead of assessing the whole set of responses, is there a way of segmenting the responses into sections as per the topic headings within the template?This would permit analysis within sections rather than across all topics.I am unable to share responses due to confidentality but I can share the blank template available at https://www.economy-ni.gov.uk/energy-strategy-call-for-evidenceDummy date in R[50]""2. Energy in Northern Ireland""
  [51] ""Q1. What lessons can we learn from elsewhere in addressing energy within an""
  [52] ""overarching climate action framework?""
  [53] ""Q2. What are the key considerations for decarbonising Northern Ireland’s energy""
  [54] ""sector given existing linkages to other jurisdictions?""
  [55] ""Q3. To what extent should Northern Ireland implement the key energy-related""
  [56] ""recommendations from the CCC ‘Reducing Emissions in Northern Ireland’""
  [57] ""report?""
  [58] ""Q4. Do you agree with the 30-year timeframe? If not, please state your preferred""
  [59] ""approach and reasons.""
  [60] “Respondent response text xxxxxxxx  blahblahvlahblah”
  [61] ""3. The Energy Transition in Northern Ireland""
  [62] ""Q5. What are the unique characteristics of Northern Ireland that need to be""
  [63] ""considered in a net zero carbon energy transition?""
  [64] ""Q6. Is your organisation undertaking or planning to undertake projects to support""
  [65] ""the energy transition? If so, please provide further details.""
  [66] “Respondent response text xxxxxxxx  blahblahvlahblah”                                         Rows 60 is the text to keep for analysis for section 2 on Energy and row 66 for Section 3. Obviously the respondents text will likely be longer than a single row. Then ignore those rows with questions and headers.All rows prior to 50 are introduction text to be ignored as well.ThanksStephen"
5051,"R-Text mining: replace abbreviations, numbers and symbols in german","['r', 'replace', 'numbers', 'text-mining', 'abbreviation']","I would like to replace the abbreviations, numbers and symbols in my text.
As my text is in german and not in english I have problems in converting it.  I tried:But this works just for an English text and not for German.
What should I add that the function also works in German?"
5052,Strings analysis: splitting strings into n parts by percentage of words,"['python', 'string', 'text-mining']","I'd need to calculate the length of each string included in the list: to split each of them into three parts: I'd be able to calculate the length of each string into the list, but I do not know how to split each string into three parts and saved them. E.g.: 
the first sentence ""I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best"" has length 201 (tokenisation) so I'd need to take I read about the use of chunk but I've no idea on how I could apply. Also, I'd need a condition that can ensure me that I am taking integer (elements such words cannot be consider 1/2) words and I am not going beyond the length. "
5053,R: Possible to extract groups of words from each sentence(rows)? and create data frame(or matrix)?,"['r', 'extract', 'text-mining']","I created lists for each word to extract words from sentences, for example like thisBut I have more than 25 words list to extract, that's very long coding. 
Is it possible to extract a group of characters(words) from text data?Below is just a pseudo set.Expected output as below"
5054,Extract from string information on date/time,"['python', 'pandas', 'string', 'text-mining']","I have some texts that generally starts with:and so on.
Basically I have information on:I would like to transform this kind of information into valuable time serie information, in order to extract this part and create a new column from that (Datetime).
In my dataset, I have one column (Date) where I have already the date of when the research was performed (for example, today), in this format: 26/05/2020 and when the search was submitted (e.g. 8:41am). 
So if the text starts with “12 minutes ago”, I should have:And for others:The important thing is to have something (string, numeric, date format) that I can plot as time series (I would like to see how many texts where posted in terms of time interval).
Any idea on how I could do this?"
5055,Is there any function to extract the text which has a specific heading from pdf,"['python', 'pdf', 'text-mining']",I have multiple paragraphs in my pdf document. Each paragraph has a unique Heading to it. How can I extract the text from the pdf under a specific heading that I am looking for
5056,How to mine multiwords by taking into account their position in the text?,"['r', 'string', 'text-mining']","I want to extract certain words positioned between years and the following comma in a given text. Although the term Mining appears before & after 2020 in text, I need the later one which is found between (2020) and ,. The same concept apply for the term Computer Science in the following text.The last line of the above code gives an output of:The output that I'm looking for is:Note: The position of those words matter. Any help is highly appreciated!"
5057,How to mine multiwords from a given text in R?,"['r', 'string', 'text-mining']","Although land is not found as a separate word in my text, the code takes from the last portion of Netherland. How I can enforce the code to strictly look for words included in words only? 
Output for the variable cntry:An output that I need for cntry:"
5058,Find a Pattern across Multiple lines with R,"['r', 'regex', 'text-mining', 'sec']","I am trying to identify a pattern across multiple lines, to be exact 2 lines. Since the pattern in either individual line is not unique I am using this approach.So far I have tried to go with the function ""grep"" but I think I am missing the correct regular expression here.This part is a modified version of the edgar package function ""getfillings"" and tries to extract only the Management's Comment/Item 2 for quarterly results. If possible I would include something after ... 2[^A] in the function that reacts to the new line and then the string ""Management...""The pattern in the plain txts which I have, looks like this:Item 2.
Management   Discussion and Analysis of Financial Condition and Results of OperationsI would appreciate any comment on how to capture this best in a regular expression with R.Example Input looks like this:21 
Item 2.
Management   Discussion and Analysis of Financial Condition and Results of Operations 
This section and other parts of this Quarterly Report on Form 10
Item 3.
Quantitative and Qualitative Disclosures About Market Risk 
There have been no material changes to the Company   market riskand the desired output would beManagement   Discussion and Analysis of Financial Condition and Results of Operations 
This section and other parts of this Quarterly Report on Form 10I need to match ""Item 2. ... Management  Discussion"" since Item 2 is not unique. How could I formulate a regular expression across two lines?"
5059,"Regex's are working, but code looks horrible","['r', 'nested', 'tidyverse', 'text-mining']","I'm cleaning up a long list of noun-phrases for further text mining.  They're supposed to be 1- or 2-word phrases, but some have / in a conjunction.  Here's what I've got:And I want:I've got something working, but it's horrible and non-extensible.Which gives me what I wanted, but blecchh!The top two problems (1) I have to run the rexex twice - once with str_detect to get the logical for the if / else and again with str_match to pull out the tokens. (2) I have do the double unnest to unwind the list structure.  And smaller problem (3) Can I get out of if / else, into case_when or switch?I'll eventually be extending this to about a dozen patterns and use-cases."
5060,"Textmining in R using qunateda package, rpart","['nlp', 'text-mining', 'rpart', 'quanteda']","I just joined stackoverflow to find solution for this error. After running the following code, R throws error (see below). Pls help me how to solve this. I am new to text mining.  Error in terms.formula(formula, data = data) :    duplicated name
  'document' in data frame using '.'"
5061,Cosine Similarity: I would like to understand the values I get,"['r', 'text-mining', 'cosine-similarity', 'doc2vec']","I am currently trying to use word embeddings and cosine simarity in order to determine the similarity of two documents written in german. I tokenized and lemmatized the Documents and so forth using the following: In order to compare both documents I used: However, even for this example (which compares finally only the words einkaufen and wetter), I get a cosine similarity of 0.51, even though the words are not related at all.Similarly, it happens for all kind of documents. When comparing German and English documents, the value is around 0.6. Isn't that really high? If both documents are german and do share some content, the value mostly excedes 0.9.Now I am trying to figure out why the values are always that high. I am aware that my code probably is really weak, as I am still a beginner. However, I really appreciate any comments and suggestions to unterstand the problem here and to improve my textmining skills. Thank you :) "
5062,removing gibberish from sentences,"['r', 'text-mining']","During text cleaning, is it possible to detect and remove junk like this from sentences:currently I'm doing something like this:but the more I review my dataframe, I found more sentences with this type of junk. How do I use something like regex to detect and remove rows with something junk like this? "
5063,Applying function to a slot of all S4 objects in a list,"['r', 'text-mining', 'nested-lists', 's4']","I'm new to R and to scripting, but I bet you can help me!I have a list of 39 imported xml that are organized in S4 objects. This data is for text-mining of biomedical literature. I need to combine all PMID slots (from all 39 S4 objects) into one, check for duplicate numbers and then apply a function to all of them. So, I've tried a bunch of different things with no success yet to apply recursively.The list is data_xml;
The slots I need are all PMID;The effect is to attain a object that can even be numeric of all the PMIDs of all the 39 S4 objects.I've included a picture to better show the structure. 
R clip"
5064,Trouble Installing qdap package into R,"['r', 'jvm', 'text-mining', 'rjava', 'qdap']","I want to install ""qdap"" package to run a text mining project. I tried installing the package in many different ways:
1. intsall.packages(""qdap"")
2. Downloading the file locally then installing
3. Using devtoolsI always had the same problem. It seems to be related to rJava package. I tried to install rJava but I wasn't successful. I'm more of a data analyst than a developer so I don't have much experience with development (using terminal commands or fixing java on my mac).I'd really like to use this ""qdap"" package. Could you please help me figure out in a simple manner?Here's the error message after I install the package:"
5065,Crawling tweet using python,"['python', 'twitter', 'data-mining', 'text-mining', 'tweepy']",I currently doing my text mining project with data twitter using python. But I wonder could I crawling twitter with specific topic but exclude specific user?I'm using this code right now but its only for the topic
5066,Beautiful Soup: How to extract text from this structure:,"['python', 'web-scraping', 'beautifulsoup', 'text-mining']","I would like to access the timestamp text inside title = """"And get this string ""23.12.2019 13:05:24""I already know to access the proper text inside this div. But it happens it's just the hour. The full timestamp is what I need.I'm using this structure currently:"
5067,apply function to values in 2D list data type dataframe,"['python', 'list', 'dataframe', 'text-mining', 'preprocessor']","I have a dataframe like this: i and text are column names and value of text column like 2D list with sens and word tokenizes.
i want to apply remove_characters_after_tokenization function to all lists in text column.I try use map func like this but got an AttributeError: 'list' object has no attributeanyone can help me"
5068,Nested for loop on cross validation and sparsity,"['for-loop', 'vector', 'sequence', 'text-mining', 'cross-validation']","I am wondering why I am not able to execute this for loop properly, it keeps giving me the following error:Error in eval(predvars, data, env) : object '<92>s' not found   This is probably due to my loop, does someone know what I am doing wrong? I am trying to change the sparse for each loop in the sequence show below, and to subsequently save the output for each loop in accLogit[I], showing the sparsity that is used as well. Without the first for loop this works perfectly fine, however, I need to train the dataset based on various sparsities in order to find the one with the highest accuracy. "
5069,What R package is suited to identifying words that are positively correlated with a binary response variable,"['r', 'dplyr', 'text-mining', 'tidytext', 'qdap']","I have a tibble that has to three columns:What R package might I use if I were interested in identifying words that tend to be present with top-rated wine (the target variable = 1)I came across Text Mining in R Text Mining with R, but this appears to be more about sentiment analysis which seems close to what I'm trying to achieve, but perhaps a bit off the mark. Any suggestions would be welcomed.I am working under the assumption that once I've completed some basic analysis I will be able to incorporate that into a logistic regression.  "
5070,pdf2txt / pdftotext / getting text from pdf with index / toc,"['pdf', 'text-mining', 'pdfminer', 'pdftotext', 'pdftextstream']","I have arhived pdf-files that have done a good job finding article titles in a newspaper, and bookmarking the locations into an index / table of contents. Is there any way I could use that information in retrieving the text on the pages as simple txt.Ideally I would print index entry in distinct form + title, print all the text up to next index entry, and then that index entry etc. In that way it would be easy to use that info later for text mining. Any suggestions on software / tools that could do this?pdf2txt -t tag does get nice text boxes of paragraphs but does not seem to catch the index. pdftotext did not seem to have a feature of it either. Any thoughts?Thanks!"
5071,What should i use for clustering word similarity? and the visualizing clustering in Python,"['python', 'data-visualization', 'cluster-analysis', 'text-mining', 'similarity']","I have a matrix of similarity word  : What is the best method for getting clusters based on the word similarity value? besides I only have the word similarity parameter. I have tried hierarchical clustering. however, this method can only determine the number of clusters manually and cannot extract the clusters."
5072,how to fix the problem of downloading fasttext-model300?,"['text-mining', 'gensim', 'similarity', 'cosine-similarity', 'sentence-similarity']","I'm using windows 10 and python 3.3. I tried to download fasttext_model300 to calculate soft cosine similarity between documents, but when I run my python file, it stops after arriving at this statement:There are no errors or not responding, It just stops without any reaction.Does anybody know why it happens?Thanks"
5073,How to use regex to get a certain number of characters before a string? (Python) [closed],"['python', 'regex', 'text-mining']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed 3 months ago.I am trying to pull years of experience from Indeed job descriptions. Almost all of the descriptions list experience like this: ""0-2 years"" or ""2+ years"" or ""2 years"". Regardless of how it is listed, the word ""years"" comes after the number of years of experience required.How could I return a certain number of characters (in this case max 4 including the space) before the word ""years"" to capture the digits?I plan on then extracting the digits if it is in string form and averaging them to return one value to be used in a machine learning model. I am removing any job descriptions which require double digit experience values (ex. 10) so this will not be an issue.So for example:Example Description: ""Ideal candidate will have 0-2 years of experience with Apache Spark.""Desired Output:Either ""0-2"" as a string so I can extract the digits manually, or even better a list of digits [0,2]"
5074,How can I analyse a text from a pandas column?,"['python', 'pandas', 'text', 'text-mining']","I'm used to make some analysis from text files in Python. I usually do something like:However, now I'm not working with a text file, but with a Pandas dataframe. How can I get the 'text' object from a Pandas column?I tried taking a look at the post
Text mining with Python and pandas, but it's not exactly what I'm looking for."
5075,How to represent a document from test set with Document-Term Matrix created from training data? (Latent Semantic Indexing),"['vectorization', 'text-mining', 'data-representation', 'term-document-matrix', 'sentence-similarity']","I build a model of document classification from the training set of documents. Classification is done by the vector representation of each document, that is, a row in the Document-Term Matrix. Then to test the model, I need the representation of each document in the test set. How can I do that since not every term has been included in the training set (hence the Document-Term Matrix)?"
5076,How to find the co-occurences of a specific term with udpipe in R?,"['text-mining', 'quanteda', 'udpipe']","I am new to the udpipe package, and I think it has great potential for the social sciences.A current project of mine to study how news articles write about networks and networking (i.e. the people kind, not computer networks). For this, I webscraped 500 articles with the search string ""network"" from a Dutch site for news about the flexible economy (this is the major source of news and discussion about e.g. self-employment). The data is in Dutch, but that should not matter for my question.What I like to use udpipe for, is to find out in what context the noun ""netwerk"" or verb ""netwerken"" is used. I tried kwic to get this (from quanteda), but that gives me just the ""window in which it occurs.I would like to use the lemma (netwerk/netwerken) with the co-occurences operator, but without specifying a second term, and only limited to that specific lemma, rather than calculating all co-occurences.Is this possible, and how?
A normal language example:
In my network, I contact a lot of people through Facebook -> I would like to get co-occurrence of network and contact (a verb)
I found most of my clients through my network -> here I would like ""my network"" + ""found my clients"".Any help is mightily appreciated!"
5077,How to remove words that start with digits from tokens?,"['r', 'text-mining', 'quanteda']","How to remove words that start with digits from tokens in quanteda? Sample words: 21st, 80s, 8th, 5k, but they can be completely different and I don't know them in advance.I have a data frame with english sentences. I transformed it to corpus by using quanteda. Next I transformed corpus to tokens and I did some cleaning like remove_punct, remove_symbols, remove_numbers, etc. However, the remove_numbers function does not delete words that start with digits. I would like to delete such words, but I don't know their exact form - it can be e.g. 21st, 22nd, etc."
5078,Need Help to Realize a Function in a Python,"['python', 'function', 'dictionary', 'text-mining']","I need to make a function that can check a given word and return another word. For example, if the input is ""a1"", the function will check for this word in the dictionary and return ""a"". I know how to code it if it is just a single input word per category using a simple if-else, but I'm still confused if a category has more than 3 words. And I plan to have a lot of data in this dictionary. So a simple if-else would need a lot of code to be written.this is the example for the input & output that i want:  Input : a2 Output : a
 Input : b3 Output : b "
5079,Text-mining/word correlation in R,"['r', 'correlation', 'text-mining']","I'm trying to make text mining or rather word correlation work in R.The bigger picture of what I'm trying to do is, I query the entire exported OpenStreetMap database for all features that are within a specific distance to various longitude-latitude locations. So far, this is working like a charm and I have gotten to the point where I have a data frame column of type character that contains all features in that specific distance where one row represents one longitude-latitude location. The data frame column can be found in this csv and a catalogue of all possible features can be found in this csv.My next step would now be to categorise the locations depending on their surrounding features. To do this, I would like to use a text mining/word correlation algorithm that is able to create categories based on features that are often present at the same locations.So in short: I have a column of type character (words separated by commas) where one row contains all features that are within a certain vicinity to a longitude-latitude location. Based on those surrounding features I would like to categorise my locations relying on correlating features.I have tried findAssocs from the tm package, which unfortunately doesn't work for neither type list, data.frame nor character.
I have also found this wonderful documentation that guides through basic text mining in R. The problem here is that it seems like I would have to convert each row of my data frame column into a document to prepare a corpus for further processing. While this might be feasible for my test case of 61 locations, it won't be so much for my final analysis of several tens-of-thousands of locations.Can anyone prod me in the right direction here? Preferably, without relying on 3rd party software like 'rapidminer'. Having everything in one R script would be a lot better for my use case.Thank you in advance. If you require any additional information, please let me know."
5080,How to combine multiple text entries for a variable once dplyr has grouped by another variable [duplicate],"['r', 'dplyr', 'text-mining']","For hundreds of matters, my data frame has daily text entries by dozens of timekeepers.  Not every timekeeper enters time each day for each matter.  Text entries can be any length.  Each entry for a matter is for work done on a different day (but for my purposes, figuring out readability measures for the text, dates don't matter).  What I would like to do is to combine for each matter all of its text entries.Here is a toy data set and what it looks like:Dplyr groups the time records by matter, but I am stumped as to how to combine the text entries for each matter so that the result is along these lines -- all text gathered for a matter:dplyr::mutate() does not work with various concatenation functions:Maybe a loop will do the job, as in ""while the matter stays the same, combine the text"" but I don't know how to write that.  Or maybe dplyr has a conditional mutate, as in ""mutate(while the matter stays the same, combine the text).""Thank you for your help."
5081,Using machine learning how do we determine if a given description about activity is under corporate social responsibility or not?,"['algorithm', 'machine-learning', 'data-science', 'text-mining']","Given a description:for example:""I'm a proactive member of the XYZ On Demand Community (ODC). This platform helps us to engage with community organizations and expand the value of our volunteering efforts."" is a csr description just by reading as the social activity is related to some contribution through company initiatives.How do we predict this using a machine learning model. The model should be able to classify it as csr description or non - csr."
5082,Getting ratings from aria-label with beautiful soup,"['web-scraping', 'beautifulsoup', 'text-mining']","I have a soup object like: and I'm trying to find the ratings from the following code, the output is a list like this, Any suggestion on getting the ratings from <div aria-label=""3 star rating"" ?"
5083,Text correlation with R,"['r', 'text-mining']","I'm working with a DF that contains several rows with Text ID, Text Corpus and count of words in said corpus. It looks something like this:With that DF I want to calculate the number of words that all rows have in common with one another. For example Text_1 and Text_2 have two words in common while Text_1 and Text_3 have just one.Once I have that, I need to display the data in a matrix similar to this one:I managed to do this with only two rows, for example Text_1 and Text_2:But I don't know how to apply this sistematically for all rows and then display the matrix I need.Any help would be very much appreciated."
5084,Named Entity Recognition For Product Names Of Clothes,"['regex', 'nlp', 'text-mining', 'spacy', 'email-parsing']","I am trying to extract product names from a plain text, the problem with product names is that they don't have a specific pattern and I don't want to give the algorithm a set of data that has fixed names I want it to be generic.I'm looking for a way to make it detect the product names as an Entity.Any help please?Here's an example of the text Order dispatched Your new clothes are on their way. Track your
  delivery with Royal Mail: VB 9593 7366 0GBOrder DetailsMen's Dark Navy Jersey Cotton Lounge Shorts Size: XL£45.00Men's Navy Cotton Jersey Lounge Pants Size: XL£60.00Delivery £0.00Total £95.00I want to extract Men's Navy Cotton Jersey Lounge 
  and 
  Men's Dark Navy Jersey Cotton Lounge ShortsAnother exampleYour order summary Delivery between 18/11/2019 and 19/11/2019 Shipping
  from O' adidas Lxcon sneakers £80.96 Delivery between 18/11/2019 and
  19/11/2019 Shipping from BOUTIQUE ANTONIA MARCELO BURLON COUNTY OF
  MILAN Confidencial striped swimsuit £97.58 Shipping Total Payment
  method £20.00 £153.90 VISAI want to extractadidas Lxcon sneakersAndMARCELO BURLON COUNTY OF MILANFor your information this text is an email of orders and I have a lot of different patterns of emails."
5085,R text mining: grouping similar words using stemDocuments in tm package,"['r', 'text-mining', 'tm', 'stemming']","I am doing text mining of around 30000 tweets, Now the problem is to make results more reliable i want to convert ""synonyms"" to similar words for ex. some user use words ""girl"", some use ""girls"", some use ""gal"". similarly ""give"",""gave"" means only one thing. same for ""come,""came"". some user use short-form like ""plz"",""pls"" etc.
Also, ""stemdocument"" from tm package is not working properly. it's is converting dance to danc, table to tabl.....is there any other good package for stemming.
I want to replace all these words by just one similar words, in order to count the correct frequency of this data. So my sentiment analysis would be more reliable.
Following is the reproducible code (i cannot include all 30000X1 dataframe here), edited it after comments by ken:right now i am getting the erro Finding a python executable with spaCy installed...
Error in set_spacy_python_option(python_executable, virtualenv, condaenv,  : 
  No python was found on system PATHAlso, waiting for your answer. Thanks."
5086,How do I get feature_importances_ from GridsearchCV,"['python', 'scikit-learn', 'random-forest', 'text-mining', 'gridsearchcv']","I am fairly new to programming and this problem might be pretty easy to fix, but I have been stuck on it for a while now and I think my approach is just plainly wrong.
As the title indicates, I have been trying to implement a gridsearch on my RandomForest prediction to find the best possible parameters for my model and then see the most important features of the model with the best parameters.
The packages I've used:After some datacleaning and preprocessing, I made a gridsearch like this, where x_features is the DataFrame with the tfidfvectorized features of my data:My idea here was, that maybe I could make it easy for myself and copy in the optimal_param1 into my RandomForestClassifier(), and fit it on my training data more or less like this:but optimal_param2 is a dict. Therefore I thought transforming it to a string and getting rid of the signs that are too much ( sub : for =, delete {, delete } ) would make it work. That obviously failed as the numbers for n_estimators, max_depth etc. were still strings and it expected integers. What i wanted to achieve in the end was to get an output of the most important features more or less like this:I realize that gs is already a complete RF model, but it does not have the attribute feature_importances_ which i was looking for.
I would be very thankful for any ideas on how to make it work."
5087,find link from nested link In R,"['r', 'text-mining', 'data-collection']","I am learning text mining using R. I am trying to find all the links in a HTML document.I tried getHTMLLinks() but it is showing following error:so I tired ""rvest"" package to find the links. The code is as follow:It give all links in vector format. These all the links are just names listed in href tag. But actually these all are hyperlinks to a table. It would be really great if anyone can help me that how can I extract the final links instead of name of these hyperlinks?"
5088,R Tidytext unnest_tokens error when using a txt file as source,"['r', 'text-mining', 'tidytext']",Very new to this topic. I am having trouble with the unnest_tokens function in the tidytext package. I have some texts stored in .txt format that I want to analyze. An example would be putting the following sentences in a txt file then read it into R:Below is my code:Then I would get the error message below:Does anyone have a solution to my problem? Thank you in advance!
5089,Is the RNN model apropiated to forecast a featured based on a multiple depended ones?,"['r', 'neural-network', 'text-mining', 'recurrent-neural-network', 'forecasting']","I guess it will be a very frequent topic and probably bored one, but I´ve passed a lot of time reading post and examples and now, I´m even more confused.I´m trying to do my final work to the university, in economics, about prediction the oil prices based on text mining on an online blog. For that, firstly I would like to build a model with cuantitave features and then add the text mining features to it, and compare how the predictions change, to demonstrate text mining advantages.The problem is I don´t now what model use. I would like to use a ANN one, and for what I was reading for, I thought a RNN could be the apropiated, due to importance of past movements in the oil price change. However, the most examples I saw, for the RNN only are used for time series with only one feature, not for a dataset with X features (I have 30 monthly numeric features in 127 observations) to predict a Y output (crude oil prices).¿Am I not able to use a RNN for this porpose? ¿What model would be the more apropiated?Thank you all,
Gonzalma."
5090,Is it possible to create a character vector in R to customize stopwords in stopwordsRemover from the featurizeText function?,"['r', 'sql-server', 'text-mining', 'microsoft-r', 'revoscaler']","stopwordsRemover from the featurizeText function in {MicrosoftML} R Documentation
takes a (dataFile = """") data file containing the terms for custom stopwords. This can only be a file stored on the file system. E.g. C:/temp. This works well. As I am using R in SQL Server, retrieving data from the file system during processing an external R script in a stored procedure is not what I want. A vector of words should be loaded.Is there a way to load custom stopwords, without using the file system?"
5091,Stop words list for r,"['r', 'text-mining', 'stop-words']",stopwords (of package tm) returns various kinds of stopwords with support for different languages. E.g.returns 175 english stop words. I would like to know if there are some tools that provide more stop words.
5092,why k-means is better in clustering than topic modelling algorithms like LDA?,"['cluster-analysis', 'data-science', 'data-mining', 'text-mining']","I want to know about the advantages of K-means in clustering essays to discover their topics. There are a lot of algorithms to do it such as K-medoid, x-means, LDA, LSA, etc. Please give me a full description of the motives to select k-means algorithms"
5093,How do i get the full strings with a RegEx search in python that only captures part of the word?,"['python', 'regex', 'string', 'text', 'text-mining']","My assignment is to search through a document, and grab the words that contain ch, cH,Ch, CH, sh, sH, Sh, and SH. What is the most effective way to grab the whole word? right now using re.findall() i get the correct count of words and location, but am only able to print the ch or sh, not the whole word that contained the letters.
 Here is my code!and here is the output:I want line 3 to print value Shadow, not 'sh'. And line 5 to print Chains, Languished, and Shameful. Here is the assignment verbatim, if interested:Open a file and using a while loop to read in each line, use regular expressions (re.search()) to find those lines that contain any lower/upper case version of the strings ""ch"" or sh"", i.e.  {ch Ch cH CH sh sH Sh SH}.  NOTE - do not enumerate all 8 possibilities in the regular expression, rather, your regular expression should be 7 characters long including the [ ] charachters.  For each sentenct that contains a ""ch"" or ""sh"" (or Ch or CH or cH etc) print out: a) the line number and the sentance; and b) the list of the words in that sentence contaiing some version of ""sh"" or ""ch""."
5094,R is not removing stop words,"['r', 'twitter', 'text-mining', 'tidy']","I have searched tweets and have the following code. This code should remove stop words, numbers and URLS. It has removed about 1000 words but a few still remain. When I get the top 10 words, words like didn't and i'm appear. Is there something I am doing incorrectly? The stop words are present in tidytext. One more clarification item the after the antijoin on the stop words, 3 of the didn't does not remain and one still does. So the antijoin is handling some of the stop words. Here is the data I get: 
Here is some sample data of the first 6 tweets in the costaRica list1 ""WHAT DID THEY EAT? asks @SheilaGunnReid, who reports:\n\nTrudeau expensed $1200 for family meals on round trip flights to Costa Rica\n\nWATCH here: \n\n#cdnpoli #TrudeauWorstPM 2""
[2] ""I swear you watch ONE Fortnite montage on Youtube and suddenly your entire reccommended section is: \nThe Box \U0001f4e6\nLet Her Go \U0001f48d\nCosta Rica \U0001f334\nBallin\U0001f3c0\nRoxanne \U0001f495\nBlueberry Faygo \U0001f347\nAddison Rae \U0001f483""
[3] ""We had just qualified for the 2010 WC, my favorite memory from playing with @USMNT. Tonight will be my first time watching the last match of qualifying vs. Costa Rica at RFK. To the fans - thank you for inspiring me and motivating me through it all with this incredible gesture ❤️ ""
[4] ""@Thornburgurr @AMCx32 @nathanjmartin @RepGregStanton @netflix That proves she didn't kill him! She didn't have motive but other people around him did. He had mob ties in Costa Rica alonh with many \""girlfriends.\"" The local sheriff who knows the case well said that Carole is not a suspect or even a person of interest.""
[5] ""@anassaifuddin You remember \U0001f60f\n\nCorrect! \U0001f919\n\nAndorra, Costa Rica, and Dominica are examples of countries that do not have an army.""
[6] ""Wooooow... should’ve been leaving to Costa Rica rn\U0001f612""   SOLUTION
There are different quotes (' versus ’). We have to sub out the curly quote for the straight quote and it finds the words in the stop words. My new code"
5095,String manipulation for classification,"['python', 'pandas', 'classification', 'text-mining']","I have a list of links such as: and so on. The above is how the column of my datadrame looks like.
As you can see, they have in common the word 'nation'.
I would like to label/group them and add one column in my dataframe to respond with a boolean (True/false; e.g. columns: Nation? option: True/False). I would need to do this in order to classify websites in a easier (and possible quicker) way. 
Do you have any suggestions on how to do it?Any help will be welcome. "
5096,String replacements in Pandas [duplicate],"['python', 'pandas', 'text-mining']","I need to replace words starting with www. with the rest of the link. For example: withI am using pandas. The column that contains the links is called COL1. I have 1000 rows. 
I have tried with  but I do not know what I should replace with in order to take the rest of the link. Could you please give me advice on it?"
5097,"How can I find the learned words, i.e. patterns of a learned language by RNN?","['deep-learning', 'lstm', 'data-mining', 'text-mining', 'recurrent-neural-network']","I trained a LSTM model that given a history of N characters it predicts the next character. In other words, this is a character-level text generator.Since this is a character level model, I wonder if it could be used to define the vocabulary of the learned language. Can I find the words that exist in the corpus, like in pattern mining?I thought about letting the model to generate words by giving it an initial state and some random input, and continue until it predicts a space or any other terminating character, but I'm looking for a better way since it doesn't perform very well. More over, I don't want to rely on criterions like terminating characters, because I want the word-mining task to be fully unsupervised."
5098,Extracting groups of unstructured text to for later NLP?,"['nlp', 'data-mining', 'text-mining', 'information-retrieval', 'information-extraction']","I am new to data mining / text mining so I am not sure that I am using the right terminology. I am attempting to come up with a process to extract groups of related content to later apply NLP and other techniques to extract the meaningful data out of it. I have starting data that looks something like this:The goal is to obtain a list like this with a unique entry for each ""listing"", grouped with relevant meta data:The full text is written by many different authors and often switches formats within a single post so I can't detect what format it is in and process the whole document. The one thing all formats have in common is that they have new line breaks rather than a dense paragraph of text. So working with that I have a few ideas on how to approach it:Option 1: RudimentaryThis option is very simple and could work when double spaced. However it fails by using a number as a heuristic to determine if it's a new group as the product names, extras, conditions could contain numbers when single spaced.Option 2: NLPThis option would attempt to classify each word in the document as Product Name, Condition, Attribute, Price. Then process the document again to group text so that it has a Name and Price and optionally the condition and extra meta data.The problem with this approach is that the extras and bundles are products as well so classifying them would determine that they are a unique entry with meta data when they belong under a ""parent"" product because of how they are spaced in the document. Option 3: Something else?My first thought was to process the document into groups first so that when NLP knows all words in this group are related to the same product. I have a list of all Product Names and a pretty good one of all Conditions. The extras, versions, and other text is unique so it may cause some issues attempting to determine how to group.It seems like it might need a mix of the two because how the author spaces them is ultimately how everything is bundled. Yet we don't immediately know if the next set of content is related to the first or a new listing without some other process.INPUTOUTPUT (JSON)"
5099,"How to do Text Mining from a HTML document, and convert it into a CSV file?","['r', 'csv', 'dataframe', 'text-mining', 'cran']","So I'm trying to do a bit of text mining from this website ""https://www.bmkg.go.id/gempabumi/gempabumi-terkini.bmkg"" - particularly from lines 452 until 1050 through the Developer's Sources. I haven't been able to do that successfully; and my goal is, after I succeed in doing so, I'll have to convert it into a dataframe with custom labels, then save it as a CSV file into my local drive. Is my logic on achieving this goal correct, or am I getting it wrong to even begin with?Here's what I have so far:Output:These are what lines 452 - 1050 look like in the HTML, from Developer Source:Any help on this would be much appreciated! Thank you :)"
5100,Practical approaches to structured data extraction from plain text: Looking for ideas & feedback,"['nlp', 'text-mining', 'information-retrieval', 'unsupervised-learning', 'information-extraction']","I am member of a Facebook group for local ride shares. The group is specific for two cities and everything that is in-between, so the post are (mostly) as such:So I've been thinking about possible ways to build a simple search engine for it where people can select a date/time and the direction where they need to go. I'm thinking that in the end, I would like to have a structured tuple such as {start: 'city A', end: 'city Z', time: '15/04/2020 14:00'}. (I'd probably get the date from the post metadata.)I'm not that advanced in NLP/text mining techniques that could make it in production, so I'm looking for some input on my ideas here:Option c) is my favorite and also the technically most interesting option, but I just started reading about this topic. Some thoughts I have about it:I would really appreciate some thoughts, comments and paper or book recommendations. With all the current down time, I'm hoping to do some hands-on work on this and get some more experience in unsupervised learning."
5101,How to get sum of score similar tags from text in elastic search,"['elasticsearch', 'lucene', 'text-mining', 'scoring', 'elasticsearch-query']","I try to use Elastic Search (version 6.8) to find most similar tags from text, and i expect to get sum of score similar tags instead of default elastic search's calculation (formula).For example, i create my_test_index and insert three documents:There is no mapping, it's default as bellow:So, I request below query:And get this response:If i set explain:true, result is:But, it's not appropriate result for me, i want to calculate sum of score similar tags like below:
I have ""electric"" word in text and tags and equal to ""electric"" tag, it gets 1.0 score and similarity to ""electrical"" tag, it gets ~0.7 score.
And ""develop"" word in text and tags, equal to ""develop"" tag, it gets 1.0 score, similarity to  ""developer"" tag, it gets ~0.8 score and similarity to ""softwares"" it gets ~0.9 score, and so on ...So, I expect this result==> sum score of _id:20 is= ~2.7, _id:21= ~1.7 and ....I was hoping someone can provide an example on how to do this or at least point me in the right direction.Thanks."
5102,"Building document-feature-matrix (quanteda) with Twitter data takes only a few minutes on one computer, but several hours on another one","['r', 'text-mining', 'quanteda', 'dfm']","I am using quanteda to build document feature matrices from different data sources. 
While building dfm with parliamentary speech data and Facebook data takes just a few minutes, it takes more than 7 hours to compile a dfm based on a Twitter dataset. The three datasets are approximately equal in size (60mb). R is updated (R version 3.5.3), RStudio is updated (Version 1.3.923) and quanteda is updated (Version 2.0.1) and I am using a MacBook Pro 2018 (OS X version 10.14.5).Running the exact same code on another machine with an older version of quanteda (version 1.5.2) takes just a few minutes instead of several hours.Unfortunately, I cannot provide a reproducible example since the data cannot be shared. Do you have any ideas what the problem might be and how I can circumvent it?Here are the sessionInfo() and code plus output from the problematic machine that needs more than 7 hours for creating the dfm: Here are the sessionInfo() and code plus output from the machine that creates the same dfm in less than a minute: "
5103,"Counting name occurence in textfile, being sensitive to duplicates","['python', 'regex', 'text', 'nlp', 'text-mining']","I have a list of names and want to count the occurrences throughout a corpus of text files.I am using a simple regex search with a dictionary to do this:The catch:I have a mix of titles and names (with different name formats as seen in the example below), with some duplicates between them.For example: 
My list includes two different people - ""Dr. Mark"" (title + surname) and ""Mark Smith"" (first name + surname).If a text file includes the string ""Dr. Mark Smith said that..."" my function marks a count for both people (instead of only for ""Mark Smith"").Is there any way to ensure only one count per substring? "
5104,"How do I find differing words in two strings, sentence-wise?","['r', 'text-mining', 'strsplit']","I am comparing two similar texts. x1 is the model text and x2 is the text with mistakes (e.g spelling, new characters etc.). I am trying to remove words found in both texts. Since my actual text is not in English I cannot use the dictionary. What I have tried is to step through each character of x1 and if it is same character in x2 then delete from x2 and move to next character of x1. Code I've been working on:Output:  What I want as result is:  'text' 'this' 'id'  'thius' 'ley’s' 'ythis' 'possiblke' "
5105,Enumerate two lists at the same time,"['python', 'string', 'list', 'nlp', 'text-mining']","I have two lists:
1.List of IPA symbols - M
2.List of single words - NNow I need to create a third list X = [N,M] where for each IPA symbol found in a single word I have to assign 1 to the new list and 0. For example if M = ['ɓ', 'u', 'l', 'i', 'r', 't', 'ə', 'w', 'a', 'b'] and for simplicity N has only two words = ['ɓuli', 'rutə'], then the output should look like 
X = [[1,1,1,1,0,0,0,0,0,0],
     [0,1,0,0,1,1,1,0,0,0]]So it's kind of co-occurence matrix but simpler - because I do not need to hold count of how many times the symbol occur in the word. I just need to assign 1 to X when a symbol occur in a word in a proper position. Maybe I am overthinking this but I can't seem to find a way to hold index of both lists.
Here is my code snippet:"
5106,How do I find most frequent words by each observation in R?,"['r', 'nlp', 'text-mining']","I am very new to NLP. Please, don't judge me strictly.I have got a very big data-frame on customers' feedback, my goal is to analyze feedbacks. I tokenized words in feedbacks, deleted stop-words (SMART). Now, I need to receive a table of most and less frequent used words.The code looks like this:The dataframe looks like this: there are lots of feedbacks (variable ""description"") and customers by whom the feedbacks were given (each customer is not unique, they can be repeated). I want to receive a table with 3 columns: a) customer name b) word c) its frequency. This ""ranking"" should be in a decreasing order."
5107,r: unnest_tokens() not working with particular file,"['r', 'nlp', 'text-mining', 'tidytext']","i am trying to run unnest_tokens() on the essay4 column of this dataset:https://github.com/rudeboybert/JSE_OkCupid/blob/master/profiles.csv.zipi have tried both unnest_tokens() and unnest_tokens_(), as well as running dput(as_tibble()) on profiles.csv to try to get the program working because of an answer i saw to a similar question that worked for somebody else, but i always get one of two errors.when i run this:i get this error:when i run this:i get this error:i have also tried running the same operations on a version of profiles.csv which hasn't had dput(as_tibble()) run on it.i can't figure out what to do here. it seems that other people have had trouble with this function because they aren't passing character vectors to it (like sending a list instead), or they forget to set stringsAsFactors = FALSE when reading in the data, which i've made sure to do.any advice for how to proceed? i wish i could link the data directly instead of linking a zip file, but the file is 1/3 of the size when it's zipped. oh, and it's not my github account, so i don't get to decide how the data is stored.anyway, thank you in advance for any insight."
5108,Some help getting started with tidytext,"['r', 'text-mining', 'topic-modeling', 'tidytext']","I have a project I'm working on in tidytext, which I'm pretty new to. My input data is currently in the form of individual .txt files in a folder. I successfully used get_sentiments() to track the positive/negative sentiments of my data, but I'm looking to do some more advanced topic modelling.https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocationI'm trying to work off of this guide, but I'm struggling to get started. It looks like the input data you need to do topic modelling is a DocumentTermMatrix, which I'm unsure how to create. Is there a way to turn the data I currently have as individual files into this format so that I can use the methods described in that guide?"
5109,Reading PDFs on the cloud,"['cloud', 'ocr', 'text-mining']","I have a large amount of pdf files (around 30000) that refer to different countries. I want to read them and form a corpus for each country. Initially, I used the pdf_text function from the pdftools package and the process was quite fast, 2 min for every country. Later on, I realised that the result was not optimal i.e. some words were not correctly rendered and I switched to the function pdf_ocr_text. The result is really accurate, but the computation time is really long (2 hours for one country). Since the reading by country are indipendent, I was thinking to parallelise the process on the Cloud, but I have never done it. Does somebody have a nice tutorial or suggestions on how to do that?"
5110,Unsupervised commands classification,"['python', 'machine-learning', 'text-mining', 'word2vec', 'latent-semantic-analysis']",How can I cluster commands such as  /bin/busybox chmod 777 /dvrHelper without using Bag-Of-Words representation? Models like LDA or Word2vec could be useful for my goal?
5111,Python glove missing module 'glove' 'Glove',"['python-3.x', 'nlp', 'text-mining', 'glove']","Here is what I performed:Installed pip3 install glove_py ok.
In Jupyter Python, import glove works ok.Problem:When I try a basic setup code to ensure all the modules are loaded and working. I have this code, which errors on message: ""NameError: name 'glove' is not defined"". Now since module glove import works ok, I have tried function 'glove' and 'Glove', both with NameError not defined. I did find libraries like 'git clone http://github.com/stanfordnlp/glove' and did download and build the code with make. This code ran ok in the console for a sample. Pip install for glove_py installed ok. But pip install for glove_python failed to install with ""Error Command errored out with exit status 1:"". glove 'git clone http://github.com/stanfordnlp/glove' download ok and build with make ok. But even with this make'd version, I was not able to get the Python import glove to find this c code make realized inside the Jupyter Python environment.I suspect that I am missing something simple, I would appreciate any insight.Python code, test run. Here is my Python code test run which failed on module not found.Directory function to see the functions inside 'gl' module, imported from glove package, no module function names showed. So this clearly shows that the import of glove as gl, had some issues."
5112,No applicable method for 'tidy' applied to an object of class “factor” in Tidytext,"['r', 'text-mining', 'tidytext']","I'm starting doing text mining in R and I've some problems. I have a csv with users comments about a page. Each row is a different comment. It only has 1 column, the one that has the comments. I was trying to use Tidy in R so I import the file (read.csv) and I get a data frame with n factor levels. 
The next step is try to tokenize the rowsThe csv looks like thisAs you can see, I get that error. I've also try to convert to character that column but I get the same error. Every example I look has a text prepare to work, so it's difficult to see how the raw texts are prepared. 
It's a rookie problem, so any advice will be appreciated. "
5113,How to compare text columns in 2 different dataframes to extract full matches and partial matches,"['python', 'text-mining', 'string-comparison', 'fuzzywuzzy']","I am pretty new to Python and am currently working on a project in which I have two dataframes. One contains all clients and one contains referrals. The objective is to figure out which referred to the organization successfully became clients. I have no Primary key to work with and so must do it based off of the following fields which, both dataframes have:Full Name,
Email,
Phone Number.From research I believe FuzzyWuzzy is the best library to use. However, I am conceptually stuck on trying to figure out how to write a code that will A. extract all direct matches from the 3 fields aboveB. extract all partial matches from the ""Full Name"" and ""Email"" field (partial matches would be useful for pulling out any potential typos)and give me these results in a new dataframe. I have tried putting all fields on one df and came up with the following code but, was met with errorsandI have successfully done this on excel using IndexMatch and Vlookup but it is too time consuming. Especially when comparing the partial matches on names. Any help would be greatly appreciated. Thanks in advance!"
5114,How to find similar tags from text using elastic search,"['elasticsearch', 'lucene', 'text-mining']","I try to use Elastic Search to find most similar tags from text.For example, I create test_index and insert two documents:So, i expect to find ""software"" tag (text or id) from ""I'm using some softwares and applications"" text.I was hoping someone can provide an example on how to do this or at least point me in the right direction.Thanks."
5115,Accessing words in a string,"['python', 'text-mining']","I just want to know how to get two different words present in a string.For Example: I have string A definedQ: I want to know how to get the words ""aaed"" and ""thn"" only from the string A. I don't need other wordsNote this is a part of text mining project which I am doing currently"
5116,Remove Everything after a specific word using R [duplicate],"['regex', 'rstudio', 'data-analysis', 'text-mining']","As the image shows, I need to remove everything after the word ""Please"" irrespective of the position of word ""Please"". I tried regex to remove everything after ""."", I need different approach for thisAny help will be appreciated."
5117,Text classification on a small unbalanced dataset: using externally derived features,"['machine-learning', 'classification', 'text-mining']","Text classification on a small unbalanced dataset of text documents (N=479; label 1: N=404 , label 2: N=44 label 3: N=31) the 3rd label contains conspiratorial documents.Since I have so few examples (in relation to to total dataset + in general), I was wondering if it would be an option to use externally derived features, perhaps by building/extracting a TF/IDF of conspiracy terms. And, in general, if using externally derived features is a valid method in ML & Text Mining research."
5118,Remove all rows that doesn't match a set of strings and recategorization of the columns,"['r', 'text-mining', 'data-cleaning']","I have a set of social media data queried from twitter API, which also included people's self-reported location. However, the location string does not default to a standard format for categorization, and sometimes there are ""trolls"" value. Here is an exampleMy plan is to obtain a CSV file with all cities names around the world at https://www.kaggle.com/max-mind/world-cities-database and import it into R as a vector, here is a small exampleWhat I want to do is to write an R function that cross-references a1 based on a2, replace all strings in a1 where it doesn't appear on a2 as NA, and replace all strings where it appears on a2 by that exact string values. For example, say that our function is f, the output of the function would be as followCan I write a function in R for this, or are there any existing R package build for this task? Thank you for the help"
5119,How to read “bullet point” and “table” anlong with other text from docx and convert into excel using python,"['python', 'docx', 'text-mining', 'python-docx']","enter image description hereI have a word file, which contains bullet listing and data inside table. I need to convert the text into an excel file which contains two fields Question and answer. below is the code i used to get the text from word but its not fetching special char and table "
5120,How do I calculate fuzz ratio between two columns?,"['python', 'pandas', 'numpy', 'data-science']",Getting started with Pandas.I want to calculate fuzz ratio for each row between the two columns so the output would be something like this:How would I be able to accomplish this? Both the columns are in the same df.
5121,How to view tokens in quanteda after applying a dictionary,"['r', 'nlp', 'access-token', 'text-mining', 'quanteda']","This is my first time asking a question here, so pleace excuse, if I am not handling it appropriately.
I used the R package quanteda to analyse text documents. My problem is now that I would like to see the text after I applied the dictionary that I developed. In order to apply the dictionary I tokenized the corpus, but then I couldn't find a function or method which allows me to see the tokenized text. I looked at the quanteda website and the cheat sheet but couldn't find any solution. 
This is basically the important part of my code: EDIT: Moved code from comment to question:I am looking for the command which would give the ""A b c"", which should now be in tokens1_dict.I would appreciate some help a lot!Best wishesYannick"
5122,Regex to match sentences with adjacent and non-adjacent word repetition in R,"['r', 'regex', 'text', 'text-mining']","I have a dataframe with sentences; in some sentences, words get used more than once:I want to match those sentences in which a word, any word, gets repeated once or more times. EDIT 1: The repeated words **can* be adjacent but they need not be. That's the reason why Regular Expression For Consecutive Duplicate Words does not provide an answer to my question. I've been modestly successful with this code:The success is just modest because some sentences are matched that should not be matched, e.g., yourself , everybody 'd be changing your hair in n it ?, while others are not matched that should be, e.g., no it 's not mother theresa , it 's saint theresa .. How can the code be improved to produce exact matches?Expected result:EDIT 2:Another question would be how to define the exact amount of repeated words. The above, imperfect, regex matches words that are repeated at least once. If I change the quantifier to {2}, thus looking for a triple occurrence of a word, I'd get this code and this result:But again the match is imperfect as the expected result would be:Any help is much appreciated!"
5123,Convert a kwic to a cvs or a similar format in R?,"['r', 'text-mining', 'quanteda']","i have a kwic data frame for a paper i write, so i have to copy paste the results in Word e.g. so is there a solution to do this? Which export option do i have?"
5124,how to assign the topics retried via LDA in R using “textmineR” package to the specific documents,"['r', 'text-mining', 'lda']","I have got 787 documents (speech - text file). Using ""textmineR"" package i got the topics for the same. I have got 3 topics as below:Can someone please suggest how do i assign each topic to the relevant document? and create a datable for the same:and so on."
5125,How to retrieve messages from non-owned discord server,"['python', 'api', 'discord', 'text-mining']","I'm writing a code for text analysis and Matlab and want to get messages from a discord server that I don't own according to a search query. First question? is that possible?Second: are there any good tutorials on how to do that in Python? (discord.py client.run() gives me a asyncio error even after installing nest_async)Thanks a lot,
Omar"
5126,Pulling specific words from PDF using regex in R,"['r', 'regex', 'text-mining']","Click here to see the PDF fileI am trying to pull Start,Finish,Job Number Description and Tutor from the PDF file above.The following code is giving me Start,Finish and Job Number, but I also need to extract the rest of the texts for respective Job numbers.Can you please help me on how to pull those texts and turn it into a table? "
5127,PDF: how to convert one column lists to multiple column data frame? - lists of people in subgroups inside groups to multiple columns,"['r', 'list', 'pdf', 'text-mining']","I have near 15 PDFs containing lists of people. This PDFs are only one column width, so it is a pure list. But in some way these lists are nested (subgroups inside subgroups inside groups...). There is no numerical data apart from the first number of each person in the list (which is very important for my analysis), and similar order info.I need to pull out from the PDF this lists and convert them into a conventional data frame.Here is an example of the structure of one PDF:This is the first PDF: http://bocyl.jcyl.es/boletines/1983/04/02/pdf/BOCYL-D-02041983-1.pdf!!! I found these documents also stored in the webpage, so in HTML format: http://bocyl.jcyl.es/html/1983/04/02/html/BOCYL-D-02041983-1.do
Maybe it is easier to take the content from them instead from the PDF?This follows as you can imagine (territory two, three, four..., with subsequent subgroups one, two, three, four,... etc.). This goes up to near 600 lines per PDF and more in the latest PDF.I need to create a data frame that follows this example structure:One row should be one personPOSITION IN LIST should refer to the order in which person Name Surname appeared in a given year (each PDF is for a year), in his TERRITORY, in his GROUP.Consider it to be something like a ranking, in which is important the order of the person.
Very few of the people of the PDF1 (year 1) will appear again in PDF2 (year 2), and then in PDF3 (year 3), etc. So, one objective behind all of this is to know how many and who does repeat year after year in this list.And also, it is important for the analysis to know the position of that person who does repeat in every year, to draw the evolution of this person, or to know if this person disappears after year X, etc.PS: pardon my English, is not my first language :("
5128,emerging trend detection with python,"['text-mining', 'lda', 'topic-modeling', 'trend']","I have achieved the following results by using Topic modeling:
Data Frame includes: Dominant_Topic, Keywords, Text and Year.
LDA Results Data Frame Image 
Now, I want to get emerging trends by column of keywords or text, by column of year.
So what trends have emerged in recent years and are growing?
Using Python code
please answer me
Thank you"
5129,Basic text mining and ETL for BigQuery with Python,"['python', 'etl', 'text-mining']","As an exercise I need to perform text mining with Python on the given data set:  https://www.kaggle.com/bigquery/patentsviewExpected results are:
- 3 most popular nouns in claim.text associated to each of the cpc_subsection.title category
- the most associated noun in claim.text to each of the 3 highest nouns. Ie. Most associated noun -> Most popular noun -> Title category.I've managed to connect to that data set, I am able to view its content.
My general idea is that I would need to create a list for every 'text' index to compare it later with every index of 'title' strings. Then a loop with a counter for every 'title' match. Highest 3 results listed at the end. However, after several hours of overcoming errors on every possible step to this point, I can not figure out how the code should look like to perform the task.Edit4:
Alright, I've made a code which provides with all matches at the end but not divided to particular nouns - just total matches.EDIT5: It's finished for the first point. I will share the code when I can."
5130,Filename too long when using keyword_search to detect pdf?,"['r', 'text-mining', 'pdftools']","I am trying to do some text mining of a pdf by searching for certain keywords.This is my code:However, I got the error message of a filename too long. How can I get over this issue?"
5131,"In Python, extracting all kind of date format?","['pandas', 'date', 'text-mining']","I want to extract all information related to a date in a text. I have my text in a Series Sdoc, where there is only one date in each row to extract.
So far, I'm using : By looking at my text, I identify all the different kind of date format, and I fill my re.findall with a lot of other format (like all the month based on this format (\d{0,2} \b[Jan]\w+ \d{2,4})
I catching a lot of them, but can't figure out how to deal with .Jan 2007I tried .[Jan], .\w+,[.Jan]... none of them works.Second problem, after this step, I have a Series with a lot of blank spaces :gives : [('', '', '', '', '', '', '', '', '', '', '', '', '', '', '2007')]
I've manage to clean it a little bit with :
    r2=rawdate.apply(pd.Series).stack().reset_index(drop=True)
    clean_r2=[tuple(filter(None, tp)) for tp in r2]
which gives me [....,('2007',),.....] but I figure there must exist a better way to clean it  ?thank you !"
5132,How to create dataframeSource in R? Unable to create a corpus that fits my needs,"['r', 'text-mining', 'tm']","A beginner here.
I have a dataset of 4 columns, basically news articles, containing columns with names: date, author, title and body (which contains text). 
I want to create a corpus, but I don't understand how to create DataframeSource, basically the arguments I am passing. I understand the VectorSource. 
Thereafter my aim is to do some basic text analysis. Thank you!"
5133,How to do Keyword matching across different dataframes in Pandas?,"['python', 'pandas', 'dataframe', 'text-mining', 'keyword']","I have 2 dataframes across which I need to map the keywords. 
The input data(df1) looks like this:This is the other dataframe (df2) which is to be used for keyword matching:I want to map the 'keyword' column in df1 to the 'cleaned_text' column in df2 based on few conditions: This is how the output dataframe(df3) should look like:"
5134,what is the correct way to segregate two columns in a csv where the data is mixed up?,"['python', 'dataframe', 'text-mining']","I want all the JEL classifications in one column (called classification) and keywords in another column (called keywords), the sample data is below. Here k/c is the first column and c/k the next"
5135,Probability Cutoff for Entities,"['r', 'text-mining', 'opennlp', 'ner']","I am using the openNLP package in R Studio to extract named entities from a corpus of text documents.For Example:As you can see in this code, the words 'WhatsApp' and 'Boris Johnson' are both annotated as named person entities. However, 'WhatsApp' is shown to have only a 69% chance of being a person, while Boris Johnson has a 94% chance. My problem then is how to modify this code to only return named entities with a probability above a certain percent of being a person? In this case, I would not want any named entities with a below 70% probability of being a person to be shown. EditAlright, I think I've arrived at a solution.New code:"
5136,how to extract main topic and sub topic words(bold word) from pdf in python?,"['database', 'data-science', 'data-mining', 'text-mining']","Is there any way to extract bold word from pdf? Any way to extract only main topic words and sub topic words? I have used many python library like PyPDF2, Text-tact and re many more. I have read the pdf file one and print page by page. But i can not print full pdf together and also not extract just high weight words means bold words means main and sub topic words."
5137,Structural topic modeling (STM): reduce number of topics in R,"['r', 'aggregate', 'text-mining', 'topic-modeling']","My question is conceptual and relates to this question this question on the calculation of estimate effects.When doing structural topic modeling (using the stm-package in R), I have the option to pre-set a number of topics into which the corpus is to be divided (K=x), or to let the program calculate inductively the number of topics (set K=0). What is the process to find the sweet spot for a number of topic that makes substantial sense, not just statistical? How do I aggregate topics that I believe are in fact one, but which the K = 0 algorithm sees as different ones?As I understand it, there are multiple options:1) Approach the ""sweet spot"" by repeatedly changing the fixed number of topics (K=20, K=25, K=30...).2) Try to reduce the number of topics by including additional stop words.3) Take the output from K = 0 and aggregate the topics manually by counting them as one. Which one is best practice?"
5138,find which string in a list is closest to a character,"['python', 'text-mining']","I have a pdf document that I have parsed into a list, say:Now I want to return the string that is closest (in distance) to the euro symbol (€). I have looked at various algos like levenshtein distance etc., but my task is actually quite simple and this distance can be merely number of characters.Looping with a condition kind of works:Result:But I want that 'aanhangwagens' that is in listTxt [1] is really close to the next text listTxt [2] (with the €), so the desired output is:for the phrase aquarium, it works fine because aquarium and € are in the same string i.e. listTxt[11]'hetzelfde adres staan tot maximaal € 1.250,-.'"
5139,how extract several variable from txt file?,"['pandas', 'numpy', 'text-mining']","For example, I have a text file (data.txt) which the content is in the following :I would like to extract the value for each variable, i used this line but seems i need to do more details,Do you have idea how to do that please ?"
5140,"Error in reading Chinese in txt: corpus() only works on character, corpus, Corpus, data.frame, kwic objects","['r', 'text-mining', 'stringr', 'corpus', 'quanteda']","I try to produce a wordcloud and obtain word frequency for a Chinese speech using R, jiebaR and corpus, but cannot make a corpus. Here is my code:My text file comprises the following paragraphs:Error begins when I create a corpus. R returns:I don't understand why the text is problematic. Grateful if you can help me solve the problem. Thank you."
5141,How to remove common word endings from a non-English corpus using tm package in R,"['r', 'text-mining', 'tm', 'corpus']","I'm trying to do some text mining, using tm package of R, on reviews that Italian users of a certain website wrote there. I scraped the texts, stored them on a corpus, did some sort of cleaning, but when I try to get the stems of the words by removing the common endings, I have problem specifying the Italian language instead of default one, i.e. English.First five lines work fine, but for the last one R gives me:So, my problem is that how can I use stemDocument on a corpus but specify the language I want to be used?"
5142,Python Text Clean remove hyperlink,"['python', 'regex', 'text-mining']","I work on a small text mining project.  I have some issues with text pre-processing. There are some text contains the hyperlink without""https"", for example:
""For more information, please visit us www.doctorpaul.org""
I want to remove this kind of hyperlinkhowever, it doesn't work well. Please let me know how to remove this kind of dirty words in my corpus.Thanks for any hint and answers!"
5143,Text Classification with Python or other Programm,"['python-3.x', 'keras', 'data-mining', 'text-mining']","im searching for a Programm that helps me with text classification or something like that or i have to code it by myself. 
A little example for my problem:
Tree 12m green
Tree blue 15m 
Tree red 19mSo the programm should help me to classify, sort and change the text
So Tree 12m green shold be: tr 12m gr (so i tap it into the programm) 
So for the next one Tree blue 15m the programm automaticly knows the order tree 15m blue and also knows the abbreviation tr 15m bl 
and so on
I have a lot of Data, so i might use TextMining Software or Keras or so on?
So if there is a programm that does that, than i can use this, or i have to code by myself
But the problem is, that the programm should learn the rules to classify by itself and not that hardcoded way i did this. Thank you a lot! I tried a litte "
5144,"Extract substrings in a text, on columns using Pandas","['python', 'string', 'pandas', 'text-mining']","I'm new in python, so.... I have a dataframe like this:So I'l like to search inside each row the text and have some result like:I rearch in the text ""love"" or ""love"" && ""cat"" and I want return the city or the name.I tried the follow code:It's throwing an error of the form ""The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."""
5145,Creating a graph of frequency of a specific word from a dataframe over a time period in R,"['r', 'twitter', 'data-science', 'data-mining', 'text-mining']","I have a dataframe of tweets in R, looking like this:Each tweet has a specific date assigned, all tweets in the dataframe are from a period of one year. I want to find out a frequency of one specific word (""Senate"" for example) over the entire period and plot a graph capturing how the frequency changed over time. I am fairly new to R and I could only think of super complicated ways to do it, but I am sure there must be some that's really easy and simple. I appreciate any suggestions."
5146,How can I download “Afinn” and “NRC” lexicon in R?,"['r', 'text-mining', 'tidytext']","I'm trying to get_sentiments(""afinn"") and the ""nrc"" but I get this message: Error: The textdata package is required to download the NRC word-emotion association lexicon. 
      Install the textdata package to access this dataset.How can I install the textdata package?"
5147,PDF Text to excel in a formatted structure,"['python-3.x', 'ocr', 'text-mining']","I have the following code which reads the text from the below shown extract of a pdfI need these details in a structured format like this :-The output i am getting with the aforesaid code is as follows :-how can i edit the above code to achieve the desired output,
Thanks !!"
5148,How to do fuzzy pattern matching with quanteda and kwic?,"['r', 'text-mining', 'quanteda']","I have texts written by doctors and I want to be able to highlight specific words in their context (5 words before and 5 words after the word I search for in their text). Say I want to search for the word 'suicidal'. I would then use the kwic function in the quanteda package:kwic(dataset, pattern = “suicidal”, window = 5)So far, so good, but say I want to allow for the possibility of typos. In this case I want to allow for three deviating characters, with no restriction on where in the word these are made.Is it possible to do this using quanteda's kwic-function?Example:would only give me the first, correctly spelled, sentence. "
5149,"how to remove everything but letters, numbers and ! ? . ; , @ ' using regex in python pandas df?","['regex', 'python-3.x', 'string', 'pandas', 'text-mining']","I am trying to remove everythin but letters, numbers and ! ? . ; , @ ' from my python pandas column text.
I have already read some other questions on the topic, but still can not make mine work. Here is an example of what I am doing:Then we have the following table:Now, tryng to remove everything but letters, numbers and ! ? . ; , @ 'First try:outputSecond tryoutputThird tryoutputAfterwars, I also tried using re.sub() function using the same regex patterns, but still did not manage to have the expected the result. Being this expected result as follows:Can anyone help me with that?Links that I have seen over the topic:Is there a way to remove everything except characters, numbers and '-' from a stringHow do check if a text column in my dataframe, contains a list of possible patterns, allowing mistyping?removing newlines from messy strings in pandas dataframe cells?https://stackabuse.com/using-regex-for-text-manipulation-in-python/"
5150,Text Mining in R with Persian,"['r', 'data-mining', 'text-mining']","I'm looking to do some v. simple data mining (frequency, bigrams, trigrams) on some facebook posts in Persian that I've collected and archived in a csv. Below is the script I would use with english language csv of facebook comments to unnest all individual words into their own column. Does anyone know of any method for applying unnest_tokens in Persian (or Dari to be specific) script? "
5151,Which clustering method is the standard way to go for text analytics?,"['python', 'cluster-analysis', 'text-mining']","Assume you have lot of text sentences which may have (or not) similarities. Now you want to cluster similar sentences for finding centroids of each cluster. Which method is the prefered way for doing this kind of clustering? K-means with TF-IDF sounds promising. Nevertheless, are there more sophisticated algorithms or better ones? Data structure is tokenized and in a one-hot encoded format."
5152,"How do check if a text column in my dataframe, contains a list of possible patterns, allowing mistyping?","['python', 'regex', 'python-3.x', 'dataframe', 'text-mining']","I have a column called 'text' in my dataframe, where there is a lot of things written. I am trying to verify if in this column there is any of the strings from a list of patterns (e.g pattern1, pattern2, pattern3). I hope to create another boolean column stating if any of those patterns were found or not. But, an important thing is to match the pattern when there are little mistyping issues. For example, if in my list of patterns I have 'mickey' and 'mouse', I want it to match with 'm0use' and 'muckey' too, not only the full correct pattern string.I tried this, using regex lib:I checked the text afterwards and could se that this is not working. Does anyone have a better idea to solve this problem?Here is a short example:And here is the resulting df:"
5153,How do I extract noun/ verbal phrases for portuguese?,"['python', 'nlp', 'text-mining', 'spacy', 'textblob']","I've found various tools to extract verbal and noun phrases in English, including in some questions here in stackoverflow. Yet, the techniques I've found only seem to work for English texts. I've tried spacy and textblob but they won't return anything for Portuguese texts (works perfectly in English).Here is what I've tried for Portuguese:
Spacy to extract specific noun phrase
The chunk in doc.noun_chunks works perfectly for English, but does anyone knows an already existent technique for Portuguese? I'm searching everywhere I know."
5154,Combine two words in a corpus with R,"['r', 'text-mining', 'corpus', 'text2vec']","So here is my code My .csv is articles from the new york times. 
I would like to combine words like ""new york"", ""south africa"", ""ellis island"" in vocabulary and not just have token like this :  ""new"" , ""york"", etc How can I do this ? Thank You for more precision: I m using these libraries1 "" LEAD Governor Cuomo with possible Presidential campaign waiting the wings took the oath office New Year Eve for second term New York chief executive LEAD Governor Cuomo with possible Presidential campaign waiting the wings ..."
5155,How to match tokens with a dictionary key and get corresponding value,"['python', 'dictionary', 'nlp', 'text-mining', 'topic-modeling']",I have a dataframe with tokens like below and i want to match with keys of a dictionary and get corresponding key and value.Dataframe:I have a dictionary like below:I want to match all the tokens of a row with keys of dictionary and get matched keys and value like below.output:
5156,Multiprocessing with text scraping,"['python', 'multiprocessing', 'text-mining']","I want to scrape <p> from pages and since there will be a couple thousands of them I want to use multiprocessing. However, it doesn't work when I try to append the result to some variableI want to append the result of scraping to the data = []I made a url_common for a base website since some pages don't start with HTTP etc.Above doesn't work, since map() doesn't accept function like aboveI tried to use it another way:I get this error while using  above function:I have not figured a way to do it so that it uses Pool and at the same time appending the result of scraping to the given variable EDITI change a little bit to see where it stops:"
5157,Text Mining : Pairwise Correlation between words by Group,"['r', 'dplyr', 'tidyverse', 'text-mining', 'tidytext']",A simple question.My Data looks like thisI am finding pairwise_cor for all possibilites of words above but I get result like:I want to get the above result by group i.e. with division_nameMy Data
5158,Using heuristic rules for extracting best fit examples from product reviews,"['nlp', 'data-science', 'text-mining', 'pattern-mining']","I am new to NLP and so I was trying to code an opinion mining technique in python for learning purposes. I first cleaned the data and then I used POS tagging (Stanford POS tagger) as mentioned in the paper. I fail to understand the next step.The author uses two heuristic rules on the POS tagged sentences to get ""Best Fit Examples"". The purpose of the paper is to extract three things from a sentence, Opinion Word (OW), Opining Target (OT) and Opinion Relation (LP).Currently I am stuck as to how the given rules will be applied to the POS tagged sentences. Are these rules to be taken literally? e.g. there must be three nouns followed by a verb and adjective to consider the sentence as ""Best Fit"" according to rule one. I have shared the image of the rules and some context on Best Fit Example. I am not getting the intuition behind the rules and thus, fail to implement it.Heuristic Rule 1 (if images link not working) : ""Space/NN saving/NN design/NN looks/VBZ nice/JJ  Rules for Best Fit Example"
5159,Reveice word frequency from dictionary based on LDA Topic Model,"['r', 'text-mining', 'lda', 'topic-modeling', 'word-frequency']","I'm pretty new to Text Mining in R. I just started working on a project to determine word frequencies in a corpus based on topic models that I've generated via running LDA algorithm on another corpus. I received the top 100 terms of 5 topic models from Corpus A and want to determine the frequency of those terms in Corpus B. I created 5 data frames for the top 100 terms of each topic. However, I receive following error when running the dtm for Corpus B:Does anyone know how to save the top terms of each topic as a dictionary and run dtm of Corpus B based on those dictionary?I think it's a bit easier to understand my procedure when looking at my current code:Maybe there exist some better approaches to solve my ""problem"". Would be very glad if someone could help me :) "
5160,NLP : Error/Unknown/misspelled text Detection model of a patient's medical text file,"['nlp', 'data-science', 'text-mining', 'medical']","I have few patient's medical record text files which i got from the internet and i want to identify/find the files which are bad quality(misspelled words/special characters between the words/Erroneous words) and files with good quality(clean text).i want to build error detection model using text mining/NLP.1)can someone please help me on the approach and solution for feature extraction and model selection.
2)Is there any medical corpus for medical records to identify the misspelled/Erroneous words."
5161,Scalable alternative to bag-of-words for text representation and pairwise cosine distance for near-duplicate detection,"['machine-learning', 'text-mining']","I built an application for detecting near-duplicate customer records using bag-of-words vectorization and cosine similarity for all pairs of records. It is very practical and fast for small to medium sized data sets, but for larger and richer data sets it does not scale well due to memory consumption.I am looking to replace this solution with something that 1) more efficiently represents my customer records, and 2) more efficiently performs similarity matching. My customer records look something like this - ""John Doe 123 Main Street Anytown, USA 12345"". I've been researching word embeddings (e.g. Word2Vec) to represent customer records efficiently, but I get the impression that this approach will not add any value. First, there are no semantic word relationships in my customer records. For example, the words in the customer record ""John Doe 123 Main Street Anytown, USA 12345"" have not meaningful context nor do they share similar meaning. Second, even with word embeddings I would still be computing pairwise similarities using some metric.Please correct me if my understading of Word2Vec (or any other embedding) is incorrect."
5162,How to used groupby and CountVectorizer() together in pandas Dataframe?,"['python', 'pandas', 'pandas-groupby', 'text-mining', 'countvectorizer']","I have this sample data. This is a CSV file. I want to create feature vectors of 'Questions' and 'Replies' columns using Bag-of-Word method (CounterVector()) and then calculate the cosine similarity between the question and their replies. So far I have this python code:Problem in this code is that the creates words dictionary (features.vocabulary_) from the whole ""Questions"" and ""Replies"" columns but my requirement is to calculate ""vocabulary"" for each thread individually and then create features vectors based on that individual dictionary. in other words in ""ThreadID"" column when values changes new vocabulary should be created.I think ""groupby"" function is used here but how? Hope the question is clear.
Please help me. I will be very thankful to you."
5163,How can i token unblanked sentence entry in textmining?,"['r', 'text-mining']","I want to cluster customer text data. For example, basic text is like:
text1:
... VARDIR.SAHİLE BAKAN KISIMDA.İLÇE BELEDİYE YE AİT....After the part of token which is below, some words were not tokened because some people did not space after . or unblanked sentence entry just like ""KISIMDA.İLÇE""
Algorithm do not token like ""KISIMDA"", ""İLÇE"". It takes bot of them like ""KISIMDA.İLÇE""How can i solve this problem?
thanks a lottoken part"
5164,How to remove just the set of numbers with / in between among other strings? [duplicate],"['r', 'regex', 'string', 'text-mining']","I need to extract the blood pressure values from a text note that is typically reported as one larger number, ""/"" over a smaller number, with the units mm HG (it's not a fraction, and only written as such).  In the 4 examples below, I want to extract 114/46, 135/67, 109/50 and 188/98 only, without space before or after and place the top number in column called SBP, and the bottom number into a column called DBP. 
Thank you in advance for your assistance.  "
5165,How to look for keywords in Pandas data frame column and assign a tag via dummy variable?,"['python', 'pandas', 'dataframe', 'text-mining']","I had no idea how to title this sorry. Basically, I have a CSV file with keywords and tags. It looks like this: I have a separate data frame that has these columns:I want to iterate through each row of my data frame, and check the description to see if it contains any of the keywords in the CSV file. If it does, I want to set the appropriate ""tag"" column equal to 1, otherwise 0, like so:This is how I created the data frame: What my df2 looks like: I tried something like this but got stuck very quick: Here's an example of data"
5166,Over-fitting text classification using random forest using R,"['r', 'text-mining', 'multiclass-classification']","I have trained models and the accuracy of a trained model comes to 99.9%. But when I fit model for test data accuracy is only 59.5%.
I am working on text classification and using the random forest.I have a few questions.confusionMatrix(PD3,train$Label )
Confusion Matrix and StatisticsPrediction HIGH LOW MEDIUM
    HIGH    116   0      0
    LOW       0 120      1
    MEDIUM    0   1    233Overall StatisticsMcnemar's Test P-Value : NA              Statistics by Class:confusionMatrix(PD4,test$Label )
Confusion Matrix and Statistics ReferenceOverall StatisticsMcnemar's Test P-Value : 4.28e-07        Statistics by Class:"
5167,when apply word2vec just characters shown not a word ؟,"['python', 'nlp', 'text-mining', 'word2vec']","this my code you can see i am tokonize the sentence to word but i am still have a problem when i apply 
word2vec model in my sentences i use Arabic text 
anaconda version 4.7.12in words1 the vocab just shown the letters    "
5168,Which Deep Learning Library could fit my Text-Data,"['text', 'deep-learning', 'text-mining']","Im currently working on a project and i would like to ask you what deep-learning library would be the best.So ive got a dataset wich consists of 1000x rows. One row is long text and the other row is a short text. The goal of my algorhytm ist to genereate the short text from the long text automatically. Like this :Valve-Operated Pilot-2/2NC-3/4-DN20         Short text: Pilot 2/2nc-3Ive tried to use Bert but I get this error :""from bert_text import run_on_dfs""You got any tipps for me? Ive used Google Collab the whole time. I dont know if thats the best option either.Thank you and have a nice day"
5169,How can I use the PCA for a term-document matrix in Python?,"['python', 'text-mining', 'pca']",I have a list of stemmed words:I am using this function to convert the list into a term-document matrix:How can I use the PCA to reduce the dimensions of the generated matrix?
5170,How to change language of termDocumentmatrix in R text-mining?,"['r', 'nlp', 'text-mining']","But, i dont integrate these processes into term document matrix which is below that i try to mine pdf file"
5171,R text mining with TM: Does a document contain words that are rare,"['r', 'text-mining', 'tm']","Using TM package in R, how can I score a document in term of its uniqueness? I want to somehow separate documents with very unique words from documents that contain often used words.I know how to find the frequently used words and least used words with e.g. findFreqTerms, but how do I score a document with regards to it's uniqueness?I am struggling to come up with a good solution."
5172,Extract a string or value based on specific word before and a % sign after in R,"['r', 'string', 'filtering', 'text-mining', 'text-extraction']","I have a Text column with thousands of rows of paragraphs, and I want to extract the values of ""Capacity > x%"". The operation sign can be >,<,=, ~... I basically need the operation sign and integer value (e.g. <40%) and place it in a column next to the it, same row. I have tried, removing before/after text, gsub, grep, grepl, string_extract, etc. None with good results.  I am not sure if the percentage sign is throwing it or I am just not getting the code structure. Appreciate your assistance please.
Here are some codes I have tried (aa is the df, TEXT is col name):"
5173,Fastest way to filter non frequent words inside lists of words,"['python-3.x', 'pandas', 'multiprocessing', 'apply', 'text-mining']","I've a dataset containing lists of tokens in csv format like this:First i want to find all the words that appears in text at least a certain amount of time, let's say 5, and this is easily done:Now i want to remove for each list of tokens those who appear inside frequentwords, to do so i'm using this code:The non multiprocess version take around 2.30-3 hours to compute, while this versione takes 1 hour.Surely it's a slow process because i've to perform the search of circa 130 milions tokens in a list of 30k elements, but i'm quite sure my code is not particularly good.Is there a faster and surely better way to achieve something like this?"
5174,Extracting text between two delimiters when the delimiters are in different formats using Python,"['python', 'regex', 'text', 'text-mining']","I'm a new Python programmer (more experience in R) using Pycharm community edition v2019 2.4, using a laptop running Windows 10.  I'm attempting to extract a block of text between two delimiters which is usually in the following format. (text is between the delimiters but on separate lines)The problem I'm experiencing is that Item 7 and Item 7A can come in many different formats due to the initial pre-processing of the text files, for example.orororItem 7 and Item 7A can, also appear in larger blocks of text.  This is an issue  beyond my control.I've examined 100 text files so far and have written the following code.This deals with some, but not all of the cases, and even then it's not detecting everything. It won't be possible to analyse the full set of text files as there will be close to 250,000 for the full study.  My questions are as follows.Any help would be appreciated."
5175,Inconsistent output of word frequency count in column pandas dataframe python,"['python', 'pandas', 'nlp', 'nltk', 'text-mining']","So I have a simple dataframe in pandas, where one of the column consist of tweet messages. Each cell or row contains a tweet message. I am trying to do a word frequency count to detect what are the top 10 words in my dataframe. Reason being to remove them from my dataset by adding them to my list of stopwords. Tried a few code snippets on my dataset, however confused as to why it yields different results when it comes to the count of frequency. Below comparison of codes. Code 1Code 2The top 10 most frequent words are the same in both codes but the values or count of word distribution/frequency differed slightly i.e Code 1 had a slightly lower count for the same list of words in Code 2. They are both analyzing the same dataset. The difference is around 100 words. The only difference I see is that Code 1 tokenizes the words where as Code 2 splits the words, but they are essentially the same thing so what am I missing here? I realized that Code 1 yields nltk.probability.FreqDist whereas Code 2 pandas.core.series.Series. Can someone kindly break this down to me and explain the difference please ? "
5176,Get topic words from LDA model using apache spark with java,"['java', 'apache-spark', 'text-mining', 'lda', 'topic-modeling']","I am new to text mining with apache spark using java. I am trying to do a LDA on text data. 
First I am using a IDF model to extract relevant words. Then I create a LDA model to get my topics. As a result I get a table with termIndices and termWeights. How can I get the topics as words from my lda model? This is the code that I use:This is the output of my code:"
5177,Function to find word in list and then print following 50 lines,"['python', 'list', 'text', 'text-mining']","I have a gigantic txt file that I read in and cleaned into a list.
I'm looking for certain words, so I wrote a quick functionwhich works fine, but how would I write the function so that it prints the word, plus the following next 50 lines or so? 
Summarizing, I want to find the text that comes after that word.  From then, I would want to create an empty df, and have the function fill in the df with a new row with the word + next 50 rows, every time it found that word.  "
5178,Text Extraction from Images,"['python', 'machine-learning', 'nlp', 'text-mining', 'text-analytics-api']","I did extraction of text from image. I got unstructured data after extracting text. I have to convert this to a structured form but I'm not able to do the so.The unstructured data extracted from image in python: The image:
Please help to convert this unstructured data to structure data. Any library or any function suggested?"
5179,using regular expression for searching multiple key words from cells,"['python', 'regex', 'text-mining', 'data-analysis', 'orange']",I have to write a code for searching regular expression from an excel sheet which has sentences grouped together. I have managed to find the key words representing each sentence. When i run the below mention code it finds only one key word from one cell and moves to next cell. I have tried to display the requirement in the table
5180,Apply function for one item from dataframe to all other items,"['python', 'text-mining', 'sentence-similarity']","I am examining similarity of text in Python. I have dataset with around 100 records and prepared a function for checking similarity - it has 2 arguments for 2 sets of words.My dataframe:... ...I would like to create a method that will iterate over dataframe for given row, and will find out let's say 2 most similar records.
for example checkSimilarity(1) or checkSimilarity(df['col'][1]) for index 1 in dataframe and will give us [3, 4] as the most similar."
5181,Extract some part of text and format it desirably in Python?,"['python-3.x', 'text', 'text-mining']","I want to extract some specific portion of information (text) from a bigger text, and export it based on my desired format. Below is an example"
5182,How can I set window size in text mining using Python,"['python', 'text-mining']","I'm a beginner in python and I want to solve this question.  Write a function performing the windowization of the YahooFinance sequence. The window size is the parameter maxspan, that you will set to 2 days (i.e. from t = 0 to t = 2). I have written code but I'm thinking it is wrong. Where am I doing wrong?"
5183,list of vectors in R - extract an element of the vectors,"['r', 'list', 'vector', 'text-mining']","I have a list which contains some texts. So each element of the list is a text. And a text is a vector of words. So I have a list of vectors.
I am doing some text-mining on that.
Now, I'm trying to extract the words that are after the word ""no"". I transformed my vectors, so now they are vectors of two words. Such as :
list(c(""want friend"", ""friend funny"", ""funny nice"", ""nice glad"", ""glad become"", ""become no"", ""no more"", ""more guys""), c(""no comfort"", ""comfort written"", ""written conduct"",""conduct prevent"", ""prevent manners"", ""matters no"", ""no one"", ""one want"", ""want be"", ""be fired""))My aim is to have a list of vectors which will be like :
list(c(""more""), c(""comfort"", ""one""))
So I would be able to see for a text i the vectoe of results by liste[i].So I have a formula to extract the word after ""no"" (in the first vector it will be ""more"").
But when I have several ""no"" in my text it doesn't work.Here is my code :That one works for a vector when there is only one ""no"" :But if I try to adapt it with a loop to see each element of the vector, and there are more than one ""no"" in the text, it doesn't work.Code :Warning message :As you can see I have only the second word which is there.I tried many things and I tried to split the code and run it and work on it piece by piece, but after spending all the morning on it I haven't found a solution..Did someone have an idea top help me ?Thank you in advance (and sorry for my english, I'm french ^^')"
5184,Text mining using Java/R interface,"['java', 'r', 'file', 'text-mining', 'jri']","I'm trying to do text mining using JRI for my file, this is my first time doing it. So this is my code:but I'm getting null as a result, I think the tm library isn't loading or something, as I said before this is my first time doing this."
5185,text mining python keys,"['python', 'regex', 'text-mining']","I have a multiline file, tab separated, which might include (or not) some keywords int the second column,Place1______________fishPlace2______________fishing sometingPlacexx_____________something missingPlace_somwhere______something else missingEHDN_______________fishing somethingHDGFE______________looking for something(the lines are uggly but i couldn't manage to make the data look like a table)I would need to, each time that the line contains 'something missing', to add an annotation at the end of the line, like ""ACTION NEEDED THERE"";I've tried someting like:or findall re function without success...Can you help me please ? Should re.findall work here ?
I've tried pattern=re.findall(""something missing"", line) but it's not working....
Sorry for asking that but i did not manage to find the right answer in the previous posts.....
Many Thanks in advance !"
5186,Countvectorizer with foreign symbols gives swapped key-values in vocabulary dictionary,"['python', 'scikit-learn', 'text-mining']","I'm using a CountVectorizer:This results in a vocabulary with the letters as keys and the index in the vocabulary as values:Now let's add some foreign (arab?) characters:See how the keys and values are swapped for the foreign characters, so the character and the index are swapped. What's happening? It looks like it's due to the fact that in some languages people read from right to left? Is this part of the behaviour of Python dictionaries?"
5187,Dynamically add text to PNG in R,"['r', 'text-mining']","I have floor plans for apartments and information in excel sheets and from the internet, and I'm analyzing them in R. I would like to dynamically add text to each room, say the square feet of the room compared to average etc. To do this, I figured I have to read the text of the png (e.g. bedroom), and merge with my information in R, then somehow add the text to the png. Has anyone done something similar, and is it even possible?Many thanks. "
5188,Find regular expression or list of regular expressions across multiple text files and extract matching lines,"['python', 'regex', 'python-3.7', 'text-mining']","Caveat: I am good at regular expressions, but I am a Python novice. I have tried to read as extensively as possible and could not find a solution that matched my scenario, so I am asking this question.I wish to accomplish the following: I had some success with code of this kind, which has allowed me to successfully print matching lines. With about six hours of Python experience in total, I felt pretty happy.My regular expression ""Bob(by)?"" will match ""Bob"" and ""Bobby"". But my code will print something like this (if I am not mistaken).Instead, I want it to print two lines (one for the ""Bob"" match and one for the ""Bobby"" match. This can be done relatively easily in grep, if I recall correctly, but I can't find anything helpful in the re module documentation."
5189,How to extract a subset from a text file and store it in a separate file?,"['python', 'text-mining']","I am currently trying to extract information from a text file using Python. I want to extract a subset from the file and store it in a separate file from everywhere it occurs in the text file. To give you an idea of what my file looks like, here is a sample:Here, I want to extract ""value"" and the corresponding number beside it. I have tried various techniques but have been unsuccessful. I tried to iterate through the file and stop at where I have ""value"" but that did not work.Here is a sample of the code:"
5190,How to get data from the web,"['python', 'html', 'database', 'web-scraping', 'text-mining']","I want get data from specific web site. For example getting data about foods from various web sites. I want to get the web data with title, link, date and the time and the description. I hope to do this using machine learning. So is there specific methods to do this?(getting web data and filter by keywords).
ThanksI tried using php. But I am not familiar  with the php.Finally I want to publish those data in web site."
5191,"I cannot use TfidfVectorizer and its method fit_transform, how can I solve this problem?","['scikit-learn', 'text-mining']","I'm trying to use TfidfVectorizer in order to find out what documents in a collection of 40000 articles belong to the same topics.In mypath I have the list of documents that I want to analyze.
vectorizer.get_feature_names() returns a numerous list of words, so I think it is correct.However when I call the method fit_transform() of TfidfVectorizer, it returns a matrix X with just one column, and I don't understand where the problem is.Do you have any suggestion?"
5192,Extracting N number of matches from a text string in R?,"['r', 'text', 'text-mining', 'stringr']","I am using stringr in R, and I have a string of text that lists titles of news articles. I want to extract these titles, but only the first N-number of titles that appear. In my example string of text, I have three article titles, but I only want to extract the first two. How can I tell str_extract to only collect the first 2 titles? Thank you.Here is my current code with the example texts.Here is the example text. The current code brings back all three matches in the string: I only want it to collect the first two matches."
5193,Refine core info from string,"['python', 'machine-learning', 'text-mining', 'n-gram']","A list of fruits with details in strings. I want to refine the core info, i.e. the fruit name from the strings.Some of them have 1 word in name (e.g. Apple, Grape), some have 2 (Water melon, Start fruit).I tried ngram way:1-gram:2-gram:Obviously it's not working well. What would be a better way? Thank you."
5194,Memory issue while calculating pairwise count,"['text-mining', 'pairwise']","When I try to create pairwise_count for my data I get the error ""Import memory Error in asMethod(object) : Cholmod error 'problem too large' "". Help me resolve this issue"
5195,"Unsupervised learning, Python, text clustering","['python', 'text-mining']","I want to do unsupervised learning. As I understand with such learning we don't know the cluseters before, right? I read about k-means alghoritm, followed mainly these two articles:And maybe this sounds stupid but this is some kind of magic for me, that following the example from towardsdatascience suddenly out of nothing  ""Python"" knows the clusters. My problem is to understand how it works from inside. I have around 100 text documents (resumes) and would like to play with this alghoritm - maybe start with 2 clusters - experienced/notExperienced but don't want just to copy the code  from tutorials."
5196,Adding custom stopwords to IBM Watson Discovery,"['python', 'text-mining', 'ibm-watson', 'stop-words', 'discovery']","I'm trying to add a custom stopwords on a colletion of Watson Discovery, but I only get the error 500 ""Error when creating 'stopwords'."". Same on both web and api (curl).I've tried:I've checked (https://cloud.ibm.com/docs/services/discovery?topic=discovery-query-concepts&locale=en):Also, I ran curl with invalid collection and environment to check the api validation (unnecessary I know), and it returned 404 ""Could not find listed collection"" as expected (ok, it's working).Am I missing something? What more can I check?curl command:Thank's"
5197,How can the bag of words text classification model classify by preset categories?,"['python', 'pandas', 'text-mining']","As a follow up to, Is there a Python text mining script to classify text with multiple classifications?, I have created the following scripts below.The two tables I am working with are a description table (I am only showing 5 records):And a category table (I am only showing 5 records)I have created the following script to remove keyword and characters from two separate tables I have, after changing them both into Panda Dataframes:I then tokenize the description and category description which contains the text, for each table.On the category table, I create a data frame where each individual token keyword for each category number is a record:which produces the following (I am only showing 5 records):I then remove duplicate keywords and then create a category dictionary, with each word linked with the category codes it has a record to:After conducting the same following steps to the other description table up to tokenization (but not flattening or making a second dictionary), I then match the description table with the category list dictionary.  It produces a table that shows for each description record, how many category keywords and their category numbers match.The problem I have so far, is getting too many duplicate values for each description.To apply a bag of words model for text classification, I can scrape the description table and the categorization, however how can I classify by categorization?  The bag of words seems to work for a whole document, not table."
5198,Detect part of a string in R (not exact match),"['r', 'string', 'text-mining', 'stringr', 'grepl']","Consider the following dataset :I m trying to detect string1 in string2 BUT my goal is to not only detect exact matching. I m looking for a way to detect the presence of string1 words in string2, whatever the order words appear. As an example, the string ""my beautiful house is cool"" should return TRUE when searching for ""my house"".I have tried to illustrate the expected behaviour of the script in the ""return"" column of above the example dataset.I have tried grepl() and str_detect() functions but it only works with exact match. Can you please help ? Thanks in advance"
5199,Count frequency of specific words in a dataset,"['r', 'text-mining', 'word-frequency']","I dont have experience with text classification, so I'm not sure how to logically solve this. I have little R knowledge, so would prefer to solve it in R. I have a 50000 * 32 data set. Of these, 7 columns contain text data like description, function, names etc. I am looking to classify each row into 6 different types if any words from a specific pre-defined list is available. I need the count of these pre-definied words occurring so that I can rule out accidental occurrences.pre-definied list: day, today, night+----------------------+---------------------------+-------+
|         col1         |           col2            | count |
+----------------------+---------------------------+-------+
| this is a sunny day  | the weather is good today |     2 |
| it is a rainy night  | the day is cloudy         |     2 |
| it is thursday today | have a good day           |     3 |
+----------------------+---------------------------+-------+"
5200,Error while finding topics quantity on Latent Dirichlet Allocation model using ldatuning library,"['text-mining', 'lda', 'text2vec']","This is the outcome error and I can tell this is because there is at least one document without some term, but I don't get why and how I can solve it.Even though I know ldatuning is made for topicmodels, I don't think there might be a huge difference to get a number to start testing, is there?"
5201,Creating LDA-format documents and vocab using pre-unnested tokens,"['r', 'text-mining', 'lda', 'unnest']","I'm working on a sentiment analysis task on a foreign language (simplified Chinese) data. While this kind of task is easy to implement in English-language text data, working on Chinese text is much tricky because the commonly-used unnest_token() approach doesn't quite work on Chinese text and one may need a custom-built stop words list to remove redundant words from the corpus. Hence, what I did was to have a colleague work out the Chinese text stemming and cleaning tasks for me, I then merged the cleaned text (which comes in the format of a list of lists) with their corresponding documents and sentiment rating, as shown in the example data at below. The cleaned texts are stored in the text column, which are to be matched with their original document stored in the documents column of the same row. However, if I want to convert the entries of the text column into LDA format (which consists of a indexed documents and a list of vocab) for further processing using tidy-type of applications, while keeping the length of documents of the new LDA object the same with the length of original documents in the example_dat, such that they all align with their doc_id, how can I achieve that?"
5202,How can I group the shop names which have location number behind?,"['nlp', 'text-mining']",I have a list of shop names which looks like this:I want to create a column to match the restaurant name like this Is there any possible way to do this automatically? Surely I cannot get all the main name I want. The list is too long
5203,Getting accented characters recognized when building a custom stopwords lexicon in R,"['r', 'text-mining', 'stop-words', 'lexicon']","I'm building a custom stopwords lexicon in R to remove accented characters. I thought that using the unicode reference would enable this, but it doesn't work and I'm having trouble thinking off different solutions, especially as some of these could not be covered by running a lexicon from another language. Current code:This words find with regular characters. "
5204,Perplexity issues using text2vec,"['for-loop', 'text-mining', 'text2vec']","I am ussing text2vec on 230k docs, as I always mention. I am trying to find the best topic number for my document term matrix by using perplexity. When I use it one by one it works perfectly fine, but when I try to use a loop to get it for a range from 2 to 25 it doesn't work and I can't tell why, could  someone please tell me what is wrong?"
5205,Extracting Body of Text from Research Articles; Several Attempted Methods,"['r', 'maven', 'text-mining', 'reproducible-research']","I need to extract the body of texts from my corpus for text mining as my code now includes references, which bias my results. All coding is performed in R using RStudio. I have tried many techniques.I have text mining code (of which only the first bit is included below), but recently found out that simply text mining a corpus of research articles is insufficient as the reference section will bias results; reference sections alone may provide another analysis, which would be a bonus.EDIT: perhaps there is an R package that I am not aware ofMy initial response was to clean the text formats after converting from pdf to text using Regex commands within quanteda. As a reference I was intending to follow: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005962&rev=1 . Their method confuses me not just in coding a parallel regex code, but in how to implement recognizing the last reference section to avoid cutting off portions of the text when ""reference"" appears prior to that section; I have been in contact with their team, but am waiting to learn more about their code since it appears they use a streamlined program now.PubChunks and LAPDF-text were my next two options the latter of which is referenced in the paper above. In order to utilize the PubChunks package I need to convert all of my pdf (now converted to text) files into XML. This should be straightforward only the packages I found (fileToPDF, pdf2xml, trickypdf) did not appear to work; this seems to be a within-R concern. (Coding relating to trickypdf is included below).For LAPDF-text, ...[see edit]... the code did not seem to run properly. There are also very limited resources out there for this package in terms of guides etc and they have shifted their focus to a larger package using different language that does happen to include LAPDF-text.EDIT: I installed java 1.6 (SE 6) and Maven 2.0 then ran the LAPDF-text installer, which seemed to work. That being said, I am still having issues with this process and mvn commands recognizing folders though am continuing to work through it.I am guessing there is someone else out there, as there are related research papers with similarly vague processes, who has done this before and has also got their hands dirty. Any recommendations is greatly appreciated.CheersShort of the RPoppler issue above the initial description should be sufficient.UPDATE: Having reached out to several research groups the TALN-UPF researchers got back to me and provided me with a pdfx java program that has allowed me to convert my pdfs easily into xml. Of course, now I learn that PubChunks is created with its sister program that extracts xmls from search engines and therefore is of little use to me. That being said, the TALN-UPF group will hopefully advise whether I can extract the body of each text via their other programs (Dr Inventor and Grobid). If this is possible then everything will be accomplished. Of course if not I will be back at RegEx."
5206,Requests from Scopus API is only writing the first page of the PDF,"['python', 'python-requests', 'text-mining', 'scopus']","I am trying to download the full text PDF versions from the Elsevier API. I am able to download the whole paper in XML, JSON and plain text form. So, the API key is working fine. However, I am not able to download the full text in PDF form. When I try to change the header to accept the PDF files, it only writes the first page of the articleI tried on a lot of different DOI's but all of them return the first page of the article.This is the request command that I am using to access the paperAnd I am writing using the following codeThere is no error but the test.pdf is only the first page of the article."
5207,How can I cluster topics outcome from Latent Dirichlet Allocation model while using text2vec in R?,"['cluster-analysis', 'text-mining', 'lda', 'text2vec']","After running LDA modeling in R using text2vec (as shown below), I want to apply hard cluster to the topics outcome to get a second layer for the categorization but how can I find a way of calculating distances among them?"
5208,Count number of rows which contain words,"['r', 'text-mining']","I have a dataset with many rows that contain fruit descriptions e.g:I need to find unique words in this description (
I've already done it) and then I have to count in how many rows are those unique words appear.
Example:I've done something like that:But the problem is that I have too many unique words so I don't want to do it manually. Any ideas?"
5209,Python: Masking Named Entities in Email Text,"['python', 'stanford-nlp', 'text-mining', 'named-entity-recognition', 'data-masking']","I have created a python script to extract named entities as follows:** Output **Now I want to mask these named entities with a pattern like ""######"" to follow GDPR regulations by hiding customer sensitive information.I have attempted options like:Applying for loop on original data frame - check Text for named entities present in named entity data frame - mask named entity with '#####'.Define a function for masking named entities in Text:dataset_unseen['Text'] = dataset_unseen.apply(Detectner,axis=1)But I get Following Error:AttributeError: (""'list' object has no attribute 'sub'"", 'occurred at index 0')How can I extract and mask named entities in Text. Any improvement to this code is highly apprciated !"
5210,Extract text from .pdf file in theory,"['pdf', 'text-mining']","I know there are thousands of ways to extract text from .pdf file - there are online converters, libraries, packages and it is possible to do it in any programming language. For the needs of my thesis I am looking for the source that explains how it works - I found some presentation that text is basically anything that is between parenthesis but when I opend the .pdf file with some notepad I don't find it (actually there are no real words). 
Is there any work, article that describes how .pdf file works? What language is used? What are the layers of it? Can we create a .pdf file in some notepad from scratch - then just save it as .pdf and see it properly? How such pdf_to_text tools (ex. in R or even JavaScript) work from the inside?
I will be so so grateful for any answers, help, links, explanations!"
5211,Create bag of words from string in R,"['r', 'text', 'text-mining']","I found many implementation for bag of words but still cannot find easy one for simple, long string. My result would like to be like:I have a problem with qdap library because in does not work on my R..."
5212,Where can I find the coherence funtion in r?,"['text-mining', 'text2vec']","Excuse me for being basic, but I want to use the 'coherence' function I found on this link to evaluate my latent dirichlet allocation topics and it isn't working with text2vec and I can't tell which library it is in, if it isn't that one."
5213,Converting corpus to dataframe returns NA's,"['r', 'text-mining', 'corpus']",I am trying to convert my corpus back to a dataframe but it returns only NA's. Please HelpCode:Expected Output:dataframe
5214,How to do tokenizing by n-gram for pdf file in R,"['r', 'tokenize', 'text-mining', 'tidytext']","I want to tokenize a pdf document by ngrams in R.
I tried to follow the instructions here 
 at https://www.tidytextmining.com/ngrams.html,
but get stuck with the unnest_tokens() function.I keep getting this error message:
Error in UseMethod(""unnest_tokens_"") : no applicable method for 'unnest_tokens_' applied to an object of class ""c('VCorpus', 'Corpus')""Is there anything I need to do before running the unnest_tokens function?
Thank you."
5215,"Extracting specific portions of pdf (Eg abstract, Introduction) in python?","['python', 'regex', 'text-mining', 'pdfminer']","I am trying to extract a specific portions of the PDF file. I am reading the pdf file as shown in the below code . This code is extracting all the information at once including all the garbage values. I want to concentrate on just the abstract , introduction, methods and conclusion.  I have tried this by using regex as well. Please let me know is there any way to extract the above information.Regex Code:"
5216,How to split cleaned text data into training and testing datasets except for random sampling,"['python', 'text', 'nlp', 'text-mining', 'sampling']","I have cleaned and de-duplicated text data with a 'count_raw_id' column which implies the number of raw ids that are mapped to one cleaned id 
A clean id represent that it is unique and has some raw ids mapped to it 
Now i don't want to split my cleaned text data('clean_df') randomly 
I need some Criteria based sampling to create two datasets out of this whole cleaned file of about 2k rows one to train the model and one to test the modelI don't want to use train_test_split of sklearn to split my data as it will my data randomly.I want some way out to query my data such that i can use some other sampling technique also i can't use stratified sampling as i don't have actual labels for these records"
5217,How to find phrases that are the same between strings in R,"['r', 'text-mining']","Let's say i have the following character stringI want to extract only those words or phrases that are shared between strings so that the result should be: ""Date of Procedure"",""Patient name"",""Type of procedure"",""Label"". I tried using tidytext but it forces me to say the n-gram size I want whereas there may be one, two or three word phrases that are shared."
5218,Is it better to go through stemming or lemmatizing when preprocessing text to apply Latent Dirichlet Allocation?,"['r', 'text-mining', 'stemming', 'lemmatization']","I am applying Latent Dirichlet Allocation to 230k texts in order to organize the data presented. I'm not sure if it would be better to apply stemming or lemmatizing in the preproessing tokenization function while using text2vec library in R.I reviewd both outcomes and they are different, even when it's the exact same word. The counts differ most of the times and some words missing in the other."
5219,How to replace the wrong spelt word with the correct one in R,"['r', 'tidyverse', 'str-replace', 'text-mining']","I know similar question might have asked but I feel my requirement is peculiar.
I have two data frames: one with wrongly spelt words and another data frame with corrected words.I need to replace each incorrect word with the correct word in another data frame.
Could you please let me know if there is any best possible way.I am trying with the following code but not getting the expected outputExpected output:"
5220,how to scrape every url with 'for',"['html', 'text-mining', 'rvest']",I wrote the code below.I put 10 URLs but the result texts came only 5.what's wrong with it?
5221,Text mining with R_ I couldn't put the right html_nodes,"['html', 'r', 'web-crawler', 'text-mining', 'rvest']",I tried to crawling this site(https://www1.president.go.kr/petitions/?c=0&only=1&order=1&page=1) with the code below.But didn't work.I guess I put inappropriate web tag in there.'petition_list' is right?What is proper tag?
5222,I am looking for the optimal number of topics while topic modeling with text2vec in R,"['r', 'text-mining', 'lda', 'topic-modeling']","I am running an lda topic modeling mechanism on 230k texts while using text2vec library from R for most of it. I've seen tutorials that mention how to run coherence graphs with different algorithms in order to select the best 'k' number of topics here and here but none of those match text2vec. In text2vec.org I can get a tutorial but at the end they encourage me to start playing with hyperparameters in order to get the best modeling out of them, but I am looking for the real optimal among all possible options.Preprossessing coding before modeling"
5223,Extracting tables from PDFs when there are multi-line rows--all solutions welcome,"['pdf', 'text-mining', 'text-recognition', 'pdf-conversion']","I have a bunch of PDFs with tables of varied structure and size that I would like to detect and extract. I would use something like tabulizeR or Tabula, but most of these tables have multiple lines per cell and these tools don't seem to be able to handle them. I've found a lot of people asking this question but no answers. If it was possible to automate the process it would be even better since I have a lot of PDFs. I'm not sure how to share an example PDF, but the image below is a sample of what some of the uglier tables look like.I mainly use R, but heck, I'll use COBOL if that's what it takes. Does someone know how to solve this?"
5224,Convert a text document/string to dataframe in Python,"['regex', 'python-3.x', 'nlp', 'text-mining', 'text-parsing']","I have extracted this text from a PDF using Apache tika. I want to split this text in such a way that I get a single row per question/answer. End result needs to be a tabular structure. 
Here's the text that I am working on (Mock up to avoid violations):I have tried splitting on ""\n\n"" ""?"" and "":"" but it does not generalize well.| Does your plan offer a travelers insurance and all(Part D) benefits? |       Yes| Select the type of your stay:| Enhanced Alternative| Describe the components of your travelers plan (select all that apply):|  Standard Cost-Sharing, Out-of-bond James, Standard Cost-management, Long Term membership| Sponsor attests that it will comply with 42 CFR 423.154: | Sponsor attests that it will comply with 42 CFR 423.154:A vertical pipe represents a column."
5225,Preserve Hyphenated words in ngrams analysis with tidytext,"['r', 'regex', 'text-mining', 'tidytext']","I am doing text analysis of biograms. I want to preserve ""complex"" words made of many ""simple"" words linked by hyphens.for example, if I have the following vector:***I edited this section to make my the output I need clearer****I want my biograms in a data.frame of dimensions 3x1 (which is the format you obtain when using unnest_tokens from tidytext :**** end of the edition****My problem is that with tidytext, the option token gets used with either ""ngrams"" (which is the sort of analysis I am performing) or with ""regex"" (which is the command I may use to condition on these hyphens)This is the code I am using at the moment:How can I do both things at the same time?thank you"
5226,R column text mining for frequency,"['frequency', 'text-mining']","I have a data that contains different ethnicity. It looks like this:
ethnicity SurveyHow can I find the frequency of each ethnicity?
Similar like this:(numbers are not accurate). 
exampleThanks"
5227,How to remove lines with a pattern if the next line matches the same pattern?,"['python', 'regex', 'pandas', 'text', 'text-mining']","I have a dataframe with a column containing logs for a ticket per row. Here is an example of the log:I want to delete all lines, which are not followed by changes. The following regex matches the content I am looking for RegEx101:Expected result per cell in dataframe['log']:How do I delete the unwanted lines in the cells? Or how can I replace the string inside the cells with the filtered string?"
5228,How to obtain TF using only TfidfVectorizer?,"['python', 'scikit-learn', 'text-mining']","I have a code like this one:where I print the CENTRAL_TERMS words which get the highest tf-idf scores over all the documents of the corpus. However, I also want to get the MOST_REPEATED_TERMS words over all the documents of the corpus. These are the words which have the highest tf scores. I know I can obtain by simply using CountVectorizer, but I want to use only TfidfVectorizer (in order to not performing first the vectorizer.fit_transform(corpus) for the TfidfVectorizer and then the vectorizer.fit_transform(corpus) for the CountVectorizer. I also know that I could use first CountVectorizer (to obtain tf scores) followed by TfidfTransformer (to obtain tf-idf scores). However, I think that there must be a way to this only using TfidfVectorizer.Let me know if there is a way to do this (any information is welcome)."
5229,How to extract string between numbers? (And keep first number in the string?),"['python', 'regex', 'text', 'text-mining', 'changelog']","I am trying to extract data from a change log using RegEx. Here is an example how the change log is structured:I was using the RegExStrom Tester to test my RegEx.So far I have: ^\w{5,6}(.|\n)*?\d{5,6} however the result includes the ID from the next ticket, which I need to avoid.Result:Expected Result:"
5230,Getting error in converting a corpus to DocumentTermMatrix in R,"['r', 'text-mining']","After I preprocessed my df_corpus and tried to convert the cleaned corpus to a TermDocumentMatrix, I keep getting this error: Error in UseMethod(""TermDocumentMatrix"", x) : 
   no applicable method for 'TermDocumentMatrix' applied to an object of class ""character"""
5231,How to correct the mispelt words in R with a user defined words datarame,"['r', 'replace', 'tidyverse', 'text-mining', 'stringr']","I fond that similar question was asked in previous posts but I feel my requirement is peculiar.
I have a dataframe which consists of one where there are reported misspell terms by reps.Reported terms.I have another data frame where I have corrected manually .Now I need to correct the spellings in the reported terms as follows:Could you please someone let me know what is the best approach way to do this task.
Thanks in advance for feedback"
5232,Returning Specific String found in text [duplicate],"['r', 'regex', 'string', 'text-mining', 'data-manipulation']","I have the following column in a dfand I have a vector of fruitI know using str_detect  can return TRUE or FALSE per observation usingbut what I would like is a column that has the variables that matched up, like the followingis there a way to accomplish this? Is this outside the str_detect domain?"
5233,Process and progress for natural language analysis of company communication?,"['nlp', 'text-mining', 'bpmn']","Assume there is a large record of all different kinds of inter-employee and customer communications (e.g. mails, chat transcripts, OCRed letters) wich should be analyzed, for sentiment analysis. I am trying to explain my customers how NLP works using BPMN. The challenge is to find an appropriate level of detail. Decision makers tend to see text analytics as a magic wand which delivers reproducible, object answers immediately. I somehow want to emphasize the iterative nature of the whole process. BPMN seems a nice way of doing so, because one could visualize the process of our efforts: If I have a huge dataset, I could have tokens at different tasks, showing how many texts of the corpora are already at what stage, and this even can be automatically. Before reinventing the wheel: Has anybody already married NLP and BPMN in some way? Are there any major flaws in my diagram? Is there a better way (maybe avoiding BPMN) to describe the process and to measure progress? Source code for the diagram: https://pastebin.com/raw/zMzuLvJH (feel free to use or modify)"
5234,Calculate aggrgeate cosine and Jaccard distance between two sets of documents,"['r', 'text-mining', 'cosine-similarity', 'text2vec']","I collected a list of abstracts from online news websites and manually labelled them, by topic, using their original labels (e.g., politics, entertainment, sports, finance, etc.). Now I want to compare the similarity in word usage in abstracts between any two topics (say, abstracts labelled ""politics"" vs. those labelled under ""finance""); however, because the number of news abstracts fall under each topic differ and the word length between any two abstracts also differ, which makes calculating document-by-document cosine similarity difficult.So what I did was to reference the text2vec vignette by dividing my example data by topic, parsing and stemming them, vectorizing the tokens in each abstract (i.e., the row entry) and building the dtm to create the vector space for comparison.While the methods listed in text2vec vignette are straightforward, the outputs are generated in matrix format. I am wondering if there's any way to get a single similarity measure (say, something between 0 and 1 or (-1, 1)) between any two sets of documents labelled under two different topics?   I provide my current code at below, a small 9-row data of news abstract that fall under 3 distinct topics are also provided (note that the number of documents belonging to each topic and their word length are all different: news pertaining to the topic ""sports"" have two entries, topic ""politics"" has four entries, and topic ""finance"" has three entries). Don't expect to get meaningful similarity result from such small data, it only serves as an example.It will be really appreciated if someone could point out ways to modify from my existing code and get a single pair-wise similarity measure between any two topics."
5235,Using tm() to mine PDFs for two and three word phrases,"['r', 'pdf', 'text', 'text-mining', 'tm']","I'm trying to mine a set of PDFs for specific two and three word phrases.
I know this question has been asked under various circumstances and   This solution partly works. However, the list does not return strings containing more than one word.  I've tried the solutions offered in these threads here, here, for example (as well as many others). Unfortunately nothing works.Also, the qdap library won't load and I wasted an hour trying to solve that problem, so this solution won't work either, even though it seems reasonably easy. As you can see, the output returns ""contract.prices"" instead of ""contract prices"" so I'm looking for a simple solution to this. File 127 includes the phrase 'contract prices' so the table should record at least one instance of this.I'm also happy to share my actual data, but I'm not sure how to save a small portion of it (it's gigantic). 
So for now I'm using a substitute with the 'crude' data."
5236,Extract a regular expression match,"['regex', 'r']","I'm trying to extract a number from a string.And do something like [0-9]+ on the string ""aaa12xxx"" and get ""12"".I thought it would be something like:And then I figured... But I got some form of response doing:There's a small detail I'm missing."
5237,How to replace an internal capital letter in a string,"['r', 'regex', 'text-mining']","I have a range of strings as follows:I would like to replace the internal capital ""N"" in the second word for each element with ""-"", so that it would like:Any suggestions? I've got the locations using the following:but I'm not sure how to replace the ""N"" if it's internal. When I try to replace the capital letter ""N"" using gsub, it replaces all occurrences of N, rather than only the internal ""N""."
5238,Python Text searching does not match values,"['python', 'text-mining']","Hello I've list of word to search in pandas dataframe. Once I execute my code it match only few of them for example 4 of 7 but there more about let's say 1000.
Here what I did.When I search ""sport"" keyword in excel on column ""cleaned_nomComplet""  it return 175.
What's wrong with my code, why I can not match ""sport""?
Thanks"
5239,Transforming my Counter tuple results (from counter) into string,"['string', 'counter', 'text-mining', 'spacy']","I'm trying to store my Counter results in a different way than default (preferably with spacy library) . For this case i need the results to be Word,value + the next separated by a new line. Right now i have the results as tuples ('word' : value). I've tried storing the results from the counter into string by using str() and by using the join function. However, i end up with all word characters on a newline. Where nout is the list with words. So here i got the right results stored from most common to least common (which is what I want but not in the right format)So instead of getting ('Elephant', 12), ('Orca', 9), ('Seal', 2), I want:"
5240,How do I remove these characters from my vector of strings,"['r', 'regex', 'string', 'text-mining']","I need a solution to how I can clean my vector of strings which has characters and symbols,
for exampleThe intended string will produceAny solution is acceptable, though I have tried using the gsub function 
Regex solution would be very much acceptable also"
5241,How do I remove words with less than 3 characters from a cell?,"['regex', 'excel', 'vba', 'text-mining']","I'm trying to create a word cloud in Tableau but I need to prep the data in Excel first. I already removed all characters that weren't alphanumeric. But now I want to remove the words like not, it, or is. I honestly just want to remove all the words with less than 3 characters. I tried the following two pieces of code using VBA. But this created an error that said Active X Can't create the component. The second thing I tried was this:This just doesn't do anything. I select a cell with only plain text. I go to ""Run Macro"" and I select it and hit run. Nothing happens.What am I doing wrong?Thanks!"
5242,Add metadata to VectorSource corpus using 'tm' library in R,"['nlp', 'text-mining', 'tm', 'corpus']","I have a csv file and I'm trying to convert it into Corpus to use the tm_map later and the apply some clustering. I read the fileTurn what I need into corpusThis is the outcome for the metadataThen I try to add the author info, so I can add the date and title afterwards, like thisbut I keep on getting thiswhenHow can I add the right metadata info to my Corpus?"
5243,Orange Error During Installing Text Add On,"['text-mining', 'orange', 'anaconda']","I am trying to install the Text add-on (version 0.7.3) for Orange (version 3.23) on my Win10, but I am getting the following error during the building ""ufal_udpipe"": I also tried to install Microsoft Visual C++, because it´s required in the log, but the problem is the same after the installation. I tried to install add-on throught the Anaconda Prompt:This process failed, because:The result of conda clean command is following:Reinstall of Orange and Anaconda didn´t help.Full extract of logs is here: Google DiskThank you for your help! 
Jakub"
5244,Unable to retrieve the text data into data frame after cleanup in R,"['r', 'nlp', 'text-mining']","I'm aware that similar questions have been asked here but I still believe my task is more complex.
I read a CSV file with 3 text columns into a data frame. I used tm package to clean the text data. I used the below code:Now my worry is how to do clean the data in all 3 cloumns at a time.
Second is, how to convert this corpus data into dataframe so that I can see whether all the data that I have imported into R is cleaned up successfully."
5245,How to extract parts of logs based on identification numbers?,"['regex', 'python-3.x', 'text', 'text-mining', 'logfile']","I am trying to extract and preprocess log data for a use case. For instance, the log consists of problem numbers with information to each ID underneath. Each element starts with:I extracted the identification numbers and saved them as a .csv file:Now what I am trying to achieve is, that for every ID in the .csv file I can append the corresponding log content, which is:Since I am rather new to Python and only started working with regex a few days ago, I was hoping for some help.Each log for an ID starts with ""#!#!identification_number###"" and ends with ""#!#!attribute5### <entry>"". I have tried the following code, but the result is empty:"
5246,Python Normalise Module Not Found Even Though It Is Installed,"['python', 'module', 'normalization', 'text-mining', 'normalize']","I have installed normalise module for python version 0.1.8 as shown here:
InstallationAnd here is my code to call the module:However, it always returns an error that no normalise module is found: ErrorAny help appreciated. Thanks."
5247,Regular expressions extracting multiple product attributes from product description,"['regex', 'text-mining']",I have a set of product descriptions from which i want to extract product attributes through regular expressions.https://regex101.com/r/HTTfNR/1I want to solve this using regular expressions.I have tried this but i am unable to get this working for all use cases of my 1step.I want to Extract x5550/E5540/E7-2860/E7-2860/E7 4830 as my 1st step. Can someone help me with a code to extract this text from above text?
5248,R Randomforest undefined column issue,"['r', 'random-forest', 'text-mining']","I am working on a text mining process and using Random forest to classify text to categories.
I am using caret package after processing my text.
I split the data to train and test,
Below is the R code after the same:When I run the last line, I get the below error:Edit 1: next error after correction:
 Error in train[1:5, ] : object of type 'closure' is not subsettableI see the term_sparse is giving me an issue and may be the text mining part, how can i improve my outcome?Not sure what the issue is.
Please help out!"
5249,The 'dictionary' parameter of TermDocumentMatrix does not work in R,"['r', 'text-mining', 'term-document-matrix']","Even though I added the keyword to 'dictionary' as below code, it doesn't extract from the sentence."
5250,How to get offset of a matched an n-gram in text,"['python', 'text-mining', 'string-matching', 'n-gram']","I would like to match a string ( n-gram) in a text, with a way to get offsets with it :string_to_match = ""many workers are very underpaid"" 
 text = ""The new york times claimed in a report that many workers are very underpaid in some africans countries.""so as result I want to get a tuple like this (""matched"", 44, 75) where 44 is the start and 75 is the end occurrence.here is the code I have build, but it works only for unigram.any tips to make that work for sequence of strings or n-grams!!"
5251,Corpus object missing text,"['r', 'text-mining', 'tm', 'corpus']","Working with 'tm' library in R.When aplying this code:It works and gives this outcome:Then I turn it into a Corpus object so I can work on it for applying some cluster analysis further on.While checking the raw data, I found out that it saves the lines as NULL when turning it into a data frame with this:So I don't get how to particularly turn the text into Corpus."
5252,Scikit Learn K-means Clustering & TfidfVectorizer: How to pass top n terms with highest tf-idf score to k-means,"['python', 'scikit-learn', 'k-means', 'text-mining', 'tfidfvectorizer']",I am clustering the text data based on TFIDF vectorizer. The code works fine. It takes entire TFIDF vectorizer output as input to the K-Means clustering and generate a scatter plots. Instead I would like to send only top n-terms based on TF-IDF scores as input to the k-means clustering. Is there a way to achieve that ? 
5253,How could you remove the similar portion from two large strings?,"['r', 'text-mining', 'string-search']","I am working on classification of some documents and a number of the documents have large sections of similar (and usually irrelevant) text.  I would like to identify and remove those similar sections, as I believe I may be able to make a better model.An example would be proposals by an organization, each of which contains the same paragraph regarding the organization's mission statement and purpose.A couple points which make it difficult:I've considered regex and looked through some packages like stringr, strdist, and the base functions, but these utilities seem useful if you already know the pattern and the pattern is much shorter, or if the documents have a similar structure.  In my case the text could be structured differently and the pattern is not predefined, but rather whatever is similar between the documents.I considered making and comparing lists of 3000-grams for each document but this didn't seem feasible or easy to implement.Below is an example of a complete solution, but really I am not even sure how to approach this problem, so information in that direction would be useful as well.Example codeOutput"
5254,transformation drops documents error in R,"['r', 'tm']","Whenever i run this code, tm_map line give me warning message as 
Warning message:
In tm_map.SimpleCorpus(docs, toSpace, ""/"") : transformation drops documents"
5255,find multiple strings using str_extract_all,['r'],I have a list of strings as follows:I also have a vector as follows:I want to find all matches of my tofind string so that the output should be:I think I can use str_extract_all but it doesn't give my the expected outputHow do I get the expected output?
5256,how to extract title from a pdf documment with R,"['r', 'pdf', 'text-mining']","I need help to extract information from a pdf file in r
(for example https://arxiv.org/pdf/1701.07008.pdf) I'm using pdftools, but sometimes pdf_info() doesn't work and in that case I can't manage to do it automatically with pdf_text()NB notice that tabulizer didn't work on my PC.
Here is the treatment I'm doing (Sorry you need to save the pdf and do it with your own path):I would like to get title and abstract most of the time."
5257,Why is Quanteda not removing words?,"['r', 'nlp', 'text-mining', 'quanteda', 'tidytext']","I am having trouble removing profanities from my n-grams.  The getProfanityWords function below correctly creates a character vector.  The whole script works in every other way, but the profanities remain.I did wonder whether it was to do with the hyphens in the 2 and 3 grams, but it applies to the 1-grams too.I have tried adding in and also similar within the makeNGrams function, but it makes no difference.What am I doing wrong?Thanks,Chris."
5258,Algorithm for measuring the similarity of two text files with different sizes,"['nlp', 'text-mining']","Suppose there are two files A and B, A is a huge data file with a size over 1GB (in text, no internal uniform data structure for the data). B is a file which might contain a small potion of data from A, and has a size below 1KB. I need an algorithm to measure how much data does B have can also be found in A. The more the data B contains is taken from A, the higher score this algorithm should return.Thanks."
5259,converting scanned pdf to readable pdf,"['python-3.x', 'text', 'nlp', 'text-mining', 'pdf-conversion']","I am trying to convert scanned pdf to readable pdf and I am using below code for the same. Firstly, I am converting the scanned document into an image and writing it back to blank pdf. It is giving output for the pdf which is not having any tables but it is not creating any images for the pdf containing tables.I would like to get the pdf with tables also get converted to the image and subsequently to readable one. is there any other package or methods in pdf2image that will do the same?"
5260,Count Bigrams independently of order of appearance,"['r', 'text-mining']",I´m trying to count bigrams independently of order like 'John Doe' and 'Doe John' should be counted together as 2.Already tried some examples using text mining such as those provided on https://www.oreilly.com/library/view/text-mining-with/9781491981641/ch04.html but couldn´t find any counting that ignores order of appearance.It counts separated like this:It should look like this:Thanks if anyone can help me.
5261,Printing text between a specific word and a closing paranthesis,"['python', 'regex', 'python-3.x', 'text-mining']","I have a text document out of which I want to extract specific Names based on the context. For example, a part of a sentence in the document goes like- ""...TO INTERVIEW VICTIM #1 (!ARIEL B. JOHNSON) ..."". I want to print just the name between the parenthesis but also want it to be searched using ""VICTIM #1"" for context.I have tried the following code. doc['sentence'] is the dataframe column where all the sentences of the document are stored as rows.It should be printing 'ARIEL B. JOHNSON'"
5262,R: Multiple matches when one includes another,"['r', 'string', 'pattern-matching', 'text-mining']","I'm trying to extract ""Maya is ,. nice"" from the string written below ("""" are not part of the string):""something ransom Maya wants to go for dinner with Shawn Maya is ,. nice""However, I keep getting ""Maya wants to go for dinner with Shawn Maya is ,. nice"" which is not what I was looking for.Any insights? I'm using stringr in R"
5263,How to dump the extracted tweets line by line into JSON,"['python', 'text-mining', 'tweepy', 'twitterapi-python']","I am trying to extract tweets from Twitter API and dumping them into a JSON format. This works just fine but in the tweets.json file there is only one row consisting of all the tweets. 
How do I get every tweet in a separate line ?Also, in some of the tweets, I still dont get the entire tweet. why is this happening? "
5264,A telegram bot for get specific data,"['text-mining', 'telegram-bot']","I want to create a bot telegram to retrieve some information from another robot or channel
I just wanted a training resource for this"
5265,Searching wikipedia through R,"['r', 'text-mining', 'wikipedia']","I have a list of names in my dataframe and I want to find a way to query them in Wikipedia, although it's not as simple as just appending the name to ""https://en.wikipedia.org/wiki/"", I want to actually query Wikipedia so that there will be a suggestion even if its not spelt correctly. So for example if I were to put in Dick Dawkins, it'd come up with Richard Dawkins. I checked and that is actually the first hit on Wikipedia.Ideally I'd want to use RVest but I don't want to manually get every url. Is this possible?"
5266,Get 'N' numbers of previous and next words from searched keyword from large string,"['c#', 'text-mining', 'string-operations']","I'm looking for solution where i can fetch -nth +nth numbers of words from my searched keyword from stringex. output would be : One use case is for instance when mapping code to
  someCurrently, I'm working on text mining subject in which I have to extract files and search a particular keyword + its sentence from the extracted string.
previously I was fetching the first sentence from string whenever I get the desired keyword. but now the requirement is changed as per above
here is the code snippetdotnetFiddlehere is the output I gotOutput:-  One use case is for instance when mapping code to some
  data source or third party API that where the names are used as keysthis gives me the first sentence where I got the desired keyword, any suggestion what changes should I do to get the new required output. "
5267,how can i classify the chapters of a pdf file and analyze the content per chapter?,"['python', 'python-3.x', 'pdf', 'text-mining', 'event-log']",I want to classify and analyze chapters and subchapters from a book in PDF format. So count the number of words and examine which word occurs how often and in which chapter. pip install PyPDF2this code already works. now I want to do more analysis like 
5268,Extracting a person's age from unstructured text in Python,"['python', 'nlp', 'pattern-matching', 'text-mining']","I have a dataset of administrative filings that include short biographies. I am trying to extract people's ages by using python and some pattern matching. Some example of sentences are:These are some of the patterns I have identified in the dataset. I want to add that there are other patterns, but I have not run into them yet, and not sure how I could get to that. I wrote the following code that works pretty well, but is pretty inefficient so will take too much time to run on the whole dataset. I have a few questions:Some sentences extracted from the dataset:"
5269,Cosine Similarity practical use cases,"['machine-learning', 'text', 'statistics', 'text-mining']","Cosine Similarity article on Wikipedia:
https://en.wikipedia.org/wiki/Cosine_similarityI see its gives a useful measure of how similar two documents are likely to be in terms of their subject matter.
can someone provides other practical use cases of using Cosine Similarity ?"
5270,Text Prediction - Zero Probability Ngrams,"['r', 'nlp', 'text-mining', 'quanteda', 'tidytext']","I’ve searched and read all I can but can’t only find that zero probability ngrams are an issue when creating a predictive language model, but not why.  If ngrams are not in the training set, we can't predict them and I can’t see how smoothing would help. Any pointers welcome..."
5271,Set CountVectorizer result to pandas.DataFrame,"['python', 'pandas', 'dataframe', 'text-mining', 'countvectorizer']","I need to set pandas.DataFrame with matrix features produced by CountVectorizer.but in the last line SaveTxt['text']=xtrain_count I got following errors!I was wondering how should I set result matrix of CountVectorizer to dataframe?
CountVectorizer result is a csr_matrix with about 20000 rows and 200000 columns and contents are integer (1 to 6) "
5272,Regex.Match whole words,"['c#', '.net', 'regex']","In C#, I want to use a regular expression to match any of these words:I want to find the whole words in the content string.  I thought this regex would do that:  but it returns true for words like participants, even though I only want the whole word pants.  How do I match only those literal words?"
5273,How to locate specific sequences of words in a sentence efficiently,"['python', 'python-3.x', 'nlp', 'text-mining']","The problem is to find a time efficient function that receives as inputs a sentence of words and a list of sequences of varying amounts of words (also known as ngrams) and returns for every sequence a list of indexes indicating where they occur in the sentence, and do it as efficiently as possible for large amounts of sequences.What I ultimately want is to replace the occurrences of ngrams in the sentence for a concatenation of the words in the sequence by ""_"".For example if my sequences are [""hello"", ""world""] and [""my"", ""problem""], and the sentence is ""hello world this is my problem can you solve it please?"" the function should return ""hello_world this is my_problem can you solve it please?""What I did is group the sequences by the amount of words each have and save that in a dictionary where the key is the amount and the value is a list of the sequences of that length.The variable ngrams is this dictionary:This works as desired but I have too many sequences and too many lines to apply this too and this is way too slow for me (it would take a month to finish)."
5274,Remove rows with character(0) from a data.frame before proceeding to dtm,"['text-mining', 'lda', 'topic-modeling', 'tidytext']","I'm analyzing a data frame of product reviews that contain some empty entries or text written in foreign language. The data also contain some customer attributes which can be used as ""features"" in later analysis.To begin with, I will first convert the reviews column into DocumentTermMatrix and then convert it to lda format, I then plan to throw in the documents and vocab objects generated from the lda process along with selected columns from the original data frame into stm's prepDocuments() function such that I can leverage the more versatile estimation functions from that package, using customer attributes as features to predict topic salience.However, because some of the empty cells, punctuation, and foreign characters might be removed during the pre-processing and thereby creating some character(0) rows in the lda's documents object, making those reviews unable to match their corresponding rows in the original data frame. Eventually, this will prevent me from generating the desired stm object from prepDocuments().Methods to remove empty documents certainly exist (such as the methods recommended in this previous thread), but I am wondering if there're ways to also remove the rows correspond to the empty documents from the original data frame such that the number of lda documents and the row dimension of the data frame that will be used as meta in the stm functions are aligned? Will indexing help?Part of my data is listed at below."
5275,Remove rows from column on whitespaces,"['python', 'pandas', 'text-mining', 'data-cleaning']","I want to keep all the rows in column, which has single word and rest of the rows which contains more than one white space to be removed.My dataframe df is:I want to create a new dataframe with only correct drug words and removing the garbage. I have tried below solutions, but it gives me a blank/empty column of drug.Could you please help me to get the correct solution? Like:"
5276,text cleaning: removing erroneous characters,"['r', 'text-mining', 'stringr']","I have a column called obligations, in it there are financial values for each fiscal year such as ""(Project Grants) FY 17$XX.XX; FY 18 est $XX.XX; FY 19 est $XX.XX; FY 16$XX.XX;"" 
I am ultimately trying to select each value and place them into new columns for the correct FY, however, to begin I am trying to use some tools (i.e. stringr) to remove the noise around the information I want. Not every instance in the column begins with (Project Grants), there are a number of them so I was going to continue using ELIF options in my if statement for the different types. The code did not remove the (Project Grants) from the text which is my issue. I think it may be better to create a function for this process, but I am new to the language and am not sure where to start or how to go about creating the function hence my choice to remove the characters first then eventually use extract() to create the columns I need.  The output would be where I have the original column Obligations..122. followed by FY16/FY17/... and so on with the value you see in the output above."
5277,separating an Arabic sentence into words results in a different number of words with different functions,"['r', 'nlp', 'text-mining', 'arabic', 'arabic-support']","I am trying to separate one Arabic sentence, Verse 38:1 of Quran, with the tm and tokenizers packages but they split the sentence differently into 3 and 4 words, respectively. Can someone explain (1) why this is and (2) what is the meaning of this difference from NLP  and Arabic-language points of view? Also, is one of them wrong? I am by no means expert in NLP nor Arabic but trying to run the codes.Here are the codes I tried:I would expect them to be equal but they are different. Can someone explain what is going on here?"
5278,Extracting all different options of references from pdf document in R with regex (multiple options/capture groups?),"['r', 'regex', 'text-mining']","I am trying to clean some pdf documents for text analysis. I am trying to grab all the references on the text and remove them. My problem is, that there are so many options to cite...
My documents are split up into single lines. 
I have a working regex, that only captures the standard format a) Author (year), something .
       ""Author, firstname, someone, else (1996), something: Analysis, Paris.\r""I want option a, b) Author (year(character)), something .c) Author (forthcoming), something . d) Author/s (eds.) (year), ....e) Author (n.d.), ....I have found all of those in my documents... There might be options I have not found yet, so if you have examples or something that grabs that as well, I'm grateful for every it of help.The working code is the following: My latest try is this: I tried to make as many pieces optional as I could, but now it grabs too much. I expect a list of all the References the regex finds, if possible with all the options. At the moment it grabs not enough (first case) or too much (second case)."
5279,Extracting links from pdfs in R with a regex,"['r', 'regex', 'text-mining']","I am trying to clean a list of pdfs of links. I want to include this in my cleaning function and therefore use regexes. And yes, I spend more time than I like to admit googling and browsing though questions here.
My pdfs are split into lines, so it is not one consecutive string.
I have a piece of code that gives me only one link as result (even though there should be many). 
All other options I tried included a lot of text I want to keep in my dataset. I have tried multiple options outside my function but they will not run on texts, only on examples. I want to catch everything from the www to the first white space after all the things that come after the .org or .html or whatever (e.g. /questions/ask/somethingelseI tried simulating some thingsMy current working state also only catches the first occurence in that particular line, instead stopping at the space (line m in my example) and then including the next link as well."
5280,Removing words containing a certain substring,"['r', 'text-mining', 'tm', 'corpus']","So I'm making a function to receive in a word corpus, and then spit out a cleaned product:This works great for the most part, however when I look at the resulting word cloud I generated I notice one thing that stands out:
the word cloud includes random words that have the term ""html"" in them.I figure I can fix this by simply adding a line in the function that removes any word that contains the substring ""http"", but I can't for the life of get around to doing that, and all the existing answers I've found seem to have to do with replacing a substring, or removing only that substring.What I want to do is:
if a substring is a part of the word, then remove that entire word.Word Cloud code I use to generate the word cloud from the corpus:"
5281,Manually inserting topic-specific stopwords,"['dplyr', 'text-mining', 'stop-words', 'tidytext']","I'm using tidytext's built-in anti_join(get_stopwords()) command to clean documents from a data of customer review of tech products, but I found out the output corpus consists primarily of tech specification (e.g., Windows 10, 720p Camera, 380.6 x 258.2 x 22.45 (inches), IntelCore, etc.) and comes with little adjectives and nouns indicative a customer's satisfaction of a product).Is there any handy ways to compile a list of tech terms to remove (such as those listed earlier) and manually insert it into get_stopwords() or equivalent functions to better identify those non-tech adjectives and nouns in customer reviews?"
5282,Is it bad to not remove stopwords when I've already set a ceiling on document frequency?,"['scikit-learn', 'nlp', 'text-mining', 'text-processing']","I'm using sklearn.feature_extraction.text.TfidfVectorizer. I'm processing text. It seems standard to remove stop words. However, it seems to me that if I already have a ceiling on document frequency, meaning I will not include tokens that are in  a large percent of the document (eg max_df=0.8), dropping stop words doesn't seem necessary. Theoretically, stop words are words that appear often and should be excluded should be excluded. This way, we don't have to debate on what to include in our list of stop words, right? It's my understanding that there is disagreement over what words are used often enough that they should be considered stop words, right? For example, scikit-learn includes ""whereby"" in its built-in list of English stop words. "
5283,R removewords tm treats stop word file as regex not verbatim,"['r', 'regex', 'text-mining']","How can I force removeWords from library(tm) to take each word in a stop word list verbatim (literally), not as a regex?Suppose I have a file stopwordlist.txt containing characters that can be misinterpreted as regular expressions:This is my codeI would expect removeWords to take each line as a verbatim stop word, for example to remove each occurrence of ""e.g."" and not the word ""ergo"" when taken as a regexp. Having some special characters confuses the interpreter saying it is not a valid regexp."
5284,How to loop over ExampleSets in Rapidminer?,"['text-mining', 'rapidminer']","I am trying to extract the data from a pdf without the data in the tables.Can someone suggest how to loop over ALL the ExampleSets in the ioo object collection?Note: Since all the ExampleSets are of different types, I couldn't append or join them."
5285,How to calculate readibility scores easily or how can i write a function for that?,"['python', 'machine-learning', 'nlp', 'text-mining']","I have to calculate readability score of a text document. Is there a package or inbuilt function. Everything on internet seems too complex. Can any one help me with that or how to write my own function? I have done pre processing of text, calculated the tfidf of document but I want to find the readability score or fog index of the document. I tried using code available on other platform but it didn't workI don't know how to get the desired results of readability scores. I would appreciate if someone would help me"
5286,"Unable to use NRC lexicon in tidytext. Error in match.arg(lexicon) : 'arg' should be one of “afinn”, “bing”, “loughran”","['r', 'text-mining', 'tidytext']","I am learning sentiment analysis in R using tidytext package. However, i am unable to set nrc as lexicon. Whenever i type get_sentiments (""nrc""), the above error is displayed. It says that lexicon coud only be ""afinn"", ""bing"" or ""loughran"". I tried updating the package (tidytext) and also installed 'syuzhet' package but still the problem exists. Please help!"
5287,Gensim Word2Vec Vocabulary: Unclear output,"['python', 'python-3.x', 'text-mining', 'gensim', 'word2vec']","I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:So then I tried to use the Word2Vec:but the output was like this:I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.Does someone know what's the problem? Thanks for your help!Appendix:Importing the bookcontent_text=[]
number_of_inputs=len(os.listdir(path))"
5288,Compare the words from a data frame and calculate a matrix with the length of the biggest word for each pair,"['r', 'dataframe', 'matrix', 'distance', 'text-mining']","I have a data frame with a number of unique words. I want to create code in R, where each word will be compared with all the words and  creates a matrix with the length of the biggest word from each pair. To be more comprehensive lets consider the follow example. I want to create a matrix that compares each word in the test and gives me the length of the biggest word. For the previous example I want to take the below matrix:How Can I do it in R?"
5289,Problem with adist function in text comparison,"['r', 'text', 'comparison', 'text-mining', 'levenshtein-distance']","I have a problem with adist function. Basically I am using the example of the RDocumentation. However, when I am trying to run added one more word I am taking these results:In the third column, third row, I am taking MMI, but I can not understand why as it is the same word ""hi"". So it has to be MM. (match, match and no insertion)Reference: https://www.rdocumentation.org/packages/utils/versions/3.6.0/topics/adistI am using another example: I am taking these results. But at least the diagonal needs to be M as is the same word. I can not understand what it is going wrong."
5290,What options are there to process text which was extracted from pdf to remove text wrapping / justified effect,"['python-3.x', 'pdf', 'text-mining']",I have been able to extract text from multiple pdf files but the original files had double line spacing within 1 column of text which was wrapped or justified so my extracted text also has alot of CR LF within. My issue is when the text wraps sentences also contain CR LF toy exampleI don't want to loose all the spacing structure like paragraphs so is there a way to unwrap (un-justify) text using python without removing all spacing in the document or intelligently join the lines back? Eventually I want to spell check the text with Spacy after additional text processing to handle non english text but the justified/wrapped text may be causing misspellings and difficultly detecting all the non english text.
5291,create any algoritmy from text fragments (chunks),"['python', 'text-mining']","I have a final job of the text mining module, and I am in difficulties to solve the problem proposed, so I requested a support to follow up the work in python:The objective is to program a function that receives as input a user text and returns the text fragments (chunks) that refer to the meals and quantities that you have requested. It is not necessary, nor is it the objective of this exercise, to build a classifier of intention prior to this function, but simply a function that we presuppose receives a phrase with the intention 'Order_food'. Nor is it objective to normalize the output (eg, it is not necessary to convert 'three' to '3' or 'pizzas' to 'pizza'). It is, therefore, an exercise of minimums.Por ejemplo: “quiero 3 bocadillos de anchoas y 2 pizzas” →Therefore, the output of the function will be an array with dictionaries of 2 elements (food and quantity). When an amount is not detected, its value will be set to '1' as the default value.best regards."
5292,Sample output from Large Corpus object in R for Large Text file,"['r', 'text-mining', 'tm', 'corpus']","I have a large Corpus Object as a result of 3 Large files (>1gb total).After cleaning the text I want to look at a random sample of the data say 1000 lines on my console to see if it is ok! I am unable to find any source on how to sample data from a Corpus class in reasonable time (1 minute). Some codes I ran were:There are a lot of solutions here to visualize the entire data, but again takes too long.The worst part is the only way to stop the processes due to code above is shutting down the console! I also looked at corpus_sample. The closest to what I want came from str(), which gave the first line of the first document and that's it in record time.This answer seemed promising but, turns out the corpus object doesn't have documents$texts in it (corp$documents$texts)P.SVery similar question asked here."
5293,Stopword not removing one word,"['python', 'filtering', 'text-mining']","i want to remove 'dan' in filtering process, but didnt work.
here is my codewords in stop_words_new is removed except 'dan'.
why?     "
5294,Visualizing network of sentences in Textrank,"['r', 'graph', 'nlp', 'text-mining', 'summarization']",I'm using the Textrank method explained here to get the summary of the text. Is there a way to plot the output of the textrank_sentences like a network of all the textrank_ids connected to each other?
5295,Mallet stops working for large data sets?,"['python', 'nlp', 'text-mining', 'lda', 'mallet']","I am trying to use LDA Mallet to assign my tweets to topics, and it works perfectly well when I feed it with up to 500,000 tweets, but it seems to stop working when I use my whole data set, which is about 2,500,000 tweets. Do you have any solutions for that?I am monitoring my CPU and RAM usage when I run my codes as one way to make sure the code is actually running (I use Jupyter notebook). I use the code below to assign my tweets to topics. The code seems to work when my data set contains fewer than 500,000 tweets: it spits out the results, and I can see python and/or java use my RAM and CPU. However, when I feed the code my entire data set, Java and Python temporarily show some CPU and RAM usage in the first few seconds, but after that the CPU usage drops to below 1 percent and the RAM usage starts to shrink gradually. I tried running the code several times, but after waiting on the code for 6-7 hours, I saw no increase in the CPU usage and the RAM usage dropped after a while. Also, the code did not produce any results. I finally had to stop the code. 
Has this happen to you? Do you have any solutions for it?
Thank you!"
5296,How to use trycatch to skip errors and move on to next in the list,"['r', 'text-mining']","I want to parse rtf files from a folder in the rtf files that resulted in errors during the lapply step.I am new to using trycatch, so how can I incorporate it in my code(the lapply step) to ignore the errors and continue with the parsing of the next rtf file?"
5297,Count the occurrences of words in a string row wise based on existing words in other columns,"['r', 'nlp', 'text-mining']",I have a data frame that has rows of strings. I want to count the occurrence of words in the rows based on what words appear in the column. How can I achieve this with the code below? Can the below code be modified somehow to achieve this or can anyone suggest another piece of code that doesn't require loops? Thanks so much in advance!The output should look like below:Current output with existing code looks like this: 
5298,Using cosine similarity for classifying documents,"['nlp', 'classification', 'data-science', 'text-mining', 'cosine-similarity']",I have a set of files for five different categories and most of them are not labelled correctly.Objective is to predict the correct category of the file whenever the same is uploaded.I used cosine similarity along with tf -idf to predict the class of the document with which cosine similarity is the maximum as of now i am getting good results but really not sure how well this will work down the road. Also why isnt cosine similarity used in building document classifiers instead of machine learning models when the categories of files are labelled correctly?Would really appreciate your feedback on my approach as well as your answer to the question.
5299,Upload .txt as one cell for further editing,"['r', 'text-mining']","My question: how to upload a .txt to appear as one cellI need to prepare my data first and want to do that in R. So I want to upload the .txt file I have to R so that it is in the shape of a single cell first. Through that I can do some clean-up first, before splitting it to rows and cells.Let's say I have:Lorem ipsum dolor sit amet, consetetur sadipscing elitr,sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.At vero eos et accusam et justo duo dolores et ea rebum. 
Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.I would like to load that to be all in one single cell to do some conversion first. With readr or any other import tool it will always be split into rows according to the line breaks."
5300,Passing multiple arguments as a list in R,"['r', 'text-mining', 'quanteda']",I wish to pass a list of arguments as a vector to another command in R. I do not want to repeat the same set of arguments every time.This is the code that I have to run 6 times for each $full_text column of data frames ranging t1 to t6.Is there a way to pass a one-time argument.
5301,Download older version of a package that has been removed from CRAN,"['r', 'text-mining', 'archive', 'cran']","I resume working on text mining after a substantial period of hiatus, but soon found out that the package RTextTools has been removed from CRAN and was no longer in maintenance. I tried to download it manually from CRAN archive usingbut my R showsthere is no package called 'RTextTools'.I then tried out the trick recommended by this earlier thread by usingBut this didn't work either.
The RTextTools package has many handy features I appreciate and I really do want to maintain an older version of it in my folder, is there any other ways I can install the archived version of this package on my R?"
5302,How to summarize email text using LDA in R,"['r', 'function', 'nlp', 'text-mining', 'lda']","I am working on complaints data analysis where I am adapting text summary technique for reducing unnecessary text and bringing out only useful text. I have used LDA - Latent Dirichlet Allocation in R for text summarization but I am not able to perform it to its full potential. Error -  Actual text: 
it is the council’s responsibility to deal with the loose manhole cover.
Could you provide an update on the next steps taken by the council.
** Trail Mails Text follows - about 50 lines of text**summarized text: 
it is the council’s responsibility to deal with the loose manhole cover.I have read the email thread, please get in contact with the numbers provided by ABC"""
5303,How to associate a date extracted from a pdf file with the data extracted from it using R?,"['r', 'pdf', 'dplyr', 'extract', 'text-mining']","I have two .pdf files that have a table inside with buy and sell stock information and a date on top right corner header of each page. See the files here. If necessary save the two .pdf files and the script below into the same folder in your computer and run the script to reproduce the problem.I want to extract just the table content from each file, join and transform it into a tibble and insert one first column (into a tibble) with dates extracted from header files.  So, if the first 5 lines in the tibble come from the first pdf file than the first 5 lines in the first column had to be filled in with the same date extract from the header of first file. If the next 2 lines after previous 5 ones come from the second file than these 2 lines in the first column had to be filled in with the same date extract from the header of the second file.  I've already extracted the table from each files, join and create a tibble as you can see below. Even create a code to extract the dates. But really, I don't know how to associate the date extracted from header to a table content of each file and insert it into the tibble. Code - Extract Table InformationCode - Extract DatesAny help?"
5304,Topic label of each document in LDA model using textmineR,"['r', 'nlp', 'text-mining', 'lda', 'topic-modeling']",I'm using textmineR to fit a LDA model to documents similar to https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html. Is it possible to get the topic label for each document in the data set?then adding the labels to the model: now I want the topic labels for each of 100 document in nih_sample$ABSTRACT_TEXT 
5305,Subsetting a character vector column into multiple columns,"['r', 'dplyr', 'text-mining', 'tidyr', 'data-manipulation']","I have the following tibble:I would like to split the colours into multiple columns according to their colour family: Cool, Warm, Neutral, with one column for each family.I can do this using mutate with map and str_subset:But I was wondering if there was a more succinct way of achieving the same result? I've tried tidyr::extract() but can't seem to get the regex right:I'm guessing it's incorrect because the OR statement matches the individual words in each group rather than breaking the string up into three substrings that contain all matched words for each group? Here is the demo. "
5306,how to remove empty value after we do preprocessing text in python,"['python', 'text-mining', 'preprocessor']","e.g
I have a tweet ""@cintya @groot @smanela https://blog...""
and I do a preprocessing process that link and mention has been deleted, and I think it should be lost. 
But in CSV, they return an empty value. How can I fix them?
Here is my codeI expect the result in CSV is deleted, but the actual output is empty value 1 row in CSV
481 is empty value, how can i remove it?"
5307,Finding tf-idf values in a announcement table,"['python', 'python-3.x', 'spyder', 'text-mining']","I want to do an analysis of an announcement.I have to calculate 'tf' and 'idf' values. But I think the values ​​are not realistic. Is there a problem with the code?""stemming"" line is announcements. 
The first announcement is 'kurs kayıt tarih progra giriş çıkış saat' For the first word (kurs), tf value must be 1/7 according to 
TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
But results is that"
5308,Link 2 annotations together in a window of up to 10 words using Ruta,"['nlp', 'text-mining', 'uima', 'ruta']","is there a way to link 2 annotations that are within a windows of 10 words, together in a new one?The following doesn't work:Entity W{1,10} Entity{->CREATE(Entity)};Thanks and all the best
Philipp"
5309,String Concatenation in Ruta,"['nlp', 'text-mining', 'uima', 'ruta']","does somebody know what is wrong with my String Concatenation in Ruta?Caused by: org.apache.uima.ruta.extensions.RutaParseRuntimeException: Error in Anonymous, line 28, ""+"": expected RPAREN, but found PLUSThanks for your help.
Philipp"
5310,Text mining a large list of Notes for Vehicle Identification Number (VIN#) with Python,"['python', 'python-3.x', 'pandas', 'text-mining', 'vin']","I have a large data set of Insurance Claims data with 2 columns. One column is a claim identifier. The other is a large string of notes that go with the claim.My goal is to text mine the Claims Notes for a specific VIN number. Typically a VIN# is in a 17 digit format. See Below: https://www.autocheck.com/vehiclehistory/autocheck/en/vinbasicsHowever, with my data, some issues arise. Sometimes only the last 6 digits were input for a VIN#. I basically need a way to process my data and grab anything that looks like a 17 digit VIN Number and return it to that row of data. I am using Python 3 and am a rookie text miner but have some basic experience using regular expressions.I am trying to create a function in python in which I can lambda apply it to the column of notes.Attempt so far:I am trying to mimic the format of the VIN in the link I provided.So something that looks for a string with following qualities:EDIT: Changed code snippet. This code example works if I make some toy examples of VINs with made up text but I am not having any success iterating through my pandas column. Each row entry has a large paragraph of text I want the function to go through each row at a time.Thank you."
5311,How to filter out lines within each row in R?,"['r', 'filter', 'text-mining', 'preprocessor']","I have been trying since many days on this problem but couldn't get the expected results.I have a dataframe containing conversations of two person A & B within each row (it is like 1 row contains entire conversation, likewise i have thousand of rows). i want to filter out lines in each row based on certain keywords.How can i do that?I have tried below lines but couldn't get exact results.can somebody please help me, i'm stuck on this since a week :(Thanks,
Naseer"
5312,Interactive learning [closed],"['machine-learning', 'nlp', 'text-mining']","
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                Closed last year.I'm new in NLP and text mining and I'm trying to build a documents classifier.
Once the model is trained, we test it on new documents (they, test-data, don't have labels). It is expected that the model is not 100% accurate; so for misclassified documents, we want interact with a user to correct these bad predictions.I've two ideas:Retrain the model where: traindata = old_traindata + data corrected by the user.After each user's rectification, update model parameters.Does this sound correct? in the second case, which kind of algorithms should I use? How efficiently can we solve this problem?"
5313,Read multiple pdf files at once and extract sentences that contain a keyword using R,"['r', 'pdf', 'text-mining']","Let's assume that I have few pdf files stored in a directory and I want to read all those pdf files at one and extract all the sentences that contain a specific keyword (in this case 'provisions') instead of manually opening each file and looking for that keyword.I have tried reading the files but how can I make R go through each pdf file to search for that keyword and output those sentences?
Here's a small piece that I have written:For file reference purpose, the links for pdf files are:I have created a directory and saved the pdf files in it. "
5314,Extracting university names from affiliation in Pubmed data with R,"['r', 'regex', 'text', 'text-mining', 'pubmed']","I've been using the extremely useful rentrez package in R to get information about author, article ID and author affiliation from the Pubmed database. This works fine but now I would like to extract information from the affiliation field. Unfortunately the affiliation field is widely unstructured, not standardized string with various types of information such as the name of university, name of department, address and more delimited by commas. Therefore text mining approach is necessary to get any useful information from this field. I tried the package easyPubmed in combination with rentrez, and even though easyPubmed package can extract some information from the affiliation field (e.g. email address, which is very useful), to my knowledge it cannot extract university name. I also tried the package pubmed.mineR, but unfortunately this also does not provide university name extraction. I startet to experiment with grep and regex functions but as I am no R expert I could not make this work.I was able to find very similar threads solving the issue with python:Regex for extracting names of colleges, universities, and institutes?How to extract university/school/college name from string in python using regular expression?But unfortunately I do not know how to convert the python regex function to an R regex function as I am not familiar with python.Here is some example data:What I would like to get:Please sorry if there is already a solution online, but I honestly googled a lot and did not find any clear solution for R. I would be very thankful for any hints and solutions to this task."
5315,How can I generate a word cloud from tokenized words in Python?,"['python', 'text-mining', 'word-cloud']","I have a code to import a txt file and get tokenized words using NLTK library (just like it is done in https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk). I did almost everything I needed easily, however I'm struggling to build a word cloud with the words I have now and I don't have any clue even after hours of search on the web.This is my code so far:I wanted to use the following code (from https://github.com/amueller/word_cloud/blob/master/examples/simple.py):But I don't know how to use my tokenized words with this."
5316,How do I append reviews text and reviews rating to a list,"['python', 'nlp', 'text-mining']","I am writing a program which analyses online reviews and based on the ratings, stores the review into review_text and the corresponding rating into review_label as either positive(4 & 5 stars) or negative(1, 2 & 3 stars).Tried the following codes to add the review text and review label information of each review without any success.I want the reviews to be stored in the list rev with the review text stored in column review_text and the review label (whether positive or negative) under review_label. It would look something like "
5317,Mining a Dataframe for a Count of Unique Words,"['python', 'python-3.x', 'text-mining', 'text-parsing']","I'm looking to take a set of strings in a dataframe and then break those strings up in order to get a count of distinct words in the strings. The ultimate idea is this:Word 1: 5 timesWord 2: 3 timesWord 3: 10 times...Word n: 13 timesThe ultimate goal is to take this set of strings, then extend the analysis into groupings of 2 words, 3 words, and so on.I've tried multiple packages, including Pandas, re, collections and so on, but nothing seems to get me where I need.That code should, ultimately lead to this kind of result:Word 1: 10Word 2: 20Word 3: 14...2-Word-Combo: 102-Word-Combo: 15...3-Word-Combo: 303-Word-Combo: 40...etc.I appreciate your help!"
5318,Read text files with paragraphs as one string using VCorpus from tm package in r,"['r', 'nlp', 'text-mining', 'tm']","I have a list of text files in my directory, all of which are documents with multiple paragraphs. I want to read those documents and do sentiment analysis.For example, I have one text document data/hello.txt with text like below:I read the document in like below (there can also be multiple documents):When I look at the document content  docs[[1]]$content It seems like it is character vector.My question is how I can read in those documents so that in each document, paragraphs are concatenated into one single character string so that I can use it for sentiment analysis. (VCorpus from tm package)Thanks a lot."
5319,importance feature xgboost for text predictions,"['python', 'nlp', 'text-mining', 'xgboost']","i have two text file for rating review positive and negative after the preprocessing the  data with nlp i make prediction with XGboost 
and i am tried to get importance feature as categorical variable but i got the number. how can i interpreted the number to categorical variable??"
5320,How to use NLTK countvectorizer in a dictionary in python?,"['python-3.x', 'pandas', 'nltk', 'text-mining', 'countvectorizer']","I have used csv reader to read my tsv file, which contains three columns lie, sentiment and review.  I have created dicitonary to read my tsv file data as shown in code below. Next. I would like to use NLTK count vectorizer to count word frequency in my ""review"" column only. I am not sure how to approach using CountVectorizer module in NLTK along with dictionary.I am expecting the word frequency of each word in the review column in pandas dataframe.In below code: Infile= Filename.tsvAny help is appreciated!Note: I am new to python, please provide explanation with code.Few data after running ""print(state)"" codeCode so far"
5321,How to load a folder (with text files) from your computer on Jupyter to be able to run analyses on them together?,"['python-3.x', 'windows', 'nlp', 'jupyter-notebook', 'text-mining']","I am trying to load a folder (containing about 1000 .txt files) on my Jupyter notebook (Python 3) from the desktop of my WINDOWS computer; so that I can proceed with my analyses relating to NLP. I am using SPaCY instead of NLTK as advised by one of the Udemy course instructors.I am a novice in the field and had been trying to read textbooks and udemy online courses but those did not help much.gen = os.walk('../text folder sample')next(gen)I am seeking your help with lines of codes which will enable my python script to load and proceed with analyzing the files. **Each .txt file is an autobiography, so I am trying to treat each of them as an independent case so that at later stages I can infer which autobiographies are similar (e.g. cluster analyses)."
5322,How to remove white spaces within a word using python?,"['python', 'python-3.x', 'text-mining', 'spacy', 'removing-whitespace']","This is the input given John plays chess and l u d o.  I want the output to be in this format (given below)John plays chess and ludo.I have tried Regular expression for removing spaces
but doesn't work for me.I expected the output John plays chess and ludo. .
But the output I got is Johnplayschessandludo"
5323,"Is there an R technique to group_by, search, and match a long data structure?","['r', 'boolean', 'match', 'tidyverse', 'text-mining']","This is a problem of finding which ids have matching words, from a list of 5 words for each id.We have a long data structure from a text mining project with an id and the word. Each group_id has 5 words. We would like to measure which words from one id are in another id. i.e. which id are similar based on there words. We have tried to use a for loop on the [row, column] but it seems there is a better way.My end goal is to have a matrix with id by id and the intersections are the number of matching words (0-5).This is the desired output:Any suggestion on completely reorganizing the data structure or solutions with this structure are welcome. Thanks!"
5324,How to find most frequnet words in a corpus in Pandas dataframe (Python),"['python-3.x', 'pandas', 'nltk', 'text-mining', 'countvectorizer']","I have Pandas dataframe that looks like following.I have tokenized my text files and used NLTK Countvectorizer to convert into pandas dataframe. In addition, I have already removed stopwords and punctuation from my coupus. I am trying to find most frequent words in my corpus in pandas dataframe. In below dataframe,words such as ""aaron"" and ""abandon"" aprreared >10 times, thus those words should be in new dataframe.Note: I am new to python, and I am  not sure how to implement this. Provide explanation with code.Subset of the dataframeI already already clean my corpus and my dataframe looks like following"
5325,Interpret the Doc2Vec Vectors Clusters Representation,"['text-mining', 'word2vec', 'doc2vec']","I am new to Doc2Vec, please bear with the naive questions.I have generated Doc2vector score i.e. using the 'Paragraph Vector' algorithm.
I have an array output for each document.I use the model.similar for doc1 and get the output - doc5 and doc10 are similar to doc1.Q1) How to summarize using the code what are the important words or high-level summary this document holds?In addition, If I use the array output and run K- means to get 5 clusters. How to define the cluster definition.Q2) I can read the documents but the number of documents is very high and doing a manual read to find the cluster definition is not possible."
5326,How to calculate precision / recall score for keywords in sklearn / Python?,"['python', 'scikit-learn', 'text-mining', 'precision-recall']","I have the following code that calculates precision/recall and F1 score for my model, which detects keywords in a document:As in this case, the words are aligned against each other, I can calculate the F1 score for this.However, what if my model doesn't know which order to place the words in? For example, if I place them like this:I will get a zero score for all the keywords even though the model correctly determined 3 out of 5. Do you know if there's an adjustment I can do to this metric in sklearn so that it doesn't care about the order of items matched (and their number for that matter as well)? Or maybe there's another metrics I could use? "
5327,The words that come always together in R,"['r', 'text-mining', 'word-frequency']","I am using R  and there is a text column in my data set, and I need to know if there is any way to know what is the words are always come together.
like most two words come together or three words ...etcFor example:So the results should be something like this "
5328,R doesn't recognize the Arabic language,"['r', 'utf-8', 'text-mining']","I am working with data in the Arabic language in R, so I set the local Arabic as shown here
Sys.setlocale(""LC_CTYPE"",""arabic"").then I opened the data and assigned it to a variable and I can read it so clear, but when I dealt with it the result on the console bar becomes symbols can't understand it like thisظ…ط¨ط§ظ„ط؛ ظپظٹظ‡ط§.it is my first time faced this problem any help?"
5329,StringToWordVector Weka Output,"['nlp', 'weka', 'text-mining']","I have an arff file containing a set of textual sentences. I would like to 
obtain the absolute frequency of each word within each sentence. I used StringToWordVector. This is the starting file @relation dataset 
 @attribute Text string 
 @date 
 'I'm a movie lover and this is one of the best museums in which ...After running StringToWordVector I get instances of this type: 
@relation dataset1
@attribute word numeric
...
{13 2, 19 2, 30 2, 33 1, 53 1, 55 4, 60 1, 61 2, 72 3, 78 1, 89 1, 90 1, 99 
1, 106 1,120 1,121 1,123 2,124 5,126 2,136 1,140 1,147 5,148 2,160 1,186 
1,198 1,202 1,248 9,253 1, ...}Since I would like to keep track of the word, instead of using a numeric id, how can I associate the textual word to the frequency obtained after the execution of the stringtowordvector command?"
5330,How to return all possible categories separated by | under one column,"['r', 'text-mining']","I have a dataset ""movie"" that has a column named ""genre"", its values are like ""Action"", ""Action|Animation"", ""Animation|Fantasy"". A movie can have more than one genre. I would like to output a list of all possible single categories (such as Adventure, Fantasy) and their frequencies. In other words, I want to know how many movies have genre ""action"", how many have ""fantasy"". I don't care about the combinations. Are there any advice on this?"
5331,How to perfom stemming and drop columns in pandas dataframe in python?,"['python', 'pandas', 'text-mining', 'stemming', 'porter-stemmer']","Below is the subset of my dataset. I am trying to clean my dataset using Porter stemmer that is available in nltk package. I would like to drop columns that are similar in their stems for example ""abandon','abondoned','abondening' should be just abondoned in my dataset. Below is the code I am trying, where I can see words/columns being stemmed. But I am not sure about how to drop those columns? I have already tokeninze and removed punctuation from the corpus.Note: I am new to Python and Textmining.Dataset Subsetcode so far.."
5332,Remove characters which repeat more than twice in a string [duplicate],"['r', 'regex', 'text-mining']","I have this text:and I want to remove the repeat characters, I tried this code https://stackoverflow.com/a/11165145/10718214and it works, but I need to remove repeat characters if it repeats more than 2, and if it repeated 2 times keep it.so the output that I expect isany help?"
5333,What's the best way to remove foreign words in review for topic extraction?,"['python', 'text-mining']","This question actually has two parts, whether removing foreign words is necessary and what's the best way to realise it.I'm a beginner trying to extract topics from English food review basically using latent dirchlet allocation in Python. The output is 5 topics each with 50 words each, and I have used NLTK to remove English stopwords.
But one (and only one) topic contains many foreign words that might not bear meanings, like ""de"" ""la"" ""et"" ""les"".Some original reviews that contain these words:-A la carte sushi is great. Pot of soup is huge and delicious.
-I would be interested in returning to try their Anticuchos, Ceviche de Mixto, Cau Cau, Aji de Gallina, and Chaufa de Camaron.
-I recommend patients in the parking lot. I would be lying if I didn't admit its some of the finest que in the country!The next step is get user vector, item vector and train, test, validate the results.Are these words meaningful, or shall they be removed?And how to remove the words?One answer in the question below suggests using NLKT set of English words, but I found the words set quite small, and words like ""de"" ""un"" cannot be removed.Another method suggest python package enchanted but it's not maintained anymore.Removing non-English words from text using PythonThe topic results I got are:pizza  burger  cheese  de  good    place   crust   sauce   burgers     order   et  service     toppings    pizzas  like    la  fresh   le  thin    restaurant  un  slice   best    great   delivery    time    pour    poutine     delicious   garlic  menu    try     pepperoni   est     taste   back    les     sandwich    meat    food    better  style   fast    plus    minutes     que     little  pie     onion   pas"
5334,Camelot treats same the same cell different rows,"['pandas', 'pdf', 'text-mining', 'python-camelot']","Camelot treats some rows as separate when actually they are not. The result is rows that should have belonged to the previous row.    I'm working with Camelot to extract data from bank statements.  The problem is that Camelot treats some rows as separate when actually they are not. ? As you can see in the image attached, the transaction on 1/9/2019 is split into 3 rows when actually it's only one. This happens when the description is more than one row (original statement attached).I tried optimizing row_tol and col_tol with no success. Any solution within Camelot? If not, what would be a quick fix in PANDAS? "
5335,Efficiently break up a string based on the nth occurrence of a substring using R,"['r', 'string', 'text', 'text-mining', 'pattern-mining']","IntroductionGiven a string in R, is it possible to get a vectorized solution (i.e. no loops) where we can break the string into blocks where each block is determined by the nth occurrence of a substring in the string.Work done with Reproducible ExampleSuppose we have several paragraphs of the famous Lorem Ipsum text.We would like to break this text into segments at every 3rd occurrence of the the word "" in"" (a space is included in order to distinguish from words which contain ""in"" as part of them, such as ""min"").I have the following solution with a while loop:We are able to get the desired output in a list with a warning (warning not shown)GoalIs it possible to improve this solution by vectorizing (i.e. using apply(), lapply(), mapply() etc.). Also, my current solution cut's off the last occurrence of the substring in a block.The current solution may not work well on extremely long strings (such as DNA sequences where we are looking for blocks with the nth occurrence of a substring of nucleotides)."
5336,How to extract categories out of short text documents?,"['nlp', 'cluster-analysis', 'text-mining', 'topic-modeling']","My data contains the answers to the open-ended question: what are the reasons for recommending the organization you work for?I want to use an algorithm / technique that, using this data, learns the categories (i.e. the reasons) that occur most frequently, and that a new answer to this question can be placed in one of these categories automatically.I initially thought of topic modeling (for example LDA), but the text documents are very short in this problem (mostly between the 1 and 10 words per document). Therefore, is this an appropriate method? Or are there other models that are suitable for this? Perhaps a cluster method?Note: the text is in Dutch"
5337,How do I 'efficiently' replace a vector of strings with another (pairwise) in a large text corpus,"['r', 'text-mining', 'gsub', 'large-data']","I have a large corpus of text in a vector of strings (app. 700.000 strings). I'm trying to replace specific words/phrases within the corpus. That is, I have a vector of app 40.000 phrases and a corresponding vector of replacements.I'm looking for an efficient way of solving the problem I can do it in a for loop, looping through each pattern + replacement. But it scales badly (3 days or so !)I'v also tried qdap::mgsub(), but it seems to scale badly as wellBoth solutions scale badly for my data with app 40.000 patterns/replacements and 700.000 txt stringsI figure there must be a more efficient way of doing this?"
5338,How can I remove words with more than 3 consecutive letters? [duplicate],"['r', 'regex', 'text-mining']","currently I m working on a text mining(internet comment) case and I really want to remove those meaningless word. for example, aaaaawwwwww, weeeeeeeeeeeee etc.I want to make a rule which can remove those words contain 3 or more consecutive letters.can anyone help me to make this possible via tm_map or base on DocumentTermMatrix ?this is what I have done so farsimulated input data:expected output:"
5339,How do I remove texts from a corpus in R?,"['r', 'data-science', 'text-mining', 'tm', 'quanteda']","I'm dividing a long document into chapters using the corpus_segment function in the tm package. After running the pattern, I'm still left with a couple of unwanted chapters. I'd like to somehow remove these from the corpus.I've tried looking this up in the documentation, but honestly can't seem to figure out the right way to go about this.This code results in the document being divided into 20 chapters, 5 of which I'd like to be removed.How would i go about removing, say ""H9368.html.4"" from this corpus?"
5340,convert R matrix to text2vec dtm,"['r', 'text-mining', 'text2vec']","I have a R matrix mat and I want to perform LDA on it. When I run lda_model$fit_transform(mat, n_iter = 20), I get an error:Is there an easy way to solve this? The source for my matrix is not text and I don't want to go into vocabularies, itoken(), etc."
5341,How to output in R all possible deviations of a word for a fixed distance value?,"['r', 'text-mining', 'tidyverse', 'stringr', 'quanteda']","I have a word and want to output in R all possible deviatons (replacement, substitution, insertion) for a fixed distance value into a vector.For instance, the word ""Cat"" and a fixed distance value of 1 results in a vector with the elements ""cot"", ""at"", ..."
5342,Word search from string in python and providing output into CSV column,"['python-3.x', 'text-mining']","A program that row-wise checks strings if it contains from the list of words and writes 1/0 in the next column named ""Result""I am looking for filtering text messages containing words like 'PNR' and Airport code (like 'LHR','JFK' etc.)Check this link for summary https://imgur.com/0JESYAy.jpg
I have 1 million row in a CSV file having text messages. How can I produce a simple boolean output in the next column with 0 or 1 depending upon message contains any of the words from Set of words.I am not an advanced programmer, I am working with python and has basic knowledge of programming.
I have done simple extraction from strings. "
5343,How to lemmatize a corpus with a particular dictionary in R?”,"['r', 'text-mining', 'lemmatization']","I'm trying to perform lemmatization on a corpus, using the function lemmatize_strings() as an argument to tm_map() of tm package. But I want to use my own dictionary (""lexico"" - first column with the full word form in lower case, while the second column has the corresponding replacement lemma).    I tried to use:But didn't work...
When I use:I have no problem!How can I put the my dictionary ""lexico"" in the fuction tm_map()?   Sorry for this question, it'is my first attempt to make some text mining, at the age of 48.To be more understandable, my corpus are composed by 2000 documents; an extract from the first document:Then worked on a dictionary file (lexico) with this configuration:When I use the function lemmatize_strings(corpus[[1]], dictionary = lexico), it works correctly and give de document of corpus nº1 lemmatized with lemmas from my dictionary.The problem that I have, is with this function:This simply destroy all my documents in the corpusThnks in advance for your reply!"
5344,Moving words in a cell to individual columns [closed],"['r', 'text', 'text-mining']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed last year.I have a csv file that has a column with multiple words in each cell. I wonder if there's any R function to move words in each cell to individual cells. 
The following are data in two cells in the dataset:arecapalm,betelnut,konkan,nature,traveldiaries,mirrorlessframes
passangerstories,chakarmanee,atranginikhil,maharashtra,indiaThanks. Any help appreciated.Chamil"
5345,(R) About stopwords in DocumentTermMatrix,"['text-mining', 'tm', 'stop-words']","I have some questions about DocumentTermMatrix() and about its stopwords.
I typed as below, but couldn't get the results that I wanted.First is that even though I used stopwords=F, the dtm still removed some stopwords such as ""is."" However, it didn't remove ""his"" although it is listed in both stopwords(""en"") and stopwords(""SMART"").
So I really don't understand what stopwords that DTM uses and why stopwords=F doesn't work. and What should I do to make it work?"
5346,Term frequencies from VCorpus and DTM do not match,"['r', 'text-mining', 'tm', 'corpus']","I calculated term frequency of test documents both from Corpus and DTM as below. But they didn't match with each other.
Can anyone tell me where the mismatch came from? Is it because I used wrong methods to extract term frequency?"
5347,Remove specific words with specific punctuation in R,"['r', 'string', 'text-mining', 'tm']","I'm working on a corpus in R that contains interrogatories in Russian.In the beginning of each question there is the names of the person speaking written.By example:President. - Are you Nikolaj Khvostov?For an analysis of these interrogatories I would like to remove these names when they come at the beginning of the line (ie, when they are used to identify the speaker), but not when they come in the middle of the text (ie, when the speaker actually says that name). So that there is a specific punctuation which follows.I tried the following code:or But nothing is removed and if I try:All the terms President and Khvostov are removed. But I need to use these names when they're used by during the intervention.What I mean is that by example:Input strings:President. - Are you Nikolaj Khvostov?Khvostov. - Yes Mr. President.Desired output:Are You Nikolaj Khvostov?Yes Mr. President.Undesired output generated by the third code block:Are you Nikolaj?Yes Mr..There an example of my data:It would be easy to do it in word, but I actually have more than 5 000 pages of interrogatory I would like to study. Word is not able to open the document without crashing my computer. That's why I hoped a code would match to help me.My data is added in R as a Large VCorpusПредседатель. — Алексей Николаевич, вы уже были допрошены один раз 15
  марта — не правда ли?Хвостов. — Да.Председатель. — Вам известно, что вы в заседании Чрезвычайной
  Следственной Комиссии?Хвостов. — Да.Председатель. — Я просил бы вас, не стесняясь рамками допроса, который
  с вас снят, рассказать нам все, что вам известно и как члену
  Государственной Думы и по должности министра внутренних дел, — о
  действиях бывших министров и прочих высших должностных лиц,
  расследованием действий которых мы заняты. Нас интересует, конечно, и
  та тема, которую вы задели при показании, данном вами г. Коровиченко,
  т.-е. тема о тех кружках, которые стояли рядом с правительством или,
  быть может, позади его и оказывали известное влияние. Эта тема
  подлежит углублению… В частности, мы просим вас остановиться на
  следующем: в своей деятельности министра внутренних дел испытывали ли
  вы, и в какой мере, и при каких обстоятельствах, — давление этих
  кружков, о которых вы уже давали показание? Вот канва… Благоволите
  начать.Хвостов. — Я должен доложить, что вполне подтверждаю все, уже данное
  мною в показании. В сущности, это показание я считаю своей
  обязанностью дать, при чем оно не касается дел должностных и вверенных
  мне по должности министра внутренних дел, а лишь того, что случалось
  узнавать по этому поводу и что не входило непосредственно в задачу
  министра внутренних дел…Председатель. — Я не понял вашу мысль… У нас не может быть здесь
  никаких тайн: вы не только в праве, но вы обязаны показать нам
  абсолютно все то, что вы знаете.Хвостов. — Кроме тех обязанностей, которые на меня возлагались по
  должности, я руководствовался долгом русского человека, потому что
  вопрос касался больного для меня места — я разумею вопрос о шпионаже…Председатель. — Не расскажете ли вы в такой последовательности: при
  каких обстоятельствах вы были назначены министром внутренних дел? Это
  — сперва… [3]Хвостов. — Может быть, вы мне разрешите взять более глубоко? Вы
  изволили сказать, что я должен показать то, что мне известно было
  раньше в качестве члена Государственной Думы… Уже издавна, в качестве
  члена Государственной Думы, я обратил внимание на немецкое влияние,
  которое, мне казалось, имелось в правительстве. Я занялся…Председатель. — Вы когда обратили на это внимание?Хвостов. — Еще членом Государственной Думы, почти в самом начале
  прибытия моего в Петроград — в 1912 году… До этого для меня, как
  служившего в провинции, те или другие влияния на петроградские сферы
  должны были оставаться в стороне… Единственный раз, когда мне пришлось
  встретиться с Распутиным, — это было в Нижнем Новгороде, когда я был
  губернатором. Ко мне приехал Распутин, мне в то время мало известный,
  о котором я слышал в виде толков, доходивших до провинции. Он
  предложил мне место министра внутренних дел. Было это, насколько я
  помню, — за неделю или дней за десять до убийства Столыпина. Я был
  удивлен его появлением, не придавал ему такого значения, какое
  впоследствии обнаружилось… Я крайне удивился возможности ухода
  Столыпина, так как в провинции нам казалось, что Столыпин — сила
  непререкаемая, нам казалось невозможным, чтобы он колебался, шатался
  или уходил… Распутин объявил мне, что он должен поговорить со мной,
  так как он послан, как он сказал, — «посмотреть мою душу»…Председатель. — Кем послан?Хвостов. — Неопределенно: из Царского послан — посмотреть мою душу…
  Это казалось мне, в то время непосвященному, несколько смешным, и я с
  ним поговорил шутовским образом, а потом, через несколько времени, я
  послал полицмейстера свезти его на вокзал. Распутин уехал…Председатель. — Вы говорите, вам показалось странным, что это
  происходило еще при жизни министра внутренних дел, — вы выразили
  сомнение по этому поводу?Хвостов. — Я выразил сомнение. Он сказал, что Столыпин должен уйти.
  Но, по правде сказать, я с ним серьезно не говорил… Я считал, что он
  одно из духовных развлечений в Царском Селе, но не считал серьезным,
  чтобы он мог иметь значение при назначении министров… Я знал, что в
  это время ко мне относился в высшей степени благосклонно бывший
  император Николай II… Его хорошие отношения ко мне завязались впервые,
  когда я был губернатором в Вологде и докладывал о возможности
  соединения вологодских рек с Сибирью чрез Урал. На это обстоятельство
  я, главным образом, напираю. Это его очень интересовало, я делал ему
  часто доклады, которые были более радужны, в смысле экономических
  перспектив, чем все остальное, что делалось в России в это время. Вот
  этим я обратил на себя его внимание… [4]В предпоследний раз, перед
  указанным приездом Распутина, я был принят государем сидя, что
  считалось высшим знаком благоволения. Разговор велся об общих
  предметах.Председатель. — А когда в последний раз перед этим визитом Распутина
  вы были в Царском?Хвостов. — Я с точностью не помню, но так месяца за полтора… Мне было
  известно от близких лиц, от иностранных посольств, что обо мне
  постоянно ведется разговор на охоте… Но этот самый приезд Распутина в
  высшей степени поразил меня, и я к нему отнесся не серьезно. После
  этого мне пришлось быть уже в Царском…Председатель. — Алексей Николаевич, вы говорите, что ваше отношение к
  этому приезду было отрицательным; но это не избавляет нас от
  необходимости несколько подробнее остановиться на нем: только ли для
  этого приезжал Распутин?Хвостов. — Исключительно для этого.Председатель. — Какой еще разговор был между вами?Хвостов. — Разговор исключительно этот. Он сказал: «Приехал посмотреть
  твою душу»…Председатель. — Т.-е. только эти несколько слов?Хвостов. — Он изъявил еще желание посмотреть мою семью… Моей семьи еще
  не было… Я считал излишним вводить его в мою семью. Я не серьезно к
  этому отнесся… Тут была ярмарка: она кончалась. Мне было не до того,
  чтобы беседовать с ним. Я отнесся к этому в высшей степени легко…
  Когда, через месяц после этого, я приехал сюда, то я увидел ко мне
  совершенно обратное отношение. Я был принят в высшей степени
  неприязненно, в высшей степени сухо, что, после предшествующих
  приемов, мне показалось неособенно приятным. Это послужило основанием
  тому, что тогда казалось странным: я ушел из губернии, пошел в
  Государственную Думу… Уйти я сразу не мог, прошло порядочное время,
  около года: но во всяком случае, в этот промежуток времени, я делал
  все возможное, чтобы попасть в Государственную Думу по Орловской
  губернии, и уже все мои мысли были за то, чтобы уйти, — видя, что
  здесь мне отрезаны все дальнейшие пути…Председатель. — Что же, уезжая, Распутин какие-нибудь угрозы делал по
  вашему адресу?Хвостов. — Болтал по обыкновению: говорил, что он уже обо мне послал
  телеграмму.Председатель. — Какого содержания?Хвостов. — Содержания совершенно не помню в подробностях… Мне потом
  достали текст (всегда на почте есть свои люди, которые сообщают потом
  подробное содержание). Но я текста не помню.Председатель. — Приблизительно, какого содержания? [5]Хвостов. — Отрицательное ко мне отношение… Что-то такое: «Хотя бог на
  нем почиет, но чего-то не достает»…Председатель. — Скажите еще: он вам говорил, от чьего имени из
  Царского Села он являлся — от отрекшегося государя или от государыни?There the begining of my code:Last part of the result for dput(head(corpus2))))"
5348,"TypeError: sequence item 0: expected str instance, list found","['python', 'text-mining']","I need your help , I am working with text mining ,text classification,and when I want to convert my list of words into string of characters ,because countvec doesn't work with list,I get an errorTypeError                                 Traceback (most recent call last)
 in 
----> 1 Xtrain="" "".join(X_train)TypeError: sequence item 0: expected str instance, list foundenter image description here"
5349,Create custom dictionary from character vector,"['r', 'text-mining', 'quanteda']","I am trying to look for specific words in corpus using dfm_lookup().I am really struggling with the dictionaries needed for the dfm_loopup().I created a character vector named ""words"" which contains all the words that should go into the dictionary.dictionary needs a list, so I am creating a list from the character vector before I am using dictionary().But then I getWhat do I have to change in the list command to get the proper output for dictionary()?Is there a simplier version to look for specific words in a dfm? Because it was really easy with the tm() package."
5350,Calculate similarity between list of words,"['python', 'data-mining', 'text-mining', 'similarity']","I want to calculate the similarity between two list of words, for example :['email','user','this','email','address','customer']is similar to this list:['email','mail','address','netmail']I want to have a higher percentage of similarity than another list, for example:
['address','ip','network'] even if address exists in the list."
5351,list of English words to refer to humans,"['text', 'nlp', 'text-mining', 'natural-language-processing']","I am trying to automatically process English sentences and detect the words which might be referring to humans. e.g. he, everybody, someone, niece, I, son, ...
I am already using NER, and have implemented some simple heuristic rules as well.
But I think, other than tricky cases which is fine if I mis-label them, the problem can be solved with a simple dictionary look-up. Is there any list of English words that I can use?"
5352,web scraping with R in country which twitter is blocked,"['r', 'text', 'twitter', 'web-scraping', 'text-mining']","I am trying to extract some data with rtweet package in R. The problem is that twitter is blocked in Iran and I have to use a software to pass the filtering. Unfortunately, I am facing this error:I am following regular steps for creating a token:Do you have any idea?Thanks,"
5353,How to correct words automatically using R?,"['r', 'text', 'text-mining', 'spell-checking', 'autocorrect']","I'm working with text mining and before processing the data I need to correct words (English language) that are stored in a .CSV file.I got what I wanted using python and the TextBlob library. First I step the original file, TextBlob fixes all the incorrect words automatically from the file and in the output, a file is generated with the corrected words.You can check the python code here:Please, could someone please tell me some package in R that does something similar? I haven't found any packages that are able to automatically correct words using R."
5354,Managing unstructured text data into a DBMS,"['sql', 'database', 'text', 'nosql', 'text-mining']","I'm competent in R and C++ and know my way around SQL queries, but not with databases and need some advice. Suppose I have a text file that looks like this:for 500,000+ ids. I want to extract queries such as ""what's the most popular book under category xxx?"" or ""which customer has the highest average helpful reviews?"" or even suggest ""which books would customer xxx enjoy?""However, I do not know how to wrangle with this type of data. I was thinking of four databases/tables (what is the proper term!):Id ASIN title group salesrank num_of_similar num_of_categories total_reviews downloaded_reviews avg_rating_reviewsId customer_name customer_date customer_rating customer_votes customer_helpfulId category_nameId similarHere is a snippet of what that would look like for the 3rd table (so you can deduce what I would imagine the 4th table to look like):However, I have no experiences with transforming unstructured text into databases and was looking for some guidance. Where do I start! Should I look into NoSQL! Or MS Access! Do I use for-loops to extract information (for instance, the category_name)? If so, how! Do I use regex? I have not found any beginner friendly tutorials on how to structure unstructured text into manageable databases, so any guidance would be severely appreciated."
5355,Why am I getting an Error in DocumentTerm Matrix in R even after using content_transformer for tolower in tm_map function?,"['r', 'text-mining', 'tm']","I have gone through many answers here , and tried to use all suggestions given in stackoverflow  but nothing seems to be working for me . Is there any order before creating document term matrix using tm package in R ?"
5356,Segment words into its sub-words/sub-concepts,"['python', 'nlp', 'text-mining', 'spacy']",What are some popular methods to find and replace concatenated words such as:The method should run on thousands of lines without knowing if there are concatenated words there in advance.I'm using SpaCy library for most of my string handling so best method would be one that works along well with SpaCy.
5357,Text Mining: Query search,"['python', 'text-mining']","I have a dictionary:I need to return the documents that contain these queries. For query1, for example, the answer should be [0,5,9].
I believe the answer should be something like that but in python:Please help. "
5358,Normalizing text using regex [duplicate],"['python', 'regex', 'replace', 'text-mining', 'tweets']","I am working with tweets and I would like to have all the variations of aa aaaa aaah ahhh replaced by a single expression 'ah'. However, using my code I also replace the single 'a' and the 'and' which I don't want to change. This way i get:But what I want is:"
5359,Why should I use ggraph() with set.seed() in R?,"['r', 'text-mining', 'ggraph']","I've recently been learning text mining with tidytext.
Today, I encountered the following:I've used the set.seed() function with other functions like sample(). But here, I don't understand why ggraph should be used with set.seed().
Can anyone help me?"
5360,Replace values in a list of lists,"['python', 'list', 'replace', 'text-mining']","I have a list of lists and I need to replace some values. I need to replace smiles with their meaning, remove # (leaving just the word after) and remove linksI tried to do something like:or but it doesn't work. I get an error saying ""expected string or bytes-like object"". 
Please help me!"
5361,Remove username from a list of lists,"['python', 'list', 'text-mining']",I have a list of lists regarding tweets and I need to remove the username.The main problem is that I don't know how to work with a list of lists. I tried the following code among other things but did not work:Please help.
5362,DocumentTermMatrix in R - sum of unique words for each row,"['r', 'matrix', 'text-mining']","I have a DocumentTermMatrix data_tags with 80.000 rows (groups of tags)
and 900.000 columns, so 900.000 different tags.
Through findFreqTerms(data_tags,2) I found out that about 462.000 tags are unique.I want to make a function where 2 things happen:
- delete these 462.000 columns, so that only tags with frequency 2 or more are left;
- create 1 new column (Uniques): sum() for each row of all the unique tags that were removed.for example, tag 3 and tag4 are unqiue (only once appears in column):Thanks in advance for the help."
5363,How do I split text with multiple sentences in a column into multiple rows in Python pandas?,"['python', 'pandas', 'text-mining', 'sentence-synthesis']","I am trying to split Comments column into multiple rows containing each sentence. I used the following StackOverflow thread for my reference as it tends to give similar result.
Reference Link: pandas: How do I split text in a column into multiple rows?
Sample data of dataframe is as below.Id  Team    Food_Text
1   X   Food is good. It is very well cooked. Delicious!
2   X   I hate the squid. Food is not cooked well. At all indeed. 
3   X   Please do not good anytime over here
4   Y   I love the fish. Awesome delicacy.
5   Y   Good for desserts. Meat tastes badEach record for 'Food_Text' can be of multiple sentences delimited by full-stop or period. I have used the following codeI am not sure why the join is not giving me a proper dataframe with more number of rows. Repetition of other columns based on index of split. So Id=1 has 3 sentences so we should have 3 records with all other data same and Food_Text column with a new sentence from a comment by ID=1. Similarly for other records. Thank You in advance for your help!
Regards,
Sohil Shah"
5364,"Error in aggregate.data.frame(as.data.frame(x), …) : arguments must have same length","['r', 'text-mining', 'topic-modeling']","Hi I'm working with the last example in this tutorial: Topics proportions over time.
https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.htmlI run it for my data with this codeHere is the excel file to the input data
https://www.mediafire.com/file/4w2hkgzzzaaax88/data.xlsx/fileI got the error when I run the line with the aggregate function, I can't find out what is going on with the aggregate, I created the ""decade"" variable the same as in the tutoria, I show it and looks ok, the theta variable is also ok.. I changed several times the aggregate function according for example to this post
 Error in aggregate.data.frame : arguments must have same lengthBut still have the same error.. please help"
5365,Lemmatize Words not functioning properly,"['r', 'text-mining', 'tm', 'lemmatization']","I'm trying to do some text mining, with a key intention of taking the words below in this data.frame, but combining ones with similar roots:The best example is received and receive. I'd like the final outcome to look like:So now, received and receive and their frequency are summed as one. Additionally, how might I clean out the entries like ’m and •?"
5366,Find similar/synonyms/context words Python,"['python', 'text-mining', 'wordnet']","Hello i'm looking to find a solution of my issue :
I Want to find a list of similar words with french and english
For example :
name could be : first name, last name, nom, prénom, username....
Postal address could be : city, country, street, ville, pays, code postale ...."
5367,Lemmatizing whole sentence in python does not work,"['python', 'pandas', 'scikit-learn', 'nltk', 'text-mining']","I am using WordNetLemmatizer() function in NLTK package in python to lemmatize the entire sentence of movie review dataset.Here is my code:review in df is the column of text reviews that I wanted to processAfter using the preprocess function on df, the new column review_clean contains cleaned text data but it still does not have lemmatized text. eg. I can see a lot words ends with -ed, -ing. Thanks in advance."
5368,how to read muliple pubmed abstract text file using pubmed.mineR function readabs,"['r', 'text-mining', 'pubmed']","I'm using R package 'pubmed.mineR' package which provide 'gene_atomization' for fetching genes from pubmed abstract. but 'readabs' function read only single text file.this code show error can anyone help for reading multiple abstract text file
I also tried 'Corpus' function of 'tm' package, it is still not working "
5369,matching keywords to a series of text comments,"['r', 'text', 'text-mining']","I have two sets of information:A csv file where every row has a comment, e.g:a. I love football
  b. Rugby is a tough game
  c. Hello WorldAnother csv file that list words related to sports, e.g:a. tennis
  b. football
  c. rugbyWhat I want to do is: 
  a. find whether any of the words in the second file appears at least once in every individual rows of the first file.
  b. If it appears at least once, it should be categorize as sports against every comment, else others.The output file should look like:I want to do this exercise in R. I explored str_detect & grepl function in R but not achieving the desire output.Your help is appreciated.Thanks"
5370,Python: Check if the sentence contains any word from List (with fuzzy match),"['python', 'text-mining', 'fuzzy-search']","I would like to extract keywords from a sentence given a list_of_keywords.I managed to extract the exact wordsIs it possible to extract words that have good similarity with the given list_of_keywords, i.e cosine similarity between two words is > 0.8For example, the keyword in the given list is 'allergy' and now the sentence is written as'a severe allergic reaction to nuts in the meal she had consumed.'the cosine distance between 'allergy' and 'allergic' can be calculated as belowHow to extract 'allergic' from the sentence as well based on the cosine similarity? "
5371,How to extract the acronym specific for my data with UMLS,"['r', 'text-mining', 'medical']",I am new to the United Medical Language System. I would like to annotate some text that gives reports about endoscopy examinations. The terminology is therefore specific to gastroenterology. Some of the text contains acronyms like TI which would mean terminal ileum. However according to UMLS TI also stands for a number of other terms that are non-gastroenterological. I would like to build a gastroenterology lexicon only from UMLS terms. Is there a way to do this?
5372,Specify the provenance of FHIR Resources generated by applying NLP over medical narratives,"['nlp', 'text-mining', 'hl7-fhir', 'information-extraction']","FHIR is a standard for health care data exchange, published by HL7®. The DocumentReference Provides metadata about the document so that the document can be
  discovered and managedThrough the Provenance one can describe entities and processes involved in producing and delivering
  or otherwise influencing that resource Nearly 80 percent of clinical information in electronic health
 records(EHRs) is ""unstructured"" and in a format that health
 information technology systems cannot use.It is therefore natural to apply computer techniques to automatically generate structured data from the medical records. For that there are several implementations available both on the market and also fully open source. For example cTAKES, CLAMP, NOBLE, ClarityNLP and others are all freely available solutions targeting this task.They all address the specific need of generating structured data from unstructured medical notes, however they all deliver the structure using their own format, that eventually could be converted into FHIR.However, a central problem is on how to represent the Provenance of the extracted information, since FHIR is - to the best of my knowledge - missing the way of connecting to the precise location within the DocumentReference object of where the information has been extracted from , with which technology, and which is the level of ""quality"" of the extracted information.Before submitting a Change Request https://gforge.hl7.org/gf/project/fhir/tracker/?action=TrackerItemBrowse to the FHIR normative, it is recommended to expose the issue to the widest community and the stackoverflow.com is one of the main recommended channels. For this purpose I am hereby looking forward opinions on the matter, and namely on how to specify the provenance of  FHIR Resources generated by applying NLP over medical narratives. For example, taking an example from the Adverse Event Corpus of Gurulingappa et al https://doi.org/10.1016/j.jbi.2012.04.008 , The question is how to represent into FHIR that such drug induced problem has been extracted from the specific bytes positions 22-34 (drug) and 43-54 (problem) from the text (the Title of the paper 1999 in this example).Currently the FHIR standard does not allow to represent the precise byte position, the quality of the extraction, and the method used to perform it."
5373,regex to remove words starting with string,"['r', 'regex', 'text', 'text-mining', 'regular-language']","I am working on a text mining case in R and I need to remove from my Corpus all the words that start with https.
I am still learning regex so your help will be much appreciated!I am trying to remove all the https(and subsequent string until the space) for the following:haciendo  principales httpstcotogmdfcq felicidades rodrigo equipo  madre   nominación   oscars  cine español   enh httpstcopupepkzwx    pasando  cataluña  insostenible  sociedad catalana  rehén   supremacistas   desar httpstcojzgilkyx   auténtica broma  sánchez critique  acuerdo  pp  andalucía   pactado  torra   apel httpstcofwevrjfpsv  puede ser     roto  espacio electoral  tres partidos  siquiera hablen  ponerse  acuerdo httpstcoydkjmydxhc  gobierno socialista   ningún proyecto  españa   quedarse   moncloa solo quiere mantener  httpstcoqwibvkxl allá   gobernado  ppopular  transformado  bien  vida   españoles  preparados  go httpstcocdntnstbpa  partido político    objetivo   mismo"
5374,determine the temporality of a sentence with POS tagging,"['r', 'text-mining', 'tidytext']","I want to find out whether an action has been carried out if will be carried out from a series of sentences.
For example: 
""I will prescribe this medication"" versus ""I prescribed this medication"" or ""He had already taken the stuff"" versus ""he may take the stuff later""I was trying a tidytext approach and decided to simply look for past participle versus future participle verbs. However when I POS tag using the only types of verbs I get are ""Verb intransitive"", ""Verb (usu participle)"" and ""Verb (transitive)"". How can I get an idea of past or future verbs or is there another POS tagger I can use?I am keen to use tidytext because I cannot install rjava which some of the other text mining packages use."
5375,Extracting full name and country code of geolocation with Twitter,"['python-3.x', 'twitter', 'text-mining']","I am trying to extract full name and country code of a ""place"" from a json file that has thousands of tweets mined from Twitter API. I have tried to write the code several different times, but the code below makes the most sense to me. I was able to extract other data (hashtags, dates, followers with similar coding).This is the error I get:AttributeError                            Traceback (most recent call last)
 in 
      3 
      4     if data[i][""place""] != ""null"":
----> 5         get_place_info  = data[i][""place""].get(""full_name"")
      6         print(get_place_info)
      7 AttributeError: 'NoneType' object has no attribute 'get'"
5376,Split row string into multiple columns in R,"['r', 'split', 'text-mining', 'arabic']","I used R and I have this string as a row and I need to split it to be as columns 'id': 1050442590754103297, 'id_str': '1050442590754103297', 'name':
  'ام رودينا ', 'screen_name': 'uclkGkQ5', 'location': None,
  'url': None, 'description': '\u200f\u200fمن زوي الاحتياجات
  الخاصه', 'translator_type': 'none', 'protected': False,
  'verified': False, 'followers_count': 1567, 'friends_count': 4019,
  'listed_count': 0, 'favourites_count': 6669, 'statuses_count': 9279,
  'created_at': 'Thu Oct 11 17:46:44 +0000 2018', 'utc_offset': None,
  'time_zone': None, 'geo_enabled': False, 'lang': 'ar',
  'contributors_enabled': False, 'is_translator': False,
  'profile_background_color': 'F5F8FA', 'profile_background_image_url':
  '', 'profile_background_image_url_https': '',
  'profile_background_tile': False, 'profile_link_color': '1DA1F2',
  'profile_sidebar_border_color': 'C0DEED',
  'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color':
  '333333', 'profile_use_background_image': True, 'profile_image_url':
  'http://pbs.twimg.com/profile_images/1059769079790268416/sJpep_V8_normal.jpg',
  'profile_image_url_https':
  'https://pbs.twimg.com/profile_images/1059769079790268416/sJpep_V8_normal.jpg',
  'profile_banner_url':
  'https://pbs.twimg.com/profile_banners/1050442590754103297/1539390015',
  'default_profile': True, 'default_profile_image': False, 'following':
  None, 'follow_request_sent': None, 'notifications': NoneI tried this code is worked but I need to specify the number of columns that I need and also I need to rename the columns at the end, so it is difficult and takes timethe result I got is and it is without columns name :and the fourth column has the rest of the string I need a code that split the rows based on comma and makes the column name the word before (:) as shown here:and the same for the rest of string 
hope you understand me and thank you "
5377,extract associated values from text documents with tables as returns,['text-mining'],"I've got a challenge to extract information from pdf documents. I've managed to translate associated information from the pdf into data frames in r. The challenge is for information buried in text, would it possible to get the information returning as tables?For example, I have a list of animals, monkey, puffin, tuna, etc as observations and their features as variables, e.g. head, tail, eyes, hair, length, weight...I ask the code to use these queries to search in relevant documents returning with a table in the format of rows as observations and columns as variables (if no values were found, return as NAs in associated cells)?  I have used R to extract parameters of interest from pdf files. My code is not flexible and only enables me to extract and tidy certain pages of interest. These pages are fairly structured appendix with tags I can use to narrow down my search.  "
5378,Structuring a large text file into a dataframe using R,"['r', 'text', 'text-mining']","I have a text file of around 20 pages with around 200 paragraphs. Each paragraph contains three lines describing information about a person like so:Now I wish to convert this large file into a dataframe where the columns represent the three variables Name, Age and Phone number and where the rows correspond to the persons.How can I convert the large text file into such a dataframe?"
5379,Parse Parts of Speech Tagged Tree Corpus with Python without NLTK,"['python', 'regex', 'parsing', 'nlp', 'text-mining']",I have tree corpus as belowI need to parse this tree and convert into a sentence form as below Is there any algorithm to parse the above content or we need to use regular expressions to do this and I do not want to use NLTK packages to do this. 
5380,I need help dropping empty rows and rows with empty spaces from dataframe,"['r', 'text-mining']","I am trying to remove all empty rows from my dataframe. Problem is the rows aren't entirely empty, some have one space, other multiple spaces and new lines.Here are examples:I have tried a few solutions. Firstly I tried removing all rows that are empty and contain no spaceBut i'm left with empty rows that contain \n or multiple lines. This also makes it difficult to remove rows based on number of charactersI also thought about removing all rows that do not start with a letterHowever this removes about a 5th of my rows, from observation it doesn't seem likely that there are that many empty spaces in my dataframe. This probably also removes rows that begin with a space but actually have lettersHere is a link to the data I am using:
Data"
5381,Problems trying to completely remove https text from my Twitter analysis,"['r', 'text-mining', 'tm']","I am trying to analyse >50000 tweets and trying to clean up this data before carrying out any further analysis.I am using the TM package to clean up my data. I am struggling to remove all URLs, I have remove http urls, but I am left with https URLs. If I try and add a custom remove for these I end up removing the https part and remaining with the remainder of the url (without the forward slashes and punctuations)Here is a sample of my tweets prior to clean upThis is what I am left withI have also tried adding this functionand end up with this"
5382,Cannot seem to extract more than 88 tweets despite mining for trending keywords,"['r', 'twitter', 'text-mining']","I'm trying to search for around 20,000 tweets using keywords that are currently trending on my timeline.However, I am only getting about 88 tweets. These are trending keywords in the entire country and it is highly unlikely that there are only 88 tweets available.Here is my code"
5383,How to incorporate user input into lookahead and lookbehind assertion in regex,"['python', 'regex', 'text', 'user-input', 'text-mining']","How do I incorporate user input together with lookahead/lookbehind assertion in regex to obtain the context of the word?Currently, I'm manually changing this part of the code (?=autogenerated) to get the terms I want, but I want the code to be more flexible to take any user input. "
5384,Apply a custom (weighted) dictionary to text based on sentiment analysis,"['inner-join', 'text-mining', 'tm', 'quanteda', 'qdap']","I am looking to adjust this code so that I can assign each one of these modal verbs with a different weight.  The idea is to use something similar to the NRC library, where we have the ""numbers"" 1-5 represent categories, rather than numbers.My problem is that when I run the following code I have that 5 ""may""s count as the same as one ""must"".  What I want is for each word to have a different weight so that when I run this analysis I can see the concentration of uses of the stronger ""must"" versus say the much weaker ""can"". *with ""tidy.DF"" being my corpus and ""school"" and ""target"" being the column names.  "
5385,How to replace a list of misspelled words with a list of correct words?,"['r', 'text-mining']",I'm trying to figure out how to replace a long list of misspelled words from a list of correct words but not sure how to do it. Please advise if possible. Thank you.I tried str_replace and gsub but it seems like because I want to implement the changes from a dataframe so it doesn't really work that way.I expect the output to be like this:
5386,Pandas dataframe Merge text rows group by ID,"['python-3.x', 'merge', 'excel-formula', 'pandas-groupby', 'text-mining']","I have a dataframe as follows:                                           I want to merge Text by ID in Python using group by clause. I want to merge 'Text' columns by grouping ID.I want to do this in Python. I have attempted following code chunks but it didn't work:groups = groupby(dataset_new, key=ID(1))dataset_new.group_by{row['Reference']}.values.each do |group| 
puts [group.first['Reference'], group.map{|r| r['Text']} * ' '] * ' | ' 
endI also attempted to merge text in excel using formulas but it is also not giving required results."
5387,How to add list items to a new dataframe column inside another list in r?,"['r', 'dplyr', 'text-mining', 'pubmed']","I'm trying to extract co-author's names and affiliations for all publications on pubmed. I was able to get the list of author's name in a dataframe, but I now need to add the affiliation with the name. I've been trying to do this, but I'm not sure how. I need to combine two lists: authors and affiliations for each authors into one.I'm using RISmed to extract the data. I want the data in the following format: Lastname, Firstname  AffiliationI don't care about the count. I suppose the other way to look at this is the following: Combine two lists together. list A is a list of dataframe: There are multiple items in this list where each item has the followinglist B is a list of lists: What I want to do is to combine these two lists together such that the affiliations show up for each authors as a column on the dataframe. The final result would be the following:"
5388,Adding text between 2 html tags,"['python', 'html', 'text-mining']","I am a 2 years student and I am working on text mining.For general let me tell you about the code it first accept pdf type text and convert that in to doc.txt file, then I process that data for couple of hundred lines then after i store all sentences in that text to the list called all_text (for th future  use) and also I select some texts and store them in to a list called summary.Finally the problem is on this part:Summary list look like thisWhat I want is read from doc.txt sentence by sentence and if that sentence is in the summary list modify that sentence by put it in to BOLD tag "" the sentence"" for all in the summary list here is small code i tried for that specific part it not help full but here it is This code did not work as I expected, I mean it works for some short sentences, but it doesn't work for the sentences like those I don't have any idea why it's not working help me please?"
5389,How to filter a dfm by documents with at least n terms in quanteda?,"['r', 'text-mining', 'quanteda']","I am analyzing text data from a round table, and I would like to know if it is possible to filter only those documents which have more than ""n"" terms?My corpus has documents which contains only 1 word, such as: ""Thanks"", ""Sometimes"", ""Really"", ""go"". I would like to remove then in order to decrease sparsity.I tried dfm_trim from quanteda but I couldn't handle it:I would expect that only 1993-Clinton, 2001-Bush and 2017-Trump would have 0, or get rid off dfm.
Obs.: This example is only for illustration purpose, it is not the text data I am analyzing."
5390,How do I extract the first word from a list of words? [closed],"['r', 'text-mining']","
Want to improve this question? Add details and clarify the problem by editing this post.
                Closed last year.I'm having trouble extracting the first word from a list of words. I've tried substring, gsub, and str_extract but still haven't figured it out. Please advise. Thank you. Here is what I'm trying to do:I'm trying to extract the first word from the list that looks like this:"
5391,Convert multiple pdf files to a CSV file,"['r', 'text', 'text-mining']","My task is to convert all pdf files as shown in the figure 1 below into a single csv file.That is, one row in CSV file contains one pdf document. I use the following code and I am struggling. Your help and comment will be appreciated.Thank you,"
5392,Getting top terms per affinity propagation cluster in scikit-learn,"['scikit-learn', 'cluster-analysis', 'text-mining']","I am trying different clustering methods for a bunch of news texts, and am struggling to find any way to find top terms per cluster for sklearns affinity propagation, and am becoming unsure if this is even possible. For k-means clustering I am using the same approach as here: https://scikit-learn.org/0.19/auto_examples/text/document_clustering.html
I would logically want to use the same X for affinity propagation as for k-means. Anyone know how producing similar results with affinity propagation would be possible?"
5393,defining the Arabic language in R,"['r', 'text-mining']","I am working on text Mining in R in Arabic language, and I had some problem with defining the Arabic language by R studio.
I set the local Arabic as shown here:Sys.setlocale(""LC_CTYPE"",""arabic"")and the Arabic is showed and i can read it, but when I tried to calculate the words frequency it doesn't define the Arabic language and it convert it to some symbols.here is my code and sample of the data:the data:the code: the tdm result all terms are symbols I cannot understand it :"
5394,"Text mining, customizing lemmatization","['python', 'dataframe', 'text-mining', 'lemmatization']","A simple DataFrame and I am applying Lemmatization to it.Some words remained unchanged so I am looking for if a smart way to customize the Lemmatization.The output is:There are some words remained unchanged: visited, discussed, received
(and ""discuss"" was changed to ""discus"")I can add on below lines before Lemmatization. But what's the better way? Can the Lemmatization be customized?btw, tried NLTK's WordNetLemmatizer, it's the same. and I read Python NLTK Lemmatization of the word 'further' with wordnet but still don't have a clue."
5395,How is the correct use of stemDocument?,"['r', 'text-mining', 'tm', 'stemming', 'snowball']","I have already read this and this questions, but I still didn't understand the use of stemDocument in tm_map. Let's follow this example:If I use:it does work! But if I use:it doesn't work. Why so?"
5396,R: save vector of text values into .txt files (each element of vector to seperate .txt file),"['r', 'text-mining']","I have a vector, where each values is a text value. To make it simple let's assume its:  c('a','b','c','d')
I managed only to save it as one big file:  How could I save each element to a separate file for example 1.txt, 2.txt, 3.txt etc?"
5397,fix misspelled words in a corpus without dictionary,"['deep-learning', 'nlp', 'nltk', 'text-mining']","We have a history of conversations between humans (any language, any vocabulary), so with a lof of spelling errors:Before running a deep learning task against this data set (find synonyms etc..), I would like to fix these errors.Is it a good idea? I've never worked with such bad quality data. Wondering if there is a ""magic solution"" to achieve this.Else I plan to use:"
5398,Extracting word document with styles associated to the content,"['formatting', 'text-mining', 'text-extraction', 'python-docx']","I'm trying to extract the format of a word document containing text in different fonts and font-sizes, images, comments etc. I have used zipfile module to extract the XML files of the word document.XML files are:I'm unable to understand the styles associated with the content present in word/document.xml.I'm trying to encapsulate the results in the following manner:Tried using python-docx to get the fonts and font-sizes but mostly the value is Nonehere's the code snippet:Results are mostly None for font and size."
5399,Counting words and word stems in a large dataframe (RStudio),"['r', 'dictionary', 'text', 'text-mining', 'stringr']","I have a large dataframe consisting of tweets, and a keyword dictionary loaded as a list that has words and word stems associated with emotion (kw_Emo). I need to find a way to count how many times any given word/word stem from kw_Emo is present each tweet. In kw_Emo, word stems are marked with an asterisk ( * ). For example, one word stem is ador*, meaning that I need to account for the presence of adorable, adore, adoring, or any pattern of letters that starts with ador…. From a previous Stack Overflow discussion (see previous question on my profile), I was greatly helped with the following solution, but it only counts exact character matches (Ex. only ador, not adorable):Load relevant package.library(stringr) Identify and remove the * from word stems in kw_Emo.for (x in 1:length(kw_Emo)) {
  if (grepl(""[*]"", kw_Emo[x]) == TRUE) {
    kw_Emo[x] <- substr(kw_Emo[x],1,nchar(kw_Emo[x])-1)
      }
    }Create new columns, one for each word/word stem from kw_Emo, with default value 0.for (x in 1:length(keywords)) {
  dataframe[, keywords[x]] <- 0}Split each Tweet to a vector of words, see if the keyword is equal to any, add +1 to the appropriate word/word stems' column.for (x in 1:nrow(dataframe)) {
  partials <- data.frame(str_split(dataframe[x,2], "" ""), stringsAsFactors=FALSE)
  partials <- partials[partials[] != """"]
  for(y in 1:length(partials)) {
    for (z in 1:length(keywords)) {
      if (keywords[z] == partials[y]) {
        dataframe[x, keywords[z]] <- dataframe[x, keywords[z]] + 1
      }
    }
  }
}Is there a way to alter this solution to account for word stems? I'm wondering if it's possible to first use a stringr pattern to replace occurrences of a word stem with the exact characters, and then use this exact match solution. For instance, something like stringr::str_replace_all(x, ""ador[a-z]+"", ""ador""). But I'm unsure how to do this with my large dictionary and numerous word stems. Maybe the loop removing [*], which essentially identifies all word stems, can be adapted somehow? Here is a reproducible sample of my dataframe, called TestTweets with the text to be analysed in a column called clean_text:dput(droplevels(head(TestTweets, 20)))Here is kw_Emo:kw_Emo <- c(""abusi*"", ""accept"", ""accepta*"", ""accepted"", 
        ""accepting"", ""accepts"", ""ache*"", ""aching"", ""active*"", ""admir*"", 
        ""ador*"", ""advantag*"", ""adventur*"", ""advers*"", ""affection*"", ""afraid"", 
        ""aggravat*"", ""aggress*"", ""agoniz*"", ""agony"", ""agree"", ""agreeab*"", 
        ""agreed"", ""agreeing"", ""agreement*"", ""agrees"", ""alarm*"", ""alone"", 
        ""alright*"", ""amaz*"", ""amor*"", ""amus*"", ""anger*"", ""angr*"", ""anguish*"", 
        ""annoy*"", ""antagoni*"", ""anxi*"", ""aok"", ""apath*"", ""appall*"", ""appreciat*"", 
        ""apprehens*"", ""argh*"", ""argu*"", ""arrogan*"", ""asham*"", ""assault*"", 
        ""asshole*"", ""assur*"", ""attachment*"", ""attract*"", ""aversi*"", ""avoid*"", 
        ""award*"", ""awesome"", ""awful"", ""awkward*"", ""bashful*"", ""bastard*"", 
        ""battl*"", ""beaten"", ""beaut*"", ""beloved"", ""benefic*"", ""benevolen*"", 
        ""benign*"", ""best"", ""better"", ""bitch*"", ""bitter*"", ""blam*"", ""bless*"", 
        ""bold*"", ""bonus*"", ""bore*"", ""boring"", ""bother*"", ""brave*"", ""bright*"", 
        ""brillian*"", ""broke"", ""burden*"", ""calm*"", ""cared"", ""carefree"", 
        ""careful*"", ""careless*"", ""cares"", ""casual"", ""casually"", ""certain*"", 
        ""challeng*"", ""champ*"", ""charit*"", ""charm*"", ""cheer*"", ""cherish*"", 
        ""chuckl*"", ""clever*"", ""comed*"", ""comfort*"", ""commitment*"", ""complain*"", 
        ""compliment*"", ""concerned"", ""confidence"", ""confident"", ""confidently"", 
        ""confront*"", ""confus*"", ""considerate"", ""contempt*"", ""contented*"", 
        ""contentment"", ""contradic*"", ""convinc*"", ""cool"", ""courag*"", ""crap"", 
        ""crappy"", ""craz*"", ""create*"", ""creati*"", ""credit*"", ""cried"", 
        ""cries"", ""critical"", ""critici*"", ""crude*"", ""cry"", ""crying"", ""cunt*"", 
        ""cut"", ""cute*"", ""cutie*"", ""cynic"", ""danger*"", ""daring"", ""darlin*"", 
        ""daze*"", ""dear*"", ""decay*"", ""defeat*"", ""defect*"", ""definite"", 
        ""definitely"", ""degrad*"", ""delectabl*"", ""delicate*"", ""delicious*"", 
        ""deligh*"", ""depress*"", ""depriv*"", ""despair*"", ""desperat*"", ""despis*"", 
        ""destruct*"", ""determina*"", ""determined"", ""devastat*"", ""difficult*"", 
        ""digni*"", ""disadvantage*"", ""disagree*"", ""disappoint*"", ""disaster*"", 
        ""discomfort*"", ""discourag*"", ""dishearten*"", ""disillusion*"", ""dislike"", 
        ""disliked"", ""dislikes"", ""disliking"", ""dismay*"", ""dissatisf*"", 
        ""distract*"", ""distraught"", ""distress*"", ""distrust*"", ""disturb*"", 
        ""divin*"", ""domina*"", ""doom*"", ""dork*"", ""doubt*"", ""dread*"", ""dull*"", 
        ""dumb*"", ""dump*"", ""dwell*"", ""dynam*"", ""eager*"", ""ease*"", ""easie*"", 
        ""easily"", ""easiness"", ""easing"", ""easy*"", ""ecsta*"", ""efficien*"", 
        ""egotis*"", ""elegan*"", ""embarrass*"", ""emotion"", ""emotional"", ""empt*"", 
        ""encourag*"", ""energ*"", ""engag*"", ""enjoy*"", ""enrag*"", ""entertain*"", 
        ""enthus*"", ""envie*"", ""envious"", ""excel*"", ""excit*"", ""excruciat*"", 
        ""exhaust*"", ""fab"", ""fabulous*"", ""fail*"", ""fake"", ""fantastic*"", 
        ""fatal*"", ""fatigu*"", ""favor*"", ""favour*"", ""fear"", ""feared"", ""fearful*"", 
        ""fearing"", ""fearless*"", ""fears"", ""feroc*"", ""festiv*"", ""feud*"", 
        ""fiery"", ""fiesta*"", ""fine"", ""fired"", ""flatter*"", ""flawless*"", 
        ""flexib*"", ""flirt*"", ""flunk*"", ""foe*"", ""fond"", ""fondly"", ""fondness"", 
        ""fool*"", ""forgave"", ""forgiv*"", ""fought"", ""frantic*"", ""freak*"", 
        ""free"", ""freeb*"", ""freed*"", ""freeing"", ""freely"", ""freeness"", 
        ""freer"", ""frees*"", ""friend*"", ""fright*"", ""frustrat*"", ""fuck"", 
        ""fucked*"", ""fucker*"", ""fuckin*"", ""fucks"", ""fume*"", ""fuming"", 
        ""fun"", ""funn*"", ""furious*"", ""fury"", ""geek*"", ""genero*"", ""gentle"", 
        ""gentler"", ""gentlest"", ""gently"", ""giggl*"", ""giver*"", ""giving"", 
        ""glad"", ""gladly"", ""glamor*"", ""glamour*"", ""gloom*"", ""glori*"", 
        ""glory"", ""goddam*"", ""gorgeous*"", ""gossip*"", ""grace"", ""graced"", 
        ""graceful*"", ""graces"", ""graci*"", ""grand"", ""grande*"", ""gratef*"", 
        ""grati*"", ""grave*"", ""great"", ""grief"", ""griev*"", ""grim*"", ""grin"", 
        ""grinn*"", ""grins"", ""grouch*"", ""grr*"", ""guilt*"", ""ha"", ""haha*"", 
        ""handsom*"", ""happi*"", ""happy"", ""harass*"", ""hated"", ""hateful*"", 
        ""hater*"", ""hates"", ""hating"", ""hatred"", ""hazy"", ""heartbreak*"", 
        ""heartbroke*"", ""heartfelt"", ""heartless*"", ""heartwarm*"", ""heh*"", 
        ""hellish"", ""helper*"", ""helpful*"", ""helping"", ""helpless*"", ""helps"", 
        ""hesita*"", ""hilarious"", ""hoho*"", ""homesick*"", ""honour*"", ""hope"", 
        ""hoped"", ""hopeful"", ""hopefully"", ""hopefulness"", ""hopeless*"", 
        ""hopes"", ""hoping"", ""horr*"", ""hostil*"", ""hug"", ""hugg*"", ""hugs"", 
        ""humiliat*"", ""humor*"", ""humour*"", ""hurra*"", ""idiot"", ""ignor*"", 
        ""impatien*"", ""impersonal"", ""impolite*"", ""importan*"", ""impress*"", 
        ""improve*"", ""improving"", ""inadequa*"", ""incentive*"", ""indecis*"", 
        ""ineffect*"", ""inferior*"", ""inhib*"", ""innocen*"", ""insecur*"", ""insincer*"", 
        ""inspir*"", ""insult*"", ""intell*"", ""interest*"", ""interrup*"", ""intimidat*"", 
        ""invigor*"", ""irrational*"", ""irrita*"", ""isolat*"", ""jaded"", ""jealous*"", 
        ""jerk"", ""jerked"", ""jerks"", ""joke*"", ""joking"", ""joll*"", ""joy*"", 
        ""keen*"", ""kidding"", ""kind"", ""kindly"", ""kindn*"", ""kiss*"", ""laidback"", 
        ""lame*"", ""laugh*"", ""lazie*"", ""lazy"", ""liabilit*"", ""libert*"", 
        ""lied"", ""lies"", ""like"", ""likeab*"", ""liked"", ""likes"", ""liking"", 
        ""livel*"", ""LMAO"", ""LOL"", ""lone*"", ""longing*"", ""lose"", ""loser*"", 
        ""loses"", ""losing"", ""loss*"", ""lost"", ""lous*"", ""love"", ""loved"", 
        ""lovely"", ""lover*"", ""loves"", ""loving*"", ""low*"", ""luck"", ""lucked"", 
        ""lucki*"", ""luckless*"", ""lucks"", ""lucky"", ""ludicrous*"", ""lying"", 
        ""mad"", ""maddening"", ""madder"", ""maddest"", ""madly"", ""magnific*"", 
        ""maniac*"", ""masochis*"", ""melanchol*"", ""merit*"", ""merr*"", ""mess"", 
        ""messy"", ""miser*"", ""miss"", ""missed"", ""misses"", ""missing"", ""mistak*"", 
        ""mock"", ""mocked"", ""mocker*"", ""mocking"", ""mocks"", ""molest*"", ""mooch*"", 
        ""mood"", ""moodi*"", ""moods"", ""moody"", ""moron*"", ""mourn*"", ""nag*"", 
        ""nast*"", ""neat*"", ""needy"", ""neglect*"", ""nerd*"", ""nervous*"", ""neurotic*"", 
        ""nice*"", ""numb*"", ""nurtur*"", ""obnoxious*"", ""obsess*"", ""offence*"", 
        ""offens*"", ""ok"", ""okay"", ""okays"", ""oks"", ""openminded*"", ""openness"", 
        ""opportun*"", ""optimal*"", ""optimi*"", ""original"", ""outgoing"", ""outrag*"", 
        ""overwhelm*"", ""pained"", ""painf*"", ""paining"", ""painl*"", ""pains"", 
        ""palatabl*"", ""panic*"", ""paradise"", ""paranoi*"", ""partie*"", ""party*"", 
        ""passion*"", ""pathetic*"", ""peculiar*"", ""perfect*"", ""personal"", 
        ""perver*"", ""pessimis*"", ""petrif*"", ""pettie*"", ""petty*"", ""phobi*"", 
        ""piss*"", ""piti*"", ""pity*"", ""play"", ""played"", ""playful*"", ""playing"", 
        ""plays"", ""pleasant*"", ""please*"", ""pleasing"", ""pleasur*"", ""poison*"", 
        ""popular*"", ""positiv*"", ""prais*"", ""precious*"", ""pressur*"", ""prettie*"", 
        ""pretty"", ""prick*"", ""pride"", ""privileg*"", ""prize*"", ""problem*"", 
        ""profit*"", ""promis*"", ""protested"", ""protesting"", ""proud*"", ""puk*"", 
        ""radian*"", ""rage*"", ""raging"", ""rancid*"", ""rape*"", ""raping"", ""rapist*"", 
        ""readiness"", ""ready"", ""reassur*"", ""reek*"", ""regret*"", ""reject*"", 
        ""relax*"", ""relief"", ""reliev*"", ""reluctan*"", ""remorse*"", ""repress*"", 
        ""resent*"", ""resign*"", ""resolv*"", ""restless*"", ""revigor*"", ""reward*"", 
        ""rich*"", ""ridicul*"", ""rigid*"", ""risk*"", ""ROFL"", ""romanc*"", ""romantic*"", 
        ""rotten"", ""rude*"", ""sad"", ""sadde*"", ""sadly"", ""sadness"", ""sarcas*"", 
        ""satisf*"", ""savage*"", ""scare*"", ""scaring"", ""scary"", ""sceptic*"", 
        ""scream*"", ""screw*"", ""selfish*"", ""sentimental*"", ""serious"", ""seriously"", 
        ""seriousness"", ""severe*"", ""shake*"", ""shaki*"", ""shaky"", ""share"", 
        ""shared"", ""shares"", ""sharing"", ""shit*"", ""shock*"", ""shook"", ""shy*"", 
        ""sigh"", ""sighed"", ""sighing"", ""sighs"", ""silli*"", ""silly"", ""sincer*"", 
        ""skeptic*"", ""smart*"", ""smil*"", ""smother*"", ""smug*"", ""snob*"", 
        ""sob"", ""sobbed"", ""sobbing"", ""sobs"", ""sociab*"", ""solemn*"", ""sorrow*"", 
        ""sorry"", ""soulmate*"", ""special"", ""splend*"", ""stammer*"", ""stank"", 
        ""startl*"", ""stink*"", ""strain*"", ""strange"", ""strength*"", ""stress*"", 
        ""strong*"", ""struggl*"", ""stubborn*"", ""stunk"", ""stunned"", ""stuns"", 
        ""stupid*"", ""stutter*"", ""succeed*"", ""success*"", ""suck"", ""sucked"", 
        ""sucker*"", ""sucks"", ""sucky"", ""sunnier"", ""sunniest"", ""sunny"", 
        ""sunshin*"", ""super"", ""superior*"", ""support"", ""supported"", ""supporter*"", 
        ""supporting"", ""supportive*"", ""supports"", ""suprem*"", ""sure*"", 
        ""surpris*"", ""suspicio*"", ""sweet"", ""sweetheart*"", ""sweetie*"", 
        ""sweetly"", ""sweetness*"", ""sweets"", ""talent*"", ""tantrum*"", ""tears"", 
        ""teas*"", ""tehe"", ""temper"", ""tempers"", ""tender*"", ""tense*"", ""tensing"", 
        ""tension*"", ""terribl*"", ""terrific*"", ""terrified"", ""terrifies"", 
        ""terrify"", ""terrifying"", ""terror*"", ""thank"", ""thanked"", ""thankf*"", 
        ""thanks"", ""thief"", ""thieve*"", ""thoughtful*"", ""threat*"", ""thrill*"", 
        ""ticked"", ""timid*"", ""toleran*"", ""tortur*"", ""tough*"", ""traged*"", 
        ""tragic*"", ""tranquil*"", ""trauma*"", ""treasur*"", ""treat"", ""trembl*"", 
        ""trick*"", ""trite"", ""triumph*"", ""trivi*"", ""troubl*"", ""TRUE"", ""trueness"", 
        ""truer"", ""truest"", ""truly"", ""trust*"", ""truth*"", ""turmoil"", ""ugh"", 
        ""ugl*"", ""unattractive"", ""uncertain*"", ""uncomfortabl*"", ""uncontrol*"", 
        ""uneas*"", ""unfortunate*"", ""unfriendly"", ""ungrateful*"", ""unhapp*"", 
        ""unimportant"", ""unimpress*"", ""unkind"", ""unlov*"", ""unpleasant"", 
        ""unprotected"", ""unsavo*"", ""unsuccessful*"", ""unsure*"", ""unwelcom*"", 
        ""upset*"", ""uptight*"", ""useful*"", ""useless*"", ""vain"", ""valuabl*"", 
        ""valuing"", ""vanity"", ""vicious*"", ""vigor*"", ""vigour*"", ""villain*"", 
        ""violat*"", ""virtuo*"", ""vital*"", ""vulnerab*"", ""vulture*"", ""warfare*"", 
        ""warm*"", ""warred"", ""weak*"", ""wealth*"", ""weapon*"", ""weep*"", ""weird*"", 
        ""welcom*"", ""well*"", ""wept"", ""whine*"", ""whining"", ""willing"", ""wimp*"", 
        ""win"", ""winn*"", ""wins"", ""wisdom"", ""wise*"", ""witch"", ""woe*"", ""won"", 
        ""wonderf*"", ""worr*"", ""worse*"", ""worship*"", ""worst"", ""wow*"", ""yay"", 
        ""yays"",""yearn*"",""stench*"")Code used from MRau's answer that hasn't worked for me:"
5400,removing prepositions from a text file in linux,"['linux', 'shell', 'sed', 'text-mining']",What I want to do is that i want to remove all prepositions in a text file in CentOS. Things like 'on of to the in at ....'. Here is my script:but at the end when i open newHam.txt nothing changes! It's the same as Ham.txt. I don't know whether this is a good approach or not. Any suggestion? Any approach??   
5401,Twitter retweets network in R,"['r', 'graph', 'package', 'text-mining']","I want to create retweets network - who retweets whom
my code is:However when I run it i receive number of errors:
Error in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  : 
  polygon edge not found
In addition: Warning messages:1: In grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x,
  x$y,  :  no font could be found for family ""Roboto Condensed"" 2: In
  grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :   no
  font could be found for family ""Roboto Condensed"" 3: In
  grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :   no
  font could be found for family ""Roboto Condensed"" 4: In
  grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :   no
  font could be found for family ""Roboto Condensed"" 5: In
  grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :   no
  font could be found for family ""Roboto Condensed""It may be Arial font problem but my font is installed and is up and running. What are the other possible reasons?How to fixt that? What about this error with ""Roboto Condensed""? I am using mac OS maybe this is OS related?Any ideas?"
5402,Add column which contains binned values of an integer column,"['r', 'r-faq']","I have a dataframe with a few columns, one of those columns is ranks, an integer between 1 and 20.  I want to create another column that contains a bin value like ""1-4"", ""5-10"", ""11-15"", ""16-20"".What is the most effective way to do this?the data frame that I have looks like this(.csv format):and I want to add another column to the dataframe, so it would be like this:The way I am doing it now is not working, as I would like to keep the data.frame intact, and just add another column if the value of df$ranked is within a given range. thank you."
5403,“object mycorpus could not found”,"['r', 'text-mining', 'word-cloud']","I'm trying to make wordcloud. But I'm getting error as ""Error in tm_map(myCorpus, content_transformer(tolower)) :    object
  'myCorpus' not found""I'm very new to R. Please suggest"
5404,How to find and replace certain keywords in a specific column of a data frame in R? [closed],"['r', 'dataframe', 'text-mining']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed last year.I want to find some specific keywords in a specific column of a data frame and replace them with other keywords that already exists in that column. For example technology (freq=2) with technologies (freq=3).
I need to do this without changing the rest of the columns in the data frame and save it in the same column in the same data  frame. In this way I can have 5 keywords of ""technologies"".
However, I have  no clue how to start doing this in rstudio specially because I have to keep the output as a data frame. Can you please guide me where to begin with?"
5405,Extract found word and 20 Words before and after it,"['r', 'text-mining']","I am using stringr to scan through a very long text. If the word is found. I want to extract not only the word, but some context, lets say twenty words before and after the word has been detected.So If I have ""Hello there, how are you?"" and I look for ""there"", I want to extract there +-1 Word:
""Hello there, how""However, I am having problems in combining str_locate and str_word since one expresses location as the character-number and the other works with the word-numberHow do I go about this? I know how to locate a word and I know how to extract words. But How do I extract words around a specific word?How do I change this code to tell word() that it needs to start from a specific word?"
5406,save a text of each row without merge,"['r', 'dataframe', 'text-mining']","I wrote a code in R 
To do some change in a text after count the number of character 
And if the number of character grater than 5 do the change 
And it is working here is my code the result As you can see it do the change but my problem is that It merge between the texts 
So If I have 50 rows then I do not know the text belong of which row 
Because at the end I need to save the change on original text The result I expect Thank you"
5407,count words with for loop in R,"['r', 'for-loop', 'text-mining', 'arabic']","I use R for text mining in Arabic language 
and I would like to check on words if it the word have more than 6 character do the some change and it is working but it return the first word only 
here is my codeand I think I need to add j++ 
but I don’t know where should I add it 
thank you"
5408,Error in gsub is too long with Arabic language in R,"['r', 'text-mining', 'gsub', 'analysis', 'data-cleaning']","I am doing text mining in R with Arabic language 
And use gsub function but I got an error as shown herehere is my codeanyone can help me ?"
5409,Complex regular expression getting less than expected,"['python', 'regex', 'text-mining']","I am trying to fiddle with a regex in Python 2.7 in order to catch numbered footnotes in a text. My text as converted from PDF looks like:Please note that numbered paragraphs that are the regular content of my text, are prefixed with a number and dot (like '5.').
 Ideally, I 'd like to get something like:My Python code for getting the footnotes is: which gives me:i.e. only the first footnote, while I need both off courseAny ideas are welcome!"
5410,How Search and Find Emojis on Sheets,"['regex', 'twitter', 'google-sheets', 'text-mining', 're2']","I'm working on a Sheets spreadsheet with thousands of tweets, and I need to find a regex or a formula that allows me to tell whether a cell contains emojis.I've found the below regex on a previous post, which does a great job at removing emojis; however, some emojis still make through (I guess because they are new) namely, 🤦‍️🤔🤣🤗🚌 and a few more. Could someone suggest an amendement to the expression?Thanks!=arrayformula(regexreplace(A2:A,""[\x{1F300}-\x{1F64F}]|[\x{2702}-\x{27B0}]|[\x{1F68}-\x{1F6C}]|[\x{1F30}-\x{1F70}]|[\x{2600}-\x{26ff}]|[\x{D83C}-\x{DBFF}\x{DC00}-\x{DFFF}]"",""""))"
5411,python group of words search implemented recursively. How to proceed?,"['python', 'recursion', 'scikit-learn', 'text-mining']","I have to look for concepts within texts. The concepts are expressed in the following way:""blue 5 house"" >>> would mean that I have to find the hits where the words blue and house appear within a distance of 5 or less words.
""little 3 cat"" would then mean finding the hits where the words little and cat appear within a distance of max 3 words. (i.e. ""little cat"", ""little annoying cat"" but not ""the cat of my grandmother is little"")I guess you get it.I have so far a (not very sophisticated) code as follows. I just implemented two nested loops that go over all the words of the text and when there is a hit of the first one start to look for the other one in the words around and adds the result to a list:So far it works ok.Imagine I am thinking about how to build on that in order to construct code something recursively that would give me the hits of:(little 3 cat) 4 third_word or even
concept1 5 concept2; where concept1=(""blue 3 cat"") and concept2=(""little 4 dollar"")???what should I think about? a class? is that already somehow contained in scikit-learn? More than a code (which I guess would be complicated) I am asking you for orientation. How to think about a problem solved with code recursively.ThanksNOTE 1: Please forget about the order ""little cat"" vs ""cat little"" thats another issue.NOTE 2: (after first answer) Be aware that this is a very simplified case, in reality I am looking at cases like this: ((concept1 n1 concept2) n2 concept 3)) n3 (concept1 n4 concept 5)"
5412,How can compare a string with a text group with PHP,"['php', 'regex', 'text-mining', 'text-manipulation']","I have a string group like (""hello"", ""hi"", ""how r u"", ""how are you"", ""how r you"", ""how are u"", etc.) and i want to create a function that can compare a string varible (like $varible = ""Helloooo"") with string group. Regex or any method is useful to me.String can be more but can not be missing for example:Hello = should be true Helloooo = should be true How r youuu !!!! = should be truehell = should be falsew are y = should be falseheey hello = should be trueheeeey hello bro = should be trueI'm talking about this string group (""hello"", ""hi"", ""how r u"", ""how are you"", ""how r you"", ""how are u"", etc.)I say ""string group"" because type doesn't have to be an array but it also may be an array. As i said any method is useful to me.Thanks for you support."
5413,How to do K-NN on Bag of words,"['python', 'text-mining']","I have a training and test set (equal in size). I have done the bag of words model and I am trying to do K-nearest neighbor on it and I'm unsure how to do the fit. Bag of words model: Trying to do KNN on the Bag of Words model and I'm unsure what i'm supposed to put in the ""knn.fit"" portion"
5414,R speed up sapply,"['r', 'performance', 'apply', 'text-mining']","I've got the following script in a loop:The problem is that it slows down considerably the loop.The data looks like this:distinct_similar_addresses:similar_addresses:The script is assessing if the address is referring to a unit or a single house.
Is there any way to perform this task faster?I'm adding a result set and an explaination so that what it does become more understandable.Result set:The code is just counting the number of names associated to a single row of address.
Indeed if the address is repeated it means that it's referring to a unit otherwise it's a single house."
5415,Remove non-English observations from dataframe,"['r', 'text', 'twitter', 'text-mining']","I have a dataframe with Twitter data. I have cleaned the Tweet text and added it as a vector, clean_text, but there are numerous observations in a non-English language that affect my text analysis. How do I remove all observations in the dataframe that are not written in English?Here is a reproducible sample of my dataframe, BrexitTweets."
5416,Unable to visualize word count plot using ggplot2 [closed],"['r', 'ggplot2', 'text-mining']","
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                Closed 2 years ago.I have a tidy document term matrix that I have successfully done sentiment analysis on and am now trying to see which words most often contribute to a positive or negative sentiment by plotting the words using ggplot2. I have this so far:But keep getting error code:
Error in count(., sentiment, term, wt = count) : unused argument (term).Does anyone have any ideas on why this error is occurring?Thanks!"
5417,Python and Regex to convert wrtitten numbers to numeric,"['python', 'regex', 'replace', 'nlp', 'text-mining']","I am trying to convert written numbers to numeric values. For example, to extract millions from this string:To:I use this function to remove any separators in the numbers first:And I have written this regex to findall of the possible patterns for common Million notations. This 1) finds digits and does a look ahead for 2) common notation for millions, 3) The ""[a-z]?"" part is to handle optional ""s"" on million or millions where I have already removed ""'"".which correctly matches Million numbers and returns:What I need to do now is to write a replacement pattern to insert ""000000"" after the digits, or to iterate through and multiply the digits by 100000. I have tried this so far:which returns:I think I need to do a look behind (?<=), however I haven't worked with this before and after several attempts I cant seem to work it through. FYI: My plan is to tackle ""Millions"" first and then to replicate the solution for Thousands (K), Billions (B), Trillions (T) and possibly for other units such as distances, currencies etc. I have searched SO and google for any solutions in NLP, text cleaning and mining articles but did not find anything. "
5418,wrong result in POS-tagging functions for returning verbs of text,"['r', 'function', 'nlp', 'text-mining', 'opennlp']","I have the following functions for returning the verbs in the text separated by ""|"". Does anyone know whats the problem with the second function which brings the wrong result?and now how it works: whereas it should return:"
5419,SMS text mining,"['nlp', 'sms', 'text-mining']","I am trying to extract numeric information from set of SMSs. The regexes fail in extracting balance and credit amounts as the patterns of the SMS is not consistent throughout the industry.
We are currently making assumptions to make it work like First Amount = Credit amount
Second Amount=Balance.
This has lot of limitations and error rate is gradually increasing.
Anyone has any alternatives to regexes?"
5420,Why would rows in document-term matrix have only zeros (wrt topic modeling in R using LDA function),"['r', 'text-mining']","I am trying to implement topic modeling on a corpus of tweet. For this, I am using the LDA() function of the topicmodels package.The document-term matrix on which I want to run the function is the following:When I try to use the function with the following code lda <- LDA(dtm, k = 5),
I get this error:This presumably means that some of the rows in dtm consist only of 0s. However, how can that be? Wouldn't that mean the corresponding document is empty (has no words)? The only processing I did on the corpus of tweets before forming the term-document and document-term matrices was to remove punctuation, any numbers, stopwords (using the SMART list and a few of my own), and extra whitespaces. It is unlikely that a document (tweet) would have consisted only of the items removed. Therefore, why do some (many?) of the rows in dtm have only zeros? Also, I am unable to clearly understand the significance and the underlying concept of the term ""sparsity"" and what a sparsity of 100% implies. Where should I look for an explanation? "
5421,Detecting similarity between non English UTF-8 texts in a large database (similar to SOUNDEX),"['mysql', 'sql', 'algorithm', 'text-mining', 'fuzzy-comparison']","I am having a problem in text-mining preprocessing and found  SOUNDEX() in mysql and in my understanding, these algorithm work well in English words.However, I am searching for such tools to work in the non-English languages,  such as Arabic and Persian alphabets in the database side and in an efficient process I don't want to check similarities in a loop. Is there any solution to find how similar multiple texts in a big database and capable of handling these alphabets completely?It is not necessary that the solution is related to AI or machine-learning, but I want to know how much and how many of the texts are related.Thanks in advanced."
5422,Regular Expression in R gives me TRUE for every input,"['r', 'regex', 'text-mining']","this is my code:I have a large vector with text, where i want to seperate each word to calculate sentiment scores. I only want to match only exact strings, which i managed to do with \\b.However, some texts matches the whole searchvector as you can see. I was not able to figure out why that is the case. Can anyone explain me what goes wrong here?"
5423,How to convert 2 column matrix to dataframe table,"['r', 'text-mining', 'word-cloud']",I want to create a bigram wordcloud in R with package tau.I got bigrams in a list as numeric. so I converted it to matrix but its without column name. I want it in a dataframe table so that I can create a bigram WordCloud with it.Please find my code below and suggest a way out.Please suggest a way on how can I create a wordcloud on bigram. unable to do with weka package      
5424,How will the document number affect the result of Gensim LDA?,"['python-3.x', 'text-mining', 'gensim', 'lda', 'topic-modeling']","I use three txt file to do a LDA project
I try to separate these three txt file with two way 
The difference among the process is:And the document number isBut the lda model create a rubbish result in corpus but a relatively good result in corpus1I use this model to train the documentThe difference in the two model is the document number, everything else is the sameWhy LDA create such a different result in this two model?"
5425,R splits up text columns when importing SPSS files,"['r', 'string', 'text-mining', 'spss']","I'm using the memisc package to read in an SPSS file with spss.system.file(""filename.sav""). One of the columns has a lot of text and is split up into a total of nine columns with a maximum of 255 characters each.I would like R not to split up the column. How can I do this?"
5426,How to apply KMeans clustering on PDF Data using python?,"['python', 'scikit-learn', 'k-means', 'text-mining', 'unsupervised-learning']",With reference to this topic: How to convert token list into wordnet lemma list using nltk?I want to show the words with similar meaning in a cluster diagram. I went through some of the methods and found KMeans is a good start to learn. I am using tf-idf vectorizer to convert my pdf data to vectorized version but ended up getting this error:Here's my updated source code: (designed to work on any pdf data source)
5427,Text mining with Python and pandas,"['python', 'pandas', 'text-mining']","this maybe is a duplicate, but I had no luck finding it...I am working on some text mining in Python with Pandas. I have words in a DataFrame and the Porter stemming next to it with some other statistics. This means similar words having exact same Porter stem can be found in this DataFrame. I would like to aggregate these similar words in a new column then drop the duplicates regarding Porter stem.What I would love to have:After removing duplicates:I tried to use, but I came no closer to my goals.Thank you for any help in advance.EDIT: code above revised"
5428,Splitting a string using a pattern in R,"['r', 'regex', 'text-mining', 'strsplit']","This question is building on my previous question regarding Splitting and grouping plain text (grouping text by chapter in dataframe)?With Shree's help I've been able to get most of my document cleaned up! Have been able to create two column from a list - the first column is chapter number and the second column is the text that belongs to that chapter, but I ran into some messier text.This is a worst case scenario example of my data:I need to get it structured like this (Chapter number and then chapter text for that chapter in ID order), so that I can apply the function from my previous post and split it cleanly:This seems like a straightforward problem where I could split the string using regex looking for Chapter # (""Chapter [0-9]"") and then split it again with similar logic to get the chapter and the text into separate rows. However, I'm stuck here after trying many attempts with str_split, gsub, separate_rows functions.Any help is appreciated."
5429,R Creating co-occurrence matrix,"['r', 'matrix', 'text', 'text-mining', 'adjacency-matrix']","My question is about text mining, and text processing.
I would like to build a co-occurrence matrix from my data.
My data is:Thanks in advance :)"
5430,TypeError: test() missing 1 required positional argument,"['python', 'pycharm', 'pytest', 'text-mining', 'nose']","I want to predict a score for each sentence in a text. I have written this test method:In a part of main method, I have:But I received this error:Error Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 59, in testPartExecutor
      yield   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 601, in run
      testMethod()   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) Exception: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging << -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging << ---------------------E
  ====================================================================== ERROR: mahsa_rnn_sent_classification.test
  ---------------------------------------------------------------------- Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) TypeError: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging << -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging << ------------------------------------------------------------------------------------------- Ran 1 test in 0.004sFAILED (errors=1)I think that test() has its argument 'sent'. How can I correct this error?"
5431,R script - PDF error: Illegal character in hex string; when I am searching for keywords,"['r', 'pdf', 'text-mining']","I am trying to count the number of keywords in multiple pdf files. When I run the code I always get this errors:If you have any suggestions, please let me know. Thank you!"
5432,Splitting and grouping plain text (grouping text by chapter in dataframe)?,"['r', 'nlp', 'text-mining', 'tidytext']","I have a data frame/tibble where I've imported a file of plain text (txt). The text very consistent and is grouped by chapter. Sometimes the chapter text is only one row, sometimes it's multiple row. Data is in one column like this:I'm trying to clean the data to have a new column for Chapter and the text from each chapter in another column, like this:I've been trying to use regex to split the and group the data at each occurance of the word 'Chapter #' (chapter followed by a number, but cannot get the result I want. Any advice is much appreciated."
5433,Remove all punctuation from text including apostrophes for tm package,"['r', 'text-mining', 'tm']","I have a of vector consisting of Tweets (just the message text) that I am cleaning for text mining purposes. I have used removePunctuation from the tm package like so: This have resulted in a vector with all punctuation removed from the text except apostrophes, which ruins my keyword searches because words touching apostrophes are not registered. For example, one of my keywords is climate but if a tweet has 'climate it won't be counted.How can I removes all the apostrophes/single quotes from my vector?Here is the header from dput for a reproducible example:"
5434,preprocessing text with oracle sql,"['sql', 'regex', 'text-mining', 'data-cleaning']","Again i will ask for your help in order to achieve some works with strings in an oracle database in order to make some preprocessing(before text mining).
So i have strings with that form in a varchar2 type table:
""#PROTESTOBR: Protestos em várias cidades brasileiras são destaque nos principais sites internacionais! O mundo começa a conhecer o Brasil de verdade! #vemprarua #vempraruarj #vempraruabrasil #ogiganteacordou #geracaoinvencivel  @J.Oliveira  Curta a Página Geração Invencível: http://www.facebook.com/GeracaoInvencive""i want to recusirvely clean my strings cleaning them from # and urls. i'm investigating two ways:
1/ as i succeeded in extracting hashtags from my strings and exported them into another table i try to figure how to make a REPLACE like query taking as paramater the whole hashtags contained in my hashtags table. 
2/ just try to clean up my text with recursive CONNECT BY LEVEL clause. As i succeeded in extracting hahstags i try using my query with whom i succeeded in extracting my hashtags as a replace query:The problem using this way is that it s returning me (as expected) as lines as the times the query encountered the pattern((regexp_substr(""my_string"", '#\S+\s?')). Results are very pretty good but, as you can guess i only want one result ^^
How can i do that? What s your advices ? How would have you proceed taking into account that i have not one string to deal with, of source, but approximatively 1 million. 
Thanks a lot for your advices!
Thanks bis because this query shown above was found thanks to stackoverflow examples."
5435,Find nonsense words in a text,"['r', 'text-mining', 'stringr', 'stringi']","I have a dataset with answers of user if they know a brand or not. Some of the users just answered nonsense, as you can see in my example. Output:My function does not work too bad, however there might be better solutions. Is there a package or better method to identify if a word was just misspelled or a person answered nonsense. 
If not, suggestions to improve that function are highly appreciated! edit: Updated some improvements :-)"
5436,Transliterate The Paragraph from Indian regional language to English,"['python', 'google-api', 'text-mining', 'transliteration']","I want translit(Transliteration) paragraph from Hindi(any regional language) to English, using python.ex.I have checked google API but it only supports javascript and HTML, where you type text using any source language and your text gets transliterated in your expected language. 
I want any rest-full cloud API which takes the text in any Indian regional language and transliterates in English, and that can be integrated with python."
5437,Extracting speaker interventions from a text using R? Or something else?,"['r', 'text-mining']","We're working on a text mining project for school on the proportion of environment-oriented speech in Quebec's National Assembly. We wanna extract a list of every speaker's interventions throughout the years.Our documents are all formatted this way:What I would like to do is write the simplest thing possible that would allow me to extract these interventions. I'm thinking something along the lines of: ""Every time you see [Mr. **** : ] OR [Mrs. **** : ], extract ALL the text until you see another occurrence of [Mr. **** : ] OR [Mrs. **** : ]. And, ideally, extract all the Mr. Smiths and the Mrs. Joneses and the Mr. Williams in separate files while keeping track of which file the interventions came from.I started writing a very basic gsub line which allowed me to replace the occurrences I wanted to replace with an @, only to realize I don't want to replace them completely but rather maybe just add an @ in front which would probably make it easier to write something that would just separate the @s in distinct files.I've just started teaching myself R for this project and I need some insight on how I should proceed next. Or should I use something else instead?"
5438,Keep special characters in a word-frequency matrix,"['r', 'text-mining', 'stringr', 'qdap']","I analyze some brands in text to find out KPI´s like Ad recognition. However brands which contain special characters are destroyed by my code so far.This is the output:Is there a package or method to archieve that H&M gets h&m, but not ""h"" and ""m"", like its two brands?edit: The wfm function has got a ... argument which SHOULD allow me to use the strip function.Does not work unfortunately."
5439,misaligned table from txt file to panda dataframe,"['python', 'text-mining']","I am trying to put a table from a text file to dataframe. The text file was created from a pdf.I am relatively new to python. I have a function that can handle a particular format of a text table. For example my code can handle when the second column only has MU../All uses/All/MU and no other text and the fourth column is smaller in length than the fifth. It can also handle if the first column has LM../(LM..)/3333(only digits) but without indentation.But this seems to be a little too hard for me. Because the second column could have sentences and words other than mentioned before and in one row the fourth column is longer than fifth and in another the first column has indentation. I am stuck with this.The final dataframe should have 5 columns  and 3 rows as belowCan anyone please help me with this? Note: 'text..' can be any sentence/multiple lines of sentences, words, numeric or special characters. I have been trying out all solutions posted on stackoverflow, tried tabula-py, pypdf2, pdfminer. Nothing seems to help. Any help would be appreciated."
5440,Details behind “augment” when applied to topic modeling,"['r', 'text-mining', 'lda', 'topic-modeling', 'tidytext']","I have a question on ""augment"" function from Silge and Robinson's ""Text Mining with R: A Tidy Approach"" textbook. Having run an LDA on a corpus, I am applying the ""augment"" to assign topics to each word.I get the results, but am not sure what takes place ""under the hood"" behind ""augment"", i.e. how the topic for each word is being determined using the Bayesian framework. Is it just based on conditional probability formula, and estimated after LDA is fit using p(topic|word)=p(word|topic)*p(topic)/p(word)?I will appreciate if someone could please provide statistical details on how ""augment"" does this. Could you also please provide references to papers where this is documented."
5441,How to quickly apply over Document Term Matrix in R,"['r', 'text-mining']","I am working on a project that requires me to iterate over a Document Term Matrix, converting all non-zero values to 1 and keeping zero values at zero. The function I'm using now takes forever to run, and I would like help optimizing the code.My code as it is right now isWhere data_dtm is a large Document Term Matrix."
5442,Trying to remove special characters and non-english words from my data R,"['r', 'text-mining', 'tm', 'quanteda']","I am trying to clean up my data to remove; i.) special characters (e.g
+_),  ii.) specific words (e.g retweet, followers, couldn, better, person) iii.) words that do not appear in the english dictionary I am using the quanteda library. My objective is to get the top 50 bigrams and plot them on a graph.This is output from my code:I have tried usingto remove special characters, but that seems to remove all characters - output is numeric(0)"
5443,Extract rows from dataframe that have keywords in them (Twitter data in RStudio),"['twitter', 'rstudio', 'extract', 'keyword', 'text-mining']","I have a large dataframe (~500,000 observations) consisting of structured Twitter data (i.e. username, rewtweet counts, text) in RStudio. I want to run a text analysis on the tweets so I can extract observations that have one or more keywords in the tweet text.I have uploaded my keywords as keywords_C <- c(""climate change"",""climate"",""climatechange"",""global warming"",""globalwarming"") . Tweet text is stored in my dataframe in a column labelled text. How do I make a new dataframe containing only observations where one or more of the keywords are present in the text column? Alternatively, can I delete observations where the keywords are not present? My dataframe is called NewCDatadput(droplevels(head(NewCData, 10)))"
5444,Removing string pattern from dataframe (Twitter data in RStudio),"['string', 'pattern-matching', 'rstudio', 'lapply', 'text-mining']","I have a large dataframe (~500,000 observations) consisting of Twitter data (i.e. username, rewtweet counts, text) in RStudio. I want to run a text analysis on the tweets, but I first need to remove retweet tags so they don't affect my keyword searches. For example, in tweets that are retweets, the text looks like this: RT @BobsAccount Great article! Can't wait to learn more. I want to remove the string attached to RT @.....   I have used lapply and gsub to remove specific characters. For example, this successfully removed ""@"" : data <- data.frame(lapply(data, function(x) {gsub(""@"","""", x)}))But I can't figure out how to remove a ""string pattern"" (i.e. any text attached to ""RT @""). Any help would be greatly appreciated!"
5445,Python Regex - Extract text between (multiple) expressions in a textfile,"['python', 'regex', 'text-mining', 'text-extraction']","I am a Python beginner and would be very thankful if you could help me with my text extraction problem. I want to extract all text, which lies between two expressions in a textfile (the beginning and end of a letter). For both, the beginning and the end of the letter there are multiple possible expressions (defined in the lists ""letter_begin"" and ""letter_end"", e.g. ""Dear"", ""to our"", etc.). I want to analyze this for a bunch of files, find below an example of how such a textfile looks like -> I want to extract all text starting from ""Dear"" till ""Douglas"". In cases where the ""letter_end"" has no match, i.e. no letter_end expression is found, the output should start from the letter_beginning and end at the very end of the text file to be analyzed.Edit: the end of ""the recorded text"" has to be after the match of ""letter_end"" and before the first line with 20 characters or more (as is the case for ""Random text here as well"" -> len=24.This is my code so far - but it is not able to flexible catch the text between the expressions (there can be anything (lines, text, numbers, signs, etc.) before the ""letter_begin"" and after the ""letter_end"")I am very thankful for every help!"
5446,"r dplyr text mining Error in eval(rhs, env, env) : object 'score' not found","['r', 'function', 'dplyr', 'text-mining']",I'm currently working on an R project and I was defining a function that would perform text mining on a specific dataset. The general idea is to have a function that counts the number of the text mined and multiply that number with each text's score. So far I've defined the function with : I am trying to calculate the score by multiplying the number of word appearances with its weightage but I got an error: Does this mean I have to define score as a function variable? Because it is a column within the dictionary. Would greatly appreciate any help and insight into this issue. Thank you!
5447,Extract text between any combination of expressions (list),"['python', 'regex', 'text-mining', 'text-extraction']","I need to extract text between two expressions (beginning & end) from a textfile (the beginning and the end of a letter, which is embedded in a larger file). The problem that I face is that there are multiple potential expressions for both, the beginning and the end of the letter.I have a list of expressions, which potentially qualify as beginning / end expressions. I need to extract all text between any combination of those expressions from a larger text (including beginning and end expression) and write it to a new file. My code so far:The above example includes ""Dear"" as letter_begin expression and ""Sincerly"" as letter_end expression. I need to have a flexible code, which is able to catch any beginning and ending letter expression from the above lists (any potential combination of the expressions; e.g. ""Dear [...] rest regards"" or ""Estimated [...] Sincerly"")"
5448,unable to create term document matrix in python(jupyter),"['python', 'text-mining']","While running the following code in jupyter to create term document matrix, I'm getting an error saying nameerror: name 'textmining' not defined.The code is as below:I checked whether textmining function is installed or not by running this code:and after running, the output is:Requirement already satisfied: textmining in c:\users\asus\anaconda3\lib\site-packages (1.0)Requirement already satisfied: stemming in c:\users\asus\anaconda3\lib\site-packages (1.0.1)What should I do counter the name error occurring during the creation of term document matrix? Is there any alternate way of creating this term document matrix?"
5449,Comparison Fuzzy R,"['r', 'dataframe', 'data-mining', 'data-science', 'text-mining']","I have two datasets, the dataset df1 has a column with the names of companies registered in our CRM and another column with the name of the sales manager. Dataset df2 has a column with the names of companies that have visited an IT event.The dataset df2, because it was manually entered by the participants, was written with spelling mistakes, abbreviations, etc. That is, similar names for the names of companies registered in CRM.So the goal is to compare the names of the companies that visited the event in dataset df2, with the names of the companies registered in the dataset df1 and assign these comparisons to the sales manager. Of course, names that are not found or that have a very distant comparison should have the NA value for the salesperson.I'm new to R and I'm trying various things with little success.Could you help me build this script?Below is the example:The final result expected is to have another df with crossed data."
5450,How to get information with python when data is heavily nested,"['python', 'parsing', 'text', 'text-mining']",I have a text file which contains some data to be mined. The structure is shown belowIn some person's record the subfield is absent and output should specify subfield to be unknown in that case. Now below is the code I use to extract the dataThe output is shown belowThe problem has been that I don't know where to print data so current I am using below statment to print data which is wrong       I would appreciate if someone could give any suggestion to retrieve the data correctly
5451,Fuzzy Matching/Join Two Data Frames of University Names [duplicate],"['r', 'merge', 'text-mining', 'fuzzy', 'fuzzyjoin']","I have a list of university names input with spelling errors and inconsistencies. I need to match them against an official list of university names to link my data together. I know fuzzy matching/join is my way to go, but I'm a bit lost on the correct method. Any help would be greatly appreciated. And I desire an output that has them merged together as closely as possible"
5452,How to remove urls without http in a text document using r,"['r', 'regex', 'text-mining']","I am trying to remove urls that may or may not start with http/https from a large text file, which I saved in urldoc in R. The url may start like tinyurl.com/ydyzzlkk or aclj.us/2y6dQKw or pic.twitter.com/ZH08wej40K. Basically I want to remove data before a '/' after finding the space and after a ""/"" until I find a space. I tried with many patterns and searched many places. Couldn't complete the task. I would help me a lot if you could give some input. This is the last statement I tried and got stuck for the above problem. 
urldoc = gsub(""?[a-z]+\..\/.[\s]$"","""", urldoc)Input would be: A disgrace to his profession. pic.twitter.com/ZH08wej40K In a major victory for religious liberty, the Admin. has eviscerated institution continuing this path. goo.gl/YmNELW nothing like the admin. proposal: tinyurl.com/ydyzzlkkOutput I am expecting is: A disgrace to his profession.  In a major victory for religious liberty, the Admin. has eviscerated institution continuing this path.  nothing like the admin. proposal: Thanks."
5453,Reference DTM row by document number and save to vector/matrix,"['r', 'text-mining']","How do I reference a particular row in a DTM by document number? In addition, how do I save this row as a vector/matrix?.Please see output of DTM below, when calling inspect(test)"
5454,R - How to simplify this text clean-up of special characters?,"['r', 'text', 'replace', 'text-mining', 'tm']","I suspect there is a way to simplify this text pre-preprocessing. However, I could not find a solution how to merge all these character replacements into a single row. Hence, to avoid all the repetition in my current solution (see below):Does anyone know how I can simplify this?Thanks!"
5455,How to resolve the error of processing multiple pdf files in R script,"['r', 'pdf', 'text-mining']","I tried and getting this Error while processing multiple pdf files:I also tried by applying for loop:but getting same above mentioned error.
How can I fix this error or work around it?"
5456,Unable to detect a unicode in R,"['r', 'string', 'nlp', 'text-mining']","In R we are trying to detect check boxes and checked boxes. The complete PDF is read through pdftools package and stored in the form of dataframe. The check boxes are stored in the form of ""U+F0A8"" character (removed < & > sign enclosing the character ""U+F0A8""as it's not visible with < & > signs ) While performing string detect or gref functions or just printing it these characters are not detected or printed. Kindly help. I have attached the screenshot for reference.Please let me know if you need more details. Thanks in advance."
5457,Subset/select from a DFM using a dictionary in quanteda,"['r', 'text-mining', 'quanteda']","I have a corpus of texts from various countries. I am trying to see how often a specific term appears in the texts for each country. To do so, I am following the example here: https://quanteda.io/articles/pkgdown/examples/plotting.html#frequency-plotsThis works fine, except that this only captures the exact term (""constitution""). I'd like to be able to capture variations of the term (e.g. ""charter of rights and freedoms"") use globs (e.g. ""*constitution*""), and count the results under the same category. I tried using a dictionary for this, but I get zero results.How can I go about achieving this?"
5458,Cosine similarity of Documents,"['r', 'text-mining', 'cosine-similarity']","Data format CSVTotal number of documents 500. number of fields 10.view of data  i want to calculate parallel cosine similarity  of Each ""Docs"" with all 500 documents, expected out put "
5459,Detecting references to tables and images in text.,"['parsing', 'nlp', 'text-mining', 'text-processing', 'text-parsing']","I am building a domain specific Q&A system. I wanted to detect whether a paragraph contains reference to a table or image or list in that section, or some other section, basically detect phrases similar to 'refer to table below...', 'see image...', 'to switch on the monitor, follow steps....' and so on."
5460,twitteR in R can't detect/specify the geocode,"['r', 'twitter', 'text-mining']","I intended to get a twitter data related to 'education' (using keyword #education as my hashtag). Yet, I want to retrieve data about it by specific geo-location too (e.g London, Tokyo). So, I type this code on R:But, I'm getting the result like this (in every code):Warning message: In doRppAPICall(""search/tweets"", n, params = params,
  retryOnRateLimit = retryOnRateLimit,  : 10000 tweets were requested
  but the API can only return 2785P.s: longitude, latitude were NA and all the tweets from A & B were the same.
     it seem twiter doesn't differ its location.How can I resolve this? Please guide me..."
5461,Remove any text inside square brackets in r,"['r', 'regex', 'text-mining', 'square-bracket']","I would like to remove all the words inside square brackets as well as the brackets themselves. For example,Like above, the length of words inside the bracket are not constant. So I need a function that can identify the position of [ and ] in order to erase all the words, numbers and symbols in between. Is there any function able to do that?  "
5462,substituting several ngrams in quanteda,"['r', 'text-mining', 'quanteda']","In my text of news articles I would like to convert several different ngrams that refer to the same political party to an acronym.  I would like to do this because I would like to avoid any sentiment dictionaries confusing the words in the party's name (Liberal Party) with the same word in different contexts (liberal helping).I can do this below with str_replace_all and I know about the token_compound() function in quanteda, but it doesn't seem to do exactly what I need.Should I somehow just preprocess the text before turning it into a corpus? Or is there a way to do this after turning it into a corpus in quanteda. Here is some expanded sample code that specifies the problem a little better:This example counts democratic in both the new democratic and the democratic sense, but I would those counted separately. "
5463,How to decode web scrapped data..?,"['python', 'web-scraping', 'text-mining']",After running above web scrapper code i get following code instead of original text output..My Question is how to convert this ascii code into text.Here it is webscrappper data.:-
5464,What R package can I use for counting the occurrence of unique strings in an array,"['r', 'text-mining', 'tm', 'word-count']","I need to count the occurrence of unique words in a series of words and assign a value of 1 when a new word appears in the series. I wonder what R package or function could do that.Thanks,
Chamil"
5465,'Word2Vec' object has no attribute 'index2word',"['python-3.x', 'text-mining', 'word2vec']","I'm getting this error ""AttributeError: 'Word2Vec' object has no attribute 'index2word'"" in following code in python. Anyone knows how can I solve it?
Acctually ""tfidf_weighted_averaged_word_vectorizer"" throws the error. ""obli.csv"" contains line of sentences. 
Thank you."
5466,Match Sentences in R,"['r', 'dataframe', 'pattern-matching', 'text-mining', 'string-matching']","I have 2 tables. Table1 is a smaller table with around 10K values.
Table 1 (Sample):Table 2 contains 1 Million Values.Table 2 (Sample):Now I want to take all words from table 1 and look into table 2 an find the best match. The order of words should not have a big influence in the match i.e ""Ripe yellow Banana"" should match perfectly with ""Yellow Ripe Banana"". ""Buy Samsung Mobile"" should match with ""Samsung Mobile"" and with ""Samsung S6"". The final output should look like this.Table 3:Would really appreciate if we can Stem and Tokenize the sentence before doing a match.I have tried the following but its not working properly and the loop takes quite a bit of time."
5467,R Term frequency from large document set,"['r', 'sorting', 'text-mining']","I have a data frame like thisI need to get the frequency by id for each word in the content which is space separated. Which is basically finding the unique terms in the column and finding the frequency and display grouped by IdI tried This gives the ungrouped solution which I'm trying to group now, but I have over 20000 unique values in content, is there an efficient way to do this?"
5468,Creating a DTM on Alteryx Designer,"['azure', 'cluster-analysis', 'text-mining', 'alteryx']","I am new to Alteryx and am trying to use it for analysing unstructured data. I have a column of description in text form and I intend to use the K-Means Clustering tool for topic modelling. For K-means to work on text, I will need to convert my text into a Document Term Matrix (DTM) so that they appear as continuous variables to the clustering tool. However, I am struggling to find a way I can convert my text to a DTM.Does anyone know a way to do so? I am currently looking at the R tool but am not exactly sure how to start too. Hoping that all of you experts here can help me out!I have looked through posts on text analysis and realized that most fell back on the Microsoft Azure ML Text Analysis Macro. However, I would like to avoid using the macro (to not be restricted to limited runs every month for scalability) and instead use tools that are available in Alteryx.Thanks to everyone in advance!"
5469,Cropping PDF files cannot crop out text for text extraction (textract and pdfminer),"['python', 'text-mining', 'pypdf2', 'pdfminer']","I’m using the python library PyPDF2 to crop many PDF files to cut out the useless information on top and bottom of academic papers (i.e. page numbers and journal information at the bottom). Then I used the library textract to extract the texts from the cropped PDF files to txt files. However, the output txt files still contains the cropped out information despite the cropping.
This also applies to pdfminer, another text extraction library (not OCR). It seems that for text extraction, as opposed to OCR, the text cannot be eliminated by simply cropping. Can anyone explain why this is the case? Any idea on how else to eliminate useless information in PDF files for text extraction?"
5470,Python Sorting a docx file,"['python', 'text-mining']","I have a custom docx file dictionary where the words to be defined are on uppercase and bold. That is the only distinction from other words. The definitions are often large and with lines between, therefor it creates a new paragraph. As Sample:AASDFG this means blah. 
  Blah comes from the aakakIt is still the same definition for ASDFGMUG meaning of mugABBA musical groupI want to separate the paragraphs correctly to sort by the word to define (Upper case word) alphabetically. This is what I haveDesired OutputAABBA definitionASDFG definitionMMUG definitionThank you!"
5471,R: Converting “special” letters into UTF-8?,"['r', 'utf-8', 'character', 'string-formatting']","I run into problems matching tables where one dataframe contains special characters and the other doesn't. Example: Doña Ana County vs. Dona Ana CountyHere is a script where you can reproduce the outputs:Example:returns:Unfortunately, following queries return nothing:However, when opening the dataframe in R Studio, it shows:Question 1: Why does the second query give no return, though ""Do\xf1a Ana County"" appears in the database?Question 2: How can I convert all ""special"" characters such as ñ into n, or similar (UTF-8?)? Is there a library or snippet for that, or definition in the header, instead of defining rules for every character? I would have to do this anyways in order to match certain columns from both tables.Thank you! "
5472,word2vec for document classification,"['text-mining', 'word2vec']","I am learning how to apply word2vec for document classification, but I am struggling with two issues following:My dataset consists of users' commments; some comments have only one word (e. g. ""husgmabb"", or a HTTP link which I simply convert it as ""URL""). Can I apply word2vec to a dataset that contains such one-word comments?My dataset is labelled as ""spam"" or ""ham""; I want to represent each document as a vector in features embedded space, then build a NN to train them. Is it a proper way for document classification?Can anyone give me some explanation, as I am just a new text mining leaner.
Many thanks!"
5473,Count number of times a word appears in each row and store in new column (dplyr),"['r', 'dplyr', 'text-mining', 'word-frequency', 'qdap']",I have a character vector containing basically paragraphs of words. I would like to count the number of times a specific word appears in each row separately and then create a new vector to hold this number. How can I achieve this with dplyr? (Any other method available is also okay).The closest I've come to a solution is on this link: Count number of times a word appears (dplyr) but it's not giving me exactly what I want.
5474,Wrting each item in a list into a separate txt file with auto-assigned filename (python=3.6),"['python-3.x', 'text-mining', 'text-processing', 'text-extraction', 'pdfminer']","I'm using textract to get plain text from PDF files. For the plain text of each PDF file in the directory, I append it to the list filetext_list. I want to write each item of the list to a separate txt file with an auto-assigned filename like ""article_1"". Here is what I did so far:The output files are ""article_0"" and ""article_1"", which are named properly. However, both files contain the text of the same item in the list. I intended them to each contain the text of a separate item in the list. Any idea why the code failed? Also, I would like to eliminate the ""\n"" elements in the text by doing something like .replace('\n', ' '), but I don't know where this would fit in the code. Thank you!"
5475,Trying to Install Orange-3 Text Mining Add In [Error] - Command failed: python python -m pip install Orange3-Text exited with non zero status,"['macos', 'text-mining', 'orange']",I am trying to install the Text add-in for Orange 3 but I get this error each time:Command failed: python python -m pip install Orange3-Text exited with non zero status.My pip is up to date too so I'm not sure what the issue is or how I can get around this. How can I resolve this?
5476,Python regex or other solution to extract text items from a string?,"['python', 'text', 'beautifulsoup', 'text-mining']","I have a string that lookls like this:And I need to get a list of the items between dots, as follows:I have tried with regex and other tools but I cant find the right, flexible* answer. *flexible = the list might have something between 1 and N elements"
5477,Identifying finance related words from a document using python,"['python', 'nltk', 'text-mining', 'wordnet']",I've been given a task to identify all the finance related words from documents. Do I need a finance wordnet (as we do for sentiment analysis) or do I need a dictionary containing finance words?
5478,R: Append multiple rows to dataframe within for-loop,"['r', 'append', 'text-mining']","I have PDF files that I made from these wikipedia pages (for example):https://en.wikipedia.org/wiki/AIM-120_AMRAAMhttps://en.wikipedia.org/wiki/AIM-9_SidewinderI have a list of keywords I want to search for within the document and extract the sentences in which they appear.I can call the file, extract the text from the PDF, pull the sentences with the keywords from the PDF.  This works if I do this with each of the keywords individually, but when I try to do this in a loop I keep getting this issue where the rows aren't appending.  Instead it's almost doing a cbind and then an error gets thrown regarding the number of columns.  Here is my code and any help you can provide as to what I can do to make this work is much appreciated.  How do I get the rows to append correctly and appear in one file per PDF?After I get the rows to append correctly the next step will be to add in the keywords as a variable to the left of the extracted sentences.  Example of ideal output:Would this be done in the loop as well or post as a separate function?Any help is appreciated."
5479,Report the mean number of characters in Corpus document,"['r', 'rstudio', 'mean', 'text-mining']","So I have a corpus setup reading bunch of text file with paragraphs in them. Now I need to find the mean of the characters in each text. Running a
 mean(nchar(apapers), na.rm =T) results in a very weird output, more than the number of characters. 
Any other way to get the mean?"
5480,Mixing text and numeric features for text classification using deep learning,"['deep-learning', 'text-mining', 'feature-engineering', 'natural-language-processing']","I have a problem about classification of text into several categories (topics). Apart from text, I have some numeric features that I believe may be useful (there are also missing values among those features). But the most important information is, of course, presented in the text. Therefore, I think deep learning approach (with a common pipeline: embedding layer + CNN or RNN with dropout + Dense layer) would be the best choice. What is the best practice to mix the current model that works only on text input with numeric features? Are there any tricks, best common practices, state-of-the-art research going on in this field? Are there any papers/experiments (on GitHub, maybe) on this topic?It'd be great if we could think of the problem in general, but for the sake of having an idea of what sort of problem we may solve, I will give a specific example. Let's suppose we have reviews from users in which they describe a problem they faced while receiving a service or purchasing an item. The target feature is multi-label: the set of tags (categories/topics) associated with the complaint that a user had (we should choose relevant ones among a few hundreds of possible topics).Then apart from the user's comment itself (which is the most important feature), we may want also to take into account some numerical features like price, waiting time, rating (customer satisfaction score), etc. This can potentially be useful for predicting some particular categories.The idea is to mix all these features somehow in a deep learning model to produce the final model. Not sure if I know much about the best ways how to do it. What are the best practices / useful tricks for this kinds of problems?"
5481,Erasing part of a text file in Python,"['python-3.x', 'text', 'text-mining']","I have a text file in my hard disk which is really big. It has around 8 million json files which are separated by comma and I want to remove the last json ; however, because it is really big I cannot do it via regular editors (Notepad++, Sublime, Visual Studio Code, ...). So, I decided to use Python, but I have no clue how to erase part of an existing file using python. Any kind of help would be appreciated.P.S: My file has such a structure:"
5482,Sequential string chunking,"['python', 'list', 'list-comprehension', 'text-mining', 'nested-lists']","I have a list of strings that I am looking to chunk into sub-lists comprising three  elements, the element in the list (i), the preceding element (i-1), and the next element (i+1). I would also be looking to iterate over every other element in the list rather than every element. More specifically, how can I go from this...To this... (starting from index position 1 to enable a preceding element);So each item in the sub-list contains every second element as its' first item, followed by the item that preceded it in the original list, followed by the item that succeeded it in the original list."
5483,R - Text Analysis - Misleading results,"['r', 'text-mining', 'tm', 'text-analysis', 'qdap']","I am doing some text analysis of comments from bank customers related to mortgages and I find a couple of things I do understand.1) After cleaning data without applying Stemming Words and checking the dimension of the TDM the number of terms (2173) is smaller than the number of documents (2373)(This is before remove stop words and being the TDM a 1-gram).2) Also, I wanted to check the 2-words frequency (rowSums(Matrix)) of the bi-gram tokenizing the TDM. The issue is that for example I have gotten as the most repeated result the 2-words ""Proble miss"". Since this grouping was already strange, I have gone to the dataset, ""Control +F"", to try to find and i could not. Questions: it seems that the code some how has stemmed these words, how is it possible? (From the top 25 bi-words, this one is the only one that seems to be stemmed). Is this not supposed to ONLY create bi-grams that are always together?SAMPLE of DATASET:"
5484,web scrapping using python notebook,"['python-3.x', 'web-scraping', 'text-mining']","I am doing web scrapping from e commerce website .
i extracted the content i want
but while printing the out put it only gives the last value
below is the code i used
would be glad if any one help me out"
5485,Summarizing R corpus with doc ID,"['r', 'text-mining', 'tm', 'corpus']","I've created a DocumentTermMatrix similar to the one in this post:Keep document ID with R corpusWhere I've maintained the doc_id so I can join the data back to a larger data set.My issue is that I can't figure out how to summarize the words and word count and keep the doc_id. I'd like to be able to join this data to an existing data set using only 3 columns (doc_id, word, freq).Without needing the doc_id, this is straight forward and I use this code to get my end result.I've tried several different approaches to this and just cannot get it to work. This is where I am now (image). I've used this code:to move the doc_id into a column in the matrix, but cannot get the numeric columns to sum and keep the doc_id associated.Any help, greatly appreciated, thanks!Expected result:doc.id    |    word      |   frequency
1         |     Apple    |        2
2         |     Apple    |        1
3         |     Banana   |        4
3         |     Orange   |        1
4         |     Pear     |       3"
5486,Part-of-Speech (POS) vs Syntactic Dependency Parsing,"['nlp', 'text-mining', 'linguistics']",I am using SpaCy for text analysis but I cannot understand the difference between Part-of-Speech (POS) and Syntactic Dependency Parsing. Both label the words in a sentence based on their role. But how exactly they are different?
5487,Problem with Analysing Turkish Text while using stopwords “tr” with R,"['r', 'text-mining']","I am analysing Turkish text in R. But there is a problem when using stopwords""tr"" 
Although, in indicated link, Turkish language is represented with ""tr"" But it still does not recognize it.here is the error: Error: Language ""tr"" not available in source ""snowball"". See stopwords_getlanguages for more information on supported languages.Any help would be appreciated. "
5488,Regex to extract a number and its unit of measure that are separated by a string from a word of interest,"['r', 'regex', 'string', 'text-mining']","I'm learning R and I'm trying to use regex to extract specific text. I would like to capture a number and the unit of measure from a recipe for a specific ingredient.For example for the following text:I would like to extract the numbers and units relating only to butter, i.e:I think this would be best done using regex, but I'm quite new to this so I'm struggling somewhat.Using str_match I can get the number in front of specific unit like this:But how could I get only the values that relate to butter and for a range of units. Is it possible to make a list of possible units (i.e. grams, tbs, Tb etc.) and ask to match any of them (so that in this example grams would match but not sticks)? Or perhaps this would be done better with some loop? I could put each sentence into a dataframe, loop through each row asking if there is 'butter' in the row search for a number in it and extract the the number and the word that follows, which should be the unit of measure.Thanks for the help. "
5489,Extracting information from text in python,"['python', 'nlp', 'nltk', 'text-mining', 'information-extraction']","I am new to the text mining. I have a CSV file. I need to go through each line and extract some information then write them into another CSV file. I am looking for specific information which I have in a dictionary. Consider below sentence: ""the application version is 1.8.2 and the variable skt.len passes the required information. file ReadMe.txt has the specifications.""My dictionary is: [""application version"", ""variable"", ""file""]I need to extract:What is the best way to extract such information from text? I am playing with NLTK and StanfordCoreNLP features. But, I could not extract the information yet. I am thinking to use regex to extract the application version. Any idea?PS: I know that this may make the task more complicated. But, sentences in each line of the CSV file may have different structures. For example: ""application version"" in one line, may be ""app version"" in another line. Or ""file"" in one line may be ""filename"" in another line. "
5490,Replacing emojis in a text,"['r', 'text-mining']","I try to replace emojis with their meaning. For this task, I use the textclean package. The lexicon does not only include the emoji description but also the byte code representation (x: column):So the result looks like this:I only want to get the description without the byte code representation because I have to clean it again. How can I apply only the ""y column"" to the text? Is their maybe a better way to deal with emojis in R?"
5491,Python web scraping and data mining,"['python', 'web-scraping', 'text-mining', 'data-extraction']","i am doing web scraping on a wikipedia page. the code seems to be right, but i have a problem at the 11th line of the code. code:error:"
5492,"R: Separate Text String by Space and Remove Tabs, Line Breaks, Etc","['r', 'text-mining', 'stringr']","After reading an HTML table, my name column appears with records as follows:The following code fails to generate the correct values in the First and Last name columnsCuriously, the First column is blank, while the Last column contains only the person's first name.How could I correctly turn this column into the First and Last column desired (i.e...Data example per recommendation of @r2evans and as appearing in correct answer code below:"
5493,Removing German stop words in R,"['r', 'text', 'text-mining', 'text-analysis']","I have survey data with a comments column. I am looking to so sentiment analysis on the responses. The problem is there are many languages in the data and I can't figure out how to eliminate multiple language stopwords from the set'nps' is my data source, nps$customer.feedback is the comments column.First I tokenize the dataGetting rid of stopwordsThen use Stemming and get_sentimets(""bing"") to show word counts by sentiment.However, ""di"" and ""die"" appear in 'negative' graph due to German text being analyzed.Can someone help?My goal is to get eliminate German stopwords using the above code. "
5494,Text mining - Stemming method without `tm` package,"['r', 'nlp', 'text-mining', 'stemming']","I´m dealing with a text mining task. Today, I have a problem with the stemming method.
I have several paragraphs in this format. These are character object, not list neither Corpus object from tm package.[1] "" andres oppenheimer intelectuales influyentes latinoamerica segun revista foreign policy editor columnista miami herald sigue recorriendo continente presenta reportajes cnn tradicional ciclo periodistico argentina presentando libro salvese pueda analiza futuro mundo automatizacion robotizacion "" I have a dictionary where some words has to be match in the corpus above. The problem is that I couldn´t do it through the stemming method. My syntax is the following: If I try:It matches the word in any position, but this is not what I want. I know that if a split the words of the corpus by, for instance the space, I can make the match as I need, but this is a more complicated aproach. I wish to do it directly from the paragraph, But without turn it into a Corpus object. Any idea? Thank you very much for your help!"
5495,"DocumentTermMatrix error in R “no applicable method for 'meta' applied to an object of class ”character""","['r', 'text-mining', 'corpus']",I am having an issue creating a DTM after running some code to clean the textual elements within a corpus.Any idea what might be causing this?Thanks for the help!
5496,Add a new stemmer to nltk,"['python', 'nlp', 'nltk', 'text-mining', 'stemming']","I have this python function that works as expected. Is it possible to save the logic as NLP stemmer?
If yes, what changes needs to be done?I will like the users to import something like this...There are a few more rules to be added to the logic. I just want to know if this is a valid (pythonic) idea."
5497,Applying “String matching to estimate similarity” to data frame,"['r', 'string', 'text-mining', 'text-analysis']","String matching to estimate similarityThe above code is exactly what I am looking for, except I cannot seem to figure out how to compare the strings between columns (the ""correct"" answer and ""given"" answer) in a data frame and then storing the output from sim.per as a new column (""similarity"") in that same data frame. I have tried .e.g, The latter also results in an error when the row is empty, which is acceptable in my dataset and should be calculated as 0 instead. Expected output should be:Any help would be appreciated! Thanks!Subset of the data:"
5498,Optimized Lemmitization method in python,"['python', 'text-mining', 'lemmatization']",I have written python script which have this below function. Lemmatized function taking so much time which is affecting the code efficiency. I am using spacy module for lemmatization.Is there any way to optimize it?Thanks in advance!
5499,From pdf text to tidy dataframe with file names in document column,"['r', 'pdf', 'text-mining', 'corpus', 'tidytext']","I want to analyse text from almost 300 pdf documents. Now I used the pdftools and tm, tidytext packages to read the text, coverted it to a corpus, then to a document-term-matrix and I finally want to structure it in a tidy dataframe.I've got a couple questions:I've got the The following reproducible script: With the following output:This seems to work out nicely but I would rather want the filenames (""'s-Gravenhage"" and ""Aa en Hunze"") as the values in the document column instead of indexed numbers. How do I do this?Desired output:Delete downloaded files and its directory from desktop running the following line:All help is much appreciated! 💐"
5500,Extracting specific segments from PDF document,"['python-3.x', 'text-mining', 'pdf-extraction']","I have a few research papers in pdf format and I want to extract just the introduction/background etc from the paper. also, I can only use python. can someone please help?"
5501,Why can't R read the text file,"['r', 'text-mining']","Try to get R read my text file and do a text mining, but following the steps it's not working, don't know what's wrong. Someone plz help meAnd it shows:And if you do thisIt shows the text is null or contains 0 characters
why?"
5502,Removing two words together (Phrase) from Text in Python 3,"['python-3.x', 'nltk', 'text-mining']",I am trying to remove words from the 20K comments. Words are stored in dataframe and its around more than 2000. Comments in different dataframe and its around 20K. Below is the example:  Expected Output:   
5503,How word association mining is generalization of n-gram language model,"['data-mining', 'text-mining', 'language-model']","I am working on text mining (reading book...) author said word association mining is actually the generalization of n-gram language model Can you please tell how word association mining is generalization of n-gram language model.
For Me word association mining is finding symptomatic relation (finding co-occurring) words and n-gram language model is compare all n-words in query to suggest or return relevant documents. "
5504,"Python parsing file that has single and double quotes, as well as contractions","['python', 'json', 'parsing', 'text-mining']","I am trying to parse a file where some of the lines may contain a combination of single quotes, double quotes, and contractions.  Each observation includes a string as shown above.  When trying to parse the data, I am running into problems when trying to parse the reviews.  For example:or"
5505,Create Co-occurrence matrix with bigrams,"['r', 'nlp', 'sparse-matrix', 'text-mining', 'text2vec']",I am looking to create a co-occurrence matrix with bigrams in stead of unigrams from a single string. I am referring the following linkshttp://text2vec.org/glove.htmlhttps://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html#3_statistical_significanceI want to create  the matrix and traverse it to create dataset as followsThe biggest catch being traversing the sentence with bigrams. Any help on this would be great
5506,To replace internet acronyms in a dataframe using dictionary,"['python', 'pandas', 'text-mining']","I'm working on a text mining project where I'm trying to replace abbreviations, slang words and internet acronyms present in text (In a dataframe column) using a manually prepared dictionary. The problem I'm facing is the code stops with the first word of the text in the dataframe column and does not replace it with lookup words from dict Here is the sample dictionary and code I use:Example Input:Desired Output:Current Output:"
5507,Text similarity approaches do not reflect “real” similarity between texts,"['python', 'nlp', 'text-mining', 'similarity', 'sentence-similarity']","I am comparing the content of CVs (.txt files with stop-words already removed) with really compact job descriptions (JDs), like this:project management,
  leadership,
  sales,
  SAP,
  marketing The CVs have around 600 words and the JDs only the words highlighted above.The problem that I am currently experiencing, and I am sure this is due to my lack of knowledge, is that when I apply similarity measures to it, I get confuse results. For example I have the CV number 1 which contains all the words from the JD, sometimes repeated more than once. I also have CV 2 which only contains the word project in comparsion to the JD.  Even though, when I apply cosine similarity, diff, jaccard distance and edit distance, all these measures return to me a higher degree of similarity between the CV2 and the JD, which for me is strange, because only one word is equal between them, while the CV1 possesses all the words from the JD. I am applying the wrong measures to assess similarity? I am sorry if this is a naive question, I am a beginner with programming. Codes followDiffCosineEdit distanceJaccard distanceAs you guys can see, the 'LucasQuadros.txt'(CV1) has a higher similarity with the 'job.txt'(Job Description), even though it only contains one word from the job description."
5508,Text matching using R when strings are dissimilar,"['r', 'text', 'text-mining', 'agrep']","I am trying to identify observations that match between two datasets, using text string vectors $contractor and $employer, and create a TRUE/FALSE indicator on whether the contractor is in the employer list.The pmatch command says there are 0 matches, but this is because the company names are sloppily entered and not spelled consistently; there are obviously matches.  I have also used the fuzzy matching command agrepl, but in my actual data the number and quality of matching varies incredibly with small changes to the accepted Levenshtein distance.There are also some answers here and here but my lack of advanced programming experience has kept me from applying the concepts there.  Any thoughts are appreciated!"
5509,Quanteda: error message while tokenizing “unable to find an inherited method for function ‘tokens’ for signature ‘”corpus“’”,"['tokenize', 'text-mining', 'topic-modeling', 'text-analysis', 'quanteda']","I have been trying to tokenise and clean my 400 txt documents before using structured topic modelling (STM). I wanted to remove punctuations, stopwords, symbols, etc. However, I get the following error message: 
""Error in (function (classes, fdef, mtable): unable to find an inherited method for function ‘tokens’ for signature ‘""corpus""’"". This is my original code:I also tried to tokenize a simple string text - just to check if it was an encoding problem while importing my txt files - but I got the same error message, plus a couple of extra ones when I tried to tokenise the the text directly, without converting it to corpus: ""Error: Unable to locate Ciao bella ciao"" and ""Error: No language specified!"". Here is my example code in case someone wants to replicate the error message:The packages that are loaded are: data.table, ggplot2, quanteda, readtext, stm, stringi, stringr, tm, textstem. Any suggestion on how I could proceed to tokenise and clean my texts?"
5510,spark_apply() of spaklyr does not work with tm library,"['text-mining', 'tm', 'sparklyr']","I am trying to use 'tm' library functionalities with spark_apply() function of sparklyr.I have created a sparkdataframe named as train from a text file:train details:Source: table [?? x 1]Database: spark_connectionline1 Thank you. To Mama Gra�a Machel,2 ""On Tuesday, xyz abc ""Then I have defined my customised function as:But when I am running the spark_apply function as follows:spark_apply(train,myfunction) , it is throwing the below error ,Error in file(con, ""r"") : cannot open the connection
In addition: Warning message:
In file(con, ""r"") :
cannot open file 'C:\Users\hp\AppData\Local\Temp\RtmpIzVe9J\file53c76462a6c_spark.log': Permission deniedI have verified the cmd files in spark bin ,and I have all the required permissions as it is my own local machine.Just to cross verify I ran the below code and it works fine:Result:Note:Would be great if anyone can look into this issue ."
5511,finding row-wise important words in text dataframe,"['r', 'dplyr', 'text-mining', 'tidytext']","I have a dataframe which looks like this:All I want to do is to find important words in each row and create a new column that should look like this:I am not sure how this can be done?I was trying bag of words, cleaning and preprocessing etc. using various packages such as tm, tidytext etc. But unable to get the desired result.Is there any alternative possible?"
5512,gensim lda permission denied when I try to save my model,"['python', 'text-mining', 'gensim', 'lda']","I've been getting some irregular behavior from an LDA topic model program and right now, it seems like my file won't save the lda model it creates... I'm really not sure why.Here's a code snippet, albeit it's going to take me more time before I could write code that's reproducible since I'm really just trying to load certain files I created beforehand.To put it sraightforwardly... I have no clue why permission is being denied when I try to save my lda model to a particular location. Right now even the original models/ directory location is giving me ""permission denied"" with this error message. It's seeming like any and all directories I could use just... won't work. This is odd behavior and I can't really find asks that talk about this error in the same context. I have found posts of people getting this error message when they actually tried storing in locations that did not exist. But for me that isn't really a question.When I first got this error... I actually started to wonder if it was because I had another lda topic model that I named topic_model_1. It was stored in the models/ subdirectory. I started to wonder if the name was a potential cause, and changed it to lda_model_topic_1 to see if that could change results... but nothing is working.Even if you can't really figure out what solution applies to my situation (especially since right now I don't have reproducible code, I just have my work)... Can someone tell me what this error message means? When and why does it come up? Maybe that's a start."
5513,Use Pytesseract to Extract Text into Table Arrays Given the Coordinates of the Table Structure,"['opencv', 'ocr', 'tesseract', 'text-mining', 'python-tesseract']","I want to extract texts from a scanned table with tesseract and put it them into arrays that have the same structure as the table. I already used opencv to detect the table structure, and obtained the coordinates of the table joints as well as the entire table structure (stored into np.array). For example, for the table in this picture:
I want pytesseract to store it into:I have used commercial OCR softwares and they always detect the table structure first, and secondly, recognize and extract texts to that detected table structure.How do I accomplish the second step with pytesseract? Answers using Tesseract in other languages are great as well. "
5514,Text mining with tm in R antiword error,"['r', 'error-handling', 'text-mining', 'tm', 'read-text']","So I'm rather new to R, and I'm learning how to mine text from this handy website: https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/I do have my own text set of .doc, .docx, and .xlsx files and I'm trying to mine them. They're located in a folder in my working directory called 'files', but I have already encountered an error after simply writing a few lines of code.The code I have so far is:At this point, after waiting for 25 seconds or so, I get the error:and the code stops running there.I have tried searching online for solutions but it seems like a fairly rare error and so I only found 1 possible solution at https://github.com/ropensci/antiword/issues/1 but that did not work for me.This solution suggested that one of my files were corrupt, and suggested using the codeto change the error to a warning to not interrupt the reading of the files. I tried that, and at first it raised the error ofAfter which, I loaded the antiword library with a library(antiword) and changed the stop( to a warning(. However, when I ran the data = readtext('files') line again, it immediately raised the errorI'm at a loss here! Any help would be appreciated. Should I be using another package in this case?"
5515,python program to put proper punctuations in a given string,"['python', 'text', 'nltk', 'text-mining']","I want to put proper punctuation marks in a given paragraph having many punctuationless sentences.e.g:-
input: hey how are you can you come today
output: hey, how are you? can you come today?I just want to split the sentences to isolate the question part and the statement part. Basically, I need to put only ""."" and ""?"" at appropriate places.Is there any protocol to do the same.
Any suggestion will be appreciated."
5516,Word correlation in R,"['r', 'correlation', 'text-mining', 'word-count', 'text-analysis']","I see some similar questions, but none of them helped me.
I have a corpus and I want to have a list, table or data frame with both the highest and lowest positive and negative correlation of a specific word.I don't want to use ""tm"". I want to build this code so I can add some features to it.Here is how I have my corpus and the frequency lists:I know I will need a matrix, but I have no idea how to go from there."
5517,Remove all sentences where number to character ratio is greater than the average in the text,"['r', 'regex', 'text', 'substring', 'text-mining']","Is it possible to find and delete all sentences containing a higher number to character ratio?
I created the following function to calculate the ratio in a given string:The task is now to find a method to calculate the ratio for a sentence(so for a character sequence between ending with a ""."") and then to delete sentences with a higher ratio from the text. For example:"
5518,Dispersion plots in R,"['r', 'plot', 'text-mining']","I am facing the following problem.
I have 12 texts and I need to create a dispersion plot for which one of them. I want R to display several dispersion plots together. Right now I have all the information that I need and I am able to create individual dispersion plots. I just don't know how to plot all of them together, as explained here (https://www.statmethods.net/advgraphs/layout.html). Here is my code:This list contains all the texts.This contains all ""times"":I have 12 books and I want to plot all of the 12 dispersion plots together, at the same time, so I can compare them. I believe it is possible, I just don't know how."
5519,R - Finding top words in each NRC sentiment and emotion using syuzhet package,"['r', 'text-mining', 'sentiment-analysis', 'tidytext']","Snapshot of the dataset:I'm getting following chart:Here is the code:The issue is that I'm not sure if the result I'm getting is correct or not. For instance, you can see 'bad' is part of multiple emotions. Also, if we inspect lyric_sentiment, we'd see that word 'shame' is present four times for 'Tim McGraw'. In reality it appears only twice in this song.What's the right approach?"
5520,Unable to understand the error in r function for stemming row wise data in dataframe,"['r', 'function', 'text-mining', 'stemming']",I have to stem the words in strings in each of rows of a dataframe and i was trying to use a function which gave me a vague error.First the dataframe with text:Then the function:I was supposed to use the above function:To get the result after stemming:But above function failed with the error:
5521,"Error in nchar(Terms(x), type = “chars”) : invalid multibyte string, element 204, when inspecting document term matrix","['r', 'text-mining']",Here is the source code that I have used:  
5522,Read a PDF file with multiple text box Like in patent files in R,"['r', 'text-mining']",I use R to analyze PDF documents. I face a problem when I try to read a PDF document with several columns. The document is read line by line and that make a mixture of the text. I would like to be able to read column after column can anyone help me please?This is how I'm reading a document
5523,R - NLP - text cleaning,"['r', 'nlp', 'text-mining']","I am new to text mining and, currently, I stuck with this kind of pattern I would like to receive only pattern = ""Hello"" and exclude all the other text.I tried the following but I failed immediately: So, I tried to break it down:->result <fo> drops, so it is a good sign but when I do the next step->result: <U+009F> remains.
Do you have some ideas?
I appreciate any kind of suggestions!
Thanks in advance!"
5524,R: How get file name with Quanteda: char_segment,"['r', 'split', 'text-mining', 'quanteda']","I am using  char_segment from Quanteda library to separate multiple documents from one file separatted by a pattern, this command works great and easily! (I did try with str_match and strsplit but without success).Lamentably I am unable to get the filename as a Variable, this is key to next analysis.exampleExample of my commands:Please any suggestion or other options to split documents are welcome."
5525,Vectorizing terms inside a column of strings in scikit-learn,"['python', 'pandas', 'scikit-learn', 'text-mining']","I have a table-like dataset with the following structure: each line has, as columns:I want to convert this to a bag-of-words-type representation in a pandas DataFrame, so that this line becomes(I'm obviously able to take a DataFrame with a single Ingredients column and convert it into a dict of lists:But I'm not able to use CountVectorizer on that. Maybe that's not even the best way to handle it. )"
5526,Trying to get random forest for text classification running,"['r', 'text-mining', 'random-forest']","I'm trying to get a randomForest running for a school project. I am trying to build a test classifier, that predicts a category (column label) based on some text.Curretly I am stuck as there seems to be a problem with my document term matrix.
This is the error:This is what the code currently looks like. The data is representative.Since I am rather new to both random forest and R there probably is a conceptual mistake.
Any idea how to solve the problem?"
5527,Is it possible to convert words stored in a matrix of N*1 into single sentence in R?,"['r', 'text-mining']",I have a matrix of 1196*1 with each row containing one word. I need to write these words into a single sentence.   Aim: I Have A Matrix. 
5528,R: compare words histogram by document,"['r', 'text', 'text-mining', 'corpus']","I am looking for a method to compare word histograms by documents belong to a folder corpus with several documnets. I did try to made:Also and did try in ggplot option:but lamentably I got error.An modified example of my code is below, but lamentably my 3 documents are plot like one and nor segmented by Docs:"
5529,How to do bi-grams topic modeling using tidy text in r?,"['r', 'text-mining', 'n-gram', 'topic-modeling', 'tidytext']","So I tried using the tidytext package to do bigrams topic modeling, by following the steps on the tidytext website: https://www.tidytextmining.com/ngrams.html.I was able to get to the ""word_counts"" part, where R calculates each bi-gram's frequency. ""word_counts"" returned a the following:The next step was to put information from above into a dtm formatMy code is below:A warning message was raised:But the ""lda_dtm"" looks like its in the right format.However when I tried to run lda, it did not work.The following warning was raised:I don't see a topic modeling tutorial on the tidy text website for bi-grams, the tutorial was specifically for unigrams. How should I adjust the format for it to work with bi-grams?"
5530,Splitting chat conversations into sentences and mapping responses,"['regex', 'python-3.x', 'text-mining']",I have the following data:I am trying to split this into Q&A format like this:This is one set of conversation with an unique ID. After the split I would like to have each the questions and answers as different columns appropriately matching each response.I tried the following:The output is as follows:
5531,Grouping similar texts in R,"['r', 'cluster-analysis', 'text-mining']","I got employee designation data having so much unique values.
I want to club together many forms of one designation together like ('Senior Manager', 'Sr. Manager', 'Sen manager', 'Snr Manager' etc). Also this data is having typo mistakes as well.What will be the best technique for clubbing many designations into one using R.Is clustering best way to solve this issue or can some other technique help better to solve my problem.I tried 'euclidean distance' and k-means but none gave satisfactory results."
5532,Why does my python text search work correctly for some rows in my csv file but not for others?,"['python', 'text', 'unicode', 'text-mining']","I wrote a python script that opens and reads a CSV file that has the following structureIt then writes a CSV file with the following structureI also have the following python codeMy script runs, the problem is that it only behaves like expected when reading certain lines of a CSV file. For some reason Python is only able to find the company name that I'm looking for in some of the rows of the CSV file.I'll give a sample input file.This is the output that I'm gettingI should be getting a value in the Consolidated Company column for every row of the output file since 'Good Company' shows up in every row of the file. However, what I'm actually seeing is that I'm only getting a value in some of the rows.I haven't been able to figure out why my script works for some of the rows of my input file but fails for other rows of my input file. I would think that my script would either work for everything or fail for everything but it doesn't, why is that?"
5533,Cosine Similarity with two Term Frequency vectors in R,"['r', 'text-mining', 'data-analysis', 'tm', 'cosine-similarity']","I made usingtm in R a DocumentTermMatrix (dtm). if I understand correctly, this matrix displays for each document how often each possible term occurs. Now I can inspect this matrix and I getHow can I now retrieve the vector of (for example) document 181288? So I will get something likeAlso, it says my dtm's sparsity is 100%, is it (by approximation) 100% empty?"
5534,Extract text from epub in Python,"['python', 'nltk', 'text-mining', 'epub']","I have written following code to extract the words of a ebook and add them to a corpus for text-mining purposes.   Unfortunately running the code gives me the error: Could anyone tell me, what I am missing?"
5535,How to extract all words after the nth word from string in R?,"['r', 'text-mining', 'data-cleaning']","The first column in my data.frame consists of strings, and the second column are unique keys. I want to extract all words after the nth word from each string, and if the string has <= n words, extract the entire string.I have over 10k rows in my data.frame and was wondering if there is a quick way of doing this other than using for loops?Thanks."
5536,Grouping word frequency [duplicate],"['r', 'text-mining']","I'm trying to text mine social policy cases.  Each case is in a row and I want to know how many of my cases refer to say Universal Credit or some new unknown issue.  I'm starting with word frequencies.I've got as far as getting my data into this format. Basically ID takes value 1,2 or 3 as there are three case studies.  Word takes value of dog or cat.I want a count of unique ID for each Word i.e there are three case studies that mention catsI'm not even sure if this is now a text mining question or whether it's some basic group or count question."
5537,How to find the trending phrases or most frequent phrases from text data in Python?,"['python', 'nlp', 'text-mining', 'n-gram', 'pos-tagger']",I have text data in the form of pandas data frame object. Each row of the text column contains single or multiple sentences. I would like to find the most frequent or trending phrases from the text. 
5538,LDA consistency in Pyspark,"['apache-spark', 'pyspark', 'apache-spark-mllib', 'text-mining', 'lda']","I am using pyspark (version 2.3.1) and I am trying to reproduce the same results given the following code:Output 1:Even if I used the seed, every time I restart the application (close and re-open the notebook) I have different results. Look at the 2nd Output:Notice that I have the same problem in the .transform phase (even using the seed). The code used was the following:Do you have any hint to help me?Many thanks,
Lorenzo"
5539,Sentence extraction from documents using NLP or Deep Learning,"['nlp', 'deep-learning', 'text-mining', 'text-extraction']","I am looking for references(Papers)/suggestions on how to use deep learning in a text extraction task.Recently I was given a task to extract important information from documents of similar type, say for example legal merger documents. I have thousands of legal merger documents as inputs. A paralegal would go through the entire document and highlight important points from the document. This is the extracted text.What I want to do: Given a document(say legal merger document) I want to use DL or NLP to extract the information from the legal document that would be similar to that of the information extracted by paralegal.I am currently using bag of words model to extract text from the document, calculating sentiment and displaying the sentences with positive or negative sentiments. This yielded very bad results.Can anyone please provide me with some references and suggestions on how to tackle this issue?"
5540,Split mixed string into columns in R,"['r', 'string', 'split', 'text-mining']","A newbie to textmining analysis and R coding. I have 200 genes with mixed string. I want to split them and paste strings (eg, cadherins, orphan receptors) in one column and numbers (eg, 2/3), number+string (eg, 7D, 7TM) in another column. 
I used strssplit to split the words. Please any suggestion on how to parse them would be helpful.  "
5541,Extract variations of a string from R column,"['r', 'nlp', 'text-mining']",I have list of keywordsI have a column which has different text in different rowsis there any way to extract variations present in the column based on keywords?the output I am looking is
5542,How to build a Subject-Verb-Object extraction model in Python?,"['python-2.7', 'structure', 'text-mining', 'spacy', 'text-analysis']","I have a pandas data frame object with one text column containing one or two sentences of text in each row. I would like to build a Subject-Verb-Object Model to extract the best SVOs from the text column for all the rows.I am completely new to this, so please do provide additional inputs as to how to proceed.Thanks!"
5543,Naive Bayes model NOT predicting anything on applying model- Predict function returning with 0 factor level,"['r', 'text-mining', 'naivebayes']","My dataset looks like the following, and I followed Classification using Naive Bayes tutorial to develop my Naive bayes model  for textmining However, I cannot predict the result of my naive bayes, even though model is built. The predict  function is returning with 0 factor level. Below is my dataset and code so far.dput(df)R code:Thanks in advance for help,"
5544,How to read csv file for text mining,"['r', 'csv', 'text-mining']","I will be using tm for text mining purpose.However, my file CSV file is weired .Below is the dput,after I used read.table  function in r. There are three column lie, sentiment and review. However the fourth coulmn contain review with no column name.I am New to R and Text mining. If I use read.csvit is getting me an error. Please suggest better approach for reading csv file.Update:Dataset:Thanks in advance, "
5545,Keep only sentences in corpus that contain specific key words (in R),"['r', 'text-mining', 'corpus', 'text-analysis', 'quanteda']","I have a corpus with txt documents. From these txt documents, I do not need all sentences, but I only want to keep certain sentences that contain specific key words. From there on, I will perform similarity measures etc.So, here is an example.
From the data_corpus_inaugural data set of the quanteda package, I only want to keep the sentences in my corpus that contain the words ""future"" and/or ""children"". I load my packages and create the corpus:Then I want to keep only those sentences that contain my key wordsFirst, let's see which documents contain these key wordsSo far, I only found out how to exclude the sentences that contain my key words, which is the opposite of what I want to do.The above command excludes sentences containing my key words. 
My idea here with the corpus_trimsentences function is to exclude all sentences BUT those containing ""future"" and/or ""children"".I tried with regular expression. However, I did not manage to do it. It does not return what I want.This is how far I got so far. I looked into the corpus_reshape and corpus_subset functions of the quanteda package but I can't figure out how to use them for my purpose.Does anyone have an idea how I can solve my problem?Thank you very much in advance. I highly appreciate any hints or ideas!"
