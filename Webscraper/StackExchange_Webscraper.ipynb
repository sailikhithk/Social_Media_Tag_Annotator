{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcCaET6D1W6o"
   },
   "source": [
    "# **Install Package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mq4yndHAYRcZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\program file\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in d:\\program file\\lib\\site-packages (from selenium) (1.25.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the 'd:\\program file\\python.exe -m pip install --upgrade pip' command.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "508JQacU1uAF"
   },
   "source": [
    "# **Import Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqknDLnm05AE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "file :  StackExchange_Webscraper.ipynb\n",
    "last modified date: 8/6/2020\n",
    "'''\n",
    "import os\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5twPjpx8Fud"
   },
   "source": [
    "# **Webscraper Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zogXrJaJ2grd"
   },
   "outputs": [],
   "source": [
    "class StackWebscraper:\n",
    "    driver = None                  \n",
    "    topicDict = {}                 \n",
    "    topicDataframe = \\\n",
    "        pd.DataFrame(columns=[      \n",
    "        'Topic Title', \n",
    "        'Tags',\n",
    "        'Leading Comment', \n",
    "        ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, webdriverPath):\n",
    "        # Set up webdriver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')     # Ignore security certificates\n",
    "        options.add_argument('--incognito')                     # Use Chrome in Incognito mode\n",
    "        #options.add_argument('--headless')                     # Run in background\n",
    "        self.driver = webdriver.Chrome( \\\n",
    "            executable_path = webdriverPath, \\\n",
    "            options = options)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # get title from a beautiful soup of a question page. return None if no title found\n",
    "    def get_title(self, soup):\n",
    "        title = soup.find('a', class_ = 'question-hyperlink')\n",
    "        \n",
    "        # check if title exists in soup\n",
    "        if title:\n",
    "            return title.get_text()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    # get tags from a beautiful soup of a question page. return None if no tags found\n",
    "    def get_tags(self, soup):\n",
    "        post_tags = soup.find_all('a', class_= 'post-tag js-gps-track')\n",
    "        \n",
    "        # check if tags exist in soup\n",
    "        if not post_tags:\n",
    "            return None\n",
    "        \n",
    "        #collect all the tags into a list and return if they exist\n",
    "        list_of_tags = []\n",
    "        for tag in post_tags:\n",
    "          list_of_tags.append(tag.get_text())\n",
    "        return list_of_tags\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get leading comment from a beautiful soup of a question page. return None if no leading comment found\n",
    "    def get_leading_comment(self, soup):\n",
    "        leading_comment = soup.find('div', class_ = 'post-text')\n",
    "        leading_comment_texts = None\n",
    "        \n",
    "        # check if leading comment exists in soup\n",
    "        if leading_comment:\n",
    "            leading_comment_texts = leading_comment.find_all('p')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # get only the text of the leading comment and remove other code sections\n",
    "        leading_comment_text = ''\n",
    "        for elem in leading_comment_texts:\n",
    "            leading_comment_text = leading_comment_text + elem.get_text()\n",
    "        return leading_comment_text\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the links from specific tags\n",
    "    def get_links_with_tags(self, url, tags_to_scrape):\n",
    "        links = []\n",
    "        \n",
    "        # loop over all tags in the list\n",
    "        for tag in tags_to_scrape:\n",
    "            \n",
    "          # for every tags run through the 1st 12 pages, each includes 50 posts\n",
    "          for i in range(12):\n",
    "            curr_url = url +'/questions/tagged/'+ tag +  '?tab=newest&pagesize=50&page=' + str(i+1)\n",
    "            self.driver.get(curr_url)\n",
    "            time.sleep(2)     #sleep to avoid too many requests (30 requests per min)\n",
    "            page = self.driver.execute_script('return document.body.innerHTML')\n",
    "            largesoup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "            soup= largesoup.find('div', id ='mainbar')\n",
    "            posts = soup.find_all('a', class_= 'question-hyperlink')\n",
    "            \n",
    "            # get link of each post in the page and save it to the list to return\n",
    "            for elem in posts:\n",
    "                if elem.get('href') not in links:\n",
    "                    links.append(elem.get('href'))\n",
    "        return links\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the data from specific tags and save into csv\n",
    "    def run_with_tags(self, url, tags_to_scrape):\n",
    "      # declare variable to store the data\n",
    "      titles = []\n",
    "      tags = []\n",
    "      comments = []\n",
    "    \n",
    "    \n",
    "      # get links of posts from given tags\n",
    "      links = self.get_links_with_tags(url, tags_to_scrape)\n",
    "    \n",
    "      #go through each link\n",
    "      for link in links:\n",
    "        \n",
    "        #go to the page from the link and get a bs object from the page\n",
    "        curr_url = url + link\n",
    "        self.driver.get(curr_url)\n",
    "        time.sleep(3)     # sleep in order to avoid too many requests error\n",
    "        page = self.driver.execute_script('return document.body.innerHTML')\n",
    "        soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "        \n",
    "        \n",
    "        # get title, tags, and leading comment of the page\n",
    "        title = self.get_title(soup)\n",
    "        list_of_tags = self.get_tags(soup)\n",
    "        leading_comment_text = self.get_leading_comment(soup)\n",
    "        \n",
    "        \n",
    "        # check if title, tags and leading comment exist\n",
    "        if not (title and list_of_tags and leading_comment_text):\n",
    "            continue\n",
    "        \n",
    "        # store the title, tags, and leading comments of the current post\n",
    "        titles.append(title)\n",
    "        tags.append(list_of_tags)\n",
    "        comments.append(leading_comment_text)\n",
    "      \n",
    "      #create dataframe and save data into csv\n",
    "      attributeDict = {\n",
    "                    'Topic Title'       :   titles,\n",
    "                    'Tags'              :   tags,\n",
    "                    'Leading Comment'   :   comments}\n",
    "\n",
    "      #print (comments)\n",
    "      self.topicDataframe =  pd.DataFrame(attributeDict)\n",
    "      self.topicDataframe.to_csv('StackOverflow.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3tdqjj13_OG"
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Local path to webdriver\n",
    "    webdriverPath = \"chromedriver.exe\"\n",
    "\n",
    "    # stackoverflow forum base URL\n",
    "    baseURL = 'https://stackoverflow.com'\n",
    "\n",
    "    # Create Stackoverflow webscraping object\n",
    "    stackWebscraper = StackWebscraper(webdriverPath)\n",
    "\n",
    "    #create a list of tags to scrape\n",
    "    tags_to_scrape = ['nlp', 'nltk', 'bert', 'word-embedding','text-classification', 'sentiment-analysis', 'tf-idf', \n",
    "                      'scikit-learn', 'text-mining', 'selenium', 'selenium-webdriver', 'web-scraping', 'splinter',\n",
    "                      'beautifulsoup', 'scrapy']\n",
    "    \n",
    "    #tags_to_scrape = ['nlp']\n",
    "    # Run webscraping and save data\n",
    "    stackWebscraper.run_with_tags(baseURL,tags_to_scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StackExchange_Webscraper.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
